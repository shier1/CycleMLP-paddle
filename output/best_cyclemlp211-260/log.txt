2022-01-20 11:04:58,004 
AMP: False
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RE_COUNT: 1
  RE_MODE: pixel
  RE_PROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 256
  BATCH_SIZE_EVAL: 8
  CROP_PCT: 0.9
  DATASET: imagenet2012
  DATA_PATH: /root/paddlejob/workspace/train_data/datasets/Light_ILSVRC2012
  IMAGE_SIZE: 224
  NUM_WORKERS: 16
EVAL: False
LOCAL_RANK: 0
MODEL:
  MIXER:
    EMBED_DIMS: [64, 128, 320, 512]
    LAYERS: [2, 2, 4, 2]
    MLP_RATIOS: [4, 4, 4, 4]
    TRANSITIONS: [True, True, True, True]
  NAME: cyclemlp_b1
  NUM_CLASSES: 1000
  PRETRAINED: None
  RESUME: Best_CycleMLP
  TYPE: CycleMLP
NGPUS: 4
REPORT_FREQ: 50
SAVE: /root/paddlejob/workspace/output//train
SAVE_FREQ: 50
SEED: 0
TAG: default
TRAIN:
  ACCUM_ITER: 1
  AUTO_AUGMENT: True
  BASE_LR: 0.0005
  COLOR_JITTER: 0.4
  CUTMIX_ALPHA: 1.0
  CUTMIX_MINMAX: None
  END_LR: 5e-06
  GRAD_CLIP: 5.0
  LAST_EPOCH: 210
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    MILESTONES: 30, 60, 90
    NAME: warmupcosine
  MIXUP_ALPHA: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  NUM_EPOCHS: 300
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: AdamW
  RANDOM_ERASE_COUNT: 1
  RANDOM_ERASE_MODE: pixel
  RANDOM_ERASE_PROB: 0.25
  RANDOM_ERASE_SPLIT: False
  SMOOTHING: 0.1
  WARMUP_EPOCHS: 20
  WARMUP_START_LR: 5e-07
  WEIGHT_DECAY: 0.05
VALIDATE_FREQ: 2
VALIDATION:
  REQUIREMENTS: 0.789
2022-01-20 11:04:58,004 ----- world_size = 4, local_rank = 0
2022-01-20 11:04:58,797 ----- Total # of train batch (single gpu): 1252
2022-01-20 11:04:58,797 ----- Total # of val batch (single gpu): 1563
2022-01-20 11:05:00,246 ----- Resume Training: Load model and optmizer from Best_CycleMLP
2022-01-20 11:05:00,247 Start training from epoch 211.
2022-01-20 11:05:00,247 Now training epoch 211. LR=0.000237
2022-01-20 11:07:15,142 Epoch[211/300], Step[0000/1252], Avg Loss: 3.1506, Avg Acc: 0.2754
2022-01-20 11:08:49,392 Epoch[211/300], Step[0050/1252], Avg Loss: 3.2319, Avg Acc: 0.4162
2022-01-20 11:10:23,356 Epoch[211/300], Step[0100/1252], Avg Loss: 3.2250, Avg Acc: 0.4292
2022-01-20 11:11:56,221 Epoch[211/300], Step[0150/1252], Avg Loss: 3.2120, Avg Acc: 0.4273
2022-01-20 11:13:30,044 Epoch[211/300], Step[0200/1252], Avg Loss: 3.1932, Avg Acc: 0.4280
2022-01-20 11:15:05,625 Epoch[211/300], Step[0250/1252], Avg Loss: 3.1786, Avg Acc: 0.4315
2022-01-20 11:16:39,015 Epoch[211/300], Step[0300/1252], Avg Loss: 3.1669, Avg Acc: 0.4379
2022-01-20 11:18:14,529 Epoch[211/300], Step[0350/1252], Avg Loss: 3.1847, Avg Acc: 0.4390
2022-01-20 11:19:49,420 Epoch[211/300], Step[0400/1252], Avg Loss: 3.1831, Avg Acc: 0.4373
2022-01-20 11:21:24,696 Epoch[211/300], Step[0450/1252], Avg Loss: 3.1901, Avg Acc: 0.4348
2022-01-20 11:22:59,138 Epoch[211/300], Step[0500/1252], Avg Loss: 3.1882, Avg Acc: 0.4364
2022-01-20 11:24:33,638 Epoch[211/300], Step[0550/1252], Avg Loss: 3.1816, Avg Acc: 0.4379
2022-01-20 11:26:09,013 Epoch[211/300], Step[0600/1252], Avg Loss: 3.1839, Avg Acc: 0.4391
2022-01-20 11:27:44,487 Epoch[211/300], Step[0650/1252], Avg Loss: 3.1862, Avg Acc: 0.4370
2022-01-20 11:29:18,693 Epoch[211/300], Step[0700/1252], Avg Loss: 3.1857, Avg Acc: 0.4385
2022-01-20 11:30:54,102 Epoch[211/300], Step[0750/1252], Avg Loss: 3.1860, Avg Acc: 0.4402
2022-01-20 11:32:28,439 Epoch[211/300], Step[0800/1252], Avg Loss: 3.1868, Avg Acc: 0.4396
2022-01-20 11:34:03,261 Epoch[211/300], Step[0850/1252], Avg Loss: 3.1831, Avg Acc: 0.4386
2022-01-20 11:35:37,932 Epoch[211/300], Step[0900/1252], Avg Loss: 3.1841, Avg Acc: 0.4391
2022-01-20 11:37:13,174 Epoch[211/300], Step[0950/1252], Avg Loss: 3.1820, Avg Acc: 0.4400
2022-01-20 11:38:48,274 Epoch[211/300], Step[1000/1252], Avg Loss: 3.1827, Avg Acc: 0.4388
2022-01-20 11:40:22,186 Epoch[211/300], Step[1050/1252], Avg Loss: 3.1829, Avg Acc: 0.4365
2022-01-20 11:41:56,500 Epoch[211/300], Step[1100/1252], Avg Loss: 3.1825, Avg Acc: 0.4358
2022-01-20 11:43:30,148 Epoch[211/300], Step[1150/1252], Avg Loss: 3.1838, Avg Acc: 0.4353
2022-01-20 11:45:06,682 Epoch[211/300], Step[1200/1252], Avg Loss: 3.1845, Avg Acc: 0.4349
2022-01-20 11:46:40,228 Epoch[211/300], Step[1250/1252], Avg Loss: 3.1848, Avg Acc: 0.4344
2022-01-20 11:46:46,481 ----- Epoch[211/300], Train Loss: 3.1848, Train Acc: 0.4344, time: 2506.23
2022-01-20 11:46:46,482 Now training epoch 212. LR=0.000232
2022-01-20 11:48:54,873 Epoch[212/300], Step[0000/1252], Avg Loss: 3.1903, Avg Acc: 0.2812
2022-01-20 11:50:27,072 Epoch[212/300], Step[0050/1252], Avg Loss: 3.2476, Avg Acc: 0.4303
2022-01-20 11:51:58,932 Epoch[212/300], Step[0100/1252], Avg Loss: 3.2055, Avg Acc: 0.4273
2022-01-20 11:53:31,035 Epoch[212/300], Step[0150/1252], Avg Loss: 3.2083, Avg Acc: 0.4308
2022-01-20 11:55:04,900 Epoch[212/300], Step[0200/1252], Avg Loss: 3.1974, Avg Acc: 0.4353
2022-01-20 11:56:38,140 Epoch[212/300], Step[0250/1252], Avg Loss: 3.1896, Avg Acc: 0.4394
2022-01-20 11:58:08,929 Epoch[212/300], Step[0300/1252], Avg Loss: 3.1902, Avg Acc: 0.4421
2022-01-20 11:59:42,554 Epoch[212/300], Step[0350/1252], Avg Loss: 3.1940, Avg Acc: 0.4395
2022-01-20 12:01:15,817 Epoch[212/300], Step[0400/1252], Avg Loss: 3.1911, Avg Acc: 0.4401
2022-01-20 12:02:49,920 Epoch[212/300], Step[0450/1252], Avg Loss: 3.1911, Avg Acc: 0.4389
2022-01-20 12:04:22,986 Epoch[212/300], Step[0500/1252], Avg Loss: 3.1892, Avg Acc: 0.4364
2022-01-20 12:05:55,429 Epoch[212/300], Step[0550/1252], Avg Loss: 3.1883, Avg Acc: 0.4366
2022-01-20 12:07:27,926 Epoch[212/300], Step[0600/1252], Avg Loss: 3.1851, Avg Acc: 0.4394
2022-01-20 12:09:02,713 Epoch[212/300], Step[0650/1252], Avg Loss: 3.1802, Avg Acc: 0.4376
2022-01-20 12:10:34,091 Epoch[212/300], Step[0700/1252], Avg Loss: 3.1719, Avg Acc: 0.4390
2022-01-20 12:12:07,623 Epoch[212/300], Step[0750/1252], Avg Loss: 3.1697, Avg Acc: 0.4408
2022-01-20 12:13:40,279 Epoch[212/300], Step[0800/1252], Avg Loss: 3.1745, Avg Acc: 0.4384
2022-01-20 12:15:14,898 Epoch[212/300], Step[0850/1252], Avg Loss: 3.1752, Avg Acc: 0.4367
2022-01-20 12:16:47,645 Epoch[212/300], Step[0900/1252], Avg Loss: 3.1786, Avg Acc: 0.4366
2022-01-20 12:18:19,708 Epoch[212/300], Step[0950/1252], Avg Loss: 3.1815, Avg Acc: 0.4365
2022-01-20 12:19:52,181 Epoch[212/300], Step[1000/1252], Avg Loss: 3.1822, Avg Acc: 0.4356
2022-01-20 12:21:26,281 Epoch[212/300], Step[1050/1252], Avg Loss: 3.1818, Avg Acc: 0.4339
2022-01-20 12:23:01,156 Epoch[212/300], Step[1100/1252], Avg Loss: 3.1846, Avg Acc: 0.4338
2022-01-20 12:24:34,125 Epoch[212/300], Step[1150/1252], Avg Loss: 3.1843, Avg Acc: 0.4333
2022-01-20 12:26:06,902 Epoch[212/300], Step[1200/1252], Avg Loss: 3.1850, Avg Acc: 0.4348
2022-01-20 12:27:37,796 Epoch[212/300], Step[1250/1252], Avg Loss: 3.1857, Avg Acc: 0.4358
2022-01-20 12:27:44,289 ----- Epoch[212/300], Train Loss: 3.1857, Train Acc: 0.4357, time: 2457.80
2022-01-20 12:27:44,290 ----- Validation after Epoch: 212
2022-01-20 12:29:26,168 Val Step[0000/1563], Avg Loss: 0.9610, Avg Acc@1: 0.7188, Avg Acc@5: 1.0000
2022-01-20 12:29:28,532 Val Step[0050/1563], Avg Loss: 1.0784, Avg Acc@1: 0.7500, Avg Acc@5: 0.9326
2022-01-20 12:29:30,773 Val Step[0100/1563], Avg Loss: 1.0943, Avg Acc@1: 0.7562, Avg Acc@5: 0.9307
2022-01-20 12:29:33,048 Val Step[0150/1563], Avg Loss: 1.0936, Avg Acc@1: 0.7575, Avg Acc@5: 0.9280
2022-01-20 12:29:35,363 Val Step[0200/1563], Avg Loss: 1.0941, Avg Acc@1: 0.7587, Avg Acc@5: 0.9289
2022-01-20 12:29:37,552 Val Step[0250/1563], Avg Loss: 1.0867, Avg Acc@1: 0.7595, Avg Acc@5: 0.9300
2022-01-20 12:29:39,780 Val Step[0300/1563], Avg Loss: 1.0902, Avg Acc@1: 0.7596, Avg Acc@5: 0.9289
2022-01-20 12:29:41,953 Val Step[0350/1563], Avg Loss: 1.0943, Avg Acc@1: 0.7586, Avg Acc@5: 0.9282
2022-01-20 12:29:44,159 Val Step[0400/1563], Avg Loss: 1.0943, Avg Acc@1: 0.7594, Avg Acc@5: 0.9278
2022-01-20 12:29:46,416 Val Step[0450/1563], Avg Loss: 1.1004, Avg Acc@1: 0.7561, Avg Acc@5: 0.9273
2022-01-20 12:29:48,642 Val Step[0500/1563], Avg Loss: 1.1046, Avg Acc@1: 0.7553, Avg Acc@5: 0.9269
2022-01-20 12:29:50,911 Val Step[0550/1563], Avg Loss: 1.1047, Avg Acc@1: 0.7550, Avg Acc@5: 0.9269
2022-01-20 12:29:53,088 Val Step[0600/1563], Avg Loss: 1.1048, Avg Acc@1: 0.7543, Avg Acc@5: 0.9269
2022-01-20 12:29:55,282 Val Step[0650/1563], Avg Loss: 1.1062, Avg Acc@1: 0.7539, Avg Acc@5: 0.9267
2022-01-20 12:29:57,516 Val Step[0700/1563], Avg Loss: 1.1057, Avg Acc@1: 0.7540, Avg Acc@5: 0.9271
2022-01-20 12:29:59,728 Val Step[0750/1563], Avg Loss: 1.1133, Avg Acc@1: 0.7519, Avg Acc@5: 0.9262
2022-01-20 12:30:02,038 Val Step[0800/1563], Avg Loss: 1.1139, Avg Acc@1: 0.7523, Avg Acc@5: 0.9260
2022-01-20 12:30:04,225 Val Step[0850/1563], Avg Loss: 1.1152, Avg Acc@1: 0.7518, Avg Acc@5: 0.9258
2022-01-20 12:30:06,524 Val Step[0900/1563], Avg Loss: 1.1121, Avg Acc@1: 0.7520, Avg Acc@5: 0.9265
2022-01-20 12:30:08,716 Val Step[0950/1563], Avg Loss: 1.1114, Avg Acc@1: 0.7526, Avg Acc@5: 0.9269
2022-01-20 12:30:10,867 Val Step[1000/1563], Avg Loss: 1.1120, Avg Acc@1: 0.7529, Avg Acc@5: 0.9268
2022-01-20 12:30:13,043 Val Step[1050/1563], Avg Loss: 1.1132, Avg Acc@1: 0.7523, Avg Acc@5: 0.9266
2022-01-20 12:30:15,268 Val Step[1100/1563], Avg Loss: 1.1126, Avg Acc@1: 0.7521, Avg Acc@5: 0.9267
2022-01-20 12:30:17,522 Val Step[1150/1563], Avg Loss: 1.1111, Avg Acc@1: 0.7520, Avg Acc@5: 0.9268
2022-01-20 12:30:19,751 Val Step[1200/1563], Avg Loss: 1.1095, Avg Acc@1: 0.7525, Avg Acc@5: 0.9270
2022-01-20 12:30:21,992 Val Step[1250/1563], Avg Loss: 1.1094, Avg Acc@1: 0.7524, Avg Acc@5: 0.9271
2022-01-20 12:30:24,088 Val Step[1300/1563], Avg Loss: 1.1127, Avg Acc@1: 0.7521, Avg Acc@5: 0.9266
2022-01-20 12:30:26,315 Val Step[1350/1563], Avg Loss: 1.1134, Avg Acc@1: 0.7515, Avg Acc@5: 0.9265
2022-01-20 12:30:28,558 Val Step[1400/1563], Avg Loss: 1.1130, Avg Acc@1: 0.7514, Avg Acc@5: 0.9264
2022-01-20 12:30:30,814 Val Step[1450/1563], Avg Loss: 1.1120, Avg Acc@1: 0.7517, Avg Acc@5: 0.9265
2022-01-20 12:30:33,038 Val Step[1500/1563], Avg Loss: 1.1114, Avg Acc@1: 0.7521, Avg Acc@5: 0.9265
2022-01-20 12:30:35,148 Val Step[1550/1563], Avg Loss: 1.1120, Avg Acc@1: 0.7519, Avg Acc@5: 0.9264
2022-01-20 12:30:37,098 ----- Epoch[212/300], Validation Loss: 1.1119, Validation Acc@1: 0.7518, Validation Acc@5: 0.9264, time: 172.81
2022-01-20 12:30:37,645 the pre best model acc:0.0000, at epoch 0
2022-01-20 12:30:37,646 current best model acc:0.7518, at epoch 212
2022-01-20 12:30:37,646 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-20 12:30:37,646 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-20 12:30:37,646 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-20 12:30:37,646 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-20 12:30:37,646 Now training epoch 213. LR=0.000228
2022-01-20 12:32:45,809 Epoch[213/300], Step[0000/1252], Avg Loss: 2.8908, Avg Acc: 0.5830
2022-01-20 12:34:18,291 Epoch[213/300], Step[0050/1252], Avg Loss: 3.1402, Avg Acc: 0.4701
2022-01-20 12:35:51,029 Epoch[213/300], Step[0100/1252], Avg Loss: 3.1479, Avg Acc: 0.4715
2022-01-20 12:37:23,380 Epoch[213/300], Step[0150/1252], Avg Loss: 3.1509, Avg Acc: 0.4561
2022-01-20 12:38:55,739 Epoch[213/300], Step[0200/1252], Avg Loss: 3.1626, Avg Acc: 0.4545
2022-01-20 12:40:29,777 Epoch[213/300], Step[0250/1252], Avg Loss: 3.1647, Avg Acc: 0.4558
2022-01-20 12:42:04,548 Epoch[213/300], Step[0300/1252], Avg Loss: 3.1604, Avg Acc: 0.4549
2022-01-20 12:43:37,879 Epoch[213/300], Step[0350/1252], Avg Loss: 3.1661, Avg Acc: 0.4534
2022-01-20 12:45:09,699 Epoch[213/300], Step[0400/1252], Avg Loss: 3.1666, Avg Acc: 0.4529
2022-01-20 12:46:43,281 Epoch[213/300], Step[0450/1252], Avg Loss: 3.1638, Avg Acc: 0.4501
2022-01-20 12:48:16,822 Epoch[213/300], Step[0500/1252], Avg Loss: 3.1642, Avg Acc: 0.4468
2022-01-20 12:49:49,214 Epoch[213/300], Step[0550/1252], Avg Loss: 3.1702, Avg Acc: 0.4459
2022-01-20 12:51:22,878 Epoch[213/300], Step[0600/1252], Avg Loss: 3.1715, Avg Acc: 0.4463
2022-01-20 12:52:56,932 Epoch[213/300], Step[0650/1252], Avg Loss: 3.1709, Avg Acc: 0.4460
2022-01-20 12:54:29,461 Epoch[213/300], Step[0700/1252], Avg Loss: 3.1735, Avg Acc: 0.4432
2022-01-20 12:56:02,829 Epoch[213/300], Step[0750/1252], Avg Loss: 3.1776, Avg Acc: 0.4414
2022-01-20 12:57:35,661 Epoch[213/300], Step[0800/1252], Avg Loss: 3.1803, Avg Acc: 0.4426
2022-01-20 12:59:08,589 Epoch[213/300], Step[0850/1252], Avg Loss: 3.1806, Avg Acc: 0.4417
2022-01-20 13:00:41,798 Epoch[213/300], Step[0900/1252], Avg Loss: 3.1822, Avg Acc: 0.4389
2022-01-20 13:02:16,192 Epoch[213/300], Step[0950/1252], Avg Loss: 3.1798, Avg Acc: 0.4390
2022-01-20 13:03:49,420 Epoch[213/300], Step[1000/1252], Avg Loss: 3.1796, Avg Acc: 0.4396
2022-01-20 13:05:24,259 Epoch[213/300], Step[1050/1252], Avg Loss: 3.1793, Avg Acc: 0.4385
2022-01-20 13:06:57,725 Epoch[213/300], Step[1100/1252], Avg Loss: 3.1799, Avg Acc: 0.4382
2022-01-20 13:08:30,060 Epoch[213/300], Step[1150/1252], Avg Loss: 3.1836, Avg Acc: 0.4380
2022-01-20 13:10:02,970 Epoch[213/300], Step[1200/1252], Avg Loss: 3.1818, Avg Acc: 0.4380
2022-01-20 13:11:36,612 Epoch[213/300], Step[1250/1252], Avg Loss: 3.1852, Avg Acc: 0.4363
2022-01-20 13:11:43,021 ----- Epoch[213/300], Train Loss: 3.1852, Train Acc: 0.4363, time: 2465.37, Best Val(epoch212) Acc@1: 0.7518
2022-01-20 13:11:43,022 Now training epoch 214. LR=0.000223
2022-01-20 13:13:54,904 Epoch[214/300], Step[0000/1252], Avg Loss: 2.4355, Avg Acc: 0.5664
2022-01-20 13:15:27,337 Epoch[214/300], Step[0050/1252], Avg Loss: 3.1839, Avg Acc: 0.4487
2022-01-20 13:16:58,869 Epoch[214/300], Step[0100/1252], Avg Loss: 3.1788, Avg Acc: 0.4478
2022-01-20 13:18:30,998 Epoch[214/300], Step[0150/1252], Avg Loss: 3.1639, Avg Acc: 0.4605
2022-01-20 13:20:03,782 Epoch[214/300], Step[0200/1252], Avg Loss: 3.1677, Avg Acc: 0.4501
2022-01-20 13:21:37,989 Epoch[214/300], Step[0250/1252], Avg Loss: 3.1609, Avg Acc: 0.4424
2022-01-20 13:23:10,724 Epoch[214/300], Step[0300/1252], Avg Loss: 3.1696, Avg Acc: 0.4459
2022-01-20 13:24:43,223 Epoch[214/300], Step[0350/1252], Avg Loss: 3.1604, Avg Acc: 0.4476
2022-01-20 13:26:16,818 Epoch[214/300], Step[0400/1252], Avg Loss: 3.1586, Avg Acc: 0.4475
2022-01-20 13:27:49,611 Epoch[214/300], Step[0450/1252], Avg Loss: 3.1623, Avg Acc: 0.4458
2022-01-20 13:29:23,127 Epoch[214/300], Step[0500/1252], Avg Loss: 3.1689, Avg Acc: 0.4451
2022-01-20 13:30:56,329 Epoch[214/300], Step[0550/1252], Avg Loss: 3.1658, Avg Acc: 0.4464
2022-01-20 13:32:30,537 Epoch[214/300], Step[0600/1252], Avg Loss: 3.1676, Avg Acc: 0.4459
2022-01-20 13:34:02,184 Epoch[214/300], Step[0650/1252], Avg Loss: 3.1683, Avg Acc: 0.4448
2022-01-20 13:35:37,360 Epoch[214/300], Step[0700/1252], Avg Loss: 3.1719, Avg Acc: 0.4434
2022-01-20 13:37:11,447 Epoch[214/300], Step[0750/1252], Avg Loss: 3.1752, Avg Acc: 0.4419
2022-01-20 13:38:44,413 Epoch[214/300], Step[0800/1252], Avg Loss: 3.1799, Avg Acc: 0.4393
2022-01-20 13:40:18,409 Epoch[214/300], Step[0850/1252], Avg Loss: 3.1842, Avg Acc: 0.4376
2022-01-20 13:41:53,288 Epoch[214/300], Step[0900/1252], Avg Loss: 3.1857, Avg Acc: 0.4379
2022-01-20 13:43:27,489 Epoch[214/300], Step[0950/1252], Avg Loss: 3.1879, Avg Acc: 0.4376
2022-01-20 13:44:59,597 Epoch[214/300], Step[1000/1252], Avg Loss: 3.1854, Avg Acc: 0.4375
2022-01-20 13:46:32,519 Epoch[214/300], Step[1050/1252], Avg Loss: 3.1853, Avg Acc: 0.4387
2022-01-20 13:48:06,418 Epoch[214/300], Step[1100/1252], Avg Loss: 3.1850, Avg Acc: 0.4391
2022-01-20 13:49:37,814 Epoch[214/300], Step[1150/1252], Avg Loss: 3.1822, Avg Acc: 0.4400
2022-01-20 13:51:11,155 Epoch[214/300], Step[1200/1252], Avg Loss: 3.1831, Avg Acc: 0.4406
2022-01-20 13:52:43,813 Epoch[214/300], Step[1250/1252], Avg Loss: 3.1847, Avg Acc: 0.4397
2022-01-20 13:52:50,568 ----- Epoch[214/300], Train Loss: 3.1847, Train Acc: 0.4397, time: 2467.54, Best Val(epoch212) Acc@1: 0.7518
2022-01-20 13:52:50,568 ----- Validation after Epoch: 214
2022-01-20 13:54:28,873 Val Step[0000/1563], Avg Loss: 0.9498, Avg Acc@1: 0.7500, Avg Acc@5: 1.0000
2022-01-20 13:54:31,173 Val Step[0050/1563], Avg Loss: 1.0696, Avg Acc@1: 0.7592, Avg Acc@5: 0.9381
2022-01-20 13:54:33,307 Val Step[0100/1563], Avg Loss: 1.0869, Avg Acc@1: 0.7621, Avg Acc@5: 0.9338
2022-01-20 13:54:35,575 Val Step[0150/1563], Avg Loss: 1.0966, Avg Acc@1: 0.7585, Avg Acc@5: 0.9300
2022-01-20 13:54:37,750 Val Step[0200/1563], Avg Loss: 1.0993, Avg Acc@1: 0.7582, Avg Acc@5: 0.9291
2022-01-20 13:54:39,891 Val Step[0250/1563], Avg Loss: 1.0926, Avg Acc@1: 0.7582, Avg Acc@5: 0.9294
2022-01-20 13:54:41,995 Val Step[0300/1563], Avg Loss: 1.0958, Avg Acc@1: 0.7590, Avg Acc@5: 0.9289
2022-01-20 13:54:44,099 Val Step[0350/1563], Avg Loss: 1.1012, Avg Acc@1: 0.7579, Avg Acc@5: 0.9284
2022-01-20 13:54:46,140 Val Step[0400/1563], Avg Loss: 1.1028, Avg Acc@1: 0.7571, Avg Acc@5: 0.9281
2022-01-20 13:54:48,309 Val Step[0450/1563], Avg Loss: 1.1098, Avg Acc@1: 0.7537, Avg Acc@5: 0.9279
2022-01-20 13:54:50,451 Val Step[0500/1563], Avg Loss: 1.1117, Avg Acc@1: 0.7524, Avg Acc@5: 0.9278
2022-01-20 13:54:52,659 Val Step[0550/1563], Avg Loss: 1.1129, Avg Acc@1: 0.7515, Avg Acc@5: 0.9277
2022-01-20 13:54:54,762 Val Step[0600/1563], Avg Loss: 1.1129, Avg Acc@1: 0.7505, Avg Acc@5: 0.9270
2022-01-20 13:54:56,761 Val Step[0650/1563], Avg Loss: 1.1150, Avg Acc@1: 0.7502, Avg Acc@5: 0.9265
2022-01-20 13:54:59,036 Val Step[0700/1563], Avg Loss: 1.1133, Avg Acc@1: 0.7510, Avg Acc@5: 0.9272
2022-01-20 13:55:01,196 Val Step[0750/1563], Avg Loss: 1.1178, Avg Acc@1: 0.7498, Avg Acc@5: 0.9265
2022-01-20 13:55:03,253 Val Step[0800/1563], Avg Loss: 1.1179, Avg Acc@1: 0.7504, Avg Acc@5: 0.9264
2022-01-20 13:55:05,293 Val Step[0850/1563], Avg Loss: 1.1192, Avg Acc@1: 0.7499, Avg Acc@5: 0.9262
2022-01-20 13:55:07,440 Val Step[0900/1563], Avg Loss: 1.1157, Avg Acc@1: 0.7503, Avg Acc@5: 0.9266
2022-01-20 13:55:09,597 Val Step[0950/1563], Avg Loss: 1.1157, Avg Acc@1: 0.7512, Avg Acc@5: 0.9268
2022-01-20 13:55:11,757 Val Step[1000/1563], Avg Loss: 1.1178, Avg Acc@1: 0.7505, Avg Acc@5: 0.9264
2022-01-20 13:55:14,053 Val Step[1050/1563], Avg Loss: 1.1195, Avg Acc@1: 0.7499, Avg Acc@5: 0.9260
2022-01-20 13:55:16,177 Val Step[1100/1563], Avg Loss: 1.1200, Avg Acc@1: 0.7496, Avg Acc@5: 0.9259
2022-01-20 13:55:18,486 Val Step[1150/1563], Avg Loss: 1.1189, Avg Acc@1: 0.7491, Avg Acc@5: 0.9263
2022-01-20 13:55:20,693 Val Step[1200/1563], Avg Loss: 1.1178, Avg Acc@1: 0.7492, Avg Acc@5: 0.9265
2022-01-20 13:55:23,039 Val Step[1250/1563], Avg Loss: 1.1173, Avg Acc@1: 0.7490, Avg Acc@5: 0.9265
2022-01-20 13:55:25,370 Val Step[1300/1563], Avg Loss: 1.1199, Avg Acc@1: 0.7487, Avg Acc@5: 0.9261
2022-01-20 13:55:27,749 Val Step[1350/1563], Avg Loss: 1.1215, Avg Acc@1: 0.7483, Avg Acc@5: 0.9258
2022-01-20 13:55:29,998 Val Step[1400/1563], Avg Loss: 1.1204, Avg Acc@1: 0.7486, Avg Acc@5: 0.9258
2022-01-20 13:55:32,323 Val Step[1450/1563], Avg Loss: 1.1195, Avg Acc@1: 0.7489, Avg Acc@5: 0.9258
2022-01-20 13:55:34,591 Val Step[1500/1563], Avg Loss: 1.1189, Avg Acc@1: 0.7493, Avg Acc@5: 0.9261
2022-01-20 13:55:36,776 Val Step[1550/1563], Avg Loss: 1.1200, Avg Acc@1: 0.7490, Avg Acc@5: 0.9258
2022-01-20 13:55:38,993 ----- Epoch[214/300], Validation Loss: 1.1198, Validation Acc@1: 0.7491, Validation Acc@5: 0.9259, time: 168.42
2022-01-20 13:55:38,993 Now training epoch 215. LR=0.000219
2022-01-20 13:57:41,373 Epoch[215/300], Step[0000/1252], Avg Loss: 2.6603, Avg Acc: 0.6846
2022-01-20 13:59:13,724 Epoch[215/300], Step[0050/1252], Avg Loss: 3.1998, Avg Acc: 0.4187
2022-01-20 14:00:45,578 Epoch[215/300], Step[0100/1252], Avg Loss: 3.1796, Avg Acc: 0.4590
2022-01-20 14:02:18,760 Epoch[215/300], Step[0150/1252], Avg Loss: 3.1566, Avg Acc: 0.4636
2022-01-20 14:03:50,837 Epoch[215/300], Step[0200/1252], Avg Loss: 3.1574, Avg Acc: 0.4535
2022-01-20 14:05:24,243 Epoch[215/300], Step[0250/1252], Avg Loss: 3.1658, Avg Acc: 0.4517
2022-01-20 14:06:57,958 Epoch[215/300], Step[0300/1252], Avg Loss: 3.1693, Avg Acc: 0.4491
2022-01-20 14:08:31,463 Epoch[215/300], Step[0350/1252], Avg Loss: 3.1667, Avg Acc: 0.4468
2022-01-20 14:10:05,032 Epoch[215/300], Step[0400/1252], Avg Loss: 3.1719, Avg Acc: 0.4491
2022-01-20 14:11:38,941 Epoch[215/300], Step[0450/1252], Avg Loss: 3.1741, Avg Acc: 0.4474
2022-01-20 14:13:11,073 Epoch[215/300], Step[0500/1252], Avg Loss: 3.1693, Avg Acc: 0.4476
2022-01-20 14:14:42,818 Epoch[215/300], Step[0550/1252], Avg Loss: 3.1658, Avg Acc: 0.4466
2022-01-20 14:16:17,244 Epoch[215/300], Step[0600/1252], Avg Loss: 3.1687, Avg Acc: 0.4446
2022-01-20 14:17:49,127 Epoch[215/300], Step[0650/1252], Avg Loss: 3.1686, Avg Acc: 0.4412
2022-01-20 14:19:23,088 Epoch[215/300], Step[0700/1252], Avg Loss: 3.1702, Avg Acc: 0.4405
2022-01-20 14:20:56,200 Epoch[215/300], Step[0750/1252], Avg Loss: 3.1686, Avg Acc: 0.4410
2022-01-20 14:22:29,307 Epoch[215/300], Step[0800/1252], Avg Loss: 3.1659, Avg Acc: 0.4414
2022-01-20 14:24:02,765 Epoch[215/300], Step[0850/1252], Avg Loss: 3.1667, Avg Acc: 0.4416
2022-01-20 14:25:35,949 Epoch[215/300], Step[0900/1252], Avg Loss: 3.1702, Avg Acc: 0.4413
2022-01-20 14:27:08,599 Epoch[215/300], Step[0950/1252], Avg Loss: 3.1709, Avg Acc: 0.4420
2022-01-20 14:28:41,461 Epoch[215/300], Step[1000/1252], Avg Loss: 3.1741, Avg Acc: 0.4410
2022-01-20 14:30:14,974 Epoch[215/300], Step[1050/1252], Avg Loss: 3.1708, Avg Acc: 0.4418
2022-01-20 14:31:48,573 Epoch[215/300], Step[1100/1252], Avg Loss: 3.1712, Avg Acc: 0.4422
2022-01-20 14:33:21,898 Epoch[215/300], Step[1150/1252], Avg Loss: 3.1711, Avg Acc: 0.4422
2022-01-20 14:34:55,662 Epoch[215/300], Step[1200/1252], Avg Loss: 3.1711, Avg Acc: 0.4419
2022-01-20 14:36:30,266 Epoch[215/300], Step[1250/1252], Avg Loss: 3.1737, Avg Acc: 0.4408
2022-01-20 14:36:36,708 ----- Epoch[215/300], Train Loss: 3.1737, Train Acc: 0.4409, time: 2457.71, Best Val(epoch212) Acc@1: 0.7518
2022-01-20 14:36:36,708 Now training epoch 216. LR=0.000214
2022-01-20 14:38:41,869 Epoch[216/300], Step[0000/1252], Avg Loss: 3.4176, Avg Acc: 0.1602
2022-01-20 14:40:14,471 Epoch[216/300], Step[0050/1252], Avg Loss: 3.2430, Avg Acc: 0.4315
2022-01-20 14:41:47,158 Epoch[216/300], Step[0100/1252], Avg Loss: 3.2073, Avg Acc: 0.4415
2022-01-20 14:43:21,073 Epoch[216/300], Step[0150/1252], Avg Loss: 3.2113, Avg Acc: 0.4423
2022-01-20 14:44:53,667 Epoch[216/300], Step[0200/1252], Avg Loss: 3.2065, Avg Acc: 0.4317
2022-01-20 14:46:26,870 Epoch[216/300], Step[0250/1252], Avg Loss: 3.1961, Avg Acc: 0.4344
2022-01-20 14:47:59,799 Epoch[216/300], Step[0300/1252], Avg Loss: 3.2040, Avg Acc: 0.4349
2022-01-20 14:49:32,478 Epoch[216/300], Step[0350/1252], Avg Loss: 3.1966, Avg Acc: 0.4351
2022-01-20 14:51:04,777 Epoch[216/300], Step[0400/1252], Avg Loss: 3.1941, Avg Acc: 0.4364
2022-01-20 14:52:38,102 Epoch[216/300], Step[0450/1252], Avg Loss: 3.1841, Avg Acc: 0.4419
2022-01-20 14:54:12,246 Epoch[216/300], Step[0500/1252], Avg Loss: 3.1840, Avg Acc: 0.4400
2022-01-20 14:55:45,215 Epoch[216/300], Step[0550/1252], Avg Loss: 3.1832, Avg Acc: 0.4392
2022-01-20 14:57:18,392 Epoch[216/300], Step[0600/1252], Avg Loss: 3.1770, Avg Acc: 0.4379
2022-01-20 14:58:52,120 Epoch[216/300], Step[0650/1252], Avg Loss: 3.1769, Avg Acc: 0.4371
2022-01-20 15:00:25,893 Epoch[216/300], Step[0700/1252], Avg Loss: 3.1766, Avg Acc: 0.4378
2022-01-20 15:01:57,452 Epoch[216/300], Step[0750/1252], Avg Loss: 3.1737, Avg Acc: 0.4386
2022-01-20 15:03:27,270 Epoch[216/300], Step[0800/1252], Avg Loss: 3.1733, Avg Acc: 0.4407
2022-01-20 15:04:58,877 Epoch[216/300], Step[0850/1252], Avg Loss: 3.1741, Avg Acc: 0.4409
2022-01-20 15:06:31,811 Epoch[216/300], Step[0900/1252], Avg Loss: 3.1763, Avg Acc: 0.4407
2022-01-20 15:08:05,103 Epoch[216/300], Step[0950/1252], Avg Loss: 3.1731, Avg Acc: 0.4402
2022-01-20 15:09:38,263 Epoch[216/300], Step[1000/1252], Avg Loss: 3.1734, Avg Acc: 0.4391
2022-01-20 15:11:12,574 Epoch[216/300], Step[1050/1252], Avg Loss: 3.1736, Avg Acc: 0.4368
2022-01-20 15:12:46,245 Epoch[216/300], Step[1100/1252], Avg Loss: 3.1715, Avg Acc: 0.4371
2022-01-20 15:14:18,586 Epoch[216/300], Step[1150/1252], Avg Loss: 3.1706, Avg Acc: 0.4370
2022-01-20 15:15:52,513 Epoch[216/300], Step[1200/1252], Avg Loss: 3.1707, Avg Acc: 0.4376
2022-01-20 15:17:25,282 Epoch[216/300], Step[1250/1252], Avg Loss: 3.1735, Avg Acc: 0.4391
2022-01-20 15:17:32,789 ----- Epoch[216/300], Train Loss: 3.1735, Train Acc: 0.4391, time: 2456.08, Best Val(epoch212) Acc@1: 0.7518
2022-01-20 15:17:32,789 ----- Validation after Epoch: 216
2022-01-20 15:19:13,059 Val Step[0000/1563], Avg Loss: 1.0590, Avg Acc@1: 0.7500, Avg Acc@5: 0.9688
2022-01-20 15:19:15,495 Val Step[0050/1563], Avg Loss: 1.0930, Avg Acc@1: 0.7488, Avg Acc@5: 0.9240
2022-01-20 15:19:17,660 Val Step[0100/1563], Avg Loss: 1.1073, Avg Acc@1: 0.7534, Avg Acc@5: 0.9251
2022-01-20 15:19:19,851 Val Step[0150/1563], Avg Loss: 1.1106, Avg Acc@1: 0.7562, Avg Acc@5: 0.9247
2022-01-20 15:19:21,968 Val Step[0200/1563], Avg Loss: 1.1058, Avg Acc@1: 0.7593, Avg Acc@5: 0.9271
2022-01-20 15:19:24,134 Val Step[0250/1563], Avg Loss: 1.0958, Avg Acc@1: 0.7606, Avg Acc@5: 0.9293
2022-01-20 15:19:26,312 Val Step[0300/1563], Avg Loss: 1.0989, Avg Acc@1: 0.7603, Avg Acc@5: 0.9288
2022-01-20 15:19:28,552 Val Step[0350/1563], Avg Loss: 1.1024, Avg Acc@1: 0.7589, Avg Acc@5: 0.9284
2022-01-20 15:19:30,761 Val Step[0400/1563], Avg Loss: 1.1012, Avg Acc@1: 0.7585, Avg Acc@5: 0.9289
2022-01-20 15:19:32,818 Val Step[0450/1563], Avg Loss: 1.1078, Avg Acc@1: 0.7566, Avg Acc@5: 0.9283
2022-01-20 15:19:34,940 Val Step[0500/1563], Avg Loss: 1.1097, Avg Acc@1: 0.7554, Avg Acc@5: 0.9279
2022-01-20 15:19:37,075 Val Step[0550/1563], Avg Loss: 1.1111, Avg Acc@1: 0.7543, Avg Acc@5: 0.9279
2022-01-20 15:19:39,268 Val Step[0600/1563], Avg Loss: 1.1103, Avg Acc@1: 0.7542, Avg Acc@5: 0.9279
2022-01-20 15:19:41,414 Val Step[0650/1563], Avg Loss: 1.1104, Avg Acc@1: 0.7546, Avg Acc@5: 0.9279
2022-01-20 15:19:43,460 Val Step[0700/1563], Avg Loss: 1.1098, Avg Acc@1: 0.7553, Avg Acc@5: 0.9284
2022-01-20 15:19:45,508 Val Step[0750/1563], Avg Loss: 1.1147, Avg Acc@1: 0.7541, Avg Acc@5: 0.9279
2022-01-20 15:19:47,634 Val Step[0800/1563], Avg Loss: 1.1156, Avg Acc@1: 0.7543, Avg Acc@5: 0.9275
2022-01-20 15:19:49,704 Val Step[0850/1563], Avg Loss: 1.1168, Avg Acc@1: 0.7537, Avg Acc@5: 0.9274
2022-01-20 15:19:51,964 Val Step[0900/1563], Avg Loss: 1.1138, Avg Acc@1: 0.7537, Avg Acc@5: 0.9279
2022-01-20 15:19:54,177 Val Step[0950/1563], Avg Loss: 1.1126, Avg Acc@1: 0.7544, Avg Acc@5: 0.9281
2022-01-20 15:19:56,265 Val Step[1000/1563], Avg Loss: 1.1139, Avg Acc@1: 0.7544, Avg Acc@5: 0.9276
2022-01-20 15:19:58,870 Val Step[1050/1563], Avg Loss: 1.1150, Avg Acc@1: 0.7537, Avg Acc@5: 0.9275
2022-01-20 15:20:01,016 Val Step[1100/1563], Avg Loss: 1.1150, Avg Acc@1: 0.7537, Avg Acc@5: 0.9277
2022-01-20 15:20:03,207 Val Step[1150/1563], Avg Loss: 1.1140, Avg Acc@1: 0.7536, Avg Acc@5: 0.9279
2022-01-20 15:20:05,337 Val Step[1200/1563], Avg Loss: 1.1126, Avg Acc@1: 0.7544, Avg Acc@5: 0.9281
2022-01-20 15:20:07,499 Val Step[1250/1563], Avg Loss: 1.1114, Avg Acc@1: 0.7545, Avg Acc@5: 0.9284
2022-01-20 15:20:09,685 Val Step[1300/1563], Avg Loss: 1.1140, Avg Acc@1: 0.7543, Avg Acc@5: 0.9281
2022-01-20 15:20:11,801 Val Step[1350/1563], Avg Loss: 1.1148, Avg Acc@1: 0.7537, Avg Acc@5: 0.9281
2022-01-20 15:20:13,845 Val Step[1400/1563], Avg Loss: 1.1144, Avg Acc@1: 0.7534, Avg Acc@5: 0.9281
2022-01-20 15:20:16,190 Val Step[1450/1563], Avg Loss: 1.1135, Avg Acc@1: 0.7536, Avg Acc@5: 0.9283
2022-01-20 15:20:18,483 Val Step[1500/1563], Avg Loss: 1.1141, Avg Acc@1: 0.7535, Avg Acc@5: 0.9284
2022-01-20 15:20:20,668 Val Step[1550/1563], Avg Loss: 1.1143, Avg Acc@1: 0.7534, Avg Acc@5: 0.9283
2022-01-20 15:20:23,834 ----- Epoch[216/300], Validation Loss: 1.1139, Validation Acc@1: 0.7535, Validation Acc@5: 0.9284, time: 171.04
2022-01-20 15:20:25,024 the pre best model acc:0.7518, at epoch 212
2022-01-20 15:20:25,024 current best model acc:0.7535, at epoch 216
2022-01-20 15:20:25,024 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-20 15:20:25,024 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-20 15:20:25,024 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-20 15:20:25,024 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-20 15:20:25,024 Now training epoch 217. LR=0.000210
2022-01-20 15:22:33,199 Epoch[217/300], Step[0000/1252], Avg Loss: 2.7105, Avg Acc: 0.3477
2022-01-20 15:24:05,540 Epoch[217/300], Step[0050/1252], Avg Loss: 3.1694, Avg Acc: 0.4430
2022-01-20 15:25:37,153 Epoch[217/300], Step[0100/1252], Avg Loss: 3.1581, Avg Acc: 0.4396
2022-01-20 15:27:09,453 Epoch[217/300], Step[0150/1252], Avg Loss: 3.1535, Avg Acc: 0.4366
2022-01-20 15:28:42,594 Epoch[217/300], Step[0200/1252], Avg Loss: 3.1502, Avg Acc: 0.4437
2022-01-20 15:30:15,657 Epoch[217/300], Step[0250/1252], Avg Loss: 3.1491, Avg Acc: 0.4450
2022-01-20 15:31:49,281 Epoch[217/300], Step[0300/1252], Avg Loss: 3.1493, Avg Acc: 0.4428
2022-01-20 15:33:22,154 Epoch[217/300], Step[0350/1252], Avg Loss: 3.1465, Avg Acc: 0.4429
2022-01-20 15:34:54,296 Epoch[217/300], Step[0400/1252], Avg Loss: 3.1591, Avg Acc: 0.4421
2022-01-20 15:36:26,794 Epoch[217/300], Step[0450/1252], Avg Loss: 3.1588, Avg Acc: 0.4428
2022-01-20 15:38:00,275 Epoch[217/300], Step[0500/1252], Avg Loss: 3.1562, Avg Acc: 0.4445
2022-01-20 15:39:31,622 Epoch[217/300], Step[0550/1252], Avg Loss: 3.1554, Avg Acc: 0.4435
2022-01-20 15:41:04,564 Epoch[217/300], Step[0600/1252], Avg Loss: 3.1568, Avg Acc: 0.4449
2022-01-20 15:42:38,171 Epoch[217/300], Step[0650/1252], Avg Loss: 3.1532, Avg Acc: 0.4468
2022-01-20 15:44:10,451 Epoch[217/300], Step[0700/1252], Avg Loss: 3.1548, Avg Acc: 0.4456
2022-01-20 15:45:43,153 Epoch[217/300], Step[0750/1252], Avg Loss: 3.1569, Avg Acc: 0.4459
2022-01-20 15:47:16,705 Epoch[217/300], Step[0800/1252], Avg Loss: 3.1601, Avg Acc: 0.4456
2022-01-20 15:48:50,497 Epoch[217/300], Step[0850/1252], Avg Loss: 3.1628, Avg Acc: 0.4456
2022-01-20 15:50:22,364 Epoch[217/300], Step[0900/1252], Avg Loss: 3.1618, Avg Acc: 0.4448
2022-01-20 15:51:56,709 Epoch[217/300], Step[0950/1252], Avg Loss: 3.1609, Avg Acc: 0.4446
2022-01-20 15:53:27,634 Epoch[217/300], Step[1000/1252], Avg Loss: 3.1605, Avg Acc: 0.4458
2022-01-20 15:55:00,963 Epoch[217/300], Step[1050/1252], Avg Loss: 3.1614, Avg Acc: 0.4459
2022-01-20 15:56:32,568 Epoch[217/300], Step[1100/1252], Avg Loss: 3.1644, Avg Acc: 0.4451
2022-01-20 15:58:07,328 Epoch[217/300], Step[1150/1252], Avg Loss: 3.1673, Avg Acc: 0.4445
2022-01-20 15:59:38,906 Epoch[217/300], Step[1200/1252], Avg Loss: 3.1662, Avg Acc: 0.4452
2022-01-20 16:01:11,262 Epoch[217/300], Step[1250/1252], Avg Loss: 3.1665, Avg Acc: 0.4453
2022-01-20 16:01:17,325 ----- Epoch[217/300], Train Loss: 3.1665, Train Acc: 0.4453, time: 2452.30, Best Val(epoch216) Acc@1: 0.7535
2022-01-20 16:01:17,325 Now training epoch 218. LR=0.000205
2022-01-20 16:03:27,252 Epoch[218/300], Step[0000/1252], Avg Loss: 2.9636, Avg Acc: 0.3809
2022-01-20 16:04:57,695 Epoch[218/300], Step[0050/1252], Avg Loss: 3.1273, Avg Acc: 0.4922
2022-01-20 16:06:31,098 Epoch[218/300], Step[0100/1252], Avg Loss: 3.1657, Avg Acc: 0.4547
2022-01-20 16:08:03,681 Epoch[218/300], Step[0150/1252], Avg Loss: 3.1533, Avg Acc: 0.4464
2022-01-20 16:09:36,595 Epoch[218/300], Step[0200/1252], Avg Loss: 3.1480, Avg Acc: 0.4439
2022-01-20 16:11:09,843 Epoch[218/300], Step[0250/1252], Avg Loss: 3.1393, Avg Acc: 0.4456
2022-01-20 16:12:43,745 Epoch[218/300], Step[0300/1252], Avg Loss: 3.1367, Avg Acc: 0.4486
2022-01-20 16:14:17,206 Epoch[218/300], Step[0350/1252], Avg Loss: 3.1357, Avg Acc: 0.4486
2022-01-20 16:15:51,144 Epoch[218/300], Step[0400/1252], Avg Loss: 3.1399, Avg Acc: 0.4511
2022-01-20 16:17:25,816 Epoch[218/300], Step[0450/1252], Avg Loss: 3.1384, Avg Acc: 0.4540
2022-01-20 16:18:58,088 Epoch[218/300], Step[0500/1252], Avg Loss: 3.1380, Avg Acc: 0.4541
2022-01-20 16:20:31,479 Epoch[218/300], Step[0550/1252], Avg Loss: 3.1390, Avg Acc: 0.4491
2022-01-20 16:22:03,545 Epoch[218/300], Step[0600/1252], Avg Loss: 3.1409, Avg Acc: 0.4524
2022-01-20 16:23:37,031 Epoch[218/300], Step[0650/1252], Avg Loss: 3.1468, Avg Acc: 0.4527
2022-01-20 16:25:10,987 Epoch[218/300], Step[0700/1252], Avg Loss: 3.1501, Avg Acc: 0.4523
2022-01-20 16:26:44,196 Epoch[218/300], Step[0750/1252], Avg Loss: 3.1509, Avg Acc: 0.4523
2022-01-20 16:28:17,427 Epoch[218/300], Step[0800/1252], Avg Loss: 3.1474, Avg Acc: 0.4528
2022-01-20 16:29:50,063 Epoch[218/300], Step[0850/1252], Avg Loss: 3.1480, Avg Acc: 0.4530
2022-01-20 16:31:23,189 Epoch[218/300], Step[0900/1252], Avg Loss: 3.1486, Avg Acc: 0.4520
2022-01-20 16:32:57,755 Epoch[218/300], Step[0950/1252], Avg Loss: 3.1516, Avg Acc: 0.4503
2022-01-20 16:34:30,192 Epoch[218/300], Step[1000/1252], Avg Loss: 3.1542, Avg Acc: 0.4493
2022-01-20 16:36:05,357 Epoch[218/300], Step[1050/1252], Avg Loss: 3.1549, Avg Acc: 0.4478
2022-01-20 16:37:38,290 Epoch[218/300], Step[1100/1252], Avg Loss: 3.1541, Avg Acc: 0.4494
2022-01-20 16:39:11,555 Epoch[218/300], Step[1150/1252], Avg Loss: 3.1539, Avg Acc: 0.4490
2022-01-20 16:40:45,431 Epoch[218/300], Step[1200/1252], Avg Loss: 3.1521, Avg Acc: 0.4481
2022-01-20 16:42:18,629 Epoch[218/300], Step[1250/1252], Avg Loss: 3.1501, Avg Acc: 0.4493
2022-01-20 16:42:24,983 ----- Epoch[218/300], Train Loss: 3.1501, Train Acc: 0.4492, time: 2467.65, Best Val(epoch216) Acc@1: 0.7535
2022-01-20 16:42:24,983 ----- Validation after Epoch: 218
2022-01-20 16:44:03,939 Val Step[0000/1563], Avg Loss: 1.0516, Avg Acc@1: 0.7500, Avg Acc@5: 0.9375
2022-01-20 16:44:06,202 Val Step[0050/1563], Avg Loss: 1.0916, Avg Acc@1: 0.7555, Avg Acc@5: 0.9350
2022-01-20 16:44:08,363 Val Step[0100/1563], Avg Loss: 1.1078, Avg Acc@1: 0.7580, Avg Acc@5: 0.9338
2022-01-20 16:44:10,523 Val Step[0150/1563], Avg Loss: 1.1158, Avg Acc@1: 0.7552, Avg Acc@5: 0.9300
2022-01-20 16:44:12,708 Val Step[0200/1563], Avg Loss: 1.1146, Avg Acc@1: 0.7587, Avg Acc@5: 0.9294
2022-01-20 16:44:14,787 Val Step[0250/1563], Avg Loss: 1.1021, Avg Acc@1: 0.7607, Avg Acc@5: 0.9307
2022-01-20 16:44:16,852 Val Step[0300/1563], Avg Loss: 1.1071, Avg Acc@1: 0.7591, Avg Acc@5: 0.9294
2022-01-20 16:44:18,923 Val Step[0350/1563], Avg Loss: 1.1099, Avg Acc@1: 0.7594, Avg Acc@5: 0.9284
2022-01-20 16:44:21,046 Val Step[0400/1563], Avg Loss: 1.1107, Avg Acc@1: 0.7582, Avg Acc@5: 0.9279
2022-01-20 16:44:23,155 Val Step[0450/1563], Avg Loss: 1.1174, Avg Acc@1: 0.7559, Avg Acc@5: 0.9273
2022-01-20 16:44:25,343 Val Step[0500/1563], Avg Loss: 1.1211, Avg Acc@1: 0.7546, Avg Acc@5: 0.9275
2022-01-20 16:44:27,622 Val Step[0550/1563], Avg Loss: 1.1219, Avg Acc@1: 0.7541, Avg Acc@5: 0.9275
2022-01-20 16:44:29,819 Val Step[0600/1563], Avg Loss: 1.1213, Avg Acc@1: 0.7540, Avg Acc@5: 0.9279
2022-01-20 16:44:31,863 Val Step[0650/1563], Avg Loss: 1.1211, Avg Acc@1: 0.7544, Avg Acc@5: 0.9285
2022-01-20 16:44:34,011 Val Step[0700/1563], Avg Loss: 1.1192, Avg Acc@1: 0.7550, Avg Acc@5: 0.9293
2022-01-20 16:44:36,129 Val Step[0750/1563], Avg Loss: 1.1240, Avg Acc@1: 0.7539, Avg Acc@5: 0.9286
2022-01-20 16:44:38,284 Val Step[0800/1563], Avg Loss: 1.1245, Avg Acc@1: 0.7547, Avg Acc@5: 0.9284
2022-01-20 16:44:40,476 Val Step[0850/1563], Avg Loss: 1.1273, Avg Acc@1: 0.7537, Avg Acc@5: 0.9281
2022-01-20 16:44:42,565 Val Step[0900/1563], Avg Loss: 1.1240, Avg Acc@1: 0.7541, Avg Acc@5: 0.9287
2022-01-20 16:44:44,697 Val Step[0950/1563], Avg Loss: 1.1230, Avg Acc@1: 0.7548, Avg Acc@5: 0.9289
2022-01-20 16:44:46,756 Val Step[1000/1563], Avg Loss: 1.1247, Avg Acc@1: 0.7546, Avg Acc@5: 0.9284
2022-01-20 16:44:48,864 Val Step[1050/1563], Avg Loss: 1.1263, Avg Acc@1: 0.7534, Avg Acc@5: 0.9280
2022-01-20 16:44:50,942 Val Step[1100/1563], Avg Loss: 1.1279, Avg Acc@1: 0.7531, Avg Acc@5: 0.9279
2022-01-20 16:44:53,006 Val Step[1150/1563], Avg Loss: 1.1269, Avg Acc@1: 0.7530, Avg Acc@5: 0.9280
2022-01-20 16:44:55,170 Val Step[1200/1563], Avg Loss: 1.1250, Avg Acc@1: 0.7537, Avg Acc@5: 0.9280
2022-01-20 16:44:57,790 Val Step[1250/1563], Avg Loss: 1.1247, Avg Acc@1: 0.7534, Avg Acc@5: 0.9284
2022-01-20 16:44:59,912 Val Step[1300/1563], Avg Loss: 1.1276, Avg Acc@1: 0.7534, Avg Acc@5: 0.9280
2022-01-20 16:45:02,200 Val Step[1350/1563], Avg Loss: 1.1292, Avg Acc@1: 0.7529, Avg Acc@5: 0.9276
2022-01-20 16:45:04,160 Val Step[1400/1563], Avg Loss: 1.1295, Avg Acc@1: 0.7527, Avg Acc@5: 0.9273
2022-01-20 16:45:06,294 Val Step[1450/1563], Avg Loss: 1.1296, Avg Acc@1: 0.7528, Avg Acc@5: 0.9271
2022-01-20 16:45:08,334 Val Step[1500/1563], Avg Loss: 1.1292, Avg Acc@1: 0.7532, Avg Acc@5: 0.9272
2022-01-20 16:45:10,300 Val Step[1550/1563], Avg Loss: 1.1293, Avg Acc@1: 0.7531, Avg Acc@5: 0.9270
2022-01-20 16:45:12,232 ----- Epoch[218/300], Validation Loss: 1.1293, Validation Acc@1: 0.7531, Validation Acc@5: 0.9270, time: 167.25
2022-01-20 16:45:12,232 Now training epoch 219. LR=0.000201
2022-01-20 16:47:10,152 Epoch[219/300], Step[0000/1252], Avg Loss: 3.4188, Avg Acc: 0.3008
2022-01-20 16:48:41,605 Epoch[219/300], Step[0050/1252], Avg Loss: 3.1823, Avg Acc: 0.4498
2022-01-20 16:50:13,929 Epoch[219/300], Step[0100/1252], Avg Loss: 3.1682, Avg Acc: 0.4522
2022-01-20 16:51:46,646 Epoch[219/300], Step[0150/1252], Avg Loss: 3.1543, Avg Acc: 0.4540
2022-01-20 16:53:15,879 Epoch[219/300], Step[0200/1252], Avg Loss: 3.1557, Avg Acc: 0.4489
2022-01-20 16:54:48,356 Epoch[219/300], Step[0250/1252], Avg Loss: 3.1508, Avg Acc: 0.4467
2022-01-20 16:56:22,434 Epoch[219/300], Step[0300/1252], Avg Loss: 3.1508, Avg Acc: 0.4499
2022-01-20 16:57:56,808 Epoch[219/300], Step[0350/1252], Avg Loss: 3.1581, Avg Acc: 0.4478
2022-01-20 16:59:30,657 Epoch[219/300], Step[0400/1252], Avg Loss: 3.1618, Avg Acc: 0.4457
2022-01-20 17:01:04,389 Epoch[219/300], Step[0450/1252], Avg Loss: 3.1561, Avg Acc: 0.4462
2022-01-20 17:02:36,422 Epoch[219/300], Step[0500/1252], Avg Loss: 3.1547, Avg Acc: 0.4445
2022-01-20 17:04:10,125 Epoch[219/300], Step[0550/1252], Avg Loss: 3.1508, Avg Acc: 0.4463
2022-01-20 17:05:41,957 Epoch[219/300], Step[0600/1252], Avg Loss: 3.1531, Avg Acc: 0.4486
2022-01-20 17:07:16,295 Epoch[219/300], Step[0650/1252], Avg Loss: 3.1549, Avg Acc: 0.4452
2022-01-20 17:08:49,615 Epoch[219/300], Step[0700/1252], Avg Loss: 3.1575, Avg Acc: 0.4438
2022-01-20 17:10:22,556 Epoch[219/300], Step[0750/1252], Avg Loss: 3.1588, Avg Acc: 0.4455
2022-01-20 17:11:54,435 Epoch[219/300], Step[0800/1252], Avg Loss: 3.1592, Avg Acc: 0.4480
2022-01-20 17:13:27,325 Epoch[219/300], Step[0850/1252], Avg Loss: 3.1611, Avg Acc: 0.4482
2022-01-20 17:15:01,398 Epoch[219/300], Step[0900/1252], Avg Loss: 3.1600, Avg Acc: 0.4499
2022-01-20 17:16:34,155 Epoch[219/300], Step[0950/1252], Avg Loss: 3.1580, Avg Acc: 0.4507
2022-01-20 17:18:07,709 Epoch[219/300], Step[1000/1252], Avg Loss: 3.1600, Avg Acc: 0.4494
2022-01-20 17:19:41,374 Epoch[219/300], Step[1050/1252], Avg Loss: 3.1581, Avg Acc: 0.4507
2022-01-20 17:21:15,059 Epoch[219/300], Step[1100/1252], Avg Loss: 3.1572, Avg Acc: 0.4505
2022-01-20 17:22:48,537 Epoch[219/300], Step[1150/1252], Avg Loss: 3.1570, Avg Acc: 0.4501
2022-01-20 17:24:20,708 Epoch[219/300], Step[1200/1252], Avg Loss: 3.1557, Avg Acc: 0.4496
2022-01-20 17:25:54,782 Epoch[219/300], Step[1250/1252], Avg Loss: 3.1541, Avg Acc: 0.4508
2022-01-20 17:26:01,152 ----- Epoch[219/300], Train Loss: 3.1541, Train Acc: 0.4508, time: 2448.91, Best Val(epoch216) Acc@1: 0.7535
2022-01-20 17:26:01,153 Now training epoch 220. LR=0.000196
2022-01-20 17:28:03,943 Epoch[220/300], Step[0000/1252], Avg Loss: 3.4800, Avg Acc: 0.5820
2022-01-20 17:29:35,229 Epoch[220/300], Step[0050/1252], Avg Loss: 3.2105, Avg Acc: 0.4228
2022-01-20 17:31:08,031 Epoch[220/300], Step[0100/1252], Avg Loss: 3.1954, Avg Acc: 0.4392
2022-01-20 17:32:42,074 Epoch[220/300], Step[0150/1252], Avg Loss: 3.1786, Avg Acc: 0.4410
2022-01-20 17:34:18,087 Epoch[220/300], Step[0200/1252], Avg Loss: 3.1866, Avg Acc: 0.4331
2022-01-20 17:35:51,642 Epoch[220/300], Step[0250/1252], Avg Loss: 3.1806, Avg Acc: 0.4384
2022-01-20 17:37:25,825 Epoch[220/300], Step[0300/1252], Avg Loss: 3.1770, Avg Acc: 0.4360
2022-01-20 17:38:59,127 Epoch[220/300], Step[0350/1252], Avg Loss: 3.1723, Avg Acc: 0.4397
2022-01-20 17:40:32,737 Epoch[220/300], Step[0400/1252], Avg Loss: 3.1708, Avg Acc: 0.4397
2022-01-20 17:42:05,816 Epoch[220/300], Step[0450/1252], Avg Loss: 3.1689, Avg Acc: 0.4426
2022-01-20 17:43:38,757 Epoch[220/300], Step[0500/1252], Avg Loss: 3.1656, Avg Acc: 0.4454
2022-01-20 17:45:10,925 Epoch[220/300], Step[0550/1252], Avg Loss: 3.1651, Avg Acc: 0.4474
2022-01-20 17:46:43,760 Epoch[220/300], Step[0600/1252], Avg Loss: 3.1648, Avg Acc: 0.4471
2022-01-20 17:48:17,087 Epoch[220/300], Step[0650/1252], Avg Loss: 3.1689, Avg Acc: 0.4449
2022-01-20 17:49:50,261 Epoch[220/300], Step[0700/1252], Avg Loss: 3.1704, Avg Acc: 0.4448
2022-01-20 17:51:23,111 Epoch[220/300], Step[0750/1252], Avg Loss: 3.1655, Avg Acc: 0.4422
2022-01-20 17:52:55,157 Epoch[220/300], Step[0800/1252], Avg Loss: 3.1650, Avg Acc: 0.4444
2022-01-20 17:54:27,832 Epoch[220/300], Step[0850/1252], Avg Loss: 3.1622, Avg Acc: 0.4451
2022-01-20 17:56:00,470 Epoch[220/300], Step[0900/1252], Avg Loss: 3.1611, Avg Acc: 0.4465
2022-01-20 17:57:33,452 Epoch[220/300], Step[0950/1252], Avg Loss: 3.1606, Avg Acc: 0.4458
2022-01-20 17:59:06,699 Epoch[220/300], Step[1000/1252], Avg Loss: 3.1614, Avg Acc: 0.4431
2022-01-20 18:00:39,921 Epoch[220/300], Step[1050/1252], Avg Loss: 3.1611, Avg Acc: 0.4424
2022-01-20 18:02:13,081 Epoch[220/300], Step[1100/1252], Avg Loss: 3.1604, Avg Acc: 0.4423
2022-01-20 18:03:46,100 Epoch[220/300], Step[1150/1252], Avg Loss: 3.1589, Avg Acc: 0.4425
2022-01-20 18:05:18,828 Epoch[220/300], Step[1200/1252], Avg Loss: 3.1601, Avg Acc: 0.4435
2022-01-20 18:06:51,741 Epoch[220/300], Step[1250/1252], Avg Loss: 3.1612, Avg Acc: 0.4435
2022-01-20 18:06:58,354 ----- Epoch[220/300], Train Loss: 3.1612, Train Acc: 0.4435, time: 2457.20, Best Val(epoch216) Acc@1: 0.7535
2022-01-20 18:06:58,354 ----- Validation after Epoch: 220
2022-01-20 18:08:25,226 Val Step[0000/1563], Avg Loss: 0.9475, Avg Acc@1: 0.7500, Avg Acc@5: 1.0000
2022-01-20 18:08:27,582 Val Step[0050/1563], Avg Loss: 1.0730, Avg Acc@1: 0.7592, Avg Acc@5: 0.9252
2022-01-20 18:08:29,940 Val Step[0100/1563], Avg Loss: 1.0917, Avg Acc@1: 0.7614, Avg Acc@5: 0.9295
2022-01-20 18:08:32,148 Val Step[0150/1563], Avg Loss: 1.0881, Avg Acc@1: 0.7647, Avg Acc@5: 0.9280
2022-01-20 18:08:34,246 Val Step[0200/1563], Avg Loss: 1.0879, Avg Acc@1: 0.7643, Avg Acc@5: 0.9305
2022-01-20 18:08:36,503 Val Step[0250/1563], Avg Loss: 1.0762, Avg Acc@1: 0.7647, Avg Acc@5: 0.9319
2022-01-20 18:08:38,735 Val Step[0300/1563], Avg Loss: 1.0792, Avg Acc@1: 0.7636, Avg Acc@5: 0.9309
2022-01-20 18:08:40,864 Val Step[0350/1563], Avg Loss: 1.0880, Avg Acc@1: 0.7618, Avg Acc@5: 0.9300
2022-01-20 18:08:42,897 Val Step[0400/1563], Avg Loss: 1.0868, Avg Acc@1: 0.7613, Avg Acc@5: 0.9292
2022-01-20 18:08:45,044 Val Step[0450/1563], Avg Loss: 1.0928, Avg Acc@1: 0.7589, Avg Acc@5: 0.9282
2022-01-20 18:08:47,140 Val Step[0500/1563], Avg Loss: 1.0951, Avg Acc@1: 0.7583, Avg Acc@5: 0.9284
2022-01-20 18:08:49,343 Val Step[0550/1563], Avg Loss: 1.0954, Avg Acc@1: 0.7570, Avg Acc@5: 0.9284
2022-01-20 18:08:51,467 Val Step[0600/1563], Avg Loss: 1.0952, Avg Acc@1: 0.7567, Avg Acc@5: 0.9285
2022-01-20 18:08:53,724 Val Step[0650/1563], Avg Loss: 1.0949, Avg Acc@1: 0.7564, Avg Acc@5: 0.9285
2022-01-20 18:08:55,760 Val Step[0700/1563], Avg Loss: 1.0936, Avg Acc@1: 0.7571, Avg Acc@5: 0.9288
2022-01-20 18:08:57,829 Val Step[0750/1563], Avg Loss: 1.0979, Avg Acc@1: 0.7552, Avg Acc@5: 0.9285
2022-01-20 18:08:59,971 Val Step[0800/1563], Avg Loss: 1.0980, Avg Acc@1: 0.7559, Avg Acc@5: 0.9283
2022-01-20 18:09:02,118 Val Step[0850/1563], Avg Loss: 1.0995, Avg Acc@1: 0.7556, Avg Acc@5: 0.9280
2022-01-20 18:09:04,232 Val Step[0900/1563], Avg Loss: 1.0969, Avg Acc@1: 0.7561, Avg Acc@5: 0.9281
2022-01-20 18:09:06,222 Val Step[0950/1563], Avg Loss: 1.0961, Avg Acc@1: 0.7564, Avg Acc@5: 0.9284
2022-01-20 18:09:08,386 Val Step[1000/1563], Avg Loss: 1.0968, Avg Acc@1: 0.7566, Avg Acc@5: 0.9282
2022-01-20 18:09:10,508 Val Step[1050/1563], Avg Loss: 1.0986, Avg Acc@1: 0.7560, Avg Acc@5: 0.9278
2022-01-20 18:09:12,717 Val Step[1100/1563], Avg Loss: 1.1000, Avg Acc@1: 0.7556, Avg Acc@5: 0.9279
2022-01-20 18:09:14,898 Val Step[1150/1563], Avg Loss: 1.0973, Avg Acc@1: 0.7561, Avg Acc@5: 0.9285
2022-01-20 18:09:17,036 Val Step[1200/1563], Avg Loss: 1.0957, Avg Acc@1: 0.7566, Avg Acc@5: 0.9286
2022-01-20 18:09:19,199 Val Step[1250/1563], Avg Loss: 1.0956, Avg Acc@1: 0.7563, Avg Acc@5: 0.9287
2022-01-20 18:09:21,458 Val Step[1300/1563], Avg Loss: 1.0984, Avg Acc@1: 0.7561, Avg Acc@5: 0.9285
2022-01-20 18:09:23,710 Val Step[1350/1563], Avg Loss: 1.0996, Avg Acc@1: 0.7556, Avg Acc@5: 0.9284
2022-01-20 18:09:25,865 Val Step[1400/1563], Avg Loss: 1.0993, Avg Acc@1: 0.7555, Avg Acc@5: 0.9282
2022-01-20 18:09:28,049 Val Step[1450/1563], Avg Loss: 1.0984, Avg Acc@1: 0.7560, Avg Acc@5: 0.9282
2022-01-20 18:09:30,313 Val Step[1500/1563], Avg Loss: 1.0986, Avg Acc@1: 0.7562, Avg Acc@5: 0.9284
2022-01-20 18:09:32,344 Val Step[1550/1563], Avg Loss: 1.0993, Avg Acc@1: 0.7562, Avg Acc@5: 0.9283
2022-01-20 18:09:34,392 ----- Epoch[220/300], Validation Loss: 1.0992, Validation Acc@1: 0.7560, Validation Acc@5: 0.9284, time: 156.03
2022-01-20 18:09:35,577 the pre best model acc:0.7535, at epoch 216
2022-01-20 18:09:35,578 current best model acc:0.7560, at epoch 220
2022-01-20 18:09:35,578 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-20 18:09:35,578 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-20 18:09:35,578 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-20 18:09:35,578 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-20 18:09:35,578 Now training epoch 221. LR=0.000192
2022-01-20 18:11:36,065 Epoch[221/300], Step[0000/1252], Avg Loss: 3.3723, Avg Acc: 0.5850
2022-01-20 18:13:09,149 Epoch[221/300], Step[0050/1252], Avg Loss: 3.1524, Avg Acc: 0.4519
2022-01-20 18:14:42,680 Epoch[221/300], Step[0100/1252], Avg Loss: 3.1606, Avg Acc: 0.4351
2022-01-20 18:16:14,903 Epoch[221/300], Step[0150/1252], Avg Loss: 3.1637, Avg Acc: 0.4308
2022-01-20 18:17:47,102 Epoch[221/300], Step[0200/1252], Avg Loss: 3.1737, Avg Acc: 0.4371
2022-01-20 18:19:19,563 Epoch[221/300], Step[0250/1252], Avg Loss: 3.1622, Avg Acc: 0.4366
2022-01-20 18:20:51,596 Epoch[221/300], Step[0300/1252], Avg Loss: 3.1590, Avg Acc: 0.4361
2022-01-20 18:22:23,975 Epoch[221/300], Step[0350/1252], Avg Loss: 3.1506, Avg Acc: 0.4388
2022-01-20 18:23:56,240 Epoch[221/300], Step[0400/1252], Avg Loss: 3.1572, Avg Acc: 0.4398
2022-01-20 18:25:29,383 Epoch[221/300], Step[0450/1252], Avg Loss: 3.1509, Avg Acc: 0.4440
2022-01-20 18:27:02,957 Epoch[221/300], Step[0500/1252], Avg Loss: 3.1503, Avg Acc: 0.4446
2022-01-20 18:28:35,315 Epoch[221/300], Step[0550/1252], Avg Loss: 3.1502, Avg Acc: 0.4460
2022-01-20 18:30:09,419 Epoch[221/300], Step[0600/1252], Avg Loss: 3.1506, Avg Acc: 0.4454
2022-01-20 18:31:42,741 Epoch[221/300], Step[0650/1252], Avg Loss: 3.1507, Avg Acc: 0.4440
2022-01-20 18:33:17,051 Epoch[221/300], Step[0700/1252], Avg Loss: 3.1509, Avg Acc: 0.4454
2022-01-20 18:34:49,487 Epoch[221/300], Step[0750/1252], Avg Loss: 3.1562, Avg Acc: 0.4439
2022-01-20 18:36:21,973 Epoch[221/300], Step[0800/1252], Avg Loss: 3.1530, Avg Acc: 0.4456
2022-01-20 18:37:55,782 Epoch[221/300], Step[0850/1252], Avg Loss: 3.1542, Avg Acc: 0.4451
2022-01-20 18:39:28,587 Epoch[221/300], Step[0900/1252], Avg Loss: 3.1525, Avg Acc: 0.4453
2022-01-20 18:41:01,353 Epoch[221/300], Step[0950/1252], Avg Loss: 3.1553, Avg Acc: 0.4434
2022-01-20 18:42:33,976 Epoch[221/300], Step[1000/1252], Avg Loss: 3.1536, Avg Acc: 0.4451
2022-01-20 18:44:06,713 Epoch[221/300], Step[1050/1252], Avg Loss: 3.1501, Avg Acc: 0.4461
2022-01-20 18:45:40,288 Epoch[221/300], Step[1100/1252], Avg Loss: 3.1470, Avg Acc: 0.4475
2022-01-20 18:47:11,589 Epoch[221/300], Step[1150/1252], Avg Loss: 3.1450, Avg Acc: 0.4484
2022-01-20 18:48:45,128 Epoch[221/300], Step[1200/1252], Avg Loss: 3.1457, Avg Acc: 0.4484
2022-01-20 18:50:17,596 Epoch[221/300], Step[1250/1252], Avg Loss: 3.1469, Avg Acc: 0.4497
2022-01-20 18:50:24,114 ----- Epoch[221/300], Train Loss: 3.1469, Train Acc: 0.4497, time: 2448.53, Best Val(epoch220) Acc@1: 0.7560
2022-01-20 18:50:24,115 Now training epoch 222. LR=0.000188
2022-01-20 18:52:27,219 Epoch[222/300], Step[0000/1252], Avg Loss: 2.8949, Avg Acc: 0.2783
2022-01-20 18:53:59,940 Epoch[222/300], Step[0050/1252], Avg Loss: 3.0767, Avg Acc: 0.4548
2022-01-20 18:55:31,890 Epoch[222/300], Step[0100/1252], Avg Loss: 3.0996, Avg Acc: 0.4650
2022-01-20 18:57:04,907 Epoch[222/300], Step[0150/1252], Avg Loss: 3.1022, Avg Acc: 0.4570
2022-01-20 18:58:38,233 Epoch[222/300], Step[0200/1252], Avg Loss: 3.1000, Avg Acc: 0.4549
2022-01-20 19:00:11,042 Epoch[222/300], Step[0250/1252], Avg Loss: 3.1040, Avg Acc: 0.4534
2022-01-20 19:01:43,347 Epoch[222/300], Step[0300/1252], Avg Loss: 3.1023, Avg Acc: 0.4558
2022-01-20 19:03:16,888 Epoch[222/300], Step[0350/1252], Avg Loss: 3.1104, Avg Acc: 0.4543
2022-01-20 19:04:50,026 Epoch[222/300], Step[0400/1252], Avg Loss: 3.1164, Avg Acc: 0.4526
2022-01-20 19:06:22,500 Epoch[222/300], Step[0450/1252], Avg Loss: 3.1232, Avg Acc: 0.4507
2022-01-20 19:07:54,511 Epoch[222/300], Step[0500/1252], Avg Loss: 3.1211, Avg Acc: 0.4503
2022-01-20 19:09:27,336 Epoch[222/300], Step[0550/1252], Avg Loss: 3.1173, Avg Acc: 0.4507
2022-01-20 19:11:00,756 Epoch[222/300], Step[0600/1252], Avg Loss: 3.1206, Avg Acc: 0.4524
2022-01-20 19:12:35,329 Epoch[222/300], Step[0650/1252], Avg Loss: 3.1230, Avg Acc: 0.4535
2022-01-20 19:14:08,805 Epoch[222/300], Step[0700/1252], Avg Loss: 3.1273, Avg Acc: 0.4521
2022-01-20 19:15:41,727 Epoch[222/300], Step[0750/1252], Avg Loss: 3.1315, Avg Acc: 0.4499
2022-01-20 19:17:14,487 Epoch[222/300], Step[0800/1252], Avg Loss: 3.1326, Avg Acc: 0.4489
2022-01-20 19:18:47,069 Epoch[222/300], Step[0850/1252], Avg Loss: 3.1360, Avg Acc: 0.4483
2022-01-20 19:20:19,115 Epoch[222/300], Step[0900/1252], Avg Loss: 3.1355, Avg Acc: 0.4487
2022-01-20 19:21:52,553 Epoch[222/300], Step[0950/1252], Avg Loss: 3.1313, Avg Acc: 0.4488
2022-01-20 19:23:26,649 Epoch[222/300], Step[1000/1252], Avg Loss: 3.1339, Avg Acc: 0.4491
2022-01-20 19:24:59,588 Epoch[222/300], Step[1050/1252], Avg Loss: 3.1332, Avg Acc: 0.4496
2022-01-20 19:26:33,448 Epoch[222/300], Step[1100/1252], Avg Loss: 3.1361, Avg Acc: 0.4485
2022-01-20 19:28:07,237 Epoch[222/300], Step[1150/1252], Avg Loss: 3.1370, Avg Acc: 0.4475
2022-01-20 19:29:40,447 Epoch[222/300], Step[1200/1252], Avg Loss: 3.1346, Avg Acc: 0.4481
2022-01-20 19:31:14,987 Epoch[222/300], Step[1250/1252], Avg Loss: 3.1361, Avg Acc: 0.4483
2022-01-20 19:31:21,630 ----- Epoch[222/300], Train Loss: 3.1361, Train Acc: 0.4483, time: 2457.51, Best Val(epoch220) Acc@1: 0.7560
2022-01-20 19:31:21,630 ----- Validation after Epoch: 222
2022-01-20 19:33:03,529 Val Step[0000/1563], Avg Loss: 0.8379, Avg Acc@1: 0.8125, Avg Acc@5: 1.0000
2022-01-20 19:33:05,819 Val Step[0050/1563], Avg Loss: 1.0894, Avg Acc@1: 0.7678, Avg Acc@5: 0.9320
2022-01-20 19:33:08,125 Val Step[0100/1563], Avg Loss: 1.1082, Avg Acc@1: 0.7645, Avg Acc@5: 0.9319
2022-01-20 19:33:10,350 Val Step[0150/1563], Avg Loss: 1.1060, Avg Acc@1: 0.7668, Avg Acc@5: 0.9292
2022-01-20 19:33:12,428 Val Step[0200/1563], Avg Loss: 1.1043, Avg Acc@1: 0.7662, Avg Acc@5: 0.9317
2022-01-20 19:33:14,483 Val Step[0250/1563], Avg Loss: 1.0953, Avg Acc@1: 0.7654, Avg Acc@5: 0.9325
2022-01-20 19:33:16,564 Val Step[0300/1563], Avg Loss: 1.0950, Avg Acc@1: 0.7655, Avg Acc@5: 0.9311
2022-01-20 19:33:18,667 Val Step[0350/1563], Avg Loss: 1.1003, Avg Acc@1: 0.7636, Avg Acc@5: 0.9310
2022-01-20 19:33:20,742 Val Step[0400/1563], Avg Loss: 1.1003, Avg Acc@1: 0.7633, Avg Acc@5: 0.9303
2022-01-20 19:33:22,948 Val Step[0450/1563], Avg Loss: 1.1083, Avg Acc@1: 0.7614, Avg Acc@5: 0.9294
2022-01-20 19:33:25,029 Val Step[0500/1563], Avg Loss: 1.1093, Avg Acc@1: 0.7610, Avg Acc@5: 0.9299
2022-01-20 19:33:27,436 Val Step[0550/1563], Avg Loss: 1.1095, Avg Acc@1: 0.7599, Avg Acc@5: 0.9295
2022-01-20 19:33:29,740 Val Step[0600/1563], Avg Loss: 1.1092, Avg Acc@1: 0.7593, Avg Acc@5: 0.9298
2022-01-20 19:33:32,031 Val Step[0650/1563], Avg Loss: 1.1093, Avg Acc@1: 0.7594, Avg Acc@5: 0.9299
2022-01-20 19:33:34,384 Val Step[0700/1563], Avg Loss: 1.1081, Avg Acc@1: 0.7592, Avg Acc@5: 0.9309
2022-01-20 19:33:36,695 Val Step[0750/1563], Avg Loss: 1.1141, Avg Acc@1: 0.7578, Avg Acc@5: 0.9301
2022-01-20 19:33:38,944 Val Step[0800/1563], Avg Loss: 1.1146, Avg Acc@1: 0.7580, Avg Acc@5: 0.9297
2022-01-20 19:33:41,017 Val Step[0850/1563], Avg Loss: 1.1158, Avg Acc@1: 0.7575, Avg Acc@5: 0.9293
2022-01-20 19:33:43,244 Val Step[0900/1563], Avg Loss: 1.1118, Avg Acc@1: 0.7582, Avg Acc@5: 0.9294
2022-01-20 19:33:45,395 Val Step[0950/1563], Avg Loss: 1.1107, Avg Acc@1: 0.7591, Avg Acc@5: 0.9297
2022-01-20 19:33:47,519 Val Step[1000/1563], Avg Loss: 1.1120, Avg Acc@1: 0.7587, Avg Acc@5: 0.9294
2022-01-20 19:33:49,774 Val Step[1050/1563], Avg Loss: 1.1136, Avg Acc@1: 0.7578, Avg Acc@5: 0.9291
2022-01-20 19:33:51,938 Val Step[1100/1563], Avg Loss: 1.1142, Avg Acc@1: 0.7576, Avg Acc@5: 0.9290
2022-01-20 19:33:54,126 Val Step[1150/1563], Avg Loss: 1.1129, Avg Acc@1: 0.7575, Avg Acc@5: 0.9291
2022-01-20 19:33:56,229 Val Step[1200/1563], Avg Loss: 1.1117, Avg Acc@1: 0.7577, Avg Acc@5: 0.9291
2022-01-20 19:33:58,327 Val Step[1250/1563], Avg Loss: 1.1110, Avg Acc@1: 0.7573, Avg Acc@5: 0.9295
2022-01-20 19:34:00,423 Val Step[1300/1563], Avg Loss: 1.1140, Avg Acc@1: 0.7568, Avg Acc@5: 0.9292
2022-01-20 19:34:02,571 Val Step[1350/1563], Avg Loss: 1.1144, Avg Acc@1: 0.7567, Avg Acc@5: 0.9295
2022-01-20 19:34:04,703 Val Step[1400/1563], Avg Loss: 1.1136, Avg Acc@1: 0.7567, Avg Acc@5: 0.9294
2022-01-20 19:34:06,834 Val Step[1450/1563], Avg Loss: 1.1132, Avg Acc@1: 0.7569, Avg Acc@5: 0.9294
2022-01-20 19:34:08,797 Val Step[1500/1563], Avg Loss: 1.1128, Avg Acc@1: 0.7572, Avg Acc@5: 0.9298
2022-01-20 19:34:10,751 Val Step[1550/1563], Avg Loss: 1.1131, Avg Acc@1: 0.7571, Avg Acc@5: 0.9297
2022-01-20 19:34:12,680 ----- Epoch[222/300], Validation Loss: 1.1128, Validation Acc@1: 0.7570, Validation Acc@5: 0.9297, time: 171.05
2022-01-20 19:34:14,019 the pre best model acc:0.7560, at epoch 220
2022-01-20 19:34:14,019 current best model acc:0.7570, at epoch 222
2022-01-20 19:34:14,019 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-20 19:34:14,019 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-20 19:34:14,019 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-20 19:34:14,019 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-20 19:34:14,019 Now training epoch 223. LR=0.000184
2022-01-20 19:36:20,946 Epoch[223/300], Step[0000/1252], Avg Loss: 3.0644, Avg Acc: 0.4561
2022-01-20 19:37:52,881 Epoch[223/300], Step[0050/1252], Avg Loss: 3.0896, Avg Acc: 0.4855
2022-01-20 19:39:25,945 Epoch[223/300], Step[0100/1252], Avg Loss: 3.1340, Avg Acc: 0.4372
2022-01-20 19:40:58,635 Epoch[223/300], Step[0150/1252], Avg Loss: 3.1375, Avg Acc: 0.4432
2022-01-20 19:42:31,138 Epoch[223/300], Step[0200/1252], Avg Loss: 3.1356, Avg Acc: 0.4462
2022-01-20 19:44:03,791 Epoch[223/300], Step[0250/1252], Avg Loss: 3.1492, Avg Acc: 0.4415
2022-01-20 19:45:38,248 Epoch[223/300], Step[0300/1252], Avg Loss: 3.1515, Avg Acc: 0.4387
2022-01-20 19:47:10,795 Epoch[223/300], Step[0350/1252], Avg Loss: 3.1566, Avg Acc: 0.4397
2022-01-20 19:48:43,691 Epoch[223/300], Step[0400/1252], Avg Loss: 3.1498, Avg Acc: 0.4426
2022-01-20 19:50:17,086 Epoch[223/300], Step[0450/1252], Avg Loss: 3.1418, Avg Acc: 0.4451
2022-01-20 19:51:51,142 Epoch[223/300], Step[0500/1252], Avg Loss: 3.1391, Avg Acc: 0.4481
2022-01-20 19:53:24,031 Epoch[223/300], Step[0550/1252], Avg Loss: 3.1470, Avg Acc: 0.4446
2022-01-20 19:54:57,126 Epoch[223/300], Step[0600/1252], Avg Loss: 3.1463, Avg Acc: 0.4419
2022-01-20 19:56:30,140 Epoch[223/300], Step[0650/1252], Avg Loss: 3.1484, Avg Acc: 0.4404
2022-01-20 19:58:01,797 Epoch[223/300], Step[0700/1252], Avg Loss: 3.1482, Avg Acc: 0.4408
2022-01-20 19:59:33,242 Epoch[223/300], Step[0750/1252], Avg Loss: 3.1516, Avg Acc: 0.4419
2022-01-20 20:01:05,767 Epoch[223/300], Step[0800/1252], Avg Loss: 3.1487, Avg Acc: 0.4439
2022-01-20 20:02:39,257 Epoch[223/300], Step[0850/1252], Avg Loss: 3.1475, Avg Acc: 0.4444
2022-01-20 20:04:13,111 Epoch[223/300], Step[0900/1252], Avg Loss: 3.1481, Avg Acc: 0.4429
2022-01-20 20:05:47,014 Epoch[223/300], Step[0950/1252], Avg Loss: 3.1466, Avg Acc: 0.4422
2022-01-20 20:07:20,129 Epoch[223/300], Step[1000/1252], Avg Loss: 3.1468, Avg Acc: 0.4437
2022-01-20 20:08:53,528 Epoch[223/300], Step[1050/1252], Avg Loss: 3.1481, Avg Acc: 0.4427
2022-01-20 20:10:27,209 Epoch[223/300], Step[1100/1252], Avg Loss: 3.1489, Avg Acc: 0.4421
2022-01-20 20:12:00,854 Epoch[223/300], Step[1150/1252], Avg Loss: 3.1498, Avg Acc: 0.4427
2022-01-20 20:13:35,164 Epoch[223/300], Step[1200/1252], Avg Loss: 3.1506, Avg Acc: 0.4422
2022-01-20 20:15:07,547 Epoch[223/300], Step[1250/1252], Avg Loss: 3.1493, Avg Acc: 0.4424
2022-01-20 20:15:13,779 ----- Epoch[223/300], Train Loss: 3.1492, Train Acc: 0.4424, time: 2459.75, Best Val(epoch222) Acc@1: 0.7570
2022-01-20 20:15:13,779 Now training epoch 224. LR=0.000179
2022-01-20 20:17:23,548 Epoch[224/300], Step[0000/1252], Avg Loss: 3.3943, Avg Acc: 0.4668
2022-01-20 20:18:56,530 Epoch[224/300], Step[0050/1252], Avg Loss: 3.1012, Avg Acc: 0.4535
2022-01-20 20:20:29,400 Epoch[224/300], Step[0100/1252], Avg Loss: 3.1084, Avg Acc: 0.4467
2022-01-20 20:22:01,663 Epoch[224/300], Step[0150/1252], Avg Loss: 3.0977, Avg Acc: 0.4365
2022-01-20 20:23:34,145 Epoch[224/300], Step[0200/1252], Avg Loss: 3.1086, Avg Acc: 0.4458
2022-01-20 20:25:07,051 Epoch[224/300], Step[0250/1252], Avg Loss: 3.1139, Avg Acc: 0.4444
2022-01-20 20:26:39,213 Epoch[224/300], Step[0300/1252], Avg Loss: 3.1143, Avg Acc: 0.4425
2022-01-20 20:28:12,910 Epoch[224/300], Step[0350/1252], Avg Loss: 3.1202, Avg Acc: 0.4415
2022-01-20 20:29:47,352 Epoch[224/300], Step[0400/1252], Avg Loss: 3.1210, Avg Acc: 0.4467
2022-01-20 20:31:20,544 Epoch[224/300], Step[0450/1252], Avg Loss: 3.1263, Avg Acc: 0.4467
2022-01-20 20:32:53,819 Epoch[224/300], Step[0500/1252], Avg Loss: 3.1289, Avg Acc: 0.4479
2022-01-20 20:34:27,388 Epoch[224/300], Step[0550/1252], Avg Loss: 3.1282, Avg Acc: 0.4497
2022-01-20 20:36:01,416 Epoch[224/300], Step[0600/1252], Avg Loss: 3.1356, Avg Acc: 0.4496
2022-01-20 20:37:34,444 Epoch[224/300], Step[0650/1252], Avg Loss: 3.1301, Avg Acc: 0.4495
2022-01-20 20:39:09,301 Epoch[224/300], Step[0700/1252], Avg Loss: 3.1332, Avg Acc: 0.4487
2022-01-20 20:40:41,576 Epoch[224/300], Step[0750/1252], Avg Loss: 3.1322, Avg Acc: 0.4498
2022-01-20 20:42:14,268 Epoch[224/300], Step[0800/1252], Avg Loss: 3.1345, Avg Acc: 0.4479
2022-01-20 20:43:46,375 Epoch[224/300], Step[0850/1252], Avg Loss: 3.1309, Avg Acc: 0.4462
2022-01-20 20:45:19,355 Epoch[224/300], Step[0900/1252], Avg Loss: 3.1338, Avg Acc: 0.4452
2022-01-20 20:46:52,760 Epoch[224/300], Step[0950/1252], Avg Loss: 3.1321, Avg Acc: 0.4446
2022-01-20 20:48:26,321 Epoch[224/300], Step[1000/1252], Avg Loss: 3.1311, Avg Acc: 0.4447
2022-01-20 20:50:01,932 Epoch[224/300], Step[1050/1252], Avg Loss: 3.1325, Avg Acc: 0.4441
2022-01-20 20:51:33,546 Epoch[224/300], Step[1100/1252], Avg Loss: 3.1318, Avg Acc: 0.4440
2022-01-20 20:53:07,028 Epoch[224/300], Step[1150/1252], Avg Loss: 3.1297, Avg Acc: 0.4451
2022-01-20 20:54:39,688 Epoch[224/300], Step[1200/1252], Avg Loss: 3.1285, Avg Acc: 0.4453
2022-01-20 20:56:13,014 Epoch[224/300], Step[1250/1252], Avg Loss: 3.1270, Avg Acc: 0.4462
2022-01-20 20:56:19,460 ----- Epoch[224/300], Train Loss: 3.1270, Train Acc: 0.4461, time: 2465.68, Best Val(epoch222) Acc@1: 0.7570
2022-01-20 20:56:19,461 ----- Validation after Epoch: 224
2022-01-20 20:58:07,148 Val Step[0000/1563], Avg Loss: 0.8900, Avg Acc@1: 0.7812, Avg Acc@5: 1.0000
2022-01-20 20:58:09,378 Val Step[0050/1563], Avg Loss: 1.0432, Avg Acc@1: 0.7696, Avg Acc@5: 0.9314
2022-01-20 20:58:11,622 Val Step[0100/1563], Avg Loss: 1.0665, Avg Acc@1: 0.7652, Avg Acc@5: 0.9298
2022-01-20 20:58:13,761 Val Step[0150/1563], Avg Loss: 1.0625, Avg Acc@1: 0.7678, Avg Acc@5: 0.9307
2022-01-20 20:58:15,920 Val Step[0200/1563], Avg Loss: 1.0649, Avg Acc@1: 0.7683, Avg Acc@5: 0.9317
2022-01-20 20:58:18,118 Val Step[0250/1563], Avg Loss: 1.0544, Avg Acc@1: 0.7679, Avg Acc@5: 0.9336
2022-01-20 20:58:20,245 Val Step[0300/1563], Avg Loss: 1.0595, Avg Acc@1: 0.7676, Avg Acc@5: 0.9321
2022-01-20 20:58:22,416 Val Step[0350/1563], Avg Loss: 1.0642, Avg Acc@1: 0.7661, Avg Acc@5: 0.9306
2022-01-20 20:58:24,567 Val Step[0400/1563], Avg Loss: 1.0636, Avg Acc@1: 0.7644, Avg Acc@5: 0.9310
2022-01-20 20:58:26,720 Val Step[0450/1563], Avg Loss: 1.0702, Avg Acc@1: 0.7621, Avg Acc@5: 0.9302
2022-01-20 20:58:28,855 Val Step[0500/1563], Avg Loss: 1.0737, Avg Acc@1: 0.7608, Avg Acc@5: 0.9301
2022-01-20 20:58:31,019 Val Step[0550/1563], Avg Loss: 1.0742, Avg Acc@1: 0.7595, Avg Acc@5: 0.9299
2022-01-20 20:58:33,228 Val Step[0600/1563], Avg Loss: 1.0730, Avg Acc@1: 0.7593, Avg Acc@5: 0.9296
2022-01-20 20:58:35,422 Val Step[0650/1563], Avg Loss: 1.0731, Avg Acc@1: 0.7595, Avg Acc@5: 0.9297
2022-01-20 20:58:37,590 Val Step[0700/1563], Avg Loss: 1.0717, Avg Acc@1: 0.7601, Avg Acc@5: 0.9302
2022-01-20 20:58:39,780 Val Step[0750/1563], Avg Loss: 1.0769, Avg Acc@1: 0.7580, Avg Acc@5: 0.9298
2022-01-20 20:58:41,937 Val Step[0800/1563], Avg Loss: 1.0771, Avg Acc@1: 0.7583, Avg Acc@5: 0.9297
2022-01-20 20:58:44,070 Val Step[0850/1563], Avg Loss: 1.0795, Avg Acc@1: 0.7572, Avg Acc@5: 0.9293
2022-01-20 20:58:46,213 Val Step[0900/1563], Avg Loss: 1.0761, Avg Acc@1: 0.7580, Avg Acc@5: 0.9296
2022-01-20 20:58:48,319 Val Step[0950/1563], Avg Loss: 1.0752, Avg Acc@1: 0.7584, Avg Acc@5: 0.9297
2022-01-20 20:58:50,372 Val Step[1000/1563], Avg Loss: 1.0766, Avg Acc@1: 0.7580, Avg Acc@5: 0.9296
2022-01-20 20:58:52,504 Val Step[1050/1563], Avg Loss: 1.0767, Avg Acc@1: 0.7575, Avg Acc@5: 0.9296
2022-01-20 20:58:54,661 Val Step[1100/1563], Avg Loss: 1.0778, Avg Acc@1: 0.7573, Avg Acc@5: 0.9293
2022-01-20 20:58:56,836 Val Step[1150/1563], Avg Loss: 1.0756, Avg Acc@1: 0.7577, Avg Acc@5: 0.9296
2022-01-20 20:58:58,949 Val Step[1200/1563], Avg Loss: 1.0745, Avg Acc@1: 0.7584, Avg Acc@5: 0.9295
2022-01-20 20:59:01,064 Val Step[1250/1563], Avg Loss: 1.0740, Avg Acc@1: 0.7586, Avg Acc@5: 0.9297
2022-01-20 20:59:03,249 Val Step[1300/1563], Avg Loss: 1.0764, Avg Acc@1: 0.7581, Avg Acc@5: 0.9295
2022-01-20 20:59:05,488 Val Step[1350/1563], Avg Loss: 1.0781, Avg Acc@1: 0.7575, Avg Acc@5: 0.9291
2022-01-20 20:59:07,753 Val Step[1400/1563], Avg Loss: 1.0777, Avg Acc@1: 0.7574, Avg Acc@5: 0.9288
2022-01-20 20:59:10,067 Val Step[1450/1563], Avg Loss: 1.0775, Avg Acc@1: 0.7578, Avg Acc@5: 0.9287
2022-01-20 20:59:12,361 Val Step[1500/1563], Avg Loss: 1.0773, Avg Acc@1: 0.7579, Avg Acc@5: 0.9290
2022-01-20 20:59:14,571 Val Step[1550/1563], Avg Loss: 1.0782, Avg Acc@1: 0.7573, Avg Acc@5: 0.9290
2022-01-20 20:59:16,622 ----- Epoch[224/300], Validation Loss: 1.0782, Validation Acc@1: 0.7573, Validation Acc@5: 0.9291, time: 177.16
2022-01-20 20:59:17,868 the pre best model acc:0.7570, at epoch 222
2022-01-20 20:59:17,869 current best model acc:0.7573, at epoch 224
2022-01-20 20:59:17,869 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-20 20:59:17,869 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-20 20:59:17,869 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-20 20:59:17,869 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-20 20:59:17,869 Now training epoch 225. LR=0.000175
2022-01-20 21:01:37,968 Epoch[225/300], Step[0000/1252], Avg Loss: 2.5867, Avg Acc: 0.6973
2022-01-20 21:03:09,867 Epoch[225/300], Step[0050/1252], Avg Loss: 3.0790, Avg Acc: 0.4472
2022-01-20 21:04:42,097 Epoch[225/300], Step[0100/1252], Avg Loss: 3.0635, Avg Acc: 0.4424
2022-01-20 21:06:14,419 Epoch[225/300], Step[0150/1252], Avg Loss: 3.0798, Avg Acc: 0.4468
2022-01-20 21:07:47,988 Epoch[225/300], Step[0200/1252], Avg Loss: 3.0828, Avg Acc: 0.4484
2022-01-20 21:09:23,890 Epoch[225/300], Step[0250/1252], Avg Loss: 3.0923, Avg Acc: 0.4471
2022-01-20 21:10:57,006 Epoch[225/300], Step[0300/1252], Avg Loss: 3.0890, Avg Acc: 0.4501
2022-01-20 21:12:29,796 Epoch[225/300], Step[0350/1252], Avg Loss: 3.0954, Avg Acc: 0.4534
2022-01-20 21:14:03,423 Epoch[225/300], Step[0400/1252], Avg Loss: 3.1007, Avg Acc: 0.4528
2022-01-20 21:15:35,056 Epoch[225/300], Step[0450/1252], Avg Loss: 3.1008, Avg Acc: 0.4531
2022-01-20 21:17:07,417 Epoch[225/300], Step[0500/1252], Avg Loss: 3.0961, Avg Acc: 0.4535
2022-01-20 21:18:41,333 Epoch[225/300], Step[0550/1252], Avg Loss: 3.1048, Avg Acc: 0.4503
2022-01-20 21:20:15,210 Epoch[225/300], Step[0600/1252], Avg Loss: 3.1096, Avg Acc: 0.4504
2022-01-20 21:21:48,456 Epoch[225/300], Step[0650/1252], Avg Loss: 3.1137, Avg Acc: 0.4510
2022-01-20 21:23:20,366 Epoch[225/300], Step[0700/1252], Avg Loss: 3.1182, Avg Acc: 0.4499
2022-01-20 21:24:53,168 Epoch[225/300], Step[0750/1252], Avg Loss: 3.1200, Avg Acc: 0.4485
2022-01-20 21:26:26,133 Epoch[225/300], Step[0800/1252], Avg Loss: 3.1202, Avg Acc: 0.4485
2022-01-20 21:27:59,711 Epoch[225/300], Step[0850/1252], Avg Loss: 3.1218, Avg Acc: 0.4485
2022-01-20 21:29:31,950 Epoch[225/300], Step[0900/1252], Avg Loss: 3.1250, Avg Acc: 0.4496
2022-01-20 21:31:05,775 Epoch[225/300], Step[0950/1252], Avg Loss: 3.1258, Avg Acc: 0.4500
2022-01-20 21:32:38,664 Epoch[225/300], Step[1000/1252], Avg Loss: 3.1245, Avg Acc: 0.4505
2022-01-20 21:34:12,029 Epoch[225/300], Step[1050/1252], Avg Loss: 3.1256, Avg Acc: 0.4507
2022-01-20 21:35:44,972 Epoch[225/300], Step[1100/1252], Avg Loss: 3.1246, Avg Acc: 0.4509
2022-01-20 21:37:18,305 Epoch[225/300], Step[1150/1252], Avg Loss: 3.1233, Avg Acc: 0.4511
2022-01-20 21:38:51,109 Epoch[225/300], Step[1200/1252], Avg Loss: 3.1237, Avg Acc: 0.4522
2022-01-20 21:40:19,944 Epoch[225/300], Step[1250/1252], Avg Loss: 3.1263, Avg Acc: 0.4509
2022-01-20 21:40:26,078 ----- Epoch[225/300], Train Loss: 3.1262, Train Acc: 0.4509, time: 2468.20, Best Val(epoch224) Acc@1: 0.7573
2022-01-20 21:40:26,079 Now training epoch 226. LR=0.000171
2022-01-20 21:42:50,509 Epoch[226/300], Step[0000/1252], Avg Loss: 3.4009, Avg Acc: 0.2402
2022-01-20 21:44:22,486 Epoch[226/300], Step[0050/1252], Avg Loss: 3.0645, Avg Acc: 0.4787
2022-01-20 21:45:55,314 Epoch[226/300], Step[0100/1252], Avg Loss: 3.0604, Avg Acc: 0.4766
2022-01-20 21:47:30,028 Epoch[226/300], Step[0150/1252], Avg Loss: 3.0885, Avg Acc: 0.4703
2022-01-20 21:49:03,241 Epoch[226/300], Step[0200/1252], Avg Loss: 3.0945, Avg Acc: 0.4647
2022-01-20 21:50:35,830 Epoch[226/300], Step[0250/1252], Avg Loss: 3.1047, Avg Acc: 0.4642
2022-01-20 21:52:09,268 Epoch[226/300], Step[0300/1252], Avg Loss: 3.1024, Avg Acc: 0.4567
2022-01-20 21:53:42,711 Epoch[226/300], Step[0350/1252], Avg Loss: 3.1074, Avg Acc: 0.4578
2022-01-20 21:55:16,539 Epoch[226/300], Step[0400/1252], Avg Loss: 3.1036, Avg Acc: 0.4510
2022-01-20 21:56:48,466 Epoch[226/300], Step[0450/1252], Avg Loss: 3.1115, Avg Acc: 0.4509
2022-01-20 21:58:22,083 Epoch[226/300], Step[0500/1252], Avg Loss: 3.1130, Avg Acc: 0.4521
2022-01-20 21:59:55,735 Epoch[226/300], Step[0550/1252], Avg Loss: 3.1119, Avg Acc: 0.4504
2022-01-20 22:01:29,131 Epoch[226/300], Step[0600/1252], Avg Loss: 3.1141, Avg Acc: 0.4522
2022-01-20 22:03:01,314 Epoch[226/300], Step[0650/1252], Avg Loss: 3.1164, Avg Acc: 0.4531
2022-01-20 22:04:33,936 Epoch[226/300], Step[0700/1252], Avg Loss: 3.1182, Avg Acc: 0.4516
2022-01-20 22:06:07,537 Epoch[226/300], Step[0750/1252], Avg Loss: 3.1184, Avg Acc: 0.4551
2022-01-20 22:07:42,821 Epoch[226/300], Step[0800/1252], Avg Loss: 3.1196, Avg Acc: 0.4546
2022-01-20 22:09:16,799 Epoch[226/300], Step[0850/1252], Avg Loss: 3.1196, Avg Acc: 0.4541
2022-01-20 22:10:49,956 Epoch[226/300], Step[0900/1252], Avg Loss: 3.1160, Avg Acc: 0.4542
2022-01-20 22:12:22,760 Epoch[226/300], Step[0950/1252], Avg Loss: 3.1188, Avg Acc: 0.4539
2022-01-20 22:13:56,109 Epoch[226/300], Step[1000/1252], Avg Loss: 3.1172, Avg Acc: 0.4555
2022-01-20 22:15:28,139 Epoch[226/300], Step[1050/1252], Avg Loss: 3.1169, Avg Acc: 0.4539
2022-01-20 22:17:00,818 Epoch[226/300], Step[1100/1252], Avg Loss: 3.1168, Avg Acc: 0.4537
2022-01-20 22:18:34,719 Epoch[226/300], Step[1150/1252], Avg Loss: 3.1174, Avg Acc: 0.4547
2022-01-20 22:20:07,153 Epoch[226/300], Step[1200/1252], Avg Loss: 3.1179, Avg Acc: 0.4551
2022-01-20 22:21:39,528 Epoch[226/300], Step[1250/1252], Avg Loss: 3.1190, Avg Acc: 0.4544
2022-01-20 22:21:45,813 ----- Epoch[226/300], Train Loss: 3.1190, Train Acc: 0.4544, time: 2479.73, Best Val(epoch224) Acc@1: 0.7573
2022-01-20 22:21:45,813 ----- Validation after Epoch: 226
2022-01-20 22:23:34,484 Val Step[0000/1563], Avg Loss: 0.9097, Avg Acc@1: 0.8125, Avg Acc@5: 1.0000
2022-01-20 22:23:36,879 Val Step[0050/1563], Avg Loss: 1.0888, Avg Acc@1: 0.7586, Avg Acc@5: 0.9295
2022-01-20 22:23:39,514 Val Step[0100/1563], Avg Loss: 1.1025, Avg Acc@1: 0.7670, Avg Acc@5: 0.9301
2022-01-20 22:23:41,810 Val Step[0150/1563], Avg Loss: 1.1028, Avg Acc@1: 0.7670, Avg Acc@5: 0.9274
2022-01-20 22:23:44,088 Val Step[0200/1563], Avg Loss: 1.1014, Avg Acc@1: 0.7685, Avg Acc@5: 0.9293
2022-01-20 22:23:46,340 Val Step[0250/1563], Avg Loss: 1.0934, Avg Acc@1: 0.7661, Avg Acc@5: 0.9315
2022-01-20 22:23:48,618 Val Step[0300/1563], Avg Loss: 1.0952, Avg Acc@1: 0.7661, Avg Acc@5: 0.9303
2022-01-20 22:23:50,908 Val Step[0350/1563], Avg Loss: 1.1024, Avg Acc@1: 0.7646, Avg Acc@5: 0.9295
2022-01-20 22:23:53,282 Val Step[0400/1563], Avg Loss: 1.1000, Avg Acc@1: 0.7645, Avg Acc@5: 0.9302
2022-01-20 22:23:55,543 Val Step[0450/1563], Avg Loss: 1.1041, Avg Acc@1: 0.7622, Avg Acc@5: 0.9301
2022-01-20 22:23:57,828 Val Step[0500/1563], Avg Loss: 1.1080, Avg Acc@1: 0.7601, Avg Acc@5: 0.9301
2022-01-20 22:24:00,100 Val Step[0550/1563], Avg Loss: 1.1099, Avg Acc@1: 0.7585, Avg Acc@5: 0.9298
2022-01-20 22:24:02,375 Val Step[0600/1563], Avg Loss: 1.1079, Avg Acc@1: 0.7587, Avg Acc@5: 0.9303
2022-01-20 22:24:04,670 Val Step[0650/1563], Avg Loss: 1.1095, Avg Acc@1: 0.7581, Avg Acc@5: 0.9302
2022-01-20 22:24:06,977 Val Step[0700/1563], Avg Loss: 1.1086, Avg Acc@1: 0.7588, Avg Acc@5: 0.9307
2022-01-20 22:24:09,264 Val Step[0750/1563], Avg Loss: 1.1150, Avg Acc@1: 0.7569, Avg Acc@5: 0.9301
2022-01-20 22:24:11,524 Val Step[0800/1563], Avg Loss: 1.1145, Avg Acc@1: 0.7577, Avg Acc@5: 0.9299
2022-01-20 22:24:13,770 Val Step[0850/1563], Avg Loss: 1.1158, Avg Acc@1: 0.7575, Avg Acc@5: 0.9299
2022-01-20 22:24:16,029 Val Step[0900/1563], Avg Loss: 1.1131, Avg Acc@1: 0.7578, Avg Acc@5: 0.9300
2022-01-20 22:24:18,200 Val Step[0950/1563], Avg Loss: 1.1118, Avg Acc@1: 0.7589, Avg Acc@5: 0.9299
2022-01-20 22:24:20,345 Val Step[1000/1563], Avg Loss: 1.1136, Avg Acc@1: 0.7587, Avg Acc@5: 0.9299
2022-01-20 22:24:22,399 Val Step[1050/1563], Avg Loss: 1.1153, Avg Acc@1: 0.7585, Avg Acc@5: 0.9293
2022-01-20 22:24:24,527 Val Step[1100/1563], Avg Loss: 1.1156, Avg Acc@1: 0.7581, Avg Acc@5: 0.9293
2022-01-20 22:24:26,709 Val Step[1150/1563], Avg Loss: 1.1146, Avg Acc@1: 0.7582, Avg Acc@5: 0.9294
2022-01-20 22:24:28,840 Val Step[1200/1563], Avg Loss: 1.1139, Avg Acc@1: 0.7585, Avg Acc@5: 0.9292
2022-01-20 22:24:31,074 Val Step[1250/1563], Avg Loss: 1.1131, Avg Acc@1: 0.7586, Avg Acc@5: 0.9293
2022-01-20 22:24:33,264 Val Step[1300/1563], Avg Loss: 1.1153, Avg Acc@1: 0.7583, Avg Acc@5: 0.9292
2022-01-20 22:24:35,344 Val Step[1350/1563], Avg Loss: 1.1160, Avg Acc@1: 0.7580, Avg Acc@5: 0.9293
2022-01-20 22:24:37,474 Val Step[1400/1563], Avg Loss: 1.1154, Avg Acc@1: 0.7578, Avg Acc@5: 0.9291
2022-01-20 22:24:39,583 Val Step[1450/1563], Avg Loss: 1.1147, Avg Acc@1: 0.7581, Avg Acc@5: 0.9291
2022-01-20 22:24:41,766 Val Step[1500/1563], Avg Loss: 1.1151, Avg Acc@1: 0.7583, Avg Acc@5: 0.9293
2022-01-20 22:24:43,843 Val Step[1550/1563], Avg Loss: 1.1162, Avg Acc@1: 0.7578, Avg Acc@5: 0.9293
2022-01-20 22:24:45,866 ----- Epoch[226/300], Validation Loss: 1.1161, Validation Acc@1: 0.7578, Validation Acc@5: 0.9293, time: 180.05
2022-01-20 22:24:47,078 the pre best model acc:0.7573, at epoch 224
2022-01-20 22:24:47,079 current best model acc:0.7578, at epoch 226
2022-01-20 22:24:47,079 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-20 22:24:47,079 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-20 22:24:47,079 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-20 22:24:47,079 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-20 22:24:47,079 Now training epoch 227. LR=0.000167
2022-01-20 22:27:01,154 Epoch[227/300], Step[0000/1252], Avg Loss: 3.2887, Avg Acc: 0.4229
2022-01-20 22:28:33,623 Epoch[227/300], Step[0050/1252], Avg Loss: 3.1474, Avg Acc: 0.4736
2022-01-20 22:30:05,277 Epoch[227/300], Step[0100/1252], Avg Loss: 3.1219, Avg Acc: 0.4772
2022-01-20 22:31:36,393 Epoch[227/300], Step[0150/1252], Avg Loss: 3.0742, Avg Acc: 0.4801
2022-01-20 22:33:09,574 Epoch[227/300], Step[0200/1252], Avg Loss: 3.0778, Avg Acc: 0.4739
2022-01-20 22:34:42,902 Epoch[227/300], Step[0250/1252], Avg Loss: 3.0875, Avg Acc: 0.4711
2022-01-20 22:36:14,179 Epoch[227/300], Step[0300/1252], Avg Loss: 3.0840, Avg Acc: 0.4748
2022-01-20 22:37:46,628 Epoch[227/300], Step[0350/1252], Avg Loss: 3.0912, Avg Acc: 0.4725
2022-01-20 22:39:19,142 Epoch[227/300], Step[0400/1252], Avg Loss: 3.0908, Avg Acc: 0.4710
2022-01-20 22:40:51,407 Epoch[227/300], Step[0450/1252], Avg Loss: 3.1013, Avg Acc: 0.4675
2022-01-20 22:42:25,054 Epoch[227/300], Step[0500/1252], Avg Loss: 3.1060, Avg Acc: 0.4676
2022-01-20 22:43:58,888 Epoch[227/300], Step[0550/1252], Avg Loss: 3.1018, Avg Acc: 0.4691
2022-01-20 22:45:31,578 Epoch[227/300], Step[0600/1252], Avg Loss: 3.1014, Avg Acc: 0.4668
2022-01-20 22:47:05,711 Epoch[227/300], Step[0650/1252], Avg Loss: 3.1074, Avg Acc: 0.4646
2022-01-20 22:48:39,040 Epoch[227/300], Step[0700/1252], Avg Loss: 3.1090, Avg Acc: 0.4615
2022-01-20 22:50:11,638 Epoch[227/300], Step[0750/1252], Avg Loss: 3.1123, Avg Acc: 0.4599
2022-01-20 22:51:45,041 Epoch[227/300], Step[0800/1252], Avg Loss: 3.1123, Avg Acc: 0.4598
2022-01-20 22:53:18,632 Epoch[227/300], Step[0850/1252], Avg Loss: 3.1107, Avg Acc: 0.4583
2022-01-20 22:54:52,362 Epoch[227/300], Step[0900/1252], Avg Loss: 3.1154, Avg Acc: 0.4580
2022-01-20 22:56:24,654 Epoch[227/300], Step[0950/1252], Avg Loss: 3.1126, Avg Acc: 0.4580
2022-01-20 22:57:58,027 Epoch[227/300], Step[1000/1252], Avg Loss: 3.1139, Avg Acc: 0.4575
2022-01-20 22:59:31,439 Epoch[227/300], Step[1050/1252], Avg Loss: 3.1119, Avg Acc: 0.4564
2022-01-20 23:01:04,086 Epoch[227/300], Step[1100/1252], Avg Loss: 3.1130, Avg Acc: 0.4558
2022-01-20 23:02:34,938 Epoch[227/300], Step[1150/1252], Avg Loss: 3.1132, Avg Acc: 0.4569
2022-01-20 23:04:07,684 Epoch[227/300], Step[1200/1252], Avg Loss: 3.1153, Avg Acc: 0.4569
2022-01-20 23:05:39,594 Epoch[227/300], Step[1250/1252], Avg Loss: 3.1179, Avg Acc: 0.4565
2022-01-20 23:05:45,579 ----- Epoch[227/300], Train Loss: 3.1179, Train Acc: 0.4565, time: 2458.50, Best Val(epoch226) Acc@1: 0.7578
2022-01-20 23:05:45,580 Now training epoch 228. LR=0.000163
2022-01-20 23:07:55,821 Epoch[228/300], Step[0000/1252], Avg Loss: 2.7610, Avg Acc: 0.5732
2022-01-20 23:09:27,599 Epoch[228/300], Step[0050/1252], Avg Loss: 3.0808, Avg Acc: 0.4049
2022-01-20 23:10:59,659 Epoch[228/300], Step[0100/1252], Avg Loss: 3.1029, Avg Acc: 0.4283
2022-01-20 23:12:32,466 Epoch[228/300], Step[0150/1252], Avg Loss: 3.1008, Avg Acc: 0.4391
2022-01-20 23:14:05,856 Epoch[228/300], Step[0200/1252], Avg Loss: 3.1027, Avg Acc: 0.4421
2022-01-20 23:15:38,756 Epoch[228/300], Step[0250/1252], Avg Loss: 3.1009, Avg Acc: 0.4511
2022-01-20 23:17:11,328 Epoch[228/300], Step[0300/1252], Avg Loss: 3.0984, Avg Acc: 0.4531
2022-01-20 23:18:43,830 Epoch[228/300], Step[0350/1252], Avg Loss: 3.1038, Avg Acc: 0.4518
2022-01-20 23:20:16,904 Epoch[228/300], Step[0400/1252], Avg Loss: 3.1034, Avg Acc: 0.4512
2022-01-20 23:21:48,338 Epoch[228/300], Step[0450/1252], Avg Loss: 3.1068, Avg Acc: 0.4510
2022-01-20 23:23:22,112 Epoch[228/300], Step[0500/1252], Avg Loss: 3.1058, Avg Acc: 0.4501
2022-01-20 23:24:56,400 Epoch[228/300], Step[0550/1252], Avg Loss: 3.1075, Avg Acc: 0.4511
2022-01-20 23:26:30,350 Epoch[228/300], Step[0600/1252], Avg Loss: 3.1052, Avg Acc: 0.4515
2022-01-20 23:28:03,633 Epoch[228/300], Step[0650/1252], Avg Loss: 3.1120, Avg Acc: 0.4510
2022-01-20 23:29:36,490 Epoch[228/300], Step[0700/1252], Avg Loss: 3.1088, Avg Acc: 0.4529
2022-01-20 23:31:09,827 Epoch[228/300], Step[0750/1252], Avg Loss: 3.1083, Avg Acc: 0.4520
2022-01-20 23:32:43,631 Epoch[228/300], Step[0800/1252], Avg Loss: 3.1101, Avg Acc: 0.4514
2022-01-20 23:34:16,316 Epoch[228/300], Step[0850/1252], Avg Loss: 3.1103, Avg Acc: 0.4505
2022-01-20 23:35:48,802 Epoch[228/300], Step[0900/1252], Avg Loss: 3.1122, Avg Acc: 0.4488
2022-01-20 23:37:21,088 Epoch[228/300], Step[0950/1252], Avg Loss: 3.1169, Avg Acc: 0.4473
2022-01-20 23:38:54,174 Epoch[228/300], Step[1000/1252], Avg Loss: 3.1198, Avg Acc: 0.4471
2022-01-20 23:40:26,591 Epoch[228/300], Step[1050/1252], Avg Loss: 3.1215, Avg Acc: 0.4463
2022-01-20 23:41:59,513 Epoch[228/300], Step[1100/1252], Avg Loss: 3.1211, Avg Acc: 0.4466
2022-01-20 23:43:34,177 Epoch[228/300], Step[1150/1252], Avg Loss: 3.1203, Avg Acc: 0.4460
2022-01-20 23:45:08,202 Epoch[228/300], Step[1200/1252], Avg Loss: 3.1216, Avg Acc: 0.4467
2022-01-20 23:46:40,514 Epoch[228/300], Step[1250/1252], Avg Loss: 3.1219, Avg Acc: 0.4473
2022-01-20 23:46:46,557 ----- Epoch[228/300], Train Loss: 3.1219, Train Acc: 0.4472, time: 2460.97, Best Val(epoch226) Acc@1: 0.7578
2022-01-20 23:46:46,557 ----- Validation after Epoch: 228
2022-01-20 23:48:20,553 Val Step[0000/1563], Avg Loss: 0.9051, Avg Acc@1: 0.8125, Avg Acc@5: 0.9688
2022-01-20 23:48:22,893 Val Step[0050/1563], Avg Loss: 1.0825, Avg Acc@1: 0.7623, Avg Acc@5: 0.9283
2022-01-20 23:48:25,140 Val Step[0100/1563], Avg Loss: 1.0950, Avg Acc@1: 0.7614, Avg Acc@5: 0.9276
2022-01-20 23:48:27,228 Val Step[0150/1563], Avg Loss: 1.0974, Avg Acc@1: 0.7620, Avg Acc@5: 0.9276
2022-01-20 23:48:29,427 Val Step[0200/1563], Avg Loss: 1.0902, Avg Acc@1: 0.7648, Avg Acc@5: 0.9291
2022-01-20 23:48:31,589 Val Step[0250/1563], Avg Loss: 1.0816, Avg Acc@1: 0.7646, Avg Acc@5: 0.9305
2022-01-20 23:48:33,690 Val Step[0300/1563], Avg Loss: 1.0845, Avg Acc@1: 0.7638, Avg Acc@5: 0.9295
2022-01-20 23:48:35,900 Val Step[0350/1563], Avg Loss: 1.0869, Avg Acc@1: 0.7639, Avg Acc@5: 0.9291
2022-01-20 23:48:38,119 Val Step[0400/1563], Avg Loss: 1.0849, Avg Acc@1: 0.7646, Avg Acc@5: 0.9292
2022-01-20 23:48:40,210 Val Step[0450/1563], Avg Loss: 1.0900, Avg Acc@1: 0.7627, Avg Acc@5: 0.9293
2022-01-20 23:48:42,421 Val Step[0500/1563], Avg Loss: 1.0937, Avg Acc@1: 0.7614, Avg Acc@5: 0.9295
2022-01-20 23:48:44,529 Val Step[0550/1563], Avg Loss: 1.0934, Avg Acc@1: 0.7610, Avg Acc@5: 0.9299
2022-01-20 23:48:46,705 Val Step[0600/1563], Avg Loss: 1.0930, Avg Acc@1: 0.7608, Avg Acc@5: 0.9300
2022-01-20 23:48:49,077 Val Step[0650/1563], Avg Loss: 1.0929, Avg Acc@1: 0.7607, Avg Acc@5: 0.9303
2022-01-20 23:48:51,398 Val Step[0700/1563], Avg Loss: 1.0900, Avg Acc@1: 0.7615, Avg Acc@5: 0.9311
2022-01-20 23:48:53,675 Val Step[0750/1563], Avg Loss: 1.0963, Avg Acc@1: 0.7596, Avg Acc@5: 0.9305
2022-01-20 23:48:55,956 Val Step[0800/1563], Avg Loss: 1.0962, Avg Acc@1: 0.7603, Avg Acc@5: 0.9302
2022-01-20 23:48:58,162 Val Step[0850/1563], Avg Loss: 1.0979, Avg Acc@1: 0.7596, Avg Acc@5: 0.9298
2022-01-20 23:49:00,425 Val Step[0900/1563], Avg Loss: 1.0946, Avg Acc@1: 0.7600, Avg Acc@5: 0.9301
2022-01-20 23:49:02,591 Val Step[0950/1563], Avg Loss: 1.0944, Avg Acc@1: 0.7604, Avg Acc@5: 0.9304
2022-01-20 23:49:04,729 Val Step[1000/1563], Avg Loss: 1.0964, Avg Acc@1: 0.7597, Avg Acc@5: 0.9299
2022-01-20 23:49:06,930 Val Step[1050/1563], Avg Loss: 1.0980, Avg Acc@1: 0.7591, Avg Acc@5: 0.9299
2022-01-20 23:49:08,993 Val Step[1100/1563], Avg Loss: 1.0992, Avg Acc@1: 0.7589, Avg Acc@5: 0.9298
2022-01-20 23:49:11,131 Val Step[1150/1563], Avg Loss: 1.0980, Avg Acc@1: 0.7590, Avg Acc@5: 0.9299
2022-01-20 23:49:13,245 Val Step[1200/1563], Avg Loss: 1.0964, Avg Acc@1: 0.7596, Avg Acc@5: 0.9301
2022-01-20 23:49:15,371 Val Step[1250/1563], Avg Loss: 1.0954, Avg Acc@1: 0.7596, Avg Acc@5: 0.9306
2022-01-20 23:49:17,599 Val Step[1300/1563], Avg Loss: 1.0976, Avg Acc@1: 0.7593, Avg Acc@5: 0.9304
2022-01-20 23:49:19,767 Val Step[1350/1563], Avg Loss: 1.0990, Avg Acc@1: 0.7589, Avg Acc@5: 0.9302
2022-01-20 23:49:21,891 Val Step[1400/1563], Avg Loss: 1.0980, Avg Acc@1: 0.7587, Avg Acc@5: 0.9301
2022-01-20 23:49:24,206 Val Step[1450/1563], Avg Loss: 1.0974, Avg Acc@1: 0.7589, Avg Acc@5: 0.9301
2022-01-20 23:49:26,404 Val Step[1500/1563], Avg Loss: 1.0976, Avg Acc@1: 0.7592, Avg Acc@5: 0.9303
2022-01-20 23:49:28,474 Val Step[1550/1563], Avg Loss: 1.0984, Avg Acc@1: 0.7589, Avg Acc@5: 0.9301
2022-01-20 23:49:30,426 ----- Epoch[228/300], Validation Loss: 1.0982, Validation Acc@1: 0.7590, Validation Acc@5: 0.9302, time: 163.87
2022-01-20 23:49:31,645 the pre best model acc:0.7578, at epoch 226
2022-01-20 23:49:31,645 current best model acc:0.7590, at epoch 228
2022-01-20 23:49:31,645 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-20 23:49:31,645 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-20 23:49:31,646 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-20 23:49:31,646 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-20 23:49:31,646 Now training epoch 229. LR=0.000159
2022-01-20 23:51:33,823 Epoch[229/300], Step[0000/1252], Avg Loss: 3.1371, Avg Acc: 0.3457
2022-01-20 23:53:05,362 Epoch[229/300], Step[0050/1252], Avg Loss: 3.1285, Avg Acc: 0.4705
2022-01-20 23:54:38,791 Epoch[229/300], Step[0100/1252], Avg Loss: 3.0797, Avg Acc: 0.4622
2022-01-20 23:56:12,760 Epoch[229/300], Step[0150/1252], Avg Loss: 3.0698, Avg Acc: 0.4634
2022-01-20 23:57:44,700 Epoch[229/300], Step[0200/1252], Avg Loss: 3.0832, Avg Acc: 0.4693
2022-01-20 23:59:18,616 Epoch[229/300], Step[0250/1252], Avg Loss: 3.0922, Avg Acc: 0.4620
2022-01-21 00:00:51,107 Epoch[229/300], Step[0300/1252], Avg Loss: 3.0918, Avg Acc: 0.4630
2022-01-21 00:02:24,837 Epoch[229/300], Step[0350/1252], Avg Loss: 3.0861, Avg Acc: 0.4626
2022-01-21 00:03:57,991 Epoch[229/300], Step[0400/1252], Avg Loss: 3.0911, Avg Acc: 0.4616
2022-01-21 00:05:31,165 Epoch[229/300], Step[0450/1252], Avg Loss: 3.0909, Avg Acc: 0.4597
2022-01-21 00:07:03,030 Epoch[229/300], Step[0500/1252], Avg Loss: 3.0979, Avg Acc: 0.4617
2022-01-21 00:08:36,708 Epoch[229/300], Step[0550/1252], Avg Loss: 3.0972, Avg Acc: 0.4617
2022-01-21 00:10:10,601 Epoch[229/300], Step[0600/1252], Avg Loss: 3.0978, Avg Acc: 0.4601
2022-01-21 00:11:43,685 Epoch[229/300], Step[0650/1252], Avg Loss: 3.1003, Avg Acc: 0.4599
2022-01-21 00:13:17,094 Epoch[229/300], Step[0700/1252], Avg Loss: 3.1021, Avg Acc: 0.4593
2022-01-21 00:14:49,326 Epoch[229/300], Step[0750/1252], Avg Loss: 3.1043, Avg Acc: 0.4589
2022-01-21 00:16:23,400 Epoch[229/300], Step[0800/1252], Avg Loss: 3.1058, Avg Acc: 0.4567
2022-01-21 00:17:56,900 Epoch[229/300], Step[0850/1252], Avg Loss: 3.1023, Avg Acc: 0.4550
2022-01-21 00:19:30,955 Epoch[229/300], Step[0900/1252], Avg Loss: 3.1009, Avg Acc: 0.4547
2022-01-21 00:21:04,693 Epoch[229/300], Step[0950/1252], Avg Loss: 3.1009, Avg Acc: 0.4544
2022-01-21 00:22:38,944 Epoch[229/300], Step[1000/1252], Avg Loss: 3.1025, Avg Acc: 0.4528
2022-01-21 00:24:13,282 Epoch[229/300], Step[1050/1252], Avg Loss: 3.1034, Avg Acc: 0.4534
2022-01-21 00:25:46,597 Epoch[229/300], Step[1100/1252], Avg Loss: 3.1081, Avg Acc: 0.4528
2022-01-21 00:27:20,296 Epoch[229/300], Step[1150/1252], Avg Loss: 3.1072, Avg Acc: 0.4528
2022-01-21 00:28:52,970 Epoch[229/300], Step[1200/1252], Avg Loss: 3.1090, Avg Acc: 0.4529
2022-01-21 00:30:25,878 Epoch[229/300], Step[1250/1252], Avg Loss: 3.1100, Avg Acc: 0.4525
2022-01-21 00:30:32,304 ----- Epoch[229/300], Train Loss: 3.1100, Train Acc: 0.4525, time: 2460.65, Best Val(epoch228) Acc@1: 0.7590
2022-01-21 00:30:32,305 Now training epoch 230. LR=0.000155
2022-01-21 00:32:37,754 Epoch[230/300], Step[0000/1252], Avg Loss: 3.1281, Avg Acc: 0.3682
2022-01-21 00:34:09,874 Epoch[230/300], Step[0050/1252], Avg Loss: 3.1035, Avg Acc: 0.4548
2022-01-21 00:35:43,448 Epoch[230/300], Step[0100/1252], Avg Loss: 3.1222, Avg Acc: 0.4461
2022-01-21 00:37:16,000 Epoch[230/300], Step[0150/1252], Avg Loss: 3.1183, Avg Acc: 0.4510
2022-01-21 00:38:49,544 Epoch[230/300], Step[0200/1252], Avg Loss: 3.1187, Avg Acc: 0.4587
2022-01-21 00:40:22,161 Epoch[230/300], Step[0250/1252], Avg Loss: 3.1047, Avg Acc: 0.4615
2022-01-21 00:41:53,661 Epoch[230/300], Step[0300/1252], Avg Loss: 3.1014, Avg Acc: 0.4610
2022-01-21 00:43:26,837 Epoch[230/300], Step[0350/1252], Avg Loss: 3.1066, Avg Acc: 0.4565
2022-01-21 00:44:59,725 Epoch[230/300], Step[0400/1252], Avg Loss: 3.1120, Avg Acc: 0.4553
2022-01-21 00:46:34,127 Epoch[230/300], Step[0450/1252], Avg Loss: 3.1084, Avg Acc: 0.4543
2022-01-21 00:48:07,917 Epoch[230/300], Step[0500/1252], Avg Loss: 3.1141, Avg Acc: 0.4530
2022-01-21 00:49:39,963 Epoch[230/300], Step[0550/1252], Avg Loss: 3.1157, Avg Acc: 0.4538
2022-01-21 00:51:13,078 Epoch[230/300], Step[0600/1252], Avg Loss: 3.1147, Avg Acc: 0.4541
2022-01-21 00:52:47,614 Epoch[230/300], Step[0650/1252], Avg Loss: 3.1114, Avg Acc: 0.4521
2022-01-21 00:54:19,822 Epoch[230/300], Step[0700/1252], Avg Loss: 3.1066, Avg Acc: 0.4515
2022-01-21 00:55:52,778 Epoch[230/300], Step[0750/1252], Avg Loss: 3.1080, Avg Acc: 0.4486
2022-01-21 00:57:25,487 Epoch[230/300], Step[0800/1252], Avg Loss: 3.1070, Avg Acc: 0.4492
2022-01-21 00:58:58,986 Epoch[230/300], Step[0850/1252], Avg Loss: 3.1083, Avg Acc: 0.4501
2022-01-21 01:00:32,069 Epoch[230/300], Step[0900/1252], Avg Loss: 3.1072, Avg Acc: 0.4505
2022-01-21 01:02:05,534 Epoch[230/300], Step[0950/1252], Avg Loss: 3.1103, Avg Acc: 0.4485
2022-01-21 01:03:38,311 Epoch[230/300], Step[1000/1252], Avg Loss: 3.1092, Avg Acc: 0.4493
2022-01-21 01:05:11,796 Epoch[230/300], Step[1050/1252], Avg Loss: 3.1100, Avg Acc: 0.4485
2022-01-21 01:06:44,236 Epoch[230/300], Step[1100/1252], Avg Loss: 3.1080, Avg Acc: 0.4492
2022-01-21 01:08:16,561 Epoch[230/300], Step[1150/1252], Avg Loss: 3.1070, Avg Acc: 0.4509
2022-01-21 01:09:49,819 Epoch[230/300], Step[1200/1252], Avg Loss: 3.1085, Avg Acc: 0.4503
2022-01-21 01:11:21,538 Epoch[230/300], Step[1250/1252], Avg Loss: 3.1079, Avg Acc: 0.4505
2022-01-21 01:11:28,133 ----- Epoch[230/300], Train Loss: 3.1080, Train Acc: 0.4505, time: 2455.82, Best Val(epoch228) Acc@1: 0.7590
2022-01-21 01:11:28,133 ----- Validation after Epoch: 230
2022-01-21 01:13:11,388 Val Step[0000/1563], Avg Loss: 0.8886, Avg Acc@1: 0.7812, Avg Acc@5: 0.9688
2022-01-21 01:13:13,654 Val Step[0050/1563], Avg Loss: 1.0648, Avg Acc@1: 0.7635, Avg Acc@5: 0.9332
2022-01-21 01:13:15,791 Val Step[0100/1563], Avg Loss: 1.0908, Avg Acc@1: 0.7602, Avg Acc@5: 0.9360
2022-01-21 01:13:17,891 Val Step[0150/1563], Avg Loss: 1.0882, Avg Acc@1: 0.7618, Avg Acc@5: 0.9340
2022-01-21 01:13:20,053 Val Step[0200/1563], Avg Loss: 1.0847, Avg Acc@1: 0.7649, Avg Acc@5: 0.9324
2022-01-21 01:13:22,256 Val Step[0250/1563], Avg Loss: 1.0752, Avg Acc@1: 0.7659, Avg Acc@5: 0.9333
2022-01-21 01:13:24,344 Val Step[0300/1563], Avg Loss: 1.0750, Avg Acc@1: 0.7662, Avg Acc@5: 0.9326
2022-01-21 01:13:26,396 Val Step[0350/1563], Avg Loss: 1.0811, Avg Acc@1: 0.7650, Avg Acc@5: 0.9321
2022-01-21 01:13:28,508 Val Step[0400/1563], Avg Loss: 1.0824, Avg Acc@1: 0.7645, Avg Acc@5: 0.9309
2022-01-21 01:13:30,802 Val Step[0450/1563], Avg Loss: 1.0867, Avg Acc@1: 0.7618, Avg Acc@5: 0.9309
2022-01-21 01:13:33,010 Val Step[0500/1563], Avg Loss: 1.0901, Avg Acc@1: 0.7606, Avg Acc@5: 0.9306
2022-01-21 01:13:35,278 Val Step[0550/1563], Avg Loss: 1.0898, Avg Acc@1: 0.7596, Avg Acc@5: 0.9311
2022-01-21 01:13:37,466 Val Step[0600/1563], Avg Loss: 1.0895, Avg Acc@1: 0.7595, Avg Acc@5: 0.9309
2022-01-21 01:13:39,599 Val Step[0650/1563], Avg Loss: 1.0896, Avg Acc@1: 0.7598, Avg Acc@5: 0.9312
2022-01-21 01:13:41,796 Val Step[0700/1563], Avg Loss: 1.0884, Avg Acc@1: 0.7604, Avg Acc@5: 0.9314
2022-01-21 01:13:43,959 Val Step[0750/1563], Avg Loss: 1.0942, Avg Acc@1: 0.7591, Avg Acc@5: 0.9309
2022-01-21 01:13:46,049 Val Step[0800/1563], Avg Loss: 1.0936, Avg Acc@1: 0.7600, Avg Acc@5: 0.9308
2022-01-21 01:13:48,187 Val Step[0850/1563], Avg Loss: 1.0957, Avg Acc@1: 0.7595, Avg Acc@5: 0.9301
2022-01-21 01:13:50,426 Val Step[0900/1563], Avg Loss: 1.0921, Avg Acc@1: 0.7599, Avg Acc@5: 0.9302
2022-01-21 01:13:52,620 Val Step[0950/1563], Avg Loss: 1.0918, Avg Acc@1: 0.7606, Avg Acc@5: 0.9303
2022-01-21 01:13:54,710 Val Step[1000/1563], Avg Loss: 1.0927, Avg Acc@1: 0.7605, Avg Acc@5: 0.9302
2022-01-21 01:13:56,843 Val Step[1050/1563], Avg Loss: 1.0945, Avg Acc@1: 0.7598, Avg Acc@5: 0.9301
2022-01-21 01:13:58,950 Val Step[1100/1563], Avg Loss: 1.0952, Avg Acc@1: 0.7597, Avg Acc@5: 0.9300
2022-01-21 01:14:01,124 Val Step[1150/1563], Avg Loss: 1.0933, Avg Acc@1: 0.7599, Avg Acc@5: 0.9303
2022-01-21 01:14:03,267 Val Step[1200/1563], Avg Loss: 1.0923, Avg Acc@1: 0.7601, Avg Acc@5: 0.9304
2022-01-21 01:14:05,379 Val Step[1250/1563], Avg Loss: 1.0914, Avg Acc@1: 0.7602, Avg Acc@5: 0.9307
2022-01-21 01:14:07,362 Val Step[1300/1563], Avg Loss: 1.0936, Avg Acc@1: 0.7597, Avg Acc@5: 0.9304
2022-01-21 01:14:09,463 Val Step[1350/1563], Avg Loss: 1.0953, Avg Acc@1: 0.7590, Avg Acc@5: 0.9301
2022-01-21 01:14:11,517 Val Step[1400/1563], Avg Loss: 1.0946, Avg Acc@1: 0.7591, Avg Acc@5: 0.9300
2022-01-21 01:14:13,623 Val Step[1450/1563], Avg Loss: 1.0936, Avg Acc@1: 0.7593, Avg Acc@5: 0.9302
2022-01-21 01:14:15,704 Val Step[1500/1563], Avg Loss: 1.0933, Avg Acc@1: 0.7596, Avg Acc@5: 0.9305
2022-01-21 01:14:17,717 Val Step[1550/1563], Avg Loss: 1.0940, Avg Acc@1: 0.7595, Avg Acc@5: 0.9303
2022-01-21 01:14:19,592 ----- Epoch[230/300], Validation Loss: 1.0939, Validation Acc@1: 0.7595, Validation Acc@5: 0.9304, time: 171.45
2022-01-21 01:14:20,815 the pre best model acc:0.7590, at epoch 228
2022-01-21 01:14:20,816 current best model acc:0.7595, at epoch 230
2022-01-21 01:14:20,816 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 01:14:20,816 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 01:14:20,816 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 01:14:20,816 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 01:14:20,816 Now training epoch 231. LR=0.000151
2022-01-21 01:16:28,244 Epoch[231/300], Step[0000/1252], Avg Loss: 2.9719, Avg Acc: 0.6484
2022-01-21 01:18:00,659 Epoch[231/300], Step[0050/1252], Avg Loss: 3.0871, Avg Acc: 0.4396
2022-01-21 01:19:32,582 Epoch[231/300], Step[0100/1252], Avg Loss: 3.0781, Avg Acc: 0.4697
2022-01-21 01:21:05,236 Epoch[231/300], Step[0150/1252], Avg Loss: 3.0775, Avg Acc: 0.4650
2022-01-21 01:22:38,625 Epoch[231/300], Step[0200/1252], Avg Loss: 3.1104, Avg Acc: 0.4605
2022-01-21 01:24:12,162 Epoch[231/300], Step[0250/1252], Avg Loss: 3.1082, Avg Acc: 0.4657
2022-01-21 01:25:45,161 Epoch[231/300], Step[0300/1252], Avg Loss: 3.1135, Avg Acc: 0.4683
2022-01-21 01:27:17,011 Epoch[231/300], Step[0350/1252], Avg Loss: 3.1075, Avg Acc: 0.4678
2022-01-21 01:28:50,882 Epoch[231/300], Step[0400/1252], Avg Loss: 3.1096, Avg Acc: 0.4698
2022-01-21 01:30:24,224 Epoch[231/300], Step[0450/1252], Avg Loss: 3.1074, Avg Acc: 0.4679
2022-01-21 01:31:56,055 Epoch[231/300], Step[0500/1252], Avg Loss: 3.0993, Avg Acc: 0.4663
2022-01-21 01:33:28,583 Epoch[231/300], Step[0550/1252], Avg Loss: 3.0977, Avg Acc: 0.4672
2022-01-21 01:35:01,812 Epoch[231/300], Step[0600/1252], Avg Loss: 3.0939, Avg Acc: 0.4660
2022-01-21 01:36:35,250 Epoch[231/300], Step[0650/1252], Avg Loss: 3.0966, Avg Acc: 0.4644
2022-01-21 01:38:08,648 Epoch[231/300], Step[0700/1252], Avg Loss: 3.0956, Avg Acc: 0.4634
2022-01-21 01:39:42,246 Epoch[231/300], Step[0750/1252], Avg Loss: 3.0989, Avg Acc: 0.4635
2022-01-21 01:41:17,881 Epoch[231/300], Step[0800/1252], Avg Loss: 3.0993, Avg Acc: 0.4641
2022-01-21 01:42:51,208 Epoch[231/300], Step[0850/1252], Avg Loss: 3.0971, Avg Acc: 0.4647
2022-01-21 01:44:23,978 Epoch[231/300], Step[0900/1252], Avg Loss: 3.0962, Avg Acc: 0.4633
2022-01-21 01:45:54,372 Epoch[231/300], Step[0950/1252], Avg Loss: 3.0955, Avg Acc: 0.4639
2022-01-21 01:47:27,526 Epoch[231/300], Step[1000/1252], Avg Loss: 3.0956, Avg Acc: 0.4625
2022-01-21 01:49:00,351 Epoch[231/300], Step[1050/1252], Avg Loss: 3.0971, Avg Acc: 0.4624
2022-01-21 01:50:34,310 Epoch[231/300], Step[1100/1252], Avg Loss: 3.0984, Avg Acc: 0.4618
2022-01-21 01:52:07,308 Epoch[231/300], Step[1150/1252], Avg Loss: 3.1004, Avg Acc: 0.4610
2022-01-21 01:53:40,145 Epoch[231/300], Step[1200/1252], Avg Loss: 3.0979, Avg Acc: 0.4606
2022-01-21 01:55:12,756 Epoch[231/300], Step[1250/1252], Avg Loss: 3.0967, Avg Acc: 0.4594
2022-01-21 01:55:19,199 ----- Epoch[231/300], Train Loss: 3.0968, Train Acc: 0.4594, time: 2458.38, Best Val(epoch230) Acc@1: 0.7595
2022-01-21 01:55:19,199 Now training epoch 232. LR=0.000147
2022-01-21 01:57:27,177 Epoch[232/300], Step[0000/1252], Avg Loss: 2.6980, Avg Acc: 0.6729
2022-01-21 01:58:58,201 Epoch[232/300], Step[0050/1252], Avg Loss: 3.0371, Avg Acc: 0.4659
2022-01-21 02:00:30,723 Epoch[232/300], Step[0100/1252], Avg Loss: 3.0723, Avg Acc: 0.4788
2022-01-21 02:02:04,066 Epoch[232/300], Step[0150/1252], Avg Loss: 3.0681, Avg Acc: 0.4667
2022-01-21 02:03:37,135 Epoch[232/300], Step[0200/1252], Avg Loss: 3.0731, Avg Acc: 0.4663
2022-01-21 02:05:09,419 Epoch[232/300], Step[0250/1252], Avg Loss: 3.0861, Avg Acc: 0.4653
2022-01-21 02:06:40,714 Epoch[232/300], Step[0300/1252], Avg Loss: 3.0889, Avg Acc: 0.4683
2022-01-21 02:08:14,612 Epoch[232/300], Step[0350/1252], Avg Loss: 3.0895, Avg Acc: 0.4659
2022-01-21 02:09:46,579 Epoch[232/300], Step[0400/1252], Avg Loss: 3.1013, Avg Acc: 0.4626
2022-01-21 02:11:18,691 Epoch[232/300], Step[0450/1252], Avg Loss: 3.1036, Avg Acc: 0.4625
2022-01-21 02:12:52,244 Epoch[232/300], Step[0500/1252], Avg Loss: 3.1008, Avg Acc: 0.4620
2022-01-21 02:14:25,803 Epoch[232/300], Step[0550/1252], Avg Loss: 3.1062, Avg Acc: 0.4616
2022-01-21 02:15:58,482 Epoch[232/300], Step[0600/1252], Avg Loss: 3.1043, Avg Acc: 0.4595
2022-01-21 02:17:31,505 Epoch[232/300], Step[0650/1252], Avg Loss: 3.1052, Avg Acc: 0.4598
2022-01-21 02:19:04,623 Epoch[232/300], Step[0700/1252], Avg Loss: 3.0983, Avg Acc: 0.4596
2022-01-21 02:20:36,549 Epoch[232/300], Step[0750/1252], Avg Loss: 3.0986, Avg Acc: 0.4588
2022-01-21 02:22:09,255 Epoch[232/300], Step[0800/1252], Avg Loss: 3.0955, Avg Acc: 0.4586
2022-01-21 02:23:43,422 Epoch[232/300], Step[0850/1252], Avg Loss: 3.0982, Avg Acc: 0.4578
2022-01-21 02:25:16,185 Epoch[232/300], Step[0900/1252], Avg Loss: 3.1001, Avg Acc: 0.4570
2022-01-21 02:26:49,704 Epoch[232/300], Step[0950/1252], Avg Loss: 3.0992, Avg Acc: 0.4571
2022-01-21 02:28:21,273 Epoch[232/300], Step[1000/1252], Avg Loss: 3.1004, Avg Acc: 0.4556
2022-01-21 02:29:53,711 Epoch[232/300], Step[1050/1252], Avg Loss: 3.1009, Avg Acc: 0.4557
2022-01-21 02:31:27,519 Epoch[232/300], Step[1100/1252], Avg Loss: 3.1006, Avg Acc: 0.4569
2022-01-21 02:32:59,885 Epoch[232/300], Step[1150/1252], Avg Loss: 3.0983, Avg Acc: 0.4574
2022-01-21 02:34:32,602 Epoch[232/300], Step[1200/1252], Avg Loss: 3.0978, Avg Acc: 0.4575
2022-01-21 02:36:05,935 Epoch[232/300], Step[1250/1252], Avg Loss: 3.1007, Avg Acc: 0.4571
2022-01-21 02:36:12,302 ----- Epoch[232/300], Train Loss: 3.1007, Train Acc: 0.4572, time: 2453.10, Best Val(epoch230) Acc@1: 0.7595
2022-01-21 02:36:12,302 ----- Validation after Epoch: 232
2022-01-21 02:37:51,796 Val Step[0000/1563], Avg Loss: 0.8830, Avg Acc@1: 0.8438, Avg Acc@5: 1.0000
2022-01-21 02:37:54,200 Val Step[0050/1563], Avg Loss: 1.0733, Avg Acc@1: 0.7653, Avg Acc@5: 0.9308
2022-01-21 02:37:56,583 Val Step[0100/1563], Avg Loss: 1.0790, Avg Acc@1: 0.7676, Avg Acc@5: 0.9325
2022-01-21 02:37:58,712 Val Step[0150/1563], Avg Loss: 1.0827, Avg Acc@1: 0.7666, Avg Acc@5: 0.9288
2022-01-21 02:38:00,903 Val Step[0200/1563], Avg Loss: 1.0846, Avg Acc@1: 0.7665, Avg Acc@5: 0.9302
2022-01-21 02:38:03,058 Val Step[0250/1563], Avg Loss: 1.0722, Avg Acc@1: 0.7682, Avg Acc@5: 0.9325
2022-01-21 02:38:05,200 Val Step[0300/1563], Avg Loss: 1.0739, Avg Acc@1: 0.7673, Avg Acc@5: 0.9310
2022-01-21 02:38:07,197 Val Step[0350/1563], Avg Loss: 1.0805, Avg Acc@1: 0.7667, Avg Acc@5: 0.9306
2022-01-21 02:38:09,290 Val Step[0400/1563], Avg Loss: 1.0815, Avg Acc@1: 0.7649, Avg Acc@5: 0.9301
2022-01-21 02:38:11,447 Val Step[0450/1563], Avg Loss: 1.0835, Avg Acc@1: 0.7629, Avg Acc@5: 0.9302
2022-01-21 02:38:13,550 Val Step[0500/1563], Avg Loss: 1.0884, Avg Acc@1: 0.7611, Avg Acc@5: 0.9301
2022-01-21 02:38:15,700 Val Step[0550/1563], Avg Loss: 1.0885, Avg Acc@1: 0.7603, Avg Acc@5: 0.9303
2022-01-21 02:38:17,794 Val Step[0600/1563], Avg Loss: 1.0882, Avg Acc@1: 0.7602, Avg Acc@5: 0.9303
2022-01-21 02:38:19,903 Val Step[0650/1563], Avg Loss: 1.0887, Avg Acc@1: 0.7603, Avg Acc@5: 0.9304
2022-01-21 02:38:22,045 Val Step[0700/1563], Avg Loss: 1.0854, Avg Acc@1: 0.7612, Avg Acc@5: 0.9312
2022-01-21 02:38:24,206 Val Step[0750/1563], Avg Loss: 1.0913, Avg Acc@1: 0.7594, Avg Acc@5: 0.9306
2022-01-21 02:38:26,297 Val Step[0800/1563], Avg Loss: 1.0919, Avg Acc@1: 0.7597, Avg Acc@5: 0.9304
2022-01-21 02:38:28,371 Val Step[0850/1563], Avg Loss: 1.0929, Avg Acc@1: 0.7594, Avg Acc@5: 0.9303
2022-01-21 02:38:30,441 Val Step[0900/1563], Avg Loss: 1.0894, Avg Acc@1: 0.7601, Avg Acc@5: 0.9308
2022-01-21 02:38:32,404 Val Step[0950/1563], Avg Loss: 1.0891, Avg Acc@1: 0.7606, Avg Acc@5: 0.9305
2022-01-21 02:38:34,403 Val Step[1000/1563], Avg Loss: 1.0916, Avg Acc@1: 0.7603, Avg Acc@5: 0.9303
2022-01-21 02:38:36,380 Val Step[1050/1563], Avg Loss: 1.0930, Avg Acc@1: 0.7594, Avg Acc@5: 0.9300
2022-01-21 02:38:38,338 Val Step[1100/1563], Avg Loss: 1.0931, Avg Acc@1: 0.7592, Avg Acc@5: 0.9300
2022-01-21 02:38:40,254 Val Step[1150/1563], Avg Loss: 1.0909, Avg Acc@1: 0.7596, Avg Acc@5: 0.9303
2022-01-21 02:38:42,246 Val Step[1200/1563], Avg Loss: 1.0899, Avg Acc@1: 0.7599, Avg Acc@5: 0.9301
2022-01-21 02:38:44,514 Val Step[1250/1563], Avg Loss: 1.0892, Avg Acc@1: 0.7596, Avg Acc@5: 0.9305
2022-01-21 02:38:46,864 Val Step[1300/1563], Avg Loss: 1.0915, Avg Acc@1: 0.7599, Avg Acc@5: 0.9303
2022-01-21 02:38:49,262 Val Step[1350/1563], Avg Loss: 1.0925, Avg Acc@1: 0.7592, Avg Acc@5: 0.9302
2022-01-21 02:38:51,619 Val Step[1400/1563], Avg Loss: 1.0920, Avg Acc@1: 0.7591, Avg Acc@5: 0.9301
2022-01-21 02:38:54,027 Val Step[1450/1563], Avg Loss: 1.0911, Avg Acc@1: 0.7596, Avg Acc@5: 0.9302
2022-01-21 02:38:56,437 Val Step[1500/1563], Avg Loss: 1.0911, Avg Acc@1: 0.7596, Avg Acc@5: 0.9306
2022-01-21 02:38:58,681 Val Step[1550/1563], Avg Loss: 1.0919, Avg Acc@1: 0.7595, Avg Acc@5: 0.9304
2022-01-21 02:39:00,874 ----- Epoch[232/300], Validation Loss: 1.0915, Validation Acc@1: 0.7596, Validation Acc@5: 0.9305, time: 168.57
2022-01-21 02:39:02,139 the pre best model acc:0.7595, at epoch 230
2022-01-21 02:39:02,140 current best model acc:0.7596, at epoch 232
2022-01-21 02:39:02,140 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 02:39:02,140 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 02:39:02,140 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 02:39:02,140 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 02:39:02,140 Now training epoch 233. LR=0.000143
2022-01-21 02:41:02,866 Epoch[233/300], Step[0000/1252], Avg Loss: 2.7015, Avg Acc: 0.6064
2022-01-21 02:42:33,979 Epoch[233/300], Step[0050/1252], Avg Loss: 3.0457, Avg Acc: 0.4480
2022-01-21 02:44:05,547 Epoch[233/300], Step[0100/1252], Avg Loss: 3.0808, Avg Acc: 0.4465
2022-01-21 02:45:38,896 Epoch[233/300], Step[0150/1252], Avg Loss: 3.0668, Avg Acc: 0.4506
2022-01-21 02:47:11,550 Epoch[233/300], Step[0200/1252], Avg Loss: 3.0804, Avg Acc: 0.4515
2022-01-21 02:48:45,248 Epoch[233/300], Step[0250/1252], Avg Loss: 3.0812, Avg Acc: 0.4490
2022-01-21 02:50:18,131 Epoch[233/300], Step[0300/1252], Avg Loss: 3.0912, Avg Acc: 0.4502
2022-01-21 02:51:51,913 Epoch[233/300], Step[0350/1252], Avg Loss: 3.0865, Avg Acc: 0.4493
2022-01-21 02:53:24,783 Epoch[233/300], Step[0400/1252], Avg Loss: 3.0798, Avg Acc: 0.4506
2022-01-21 02:54:57,962 Epoch[233/300], Step[0450/1252], Avg Loss: 3.0817, Avg Acc: 0.4503
2022-01-21 02:56:31,450 Epoch[233/300], Step[0500/1252], Avg Loss: 3.0870, Avg Acc: 0.4531
2022-01-21 02:58:03,478 Epoch[233/300], Step[0550/1252], Avg Loss: 3.0882, Avg Acc: 0.4558
2022-01-21 02:59:38,809 Epoch[233/300], Step[0600/1252], Avg Loss: 3.0856, Avg Acc: 0.4536
2022-01-21 03:01:11,858 Epoch[233/300], Step[0650/1252], Avg Loss: 3.0885, Avg Acc: 0.4522
2022-01-21 03:02:45,169 Epoch[233/300], Step[0700/1252], Avg Loss: 3.0867, Avg Acc: 0.4545
2022-01-21 03:04:17,257 Epoch[233/300], Step[0750/1252], Avg Loss: 3.0852, Avg Acc: 0.4559
2022-01-21 03:05:49,241 Epoch[233/300], Step[0800/1252], Avg Loss: 3.0865, Avg Acc: 0.4562
2022-01-21 03:07:21,063 Epoch[233/300], Step[0850/1252], Avg Loss: 3.0850, Avg Acc: 0.4575
2022-01-21 03:08:54,198 Epoch[233/300], Step[0900/1252], Avg Loss: 3.0814, Avg Acc: 0.4590
2022-01-21 03:10:27,180 Epoch[233/300], Step[0950/1252], Avg Loss: 3.0819, Avg Acc: 0.4584
2022-01-21 03:12:01,125 Epoch[233/300], Step[1000/1252], Avg Loss: 3.0824, Avg Acc: 0.4578
2022-01-21 03:13:34,043 Epoch[233/300], Step[1050/1252], Avg Loss: 3.0846, Avg Acc: 0.4560
2022-01-21 03:15:07,245 Epoch[233/300], Step[1100/1252], Avg Loss: 3.0867, Avg Acc: 0.4541
2022-01-21 03:16:39,969 Epoch[233/300], Step[1150/1252], Avg Loss: 3.0899, Avg Acc: 0.4537
2022-01-21 03:18:15,550 Epoch[233/300], Step[1200/1252], Avg Loss: 3.0911, Avg Acc: 0.4537
2022-01-21 03:19:49,494 Epoch[233/300], Step[1250/1252], Avg Loss: 3.0928, Avg Acc: 0.4532
2022-01-21 03:19:55,858 ----- Epoch[233/300], Train Loss: 3.0928, Train Acc: 0.4531, time: 2453.71, Best Val(epoch232) Acc@1: 0.7596
2022-01-21 03:19:55,858 Now training epoch 234. LR=0.000140
2022-01-21 03:22:00,163 Epoch[234/300], Step[0000/1252], Avg Loss: 2.5644, Avg Acc: 0.1484
2022-01-21 03:23:33,661 Epoch[234/300], Step[0050/1252], Avg Loss: 3.0898, Avg Acc: 0.4672
2022-01-21 03:25:06,775 Epoch[234/300], Step[0100/1252], Avg Loss: 3.0777, Avg Acc: 0.4503
2022-01-21 03:26:38,958 Epoch[234/300], Step[0150/1252], Avg Loss: 3.0638, Avg Acc: 0.4653
2022-01-21 03:28:11,764 Epoch[234/300], Step[0200/1252], Avg Loss: 3.0587, Avg Acc: 0.4655
2022-01-21 03:29:45,399 Epoch[234/300], Step[0250/1252], Avg Loss: 3.0609, Avg Acc: 0.4537
2022-01-21 03:31:19,311 Epoch[234/300], Step[0300/1252], Avg Loss: 3.0705, Avg Acc: 0.4525
2022-01-21 03:32:51,978 Epoch[234/300], Step[0350/1252], Avg Loss: 3.0653, Avg Acc: 0.4587
2022-01-21 03:34:26,105 Epoch[234/300], Step[0400/1252], Avg Loss: 3.0648, Avg Acc: 0.4629
2022-01-21 03:36:00,911 Epoch[234/300], Step[0450/1252], Avg Loss: 3.0686, Avg Acc: 0.4615
2022-01-21 03:37:33,603 Epoch[234/300], Step[0500/1252], Avg Loss: 3.0741, Avg Acc: 0.4600
2022-01-21 03:39:07,821 Epoch[234/300], Step[0550/1252], Avg Loss: 3.0759, Avg Acc: 0.4606
2022-01-21 03:40:40,471 Epoch[234/300], Step[0600/1252], Avg Loss: 3.0747, Avg Acc: 0.4612
2022-01-21 03:42:13,660 Epoch[234/300], Step[0650/1252], Avg Loss: 3.0719, Avg Acc: 0.4600
2022-01-21 03:43:47,296 Epoch[234/300], Step[0700/1252], Avg Loss: 3.0746, Avg Acc: 0.4610
2022-01-21 03:45:21,393 Epoch[234/300], Step[0750/1252], Avg Loss: 3.0715, Avg Acc: 0.4613
2022-01-21 03:46:54,206 Epoch[234/300], Step[0800/1252], Avg Loss: 3.0709, Avg Acc: 0.4590
2022-01-21 03:48:27,124 Epoch[234/300], Step[0850/1252], Avg Loss: 3.0715, Avg Acc: 0.4593
2022-01-21 03:49:59,504 Epoch[234/300], Step[0900/1252], Avg Loss: 3.0749, Avg Acc: 0.4563
2022-01-21 03:51:32,333 Epoch[234/300], Step[0950/1252], Avg Loss: 3.0770, Avg Acc: 0.4563
2022-01-21 03:53:06,035 Epoch[234/300], Step[1000/1252], Avg Loss: 3.0745, Avg Acc: 0.4563
2022-01-21 03:54:38,835 Epoch[234/300], Step[1050/1252], Avg Loss: 3.0774, Avg Acc: 0.4542
2022-01-21 03:56:12,560 Epoch[234/300], Step[1100/1252], Avg Loss: 3.0754, Avg Acc: 0.4571
2022-01-21 03:57:45,127 Epoch[234/300], Step[1150/1252], Avg Loss: 3.0765, Avg Acc: 0.4583
2022-01-21 03:59:18,360 Epoch[234/300], Step[1200/1252], Avg Loss: 3.0797, Avg Acc: 0.4567
2022-01-21 04:00:51,795 Epoch[234/300], Step[1250/1252], Avg Loss: 3.0826, Avg Acc: 0.4569
2022-01-21 04:00:58,319 ----- Epoch[234/300], Train Loss: 3.0826, Train Acc: 0.4569, time: 2462.46, Best Val(epoch232) Acc@1: 0.7596
2022-01-21 04:00:58,320 ----- Validation after Epoch: 234
2022-01-21 04:02:31,432 Val Step[0000/1563], Avg Loss: 0.8951, Avg Acc@1: 0.8438, Avg Acc@5: 0.9688
2022-01-21 04:02:33,642 Val Step[0050/1563], Avg Loss: 1.0615, Avg Acc@1: 0.7678, Avg Acc@5: 0.9332
2022-01-21 04:02:35,796 Val Step[0100/1563], Avg Loss: 1.0849, Avg Acc@1: 0.7642, Avg Acc@5: 0.9353
2022-01-21 04:02:38,076 Val Step[0150/1563], Avg Loss: 1.0883, Avg Acc@1: 0.7661, Avg Acc@5: 0.9334
2022-01-21 04:02:40,203 Val Step[0200/1563], Avg Loss: 1.0893, Avg Acc@1: 0.7659, Avg Acc@5: 0.9325
2022-01-21 04:02:42,380 Val Step[0250/1563], Avg Loss: 1.0802, Avg Acc@1: 0.7664, Avg Acc@5: 0.9334
2022-01-21 04:02:44,612 Val Step[0300/1563], Avg Loss: 1.0798, Avg Acc@1: 0.7664, Avg Acc@5: 0.9317
2022-01-21 04:02:46,721 Val Step[0350/1563], Avg Loss: 1.0842, Avg Acc@1: 0.7665, Avg Acc@5: 0.9306
2022-01-21 04:02:48,907 Val Step[0400/1563], Avg Loss: 1.0825, Avg Acc@1: 0.7663, Avg Acc@5: 0.9307
2022-01-21 04:02:51,103 Val Step[0450/1563], Avg Loss: 1.0846, Avg Acc@1: 0.7648, Avg Acc@5: 0.9306
2022-01-21 04:02:53,298 Val Step[0500/1563], Avg Loss: 1.0871, Avg Acc@1: 0.7638, Avg Acc@5: 0.9311
2022-01-21 04:02:55,593 Val Step[0550/1563], Avg Loss: 1.0871, Avg Acc@1: 0.7633, Avg Acc@5: 0.9316
2022-01-21 04:02:57,789 Val Step[0600/1563], Avg Loss: 1.0877, Avg Acc@1: 0.7630, Avg Acc@5: 0.9315
2022-01-21 04:02:59,967 Val Step[0650/1563], Avg Loss: 1.0890, Avg Acc@1: 0.7632, Avg Acc@5: 0.9316
2022-01-21 04:03:02,141 Val Step[0700/1563], Avg Loss: 1.0884, Avg Acc@1: 0.7634, Avg Acc@5: 0.9325
2022-01-21 04:03:04,306 Val Step[0750/1563], Avg Loss: 1.0940, Avg Acc@1: 0.7616, Avg Acc@5: 0.9321
2022-01-21 04:03:06,518 Val Step[0800/1563], Avg Loss: 1.0954, Avg Acc@1: 0.7617, Avg Acc@5: 0.9319
2022-01-21 04:03:08,692 Val Step[0850/1563], Avg Loss: 1.0960, Avg Acc@1: 0.7614, Avg Acc@5: 0.9318
2022-01-21 04:03:10,748 Val Step[0900/1563], Avg Loss: 1.0922, Avg Acc@1: 0.7622, Avg Acc@5: 0.9321
2022-01-21 04:03:12,754 Val Step[0950/1563], Avg Loss: 1.0914, Avg Acc@1: 0.7626, Avg Acc@5: 0.9318
2022-01-21 04:03:14,820 Val Step[1000/1563], Avg Loss: 1.0936, Avg Acc@1: 0.7622, Avg Acc@5: 0.9313
2022-01-21 04:03:16,879 Val Step[1050/1563], Avg Loss: 1.0947, Avg Acc@1: 0.7615, Avg Acc@5: 0.9310
2022-01-21 04:03:18,993 Val Step[1100/1563], Avg Loss: 1.0953, Avg Acc@1: 0.7611, Avg Acc@5: 0.9312
2022-01-21 04:03:21,001 Val Step[1150/1563], Avg Loss: 1.0934, Avg Acc@1: 0.7613, Avg Acc@5: 0.9314
2022-01-21 04:03:23,172 Val Step[1200/1563], Avg Loss: 1.0919, Avg Acc@1: 0.7616, Avg Acc@5: 0.9316
2022-01-21 04:03:25,256 Val Step[1250/1563], Avg Loss: 1.0913, Avg Acc@1: 0.7612, Avg Acc@5: 0.9318
2022-01-21 04:03:27,538 Val Step[1300/1563], Avg Loss: 1.0937, Avg Acc@1: 0.7611, Avg Acc@5: 0.9315
2022-01-21 04:03:29,879 Val Step[1350/1563], Avg Loss: 1.0942, Avg Acc@1: 0.7606, Avg Acc@5: 0.9314
2022-01-21 04:03:32,109 Val Step[1400/1563], Avg Loss: 1.0936, Avg Acc@1: 0.7602, Avg Acc@5: 0.9315
2022-01-21 04:03:34,294 Val Step[1450/1563], Avg Loss: 1.0922, Avg Acc@1: 0.7605, Avg Acc@5: 0.9316
2022-01-21 04:03:36,411 Val Step[1500/1563], Avg Loss: 1.0920, Avg Acc@1: 0.7606, Avg Acc@5: 0.9318
2022-01-21 04:03:38,462 Val Step[1550/1563], Avg Loss: 1.0924, Avg Acc@1: 0.7604, Avg Acc@5: 0.9316
2022-01-21 04:03:40,732 ----- Epoch[234/300], Validation Loss: 1.0924, Validation Acc@1: 0.7604, Validation Acc@5: 0.9316, time: 162.41
2022-01-21 04:03:41,950 the pre best model acc:0.7596, at epoch 232
2022-01-21 04:03:42,244 current best model acc:0.7604, at epoch 234
2022-01-21 04:03:42,245 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 04:03:42,245 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 04:03:42,245 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 04:03:42,245 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 04:03:42,245 Now training epoch 235. LR=0.000136
2022-01-21 04:05:59,472 Epoch[235/300], Step[0000/1252], Avg Loss: 3.1526, Avg Acc: 0.4385
2022-01-21 04:07:30,955 Epoch[235/300], Step[0050/1252], Avg Loss: 3.0514, Avg Acc: 0.4467
2022-01-21 04:09:02,593 Epoch[235/300], Step[0100/1252], Avg Loss: 3.0897, Avg Acc: 0.4456
2022-01-21 04:10:36,008 Epoch[235/300], Step[0150/1252], Avg Loss: 3.1027, Avg Acc: 0.4500
2022-01-21 04:12:08,002 Epoch[235/300], Step[0200/1252], Avg Loss: 3.1031, Avg Acc: 0.4525
2022-01-21 04:13:41,117 Epoch[235/300], Step[0250/1252], Avg Loss: 3.1037, Avg Acc: 0.4502
2022-01-21 04:15:14,148 Epoch[235/300], Step[0300/1252], Avg Loss: 3.0995, Avg Acc: 0.4502
2022-01-21 04:16:47,223 Epoch[235/300], Step[0350/1252], Avg Loss: 3.0988, Avg Acc: 0.4510
2022-01-21 04:18:20,189 Epoch[235/300], Step[0400/1252], Avg Loss: 3.1023, Avg Acc: 0.4541
2022-01-21 04:19:52,427 Epoch[235/300], Step[0450/1252], Avg Loss: 3.1055, Avg Acc: 0.4562
2022-01-21 04:21:25,250 Epoch[235/300], Step[0500/1252], Avg Loss: 3.0996, Avg Acc: 0.4543
2022-01-21 04:22:57,626 Epoch[235/300], Step[0550/1252], Avg Loss: 3.0921, Avg Acc: 0.4557
2022-01-21 04:24:29,632 Epoch[235/300], Step[0600/1252], Avg Loss: 3.0896, Avg Acc: 0.4555
2022-01-21 04:26:02,034 Epoch[235/300], Step[0650/1252], Avg Loss: 3.0872, Avg Acc: 0.4550
2022-01-21 04:27:32,766 Epoch[235/300], Step[0700/1252], Avg Loss: 3.0872, Avg Acc: 0.4530
2022-01-21 04:29:04,901 Epoch[235/300], Step[0750/1252], Avg Loss: 3.0870, Avg Acc: 0.4525
2022-01-21 04:30:36,191 Epoch[235/300], Step[0800/1252], Avg Loss: 3.0927, Avg Acc: 0.4526
2022-01-21 04:32:09,002 Epoch[235/300], Step[0850/1252], Avg Loss: 3.0965, Avg Acc: 0.4519
2022-01-21 04:33:41,758 Epoch[235/300], Step[0900/1252], Avg Loss: 3.0949, Avg Acc: 0.4524
2022-01-21 04:35:15,642 Epoch[235/300], Step[0950/1252], Avg Loss: 3.0929, Avg Acc: 0.4527
2022-01-21 04:36:50,271 Epoch[235/300], Step[1000/1252], Avg Loss: 3.0917, Avg Acc: 0.4533
2022-01-21 04:38:23,018 Epoch[235/300], Step[1050/1252], Avg Loss: 3.0902, Avg Acc: 0.4534
2022-01-21 04:39:55,583 Epoch[235/300], Step[1100/1252], Avg Loss: 3.0946, Avg Acc: 0.4524
2022-01-21 04:41:27,756 Epoch[235/300], Step[1150/1252], Avg Loss: 3.0962, Avg Acc: 0.4533
2022-01-21 04:43:00,196 Epoch[235/300], Step[1200/1252], Avg Loss: 3.0958, Avg Acc: 0.4530
2022-01-21 04:44:33,805 Epoch[235/300], Step[1250/1252], Avg Loss: 3.0985, Avg Acc: 0.4532
2022-01-21 04:44:40,408 ----- Epoch[235/300], Train Loss: 3.0984, Train Acc: 0.4532, time: 2458.16, Best Val(epoch234) Acc@1: 0.7604
2022-01-21 04:44:40,408 Now training epoch 236. LR=0.000132
2022-01-21 04:46:57,295 Epoch[236/300], Step[0000/1252], Avg Loss: 3.4735, Avg Acc: 0.5098
2022-01-21 04:48:28,533 Epoch[236/300], Step[0050/1252], Avg Loss: 3.0902, Avg Acc: 0.4679
2022-01-21 04:49:59,102 Epoch[236/300], Step[0100/1252], Avg Loss: 3.0746, Avg Acc: 0.4667
2022-01-21 04:51:30,809 Epoch[236/300], Step[0150/1252], Avg Loss: 3.0769, Avg Acc: 0.4530
2022-01-21 04:53:03,463 Epoch[236/300], Step[0200/1252], Avg Loss: 3.0734, Avg Acc: 0.4515
2022-01-21 04:54:35,458 Epoch[236/300], Step[0250/1252], Avg Loss: 3.0628, Avg Acc: 0.4543
2022-01-21 04:56:10,808 Epoch[236/300], Step[0300/1252], Avg Loss: 3.0648, Avg Acc: 0.4547
2022-01-21 04:57:43,792 Epoch[236/300], Step[0350/1252], Avg Loss: 3.0623, Avg Acc: 0.4574
2022-01-21 04:59:16,884 Epoch[236/300], Step[0400/1252], Avg Loss: 3.0735, Avg Acc: 0.4554
2022-01-21 05:00:49,408 Epoch[236/300], Step[0450/1252], Avg Loss: 3.0766, Avg Acc: 0.4534
2022-01-21 05:02:23,114 Epoch[236/300], Step[0500/1252], Avg Loss: 3.0801, Avg Acc: 0.4494
2022-01-21 05:03:54,977 Epoch[236/300], Step[0550/1252], Avg Loss: 3.0798, Avg Acc: 0.4515
2022-01-21 05:05:28,427 Epoch[236/300], Step[0600/1252], Avg Loss: 3.0815, Avg Acc: 0.4496
2022-01-21 05:07:01,390 Epoch[236/300], Step[0650/1252], Avg Loss: 3.0795, Avg Acc: 0.4485
2022-01-21 05:08:33,897 Epoch[236/300], Step[0700/1252], Avg Loss: 3.0783, Avg Acc: 0.4498
2022-01-21 05:10:07,510 Epoch[236/300], Step[0750/1252], Avg Loss: 3.0775, Avg Acc: 0.4512
2022-01-21 05:11:40,082 Epoch[236/300], Step[0800/1252], Avg Loss: 3.0797, Avg Acc: 0.4526
2022-01-21 05:13:13,698 Epoch[236/300], Step[0850/1252], Avg Loss: 3.0816, Avg Acc: 0.4518
2022-01-21 05:14:47,108 Epoch[236/300], Step[0900/1252], Avg Loss: 3.0862, Avg Acc: 0.4517
2022-01-21 05:16:20,499 Epoch[236/300], Step[0950/1252], Avg Loss: 3.0855, Avg Acc: 0.4500
2022-01-21 05:17:53,431 Epoch[236/300], Step[1000/1252], Avg Loss: 3.0869, Avg Acc: 0.4490
2022-01-21 05:19:26,475 Epoch[236/300], Step[1050/1252], Avg Loss: 3.0886, Avg Acc: 0.4487
2022-01-21 05:21:00,695 Epoch[236/300], Step[1100/1252], Avg Loss: 3.0896, Avg Acc: 0.4489
2022-01-21 05:22:33,932 Epoch[236/300], Step[1150/1252], Avg Loss: 3.0894, Avg Acc: 0.4498
2022-01-21 05:24:05,936 Epoch[236/300], Step[1200/1252], Avg Loss: 3.0867, Avg Acc: 0.4515
2022-01-21 05:25:38,906 Epoch[236/300], Step[1250/1252], Avg Loss: 3.0860, Avg Acc: 0.4525
2022-01-21 05:25:44,993 ----- Epoch[236/300], Train Loss: 3.0860, Train Acc: 0.4526, time: 2464.58, Best Val(epoch234) Acc@1: 0.7604
2022-01-21 05:25:44,993 ----- Validation after Epoch: 236
2022-01-21 05:27:22,622 Val Step[0000/1563], Avg Loss: 0.7156, Avg Acc@1: 0.8438, Avg Acc@5: 1.0000
2022-01-21 05:27:24,991 Val Step[0050/1563], Avg Loss: 1.0363, Avg Acc@1: 0.7727, Avg Acc@5: 0.9363
2022-01-21 05:27:27,232 Val Step[0100/1563], Avg Loss: 1.0651, Avg Acc@1: 0.7676, Avg Acc@5: 0.9344
2022-01-21 05:27:29,453 Val Step[0150/1563], Avg Loss: 1.0679, Avg Acc@1: 0.7680, Avg Acc@5: 0.9307
2022-01-21 05:27:31,680 Val Step[0200/1563], Avg Loss: 1.0729, Avg Acc@1: 0.7677, Avg Acc@5: 0.9313
2022-01-21 05:27:33,853 Val Step[0250/1563], Avg Loss: 1.0599, Avg Acc@1: 0.7678, Avg Acc@5: 0.9326
2022-01-21 05:27:35,892 Val Step[0300/1563], Avg Loss: 1.0606, Avg Acc@1: 0.7668, Avg Acc@5: 0.9322
2022-01-21 05:27:38,113 Val Step[0350/1563], Avg Loss: 1.0660, Avg Acc@1: 0.7657, Avg Acc@5: 0.9322
2022-01-21 05:27:40,364 Val Step[0400/1563], Avg Loss: 1.0648, Avg Acc@1: 0.7657, Avg Acc@5: 0.9316
2022-01-21 05:27:42,553 Val Step[0450/1563], Avg Loss: 1.0712, Avg Acc@1: 0.7631, Avg Acc@5: 0.9311
2022-01-21 05:27:44,733 Val Step[0500/1563], Avg Loss: 1.0734, Avg Acc@1: 0.7620, Avg Acc@5: 0.9312
2022-01-21 05:27:46,999 Val Step[0550/1563], Avg Loss: 1.0745, Avg Acc@1: 0.7607, Avg Acc@5: 0.9314
2022-01-21 05:27:49,068 Val Step[0600/1563], Avg Loss: 1.0752, Avg Acc@1: 0.7601, Avg Acc@5: 0.9314
2022-01-21 05:27:51,219 Val Step[0650/1563], Avg Loss: 1.0745, Avg Acc@1: 0.7603, Avg Acc@5: 0.9315
2022-01-21 05:27:53,355 Val Step[0700/1563], Avg Loss: 1.0727, Avg Acc@1: 0.7609, Avg Acc@5: 0.9322
2022-01-21 05:27:55,492 Val Step[0750/1563], Avg Loss: 1.0787, Avg Acc@1: 0.7593, Avg Acc@5: 0.9318
2022-01-21 05:27:57,736 Val Step[0800/1563], Avg Loss: 1.0788, Avg Acc@1: 0.7596, Avg Acc@5: 0.9319
2022-01-21 05:27:59,875 Val Step[0850/1563], Avg Loss: 1.0802, Avg Acc@1: 0.7594, Avg Acc@5: 0.9314
2022-01-21 05:28:02,001 Val Step[0900/1563], Avg Loss: 1.0761, Avg Acc@1: 0.7603, Avg Acc@5: 0.9320
2022-01-21 05:28:04,117 Val Step[0950/1563], Avg Loss: 1.0759, Avg Acc@1: 0.7611, Avg Acc@5: 0.9322
2022-01-21 05:28:06,339 Val Step[1000/1563], Avg Loss: 1.0780, Avg Acc@1: 0.7611, Avg Acc@5: 0.9316
2022-01-21 05:28:08,488 Val Step[1050/1563], Avg Loss: 1.0800, Avg Acc@1: 0.7606, Avg Acc@5: 0.9314
2022-01-21 05:28:10,639 Val Step[1100/1563], Avg Loss: 1.0804, Avg Acc@1: 0.7605, Avg Acc@5: 0.9313
2022-01-21 05:28:12,594 Val Step[1150/1563], Avg Loss: 1.0786, Avg Acc@1: 0.7606, Avg Acc@5: 0.9318
2022-01-21 05:28:14,758 Val Step[1200/1563], Avg Loss: 1.0766, Avg Acc@1: 0.7611, Avg Acc@5: 0.9320
2022-01-21 05:28:16,895 Val Step[1250/1563], Avg Loss: 1.0764, Avg Acc@1: 0.7603, Avg Acc@5: 0.9322
2022-01-21 05:28:19,111 Val Step[1300/1563], Avg Loss: 1.0788, Avg Acc@1: 0.7602, Avg Acc@5: 0.9318
2022-01-21 05:28:21,320 Val Step[1350/1563], Avg Loss: 1.0802, Avg Acc@1: 0.7598, Avg Acc@5: 0.9315
2022-01-21 05:28:23,471 Val Step[1400/1563], Avg Loss: 1.0793, Avg Acc@1: 0.7595, Avg Acc@5: 0.9316
2022-01-21 05:28:25,664 Val Step[1450/1563], Avg Loss: 1.0780, Avg Acc@1: 0.7598, Avg Acc@5: 0.9318
2022-01-21 05:28:27,900 Val Step[1500/1563], Avg Loss: 1.0776, Avg Acc@1: 0.7599, Avg Acc@5: 0.9320
2022-01-21 05:28:30,038 Val Step[1550/1563], Avg Loss: 1.0781, Avg Acc@1: 0.7599, Avg Acc@5: 0.9317
2022-01-21 05:28:32,042 ----- Epoch[236/300], Validation Loss: 1.0780, Validation Acc@1: 0.7598, Validation Acc@5: 0.9318, time: 167.05
2022-01-21 05:28:32,042 Now training epoch 237. LR=0.000129
2022-01-21 05:30:40,350 Epoch[237/300], Step[0000/1252], Avg Loss: 3.4488, Avg Acc: 0.4385
2022-01-21 05:32:12,802 Epoch[237/300], Step[0050/1252], Avg Loss: 3.1210, Avg Acc: 0.4737
2022-01-21 05:33:44,864 Epoch[237/300], Step[0100/1252], Avg Loss: 3.1037, Avg Acc: 0.4770
2022-01-21 05:35:19,939 Epoch[237/300], Step[0150/1252], Avg Loss: 3.0924, Avg Acc: 0.4698
2022-01-21 05:36:52,649 Epoch[237/300], Step[0200/1252], Avg Loss: 3.0858, Avg Acc: 0.4598
2022-01-21 05:38:26,113 Epoch[237/300], Step[0250/1252], Avg Loss: 3.0831, Avg Acc: 0.4597
2022-01-21 05:39:58,992 Epoch[237/300], Step[0300/1252], Avg Loss: 3.0903, Avg Acc: 0.4598
2022-01-21 05:41:31,691 Epoch[237/300], Step[0350/1252], Avg Loss: 3.0815, Avg Acc: 0.4601
2022-01-21 05:43:04,237 Epoch[237/300], Step[0400/1252], Avg Loss: 3.0804, Avg Acc: 0.4579
2022-01-21 05:44:38,670 Epoch[237/300], Step[0450/1252], Avg Loss: 3.0734, Avg Acc: 0.4585
2022-01-21 05:46:12,394 Epoch[237/300], Step[0500/1252], Avg Loss: 3.0817, Avg Acc: 0.4566
2022-01-21 05:47:45,811 Epoch[237/300], Step[0550/1252], Avg Loss: 3.0825, Avg Acc: 0.4561
2022-01-21 05:49:19,391 Epoch[237/300], Step[0600/1252], Avg Loss: 3.0823, Avg Acc: 0.4546
2022-01-21 05:50:52,087 Epoch[237/300], Step[0650/1252], Avg Loss: 3.0854, Avg Acc: 0.4536
2022-01-21 05:52:24,516 Epoch[237/300], Step[0700/1252], Avg Loss: 3.0856, Avg Acc: 0.4558
2022-01-21 05:53:58,448 Epoch[237/300], Step[0750/1252], Avg Loss: 3.0866, Avg Acc: 0.4538
2022-01-21 05:55:33,077 Epoch[237/300], Step[0800/1252], Avg Loss: 3.0893, Avg Acc: 0.4513
2022-01-21 05:57:06,002 Epoch[237/300], Step[0850/1252], Avg Loss: 3.0872, Avg Acc: 0.4533
2022-01-21 05:58:39,054 Epoch[237/300], Step[0900/1252], Avg Loss: 3.0849, Avg Acc: 0.4544
2022-01-21 06:00:12,070 Epoch[237/300], Step[0950/1252], Avg Loss: 3.0859, Avg Acc: 0.4550
2022-01-21 06:01:44,322 Epoch[237/300], Step[1000/1252], Avg Loss: 3.0866, Avg Acc: 0.4555
2022-01-21 06:03:18,191 Epoch[237/300], Step[1050/1252], Avg Loss: 3.0859, Avg Acc: 0.4551
2022-01-21 06:04:51,540 Epoch[237/300], Step[1100/1252], Avg Loss: 3.0851, Avg Acc: 0.4563
2022-01-21 06:06:24,581 Epoch[237/300], Step[1150/1252], Avg Loss: 3.0830, Avg Acc: 0.4571
2022-01-21 06:07:56,761 Epoch[237/300], Step[1200/1252], Avg Loss: 3.0844, Avg Acc: 0.4574
2022-01-21 06:09:30,257 Epoch[237/300], Step[1250/1252], Avg Loss: 3.0818, Avg Acc: 0.4578
2022-01-21 06:09:36,810 ----- Epoch[237/300], Train Loss: 3.0818, Train Acc: 0.4578, time: 2464.76, Best Val(epoch234) Acc@1: 0.7604
2022-01-21 06:09:36,810 Now training epoch 238. LR=0.000125
2022-01-21 06:11:51,706 Epoch[238/300], Step[0000/1252], Avg Loss: 3.3473, Avg Acc: 0.4834
2022-01-21 06:13:25,027 Epoch[238/300], Step[0050/1252], Avg Loss: 3.1160, Avg Acc: 0.4513
2022-01-21 06:14:56,087 Epoch[238/300], Step[0100/1252], Avg Loss: 3.0936, Avg Acc: 0.4427
2022-01-21 06:16:28,396 Epoch[238/300], Step[0150/1252], Avg Loss: 3.0831, Avg Acc: 0.4423
2022-01-21 06:18:01,967 Epoch[238/300], Step[0200/1252], Avg Loss: 3.0786, Avg Acc: 0.4510
2022-01-21 06:19:35,469 Epoch[238/300], Step[0250/1252], Avg Loss: 3.0708, Avg Acc: 0.4532
2022-01-21 06:21:09,533 Epoch[238/300], Step[0300/1252], Avg Loss: 3.0694, Avg Acc: 0.4523
2022-01-21 06:22:41,806 Epoch[238/300], Step[0350/1252], Avg Loss: 3.0664, Avg Acc: 0.4551
2022-01-21 06:24:15,079 Epoch[238/300], Step[0400/1252], Avg Loss: 3.0595, Avg Acc: 0.4557
2022-01-21 06:25:49,470 Epoch[238/300], Step[0450/1252], Avg Loss: 3.0605, Avg Acc: 0.4550
2022-01-21 06:27:22,871 Epoch[238/300], Step[0500/1252], Avg Loss: 3.0585, Avg Acc: 0.4570
2022-01-21 06:28:55,416 Epoch[238/300], Step[0550/1252], Avg Loss: 3.0620, Avg Acc: 0.4595
2022-01-21 06:30:27,376 Epoch[238/300], Step[0600/1252], Avg Loss: 3.0686, Avg Acc: 0.4593
2022-01-21 06:32:00,672 Epoch[238/300], Step[0650/1252], Avg Loss: 3.0640, Avg Acc: 0.4595
2022-01-21 06:33:35,029 Epoch[238/300], Step[0700/1252], Avg Loss: 3.0639, Avg Acc: 0.4581
2022-01-21 06:35:08,321 Epoch[238/300], Step[0750/1252], Avg Loss: 3.0654, Avg Acc: 0.4590
2022-01-21 06:36:39,621 Epoch[238/300], Step[0800/1252], Avg Loss: 3.0656, Avg Acc: 0.4589
2022-01-21 06:38:12,817 Epoch[238/300], Step[0850/1252], Avg Loss: 3.0619, Avg Acc: 0.4595
2022-01-21 06:39:46,403 Epoch[238/300], Step[0900/1252], Avg Loss: 3.0652, Avg Acc: 0.4589
2022-01-21 06:41:18,916 Epoch[238/300], Step[0950/1252], Avg Loss: 3.0675, Avg Acc: 0.4590
2022-01-21 06:42:52,959 Epoch[238/300], Step[1000/1252], Avg Loss: 3.0692, Avg Acc: 0.4594
2022-01-21 06:44:25,478 Epoch[238/300], Step[1050/1252], Avg Loss: 3.0698, Avg Acc: 0.4592
2022-01-21 06:45:58,636 Epoch[238/300], Step[1100/1252], Avg Loss: 3.0690, Avg Acc: 0.4584
2022-01-21 06:47:30,538 Epoch[238/300], Step[1150/1252], Avg Loss: 3.0716, Avg Acc: 0.4575
2022-01-21 06:49:03,816 Epoch[238/300], Step[1200/1252], Avg Loss: 3.0699, Avg Acc: 0.4595
2022-01-21 06:50:37,203 Epoch[238/300], Step[1250/1252], Avg Loss: 3.0683, Avg Acc: 0.4594
2022-01-21 06:50:43,709 ----- Epoch[238/300], Train Loss: 3.0682, Train Acc: 0.4594, time: 2466.89, Best Val(epoch234) Acc@1: 0.7604
2022-01-21 06:50:43,709 ----- Validation after Epoch: 238
2022-01-21 06:52:14,768 Val Step[0000/1563], Avg Loss: 0.8879, Avg Acc@1: 0.8125, Avg Acc@5: 0.9688
2022-01-21 06:52:17,502 Val Step[0050/1563], Avg Loss: 1.0541, Avg Acc@1: 0.7635, Avg Acc@5: 0.9363
2022-01-21 06:52:19,975 Val Step[0100/1563], Avg Loss: 1.0736, Avg Acc@1: 0.7673, Avg Acc@5: 0.9335
2022-01-21 06:52:22,147 Val Step[0150/1563], Avg Loss: 1.0837, Avg Acc@1: 0.7649, Avg Acc@5: 0.9300
2022-01-21 06:52:24,502 Val Step[0200/1563], Avg Loss: 1.0804, Avg Acc@1: 0.7674, Avg Acc@5: 0.9310
2022-01-21 06:52:26,842 Val Step[0250/1563], Avg Loss: 1.0652, Avg Acc@1: 0.7695, Avg Acc@5: 0.9328
2022-01-21 06:52:29,212 Val Step[0300/1563], Avg Loss: 1.0693, Avg Acc@1: 0.7676, Avg Acc@5: 0.9313
2022-01-21 06:52:31,772 Val Step[0350/1563], Avg Loss: 1.0746, Avg Acc@1: 0.7676, Avg Acc@5: 0.9314
2022-01-21 06:52:34,174 Val Step[0400/1563], Avg Loss: 1.0754, Avg Acc@1: 0.7673, Avg Acc@5: 0.9310
2022-01-21 06:52:36,524 Val Step[0450/1563], Avg Loss: 1.0815, Avg Acc@1: 0.7650, Avg Acc@5: 0.9305
2022-01-21 06:52:38,953 Val Step[0500/1563], Avg Loss: 1.0832, Avg Acc@1: 0.7645, Avg Acc@5: 0.9309
2022-01-21 06:52:41,365 Val Step[0550/1563], Avg Loss: 1.0828, Avg Acc@1: 0.7640, Avg Acc@5: 0.9311
2022-01-21 06:52:43,572 Val Step[0600/1563], Avg Loss: 1.0846, Avg Acc@1: 0.7635, Avg Acc@5: 0.9307
2022-01-21 06:52:45,635 Val Step[0650/1563], Avg Loss: 1.0853, Avg Acc@1: 0.7636, Avg Acc@5: 0.9307
2022-01-21 06:52:47,858 Val Step[0700/1563], Avg Loss: 1.0822, Avg Acc@1: 0.7645, Avg Acc@5: 0.9317
2022-01-21 06:52:49,952 Val Step[0750/1563], Avg Loss: 1.0891, Avg Acc@1: 0.7622, Avg Acc@5: 0.9312
2022-01-21 06:52:52,072 Val Step[0800/1563], Avg Loss: 1.0891, Avg Acc@1: 0.7624, Avg Acc@5: 0.9309
2022-01-21 06:52:54,168 Val Step[0850/1563], Avg Loss: 1.0906, Avg Acc@1: 0.7618, Avg Acc@5: 0.9306
2022-01-21 06:52:56,253 Val Step[0900/1563], Avg Loss: 1.0870, Avg Acc@1: 0.7620, Avg Acc@5: 0.9308
2022-01-21 06:52:58,401 Val Step[0950/1563], Avg Loss: 1.0866, Avg Acc@1: 0.7623, Avg Acc@5: 0.9309
2022-01-21 06:53:00,473 Val Step[1000/1563], Avg Loss: 1.0881, Avg Acc@1: 0.7624, Avg Acc@5: 0.9305
2022-01-21 06:53:02,754 Val Step[1050/1563], Avg Loss: 1.0897, Avg Acc@1: 0.7614, Avg Acc@5: 0.9301
2022-01-21 06:53:04,999 Val Step[1100/1563], Avg Loss: 1.0909, Avg Acc@1: 0.7611, Avg Acc@5: 0.9301
2022-01-21 06:53:07,178 Val Step[1150/1563], Avg Loss: 1.0893, Avg Acc@1: 0.7616, Avg Acc@5: 0.9300
2022-01-21 06:53:09,394 Val Step[1200/1563], Avg Loss: 1.0883, Avg Acc@1: 0.7622, Avg Acc@5: 0.9301
2022-01-21 06:53:11,668 Val Step[1250/1563], Avg Loss: 1.0881, Avg Acc@1: 0.7618, Avg Acc@5: 0.9307
2022-01-21 06:53:13,932 Val Step[1300/1563], Avg Loss: 1.0913, Avg Acc@1: 0.7615, Avg Acc@5: 0.9303
2022-01-21 06:53:16,240 Val Step[1350/1563], Avg Loss: 1.0924, Avg Acc@1: 0.7611, Avg Acc@5: 0.9303
2022-01-21 06:53:18,477 Val Step[1400/1563], Avg Loss: 1.0919, Avg Acc@1: 0.7610, Avg Acc@5: 0.9303
2022-01-21 06:53:20,723 Val Step[1450/1563], Avg Loss: 1.0904, Avg Acc@1: 0.7614, Avg Acc@5: 0.9307
2022-01-21 06:53:22,941 Val Step[1500/1563], Avg Loss: 1.0903, Avg Acc@1: 0.7615, Avg Acc@5: 0.9309
2022-01-21 06:53:25,070 Val Step[1550/1563], Avg Loss: 1.0907, Avg Acc@1: 0.7616, Avg Acc@5: 0.9308
2022-01-21 06:53:27,072 ----- Epoch[238/300], Validation Loss: 1.0907, Validation Acc@1: 0.7616, Validation Acc@5: 0.9308, time: 163.36
2022-01-21 06:53:28,344 the pre best model acc:0.7604, at epoch 234
2022-01-21 06:53:28,345 current best model acc:0.7616, at epoch 238
2022-01-21 06:53:28,345 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 06:53:28,345 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 06:53:28,345 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 06:53:28,345 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 06:53:28,345 Now training epoch 239. LR=0.000121
2022-01-21 06:55:30,013 Epoch[239/300], Step[0000/1252], Avg Loss: 3.0449, Avg Acc: 0.3594
2022-01-21 06:57:00,025 Epoch[239/300], Step[0050/1252], Avg Loss: 3.0590, Avg Acc: 0.4849
2022-01-21 06:58:31,815 Epoch[239/300], Step[0100/1252], Avg Loss: 3.1232, Avg Acc: 0.4577
2022-01-21 07:00:04,491 Epoch[239/300], Step[0150/1252], Avg Loss: 3.1165, Avg Acc: 0.4532
2022-01-21 07:01:37,183 Epoch[239/300], Step[0200/1252], Avg Loss: 3.1134, Avg Acc: 0.4572
2022-01-21 07:03:10,247 Epoch[239/300], Step[0250/1252], Avg Loss: 3.0989, Avg Acc: 0.4495
2022-01-21 07:04:41,570 Epoch[239/300], Step[0300/1252], Avg Loss: 3.0921, Avg Acc: 0.4493
2022-01-21 07:06:14,194 Epoch[239/300], Step[0350/1252], Avg Loss: 3.0885, Avg Acc: 0.4490
2022-01-21 07:07:47,348 Epoch[239/300], Step[0400/1252], Avg Loss: 3.0847, Avg Acc: 0.4499
2022-01-21 07:09:20,183 Epoch[239/300], Step[0450/1252], Avg Loss: 3.0809, Avg Acc: 0.4488
2022-01-21 07:10:53,834 Epoch[239/300], Step[0500/1252], Avg Loss: 3.0783, Avg Acc: 0.4482
2022-01-21 07:12:28,591 Epoch[239/300], Step[0550/1252], Avg Loss: 3.0769, Avg Acc: 0.4491
2022-01-21 07:14:02,028 Epoch[239/300], Step[0600/1252], Avg Loss: 3.0707, Avg Acc: 0.4491
2022-01-21 07:15:34,775 Epoch[239/300], Step[0650/1252], Avg Loss: 3.0715, Avg Acc: 0.4495
2022-01-21 07:17:08,458 Epoch[239/300], Step[0700/1252], Avg Loss: 3.0670, Avg Acc: 0.4508
2022-01-21 07:18:42,027 Epoch[239/300], Step[0750/1252], Avg Loss: 3.0653, Avg Acc: 0.4494
2022-01-21 07:20:13,860 Epoch[239/300], Step[0800/1252], Avg Loss: 3.0678, Avg Acc: 0.4512
2022-01-21 07:21:45,824 Epoch[239/300], Step[0850/1252], Avg Loss: 3.0640, Avg Acc: 0.4527
2022-01-21 07:23:19,648 Epoch[239/300], Step[0900/1252], Avg Loss: 3.0640, Avg Acc: 0.4520
2022-01-21 07:24:52,363 Epoch[239/300], Step[0950/1252], Avg Loss: 3.0634, Avg Acc: 0.4534
2022-01-21 07:26:25,236 Epoch[239/300], Step[1000/1252], Avg Loss: 3.0637, Avg Acc: 0.4545
2022-01-21 07:27:58,535 Epoch[239/300], Step[1050/1252], Avg Loss: 3.0642, Avg Acc: 0.4543
2022-01-21 07:29:31,432 Epoch[239/300], Step[1100/1252], Avg Loss: 3.0632, Avg Acc: 0.4534
2022-01-21 07:31:04,553 Epoch[239/300], Step[1150/1252], Avg Loss: 3.0626, Avg Acc: 0.4533
2022-01-21 07:32:37,776 Epoch[239/300], Step[1200/1252], Avg Loss: 3.0625, Avg Acc: 0.4543
2022-01-21 07:34:10,186 Epoch[239/300], Step[1250/1252], Avg Loss: 3.0632, Avg Acc: 0.4535
2022-01-21 07:34:16,429 ----- Epoch[239/300], Train Loss: 3.0632, Train Acc: 0.4535, time: 2448.08, Best Val(epoch238) Acc@1: 0.7616
2022-01-21 07:34:16,429 Now training epoch 240. LR=0.000118
2022-01-21 07:36:19,055 Epoch[240/300], Step[0000/1252], Avg Loss: 3.2827, Avg Acc: 0.3359
2022-01-21 07:37:50,792 Epoch[240/300], Step[0050/1252], Avg Loss: 3.0339, Avg Acc: 0.4622
2022-01-21 07:39:22,979 Epoch[240/300], Step[0100/1252], Avg Loss: 3.0670, Avg Acc: 0.4563
2022-01-21 07:40:55,826 Epoch[240/300], Step[0150/1252], Avg Loss: 3.0573, Avg Acc: 0.4524
2022-01-21 07:42:28,569 Epoch[240/300], Step[0200/1252], Avg Loss: 3.0407, Avg Acc: 0.4573
2022-01-21 07:44:01,078 Epoch[240/300], Step[0250/1252], Avg Loss: 3.0511, Avg Acc: 0.4571
2022-01-21 07:45:33,059 Epoch[240/300], Step[0300/1252], Avg Loss: 3.0568, Avg Acc: 0.4593
2022-01-21 07:47:06,424 Epoch[240/300], Step[0350/1252], Avg Loss: 3.0563, Avg Acc: 0.4622
2022-01-21 07:48:39,533 Epoch[240/300], Step[0400/1252], Avg Loss: 3.0593, Avg Acc: 0.4598
2022-01-21 07:50:12,299 Epoch[240/300], Step[0450/1252], Avg Loss: 3.0602, Avg Acc: 0.4609
2022-01-21 07:51:47,600 Epoch[240/300], Step[0500/1252], Avg Loss: 3.0592, Avg Acc: 0.4600
2022-01-21 07:53:20,826 Epoch[240/300], Step[0550/1252], Avg Loss: 3.0556, Avg Acc: 0.4567
2022-01-21 07:54:54,728 Epoch[240/300], Step[0600/1252], Avg Loss: 3.0589, Avg Acc: 0.4554
2022-01-21 07:56:26,754 Epoch[240/300], Step[0650/1252], Avg Loss: 3.0589, Avg Acc: 0.4567
2022-01-21 07:58:00,671 Epoch[240/300], Step[0700/1252], Avg Loss: 3.0631, Avg Acc: 0.4547
2022-01-21 07:59:33,805 Epoch[240/300], Step[0750/1252], Avg Loss: 3.0620, Avg Acc: 0.4538
2022-01-21 08:01:07,915 Epoch[240/300], Step[0800/1252], Avg Loss: 3.0637, Avg Acc: 0.4541
2022-01-21 08:02:41,824 Epoch[240/300], Step[0850/1252], Avg Loss: 3.0641, Avg Acc: 0.4535
2022-01-21 08:04:14,347 Epoch[240/300], Step[0900/1252], Avg Loss: 3.0617, Avg Acc: 0.4546
2022-01-21 08:05:46,017 Epoch[240/300], Step[0950/1252], Avg Loss: 3.0622, Avg Acc: 0.4565
2022-01-21 08:07:19,533 Epoch[240/300], Step[1000/1252], Avg Loss: 3.0639, Avg Acc: 0.4561
2022-01-21 08:08:51,908 Epoch[240/300], Step[1050/1252], Avg Loss: 3.0613, Avg Acc: 0.4569
2022-01-21 08:10:27,112 Epoch[240/300], Step[1100/1252], Avg Loss: 3.0627, Avg Acc: 0.4563
2022-01-21 08:12:00,202 Epoch[240/300], Step[1150/1252], Avg Loss: 3.0631, Avg Acc: 0.4558
2022-01-21 08:13:32,077 Epoch[240/300], Step[1200/1252], Avg Loss: 3.0625, Avg Acc: 0.4568
2022-01-21 08:15:05,364 Epoch[240/300], Step[1250/1252], Avg Loss: 3.0623, Avg Acc: 0.4559
2022-01-21 08:15:12,070 ----- Epoch[240/300], Train Loss: 3.0623, Train Acc: 0.4559, time: 2455.64, Best Val(epoch238) Acc@1: 0.7616
2022-01-21 08:15:12,071 ----- Validation after Epoch: 240
2022-01-21 08:16:46,228 Val Step[0000/1563], Avg Loss: 0.8817, Avg Acc@1: 0.8438, Avg Acc@5: 1.0000
2022-01-21 08:16:48,481 Val Step[0050/1563], Avg Loss: 1.0511, Avg Acc@1: 0.7690, Avg Acc@5: 0.9332
2022-01-21 08:16:50,662 Val Step[0100/1563], Avg Loss: 1.0729, Avg Acc@1: 0.7698, Avg Acc@5: 0.9335
2022-01-21 08:16:52,778 Val Step[0150/1563], Avg Loss: 1.0768, Avg Acc@1: 0.7703, Avg Acc@5: 0.9313
2022-01-21 08:16:54,938 Val Step[0200/1563], Avg Loss: 1.0783, Avg Acc@1: 0.7710, Avg Acc@5: 0.9322
2022-01-21 08:16:57,076 Val Step[0250/1563], Avg Loss: 1.0677, Avg Acc@1: 0.7723, Avg Acc@5: 0.9339
2022-01-21 08:16:59,140 Val Step[0300/1563], Avg Loss: 1.0678, Avg Acc@1: 0.7718, Avg Acc@5: 0.9336
2022-01-21 08:17:01,385 Val Step[0350/1563], Avg Loss: 1.0718, Avg Acc@1: 0.7706, Avg Acc@5: 0.9330
2022-01-21 08:17:03,610 Val Step[0400/1563], Avg Loss: 1.0698, Avg Acc@1: 0.7716, Avg Acc@5: 0.9331
2022-01-21 08:17:05,710 Val Step[0450/1563], Avg Loss: 1.0741, Avg Acc@1: 0.7693, Avg Acc@5: 0.9331
2022-01-21 08:17:07,895 Val Step[0500/1563], Avg Loss: 1.0762, Avg Acc@1: 0.7684, Avg Acc@5: 0.9339
2022-01-21 08:17:10,061 Val Step[0550/1563], Avg Loss: 1.0778, Avg Acc@1: 0.7675, Avg Acc@5: 0.9343
2022-01-21 08:17:12,306 Val Step[0600/1563], Avg Loss: 1.0776, Avg Acc@1: 0.7667, Avg Acc@5: 0.9340
2022-01-21 08:17:14,378 Val Step[0650/1563], Avg Loss: 1.0788, Avg Acc@1: 0.7665, Avg Acc@5: 0.9339
2022-01-21 08:17:16,546 Val Step[0700/1563], Avg Loss: 1.0772, Avg Acc@1: 0.7673, Avg Acc@5: 0.9345
2022-01-21 08:17:18,607 Val Step[0750/1563], Avg Loss: 1.0826, Avg Acc@1: 0.7652, Avg Acc@5: 0.9340
2022-01-21 08:17:20,767 Val Step[0800/1563], Avg Loss: 1.0828, Avg Acc@1: 0.7654, Avg Acc@5: 0.9336
2022-01-21 08:17:23,049 Val Step[0850/1563], Avg Loss: 1.0842, Avg Acc@1: 0.7651, Avg Acc@5: 0.9329
2022-01-21 08:17:25,272 Val Step[0900/1563], Avg Loss: 1.0810, Avg Acc@1: 0.7654, Avg Acc@5: 0.9332
2022-01-21 08:17:27,407 Val Step[0950/1563], Avg Loss: 1.0798, Avg Acc@1: 0.7664, Avg Acc@5: 0.9332
2022-01-21 08:17:29,473 Val Step[1000/1563], Avg Loss: 1.0817, Avg Acc@1: 0.7660, Avg Acc@5: 0.9325
2022-01-21 08:17:31,593 Val Step[1050/1563], Avg Loss: 1.0835, Avg Acc@1: 0.7651, Avg Acc@5: 0.9321
2022-01-21 08:17:33,821 Val Step[1100/1563], Avg Loss: 1.0838, Avg Acc@1: 0.7647, Avg Acc@5: 0.9321
2022-01-21 08:17:36,021 Val Step[1150/1563], Avg Loss: 1.0819, Avg Acc@1: 0.7647, Avg Acc@5: 0.9322
2022-01-21 08:17:38,397 Val Step[1200/1563], Avg Loss: 1.0810, Avg Acc@1: 0.7654, Avg Acc@5: 0.9323
2022-01-21 08:17:40,676 Val Step[1250/1563], Avg Loss: 1.0803, Avg Acc@1: 0.7649, Avg Acc@5: 0.9327
2022-01-21 08:17:42,920 Val Step[1300/1563], Avg Loss: 1.0834, Avg Acc@1: 0.7644, Avg Acc@5: 0.9323
2022-01-21 08:17:45,140 Val Step[1350/1563], Avg Loss: 1.0851, Avg Acc@1: 0.7637, Avg Acc@5: 0.9320
2022-01-21 08:17:47,451 Val Step[1400/1563], Avg Loss: 1.0843, Avg Acc@1: 0.7637, Avg Acc@5: 0.9321
2022-01-21 08:17:49,813 Val Step[1450/1563], Avg Loss: 1.0829, Avg Acc@1: 0.7641, Avg Acc@5: 0.9321
2022-01-21 08:17:52,145 Val Step[1500/1563], Avg Loss: 1.0829, Avg Acc@1: 0.7643, Avg Acc@5: 0.9324
2022-01-21 08:17:54,354 Val Step[1550/1563], Avg Loss: 1.0833, Avg Acc@1: 0.7641, Avg Acc@5: 0.9323
2022-01-21 08:17:56,484 ----- Epoch[240/300], Validation Loss: 1.0832, Validation Acc@1: 0.7641, Validation Acc@5: 0.9323, time: 164.41
2022-01-21 08:17:57,887 the pre best model acc:0.7616, at epoch 238
2022-01-21 08:17:57,887 current best model acc:0.7641, at epoch 240
2022-01-21 08:17:57,887 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 08:17:57,887 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 08:17:57,887 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 08:17:57,887 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 08:17:57,888 Now training epoch 241. LR=0.000115
2022-01-21 08:19:56,256 Epoch[241/300], Step[0000/1252], Avg Loss: 3.4017, Avg Acc: 0.5254
2022-01-21 08:21:30,186 Epoch[241/300], Step[0050/1252], Avg Loss: 3.0275, Avg Acc: 0.4861
2022-01-21 08:23:02,226 Epoch[241/300], Step[0100/1252], Avg Loss: 3.0651, Avg Acc: 0.4694
2022-01-21 08:24:34,013 Epoch[241/300], Step[0150/1252], Avg Loss: 3.0593, Avg Acc: 0.4755
2022-01-21 08:26:05,751 Epoch[241/300], Step[0200/1252], Avg Loss: 3.0413, Avg Acc: 0.4785
2022-01-21 08:27:39,127 Epoch[241/300], Step[0250/1252], Avg Loss: 3.0429, Avg Acc: 0.4711
2022-01-21 08:29:11,724 Epoch[241/300], Step[0300/1252], Avg Loss: 3.0425, Avg Acc: 0.4738
2022-01-21 08:30:45,814 Epoch[241/300], Step[0350/1252], Avg Loss: 3.0397, Avg Acc: 0.4733
2022-01-21 08:32:18,340 Epoch[241/300], Step[0400/1252], Avg Loss: 3.0311, Avg Acc: 0.4732
2022-01-21 08:33:51,597 Epoch[241/300], Step[0450/1252], Avg Loss: 3.0246, Avg Acc: 0.4732
2022-01-21 08:35:24,635 Epoch[241/300], Step[0500/1252], Avg Loss: 3.0240, Avg Acc: 0.4725
2022-01-21 08:36:58,377 Epoch[241/300], Step[0550/1252], Avg Loss: 3.0269, Avg Acc: 0.4720
2022-01-21 08:38:32,348 Epoch[241/300], Step[0600/1252], Avg Loss: 3.0335, Avg Acc: 0.4681
2022-01-21 08:40:05,566 Epoch[241/300], Step[0650/1252], Avg Loss: 3.0399, Avg Acc: 0.4701
2022-01-21 08:41:38,530 Epoch[241/300], Step[0700/1252], Avg Loss: 3.0475, Avg Acc: 0.4698
2022-01-21 08:43:11,180 Epoch[241/300], Step[0750/1252], Avg Loss: 3.0498, Avg Acc: 0.4707
2022-01-21 08:44:43,801 Epoch[241/300], Step[0800/1252], Avg Loss: 3.0511, Avg Acc: 0.4700
2022-01-21 08:46:16,474 Epoch[241/300], Step[0850/1252], Avg Loss: 3.0511, Avg Acc: 0.4693
2022-01-21 08:47:48,317 Epoch[241/300], Step[0900/1252], Avg Loss: 3.0503, Avg Acc: 0.4712
2022-01-21 08:49:22,171 Epoch[241/300], Step[0950/1252], Avg Loss: 3.0523, Avg Acc: 0.4728
2022-01-21 08:50:54,701 Epoch[241/300], Step[1000/1252], Avg Loss: 3.0520, Avg Acc: 0.4712
2022-01-21 08:52:27,501 Epoch[241/300], Step[1050/1252], Avg Loss: 3.0511, Avg Acc: 0.4713
2022-01-21 08:54:01,525 Epoch[241/300], Step[1100/1252], Avg Loss: 3.0496, Avg Acc: 0.4707
2022-01-21 08:55:34,413 Epoch[241/300], Step[1150/1252], Avg Loss: 3.0498, Avg Acc: 0.4710
2022-01-21 08:57:07,691 Epoch[241/300], Step[1200/1252], Avg Loss: 3.0504, Avg Acc: 0.4714
2022-01-21 08:58:41,880 Epoch[241/300], Step[1250/1252], Avg Loss: 3.0491, Avg Acc: 0.4705
2022-01-21 08:58:48,189 ----- Epoch[241/300], Train Loss: 3.0490, Train Acc: 0.4705, time: 2450.30, Best Val(epoch240) Acc@1: 0.7641
2022-01-21 08:58:48,189 Now training epoch 242. LR=0.000111
2022-01-21 09:00:48,830 Epoch[242/300], Step[0000/1252], Avg Loss: 2.9696, Avg Acc: 0.4727
2022-01-21 09:02:23,199 Epoch[242/300], Step[0050/1252], Avg Loss: 2.9977, Avg Acc: 0.4847
2022-01-21 09:03:55,612 Epoch[242/300], Step[0100/1252], Avg Loss: 3.0252, Avg Acc: 0.4796
2022-01-21 09:05:26,681 Epoch[242/300], Step[0150/1252], Avg Loss: 3.0350, Avg Acc: 0.4647
2022-01-21 09:06:59,450 Epoch[242/300], Step[0200/1252], Avg Loss: 3.0377, Avg Acc: 0.4642
2022-01-21 09:08:34,495 Epoch[242/300], Step[0250/1252], Avg Loss: 3.0396, Avg Acc: 0.4643
2022-01-21 09:10:08,809 Epoch[242/300], Step[0300/1252], Avg Loss: 3.0466, Avg Acc: 0.4648
2022-01-21 09:11:41,330 Epoch[242/300], Step[0350/1252], Avg Loss: 3.0543, Avg Acc: 0.4630
2022-01-21 09:13:15,271 Epoch[242/300], Step[0400/1252], Avg Loss: 3.0550, Avg Acc: 0.4645
2022-01-21 09:14:47,092 Epoch[242/300], Step[0450/1252], Avg Loss: 3.0587, Avg Acc: 0.4662
2022-01-21 09:16:19,112 Epoch[242/300], Step[0500/1252], Avg Loss: 3.0580, Avg Acc: 0.4675
2022-01-21 09:17:52,310 Epoch[242/300], Step[0550/1252], Avg Loss: 3.0595, Avg Acc: 0.4638
2022-01-21 09:19:26,497 Epoch[242/300], Step[0600/1252], Avg Loss: 3.0620, Avg Acc: 0.4628
2022-01-21 09:21:00,625 Epoch[242/300], Step[0650/1252], Avg Loss: 3.0613, Avg Acc: 0.4644
2022-01-21 09:22:34,454 Epoch[242/300], Step[0700/1252], Avg Loss: 3.0645, Avg Acc: 0.4634
2022-01-21 09:24:06,944 Epoch[242/300], Step[0750/1252], Avg Loss: 3.0612, Avg Acc: 0.4637
2022-01-21 09:25:38,726 Epoch[242/300], Step[0800/1252], Avg Loss: 3.0669, Avg Acc: 0.4640
2022-01-21 09:27:11,239 Epoch[242/300], Step[0850/1252], Avg Loss: 3.0684, Avg Acc: 0.4644
2022-01-21 09:28:44,861 Epoch[242/300], Step[0900/1252], Avg Loss: 3.0662, Avg Acc: 0.4654
2022-01-21 09:30:18,768 Epoch[242/300], Step[0950/1252], Avg Loss: 3.0664, Avg Acc: 0.4651
2022-01-21 09:31:51,047 Epoch[242/300], Step[1000/1252], Avg Loss: 3.0642, Avg Acc: 0.4657
2022-01-21 09:33:23,525 Epoch[242/300], Step[1050/1252], Avg Loss: 3.0627, Avg Acc: 0.4669
2022-01-21 09:34:56,467 Epoch[242/300], Step[1100/1252], Avg Loss: 3.0611, Avg Acc: 0.4668
2022-01-21 09:36:30,750 Epoch[242/300], Step[1150/1252], Avg Loss: 3.0605, Avg Acc: 0.4670
2022-01-21 09:38:04,886 Epoch[242/300], Step[1200/1252], Avg Loss: 3.0607, Avg Acc: 0.4671
2022-01-21 09:39:36,923 Epoch[242/300], Step[1250/1252], Avg Loss: 3.0583, Avg Acc: 0.4685
2022-01-21 09:39:43,124 ----- Epoch[242/300], Train Loss: 3.0582, Train Acc: 0.4684, time: 2454.93, Best Val(epoch240) Acc@1: 0.7641
2022-01-21 09:39:43,124 ----- Validation after Epoch: 242
2022-01-21 09:41:24,448 Val Step[0000/1563], Avg Loss: 0.8723, Avg Acc@1: 0.8438, Avg Acc@5: 0.9688
2022-01-21 09:41:26,705 Val Step[0050/1563], Avg Loss: 1.0449, Avg Acc@1: 0.7806, Avg Acc@5: 0.9314
2022-01-21 09:41:28,944 Val Step[0100/1563], Avg Loss: 1.0727, Avg Acc@1: 0.7766, Avg Acc@5: 0.9307
2022-01-21 09:41:31,076 Val Step[0150/1563], Avg Loss: 1.0816, Avg Acc@1: 0.7734, Avg Acc@5: 0.9294
2022-01-21 09:41:33,361 Val Step[0200/1563], Avg Loss: 1.0787, Avg Acc@1: 0.7738, Avg Acc@5: 0.9310
2022-01-21 09:41:35,569 Val Step[0250/1563], Avg Loss: 1.0681, Avg Acc@1: 0.7742, Avg Acc@5: 0.9329
2022-01-21 09:41:37,740 Val Step[0300/1563], Avg Loss: 1.0697, Avg Acc@1: 0.7736, Avg Acc@5: 0.9319
2022-01-21 09:41:39,897 Val Step[0350/1563], Avg Loss: 1.0751, Avg Acc@1: 0.7720, Avg Acc@5: 0.9317
2022-01-21 09:41:42,074 Val Step[0400/1563], Avg Loss: 1.0731, Avg Acc@1: 0.7721, Avg Acc@5: 0.9317
2022-01-21 09:41:44,149 Val Step[0450/1563], Avg Loss: 1.0770, Avg Acc@1: 0.7702, Avg Acc@5: 0.9318
2022-01-21 09:41:46,209 Val Step[0500/1563], Avg Loss: 1.0769, Avg Acc@1: 0.7701, Avg Acc@5: 0.9326
2022-01-21 09:41:48,313 Val Step[0550/1563], Avg Loss: 1.0789, Avg Acc@1: 0.7685, Avg Acc@5: 0.9330
2022-01-21 09:41:50,480 Val Step[0600/1563], Avg Loss: 1.0784, Avg Acc@1: 0.7677, Avg Acc@5: 0.9329
2022-01-21 09:41:52,743 Val Step[0650/1563], Avg Loss: 1.0782, Avg Acc@1: 0.7672, Avg Acc@5: 0.9332
2022-01-21 09:41:54,941 Val Step[0700/1563], Avg Loss: 1.0765, Avg Acc@1: 0.7672, Avg Acc@5: 0.9342
2022-01-21 09:41:57,249 Val Step[0750/1563], Avg Loss: 1.0833, Avg Acc@1: 0.7648, Avg Acc@5: 0.9334
2022-01-21 09:41:59,354 Val Step[0800/1563], Avg Loss: 1.0838, Avg Acc@1: 0.7655, Avg Acc@5: 0.9328
2022-01-21 09:42:01,540 Val Step[0850/1563], Avg Loss: 1.0851, Avg Acc@1: 0.7650, Avg Acc@5: 0.9323
2022-01-21 09:42:03,843 Val Step[0900/1563], Avg Loss: 1.0818, Avg Acc@1: 0.7658, Avg Acc@5: 0.9327
2022-01-21 09:42:06,105 Val Step[0950/1563], Avg Loss: 1.0810, Avg Acc@1: 0.7663, Avg Acc@5: 0.9328
2022-01-21 09:42:08,364 Val Step[1000/1563], Avg Loss: 1.0827, Avg Acc@1: 0.7658, Avg Acc@5: 0.9327
2022-01-21 09:42:10,694 Val Step[1050/1563], Avg Loss: 1.0842, Avg Acc@1: 0.7649, Avg Acc@5: 0.9324
2022-01-21 09:42:12,952 Val Step[1100/1563], Avg Loss: 1.0842, Avg Acc@1: 0.7649, Avg Acc@5: 0.9323
2022-01-21 09:42:15,223 Val Step[1150/1563], Avg Loss: 1.0827, Avg Acc@1: 0.7651, Avg Acc@5: 0.9329
2022-01-21 09:42:17,352 Val Step[1200/1563], Avg Loss: 1.0813, Avg Acc@1: 0.7655, Avg Acc@5: 0.9330
2022-01-21 09:42:19,477 Val Step[1250/1563], Avg Loss: 1.0802, Avg Acc@1: 0.7656, Avg Acc@5: 0.9333
2022-01-21 09:42:21,580 Val Step[1300/1563], Avg Loss: 1.0825, Avg Acc@1: 0.7653, Avg Acc@5: 0.9331
2022-01-21 09:42:23,684 Val Step[1350/1563], Avg Loss: 1.0842, Avg Acc@1: 0.7648, Avg Acc@5: 0.9329
2022-01-21 09:42:25,855 Val Step[1400/1563], Avg Loss: 1.0833, Avg Acc@1: 0.7646, Avg Acc@5: 0.9332
2022-01-21 09:42:28,045 Val Step[1450/1563], Avg Loss: 1.0823, Avg Acc@1: 0.7650, Avg Acc@5: 0.9333
2022-01-21 09:42:30,339 Val Step[1500/1563], Avg Loss: 1.0821, Avg Acc@1: 0.7654, Avg Acc@5: 0.9333
2022-01-21 09:42:32,513 Val Step[1550/1563], Avg Loss: 1.0826, Avg Acc@1: 0.7651, Avg Acc@5: 0.9331
2022-01-21 09:42:34,464 ----- Epoch[242/300], Validation Loss: 1.0826, Validation Acc@1: 0.7651, Validation Acc@5: 0.9331, time: 171.34
2022-01-21 09:42:35,657 the pre best model acc:0.7641, at epoch 240
2022-01-21 09:42:35,657 current best model acc:0.7651, at epoch 242
2022-01-21 09:42:35,658 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 09:42:35,658 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 09:42:35,658 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 09:42:35,658 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 09:42:35,658 Now training epoch 243. LR=0.000108
2022-01-21 09:44:38,309 Epoch[243/300], Step[0000/1252], Avg Loss: 2.9773, Avg Acc: 0.7012
2022-01-21 09:46:11,340 Epoch[243/300], Step[0050/1252], Avg Loss: 3.0505, Avg Acc: 0.4695
2022-01-21 09:47:45,155 Epoch[243/300], Step[0100/1252], Avg Loss: 3.0639, Avg Acc: 0.4709
2022-01-21 09:49:18,631 Epoch[243/300], Step[0150/1252], Avg Loss: 3.0545, Avg Acc: 0.4707
2022-01-21 09:50:52,125 Epoch[243/300], Step[0200/1252], Avg Loss: 3.0489, Avg Acc: 0.4621
2022-01-21 09:52:23,655 Epoch[243/300], Step[0250/1252], Avg Loss: 3.0531, Avg Acc: 0.4588
2022-01-21 09:53:55,848 Epoch[243/300], Step[0300/1252], Avg Loss: 3.0491, Avg Acc: 0.4637
2022-01-21 09:55:28,365 Epoch[243/300], Step[0350/1252], Avg Loss: 3.0478, Avg Acc: 0.4656
2022-01-21 09:57:01,140 Epoch[243/300], Step[0400/1252], Avg Loss: 3.0542, Avg Acc: 0.4625
2022-01-21 09:58:33,972 Epoch[243/300], Step[0450/1252], Avg Loss: 3.0655, Avg Acc: 0.4613
2022-01-21 10:00:07,870 Epoch[243/300], Step[0500/1252], Avg Loss: 3.0621, Avg Acc: 0.4626
2022-01-21 10:01:39,890 Epoch[243/300], Step[0550/1252], Avg Loss: 3.0608, Avg Acc: 0.4643
2022-01-21 10:03:11,771 Epoch[243/300], Step[0600/1252], Avg Loss: 3.0595, Avg Acc: 0.4673
2022-01-21 10:04:46,125 Epoch[243/300], Step[0650/1252], Avg Loss: 3.0597, Avg Acc: 0.4662
2022-01-21 10:06:21,107 Epoch[243/300], Step[0700/1252], Avg Loss: 3.0572, Avg Acc: 0.4672
2022-01-21 10:07:54,720 Epoch[243/300], Step[0750/1252], Avg Loss: 3.0577, Avg Acc: 0.4662
2022-01-21 10:09:27,757 Epoch[243/300], Step[0800/1252], Avg Loss: 3.0539, Avg Acc: 0.4645
2022-01-21 10:11:01,681 Epoch[243/300], Step[0850/1252], Avg Loss: 3.0558, Avg Acc: 0.4642
2022-01-21 10:12:35,261 Epoch[243/300], Step[0900/1252], Avg Loss: 3.0563, Avg Acc: 0.4643
2022-01-21 10:14:08,158 Epoch[243/300], Step[0950/1252], Avg Loss: 3.0552, Avg Acc: 0.4635
2022-01-21 10:15:41,242 Epoch[243/300], Step[1000/1252], Avg Loss: 3.0545, Avg Acc: 0.4645
2022-01-21 10:17:14,346 Epoch[243/300], Step[1050/1252], Avg Loss: 3.0568, Avg Acc: 0.4639
2022-01-21 10:18:47,735 Epoch[243/300], Step[1100/1252], Avg Loss: 3.0575, Avg Acc: 0.4630
2022-01-21 10:20:18,962 Epoch[243/300], Step[1150/1252], Avg Loss: 3.0587, Avg Acc: 0.4632
2022-01-21 10:21:51,638 Epoch[243/300], Step[1200/1252], Avg Loss: 3.0587, Avg Acc: 0.4641
2022-01-21 10:23:24,001 Epoch[243/300], Step[1250/1252], Avg Loss: 3.0608, Avg Acc: 0.4639
2022-01-21 10:23:30,735 ----- Epoch[243/300], Train Loss: 3.0608, Train Acc: 0.4639, time: 2455.07, Best Val(epoch242) Acc@1: 0.7651
2022-01-21 10:23:30,735 Now training epoch 244. LR=0.000105
2022-01-21 10:25:40,837 Epoch[244/300], Step[0000/1252], Avg Loss: 2.7330, Avg Acc: 0.3730
2022-01-21 10:27:15,733 Epoch[244/300], Step[0050/1252], Avg Loss: 2.9806, Avg Acc: 0.4555
2022-01-21 10:28:49,613 Epoch[244/300], Step[0100/1252], Avg Loss: 3.0146, Avg Acc: 0.4558
2022-01-21 10:30:21,841 Epoch[244/300], Step[0150/1252], Avg Loss: 3.0278, Avg Acc: 0.4629
2022-01-21 10:31:55,254 Epoch[244/300], Step[0200/1252], Avg Loss: 3.0195, Avg Acc: 0.4679
2022-01-21 10:33:25,944 Epoch[244/300], Step[0250/1252], Avg Loss: 3.0148, Avg Acc: 0.4747
2022-01-21 10:34:59,648 Epoch[244/300], Step[0300/1252], Avg Loss: 3.0145, Avg Acc: 0.4743
2022-01-21 10:36:31,427 Epoch[244/300], Step[0350/1252], Avg Loss: 3.0083, Avg Acc: 0.4770
2022-01-21 10:38:04,362 Epoch[244/300], Step[0400/1252], Avg Loss: 3.0091, Avg Acc: 0.4742
2022-01-21 10:39:37,925 Epoch[244/300], Step[0450/1252], Avg Loss: 3.0096, Avg Acc: 0.4713
2022-01-21 10:41:11,302 Epoch[244/300], Step[0500/1252], Avg Loss: 3.0158, Avg Acc: 0.4708
2022-01-21 10:42:44,237 Epoch[244/300], Step[0550/1252], Avg Loss: 3.0176, Avg Acc: 0.4720
2022-01-21 10:44:17,067 Epoch[244/300], Step[0600/1252], Avg Loss: 3.0224, Avg Acc: 0.4703
2022-01-21 10:45:52,029 Epoch[244/300], Step[0650/1252], Avg Loss: 3.0249, Avg Acc: 0.4699
2022-01-21 10:47:24,429 Epoch[244/300], Step[0700/1252], Avg Loss: 3.0321, Avg Acc: 0.4686
2022-01-21 10:48:57,615 Epoch[244/300], Step[0750/1252], Avg Loss: 3.0327, Avg Acc: 0.4679
2022-01-21 10:50:31,529 Epoch[244/300], Step[0800/1252], Avg Loss: 3.0302, Avg Acc: 0.4686
2022-01-21 10:52:03,350 Epoch[244/300], Step[0850/1252], Avg Loss: 3.0302, Avg Acc: 0.4689
2022-01-21 10:53:36,954 Epoch[244/300], Step[0900/1252], Avg Loss: 3.0324, Avg Acc: 0.4694
2022-01-21 10:55:08,406 Epoch[244/300], Step[0950/1252], Avg Loss: 3.0326, Avg Acc: 0.4695
2022-01-21 10:56:40,621 Epoch[244/300], Step[1000/1252], Avg Loss: 3.0299, Avg Acc: 0.4697
2022-01-21 10:58:12,914 Epoch[244/300], Step[1050/1252], Avg Loss: 3.0307, Avg Acc: 0.4693
2022-01-21 10:59:44,606 Epoch[244/300], Step[1100/1252], Avg Loss: 3.0307, Avg Acc: 0.4688
2022-01-21 11:01:18,156 Epoch[244/300], Step[1150/1252], Avg Loss: 3.0331, Avg Acc: 0.4675
2022-01-21 11:02:51,369 Epoch[244/300], Step[1200/1252], Avg Loss: 3.0347, Avg Acc: 0.4671
2022-01-21 11:04:23,523 Epoch[244/300], Step[1250/1252], Avg Loss: 3.0328, Avg Acc: 0.4676
2022-01-21 11:04:30,294 ----- Epoch[244/300], Train Loss: 3.0328, Train Acc: 0.4676, time: 2459.55, Best Val(epoch242) Acc@1: 0.7651
2022-01-21 11:04:30,294 ----- Validation after Epoch: 244
2022-01-21 11:06:04,992 Val Step[0000/1563], Avg Loss: 0.7598, Avg Acc@1: 0.8125, Avg Acc@5: 1.0000
2022-01-21 11:06:07,681 Val Step[0050/1563], Avg Loss: 1.0139, Avg Acc@1: 0.7825, Avg Acc@5: 0.9363
2022-01-21 11:06:09,851 Val Step[0100/1563], Avg Loss: 1.0422, Avg Acc@1: 0.7729, Avg Acc@5: 0.9335
2022-01-21 11:06:12,091 Val Step[0150/1563], Avg Loss: 1.0496, Avg Acc@1: 0.7707, Avg Acc@5: 0.9327
2022-01-21 11:06:14,258 Val Step[0200/1563], Avg Loss: 1.0538, Avg Acc@1: 0.7707, Avg Acc@5: 0.9328
2022-01-21 11:06:16,384 Val Step[0250/1563], Avg Loss: 1.0432, Avg Acc@1: 0.7722, Avg Acc@5: 0.9345
2022-01-21 11:06:18,432 Val Step[0300/1563], Avg Loss: 1.0441, Avg Acc@1: 0.7718, Avg Acc@5: 0.9345
2022-01-21 11:06:20,555 Val Step[0350/1563], Avg Loss: 1.0504, Avg Acc@1: 0.7704, Avg Acc@5: 0.9334
2022-01-21 11:06:22,745 Val Step[0400/1563], Avg Loss: 1.0486, Avg Acc@1: 0.7708, Avg Acc@5: 0.9330
2022-01-21 11:06:24,902 Val Step[0450/1563], Avg Loss: 1.0530, Avg Acc@1: 0.7681, Avg Acc@5: 0.9328
2022-01-21 11:06:27,006 Val Step[0500/1563], Avg Loss: 1.0535, Avg Acc@1: 0.7674, Avg Acc@5: 0.9333
2022-01-21 11:06:29,308 Val Step[0550/1563], Avg Loss: 1.0548, Avg Acc@1: 0.7668, Avg Acc@5: 0.9331
2022-01-21 11:06:31,428 Val Step[0600/1563], Avg Loss: 1.0536, Avg Acc@1: 0.7671, Avg Acc@5: 0.9333
2022-01-21 11:06:33,716 Val Step[0650/1563], Avg Loss: 1.0534, Avg Acc@1: 0.7670, Avg Acc@5: 0.9332
2022-01-21 11:06:36,109 Val Step[0700/1563], Avg Loss: 1.0519, Avg Acc@1: 0.7670, Avg Acc@5: 0.9340
2022-01-21 11:06:38,321 Val Step[0750/1563], Avg Loss: 1.0574, Avg Acc@1: 0.7652, Avg Acc@5: 0.9332
2022-01-21 11:06:40,610 Val Step[0800/1563], Avg Loss: 1.0577, Avg Acc@1: 0.7656, Avg Acc@5: 0.9331
2022-01-21 11:06:42,875 Val Step[0850/1563], Avg Loss: 1.0589, Avg Acc@1: 0.7652, Avg Acc@5: 0.9327
2022-01-21 11:06:45,193 Val Step[0900/1563], Avg Loss: 1.0561, Avg Acc@1: 0.7657, Avg Acc@5: 0.9330
2022-01-21 11:06:47,440 Val Step[0950/1563], Avg Loss: 1.0559, Avg Acc@1: 0.7660, Avg Acc@5: 0.9329
2022-01-21 11:06:49,496 Val Step[1000/1563], Avg Loss: 1.0567, Avg Acc@1: 0.7660, Avg Acc@5: 0.9329
2022-01-21 11:06:51,677 Val Step[1050/1563], Avg Loss: 1.0583, Avg Acc@1: 0.7654, Avg Acc@5: 0.9325
2022-01-21 11:06:53,739 Val Step[1100/1563], Avg Loss: 1.0585, Avg Acc@1: 0.7652, Avg Acc@5: 0.9326
2022-01-21 11:06:55,887 Val Step[1150/1563], Avg Loss: 1.0573, Avg Acc@1: 0.7653, Avg Acc@5: 0.9328
2022-01-21 11:06:58,064 Val Step[1200/1563], Avg Loss: 1.0561, Avg Acc@1: 0.7658, Avg Acc@5: 0.9328
2022-01-21 11:07:00,171 Val Step[1250/1563], Avg Loss: 1.0556, Avg Acc@1: 0.7655, Avg Acc@5: 0.9331
2022-01-21 11:07:02,315 Val Step[1300/1563], Avg Loss: 1.0586, Avg Acc@1: 0.7652, Avg Acc@5: 0.9327
2022-01-21 11:07:04,408 Val Step[1350/1563], Avg Loss: 1.0606, Avg Acc@1: 0.7648, Avg Acc@5: 0.9322
2022-01-21 11:07:06,658 Val Step[1400/1563], Avg Loss: 1.0603, Avg Acc@1: 0.7645, Avg Acc@5: 0.9322
2022-01-21 11:07:08,749 Val Step[1450/1563], Avg Loss: 1.0597, Avg Acc@1: 0.7646, Avg Acc@5: 0.9322
2022-01-21 11:07:10,874 Val Step[1500/1563], Avg Loss: 1.0599, Avg Acc@1: 0.7648, Avg Acc@5: 0.9325
2022-01-21 11:07:12,870 Val Step[1550/1563], Avg Loss: 1.0603, Avg Acc@1: 0.7645, Avg Acc@5: 0.9323
2022-01-21 11:07:14,826 ----- Epoch[244/300], Validation Loss: 1.0601, Validation Acc@1: 0.7645, Validation Acc@5: 0.9324, time: 164.53
2022-01-21 11:07:14,826 Now training epoch 245. LR=0.000101
2022-01-21 11:09:18,486 Epoch[245/300], Step[0000/1252], Avg Loss: 3.7080, Avg Acc: 0.4131
2022-01-21 11:10:50,704 Epoch[245/300], Step[0050/1252], Avg Loss: 2.9667, Avg Acc: 0.4715
2022-01-21 11:12:22,540 Epoch[245/300], Step[0100/1252], Avg Loss: 3.0232, Avg Acc: 0.4735
2022-01-21 11:13:55,262 Epoch[245/300], Step[0150/1252], Avg Loss: 3.0126, Avg Acc: 0.4763
2022-01-21 11:15:26,438 Epoch[245/300], Step[0200/1252], Avg Loss: 3.0233, Avg Acc: 0.4725
2022-01-21 11:17:00,147 Epoch[245/300], Step[0250/1252], Avg Loss: 3.0259, Avg Acc: 0.4648
2022-01-21 11:18:32,966 Epoch[245/300], Step[0300/1252], Avg Loss: 3.0264, Avg Acc: 0.4622
2022-01-21 11:20:05,271 Epoch[245/300], Step[0350/1252], Avg Loss: 3.0246, Avg Acc: 0.4680
2022-01-21 11:21:38,211 Epoch[245/300], Step[0400/1252], Avg Loss: 3.0267, Avg Acc: 0.4664
2022-01-21 11:23:11,333 Epoch[245/300], Step[0450/1252], Avg Loss: 3.0298, Avg Acc: 0.4649
2022-01-21 11:24:47,185 Epoch[245/300], Step[0500/1252], Avg Loss: 3.0338, Avg Acc: 0.4635
2022-01-21 11:26:20,812 Epoch[245/300], Step[0550/1252], Avg Loss: 3.0389, Avg Acc: 0.4636
2022-01-21 11:27:53,041 Epoch[245/300], Step[0600/1252], Avg Loss: 3.0377, Avg Acc: 0.4660
2022-01-21 11:29:25,974 Epoch[245/300], Step[0650/1252], Avg Loss: 3.0409, Avg Acc: 0.4641
2022-01-21 11:31:00,048 Epoch[245/300], Step[0700/1252], Avg Loss: 3.0447, Avg Acc: 0.4642
2022-01-21 11:32:32,636 Epoch[245/300], Step[0750/1252], Avg Loss: 3.0479, Avg Acc: 0.4629
2022-01-21 11:34:06,096 Epoch[245/300], Step[0800/1252], Avg Loss: 3.0446, Avg Acc: 0.4649
2022-01-21 11:35:39,842 Epoch[245/300], Step[0850/1252], Avg Loss: 3.0443, Avg Acc: 0.4652
2022-01-21 11:37:14,043 Epoch[245/300], Step[0900/1252], Avg Loss: 3.0441, Avg Acc: 0.4652
2022-01-21 11:38:48,076 Epoch[245/300], Step[0950/1252], Avg Loss: 3.0450, Avg Acc: 0.4646
2022-01-21 11:40:19,608 Epoch[245/300], Step[1000/1252], Avg Loss: 3.0457, Avg Acc: 0.4634
2022-01-21 11:41:54,032 Epoch[245/300], Step[1050/1252], Avg Loss: 3.0469, Avg Acc: 0.4640
2022-01-21 11:43:27,374 Epoch[245/300], Step[1100/1252], Avg Loss: 3.0459, Avg Acc: 0.4635
2022-01-21 11:45:02,600 Epoch[245/300], Step[1150/1252], Avg Loss: 3.0482, Avg Acc: 0.4632
2022-01-21 11:46:34,929 Epoch[245/300], Step[1200/1252], Avg Loss: 3.0484, Avg Acc: 0.4634
2022-01-21 11:48:07,577 Epoch[245/300], Step[1250/1252], Avg Loss: 3.0466, Avg Acc: 0.4633
2022-01-21 11:48:13,632 ----- Epoch[245/300], Train Loss: 3.0466, Train Acc: 0.4633, time: 2458.80, Best Val(epoch242) Acc@1: 0.7651
2022-01-21 11:48:13,632 Now training epoch 246. LR=0.000098
2022-01-21 11:50:16,061 Epoch[246/300], Step[0000/1252], Avg Loss: 3.3837, Avg Acc: 0.4990
2022-01-21 11:51:48,241 Epoch[246/300], Step[0050/1252], Avg Loss: 3.1114, Avg Acc: 0.4602
2022-01-21 11:53:19,842 Epoch[246/300], Step[0100/1252], Avg Loss: 3.0641, Avg Acc: 0.4698
2022-01-21 11:54:51,742 Epoch[246/300], Step[0150/1252], Avg Loss: 3.0603, Avg Acc: 0.4693
2022-01-21 11:56:24,190 Epoch[246/300], Step[0200/1252], Avg Loss: 3.0608, Avg Acc: 0.4645
2022-01-21 11:57:56,627 Epoch[246/300], Step[0250/1252], Avg Loss: 3.0694, Avg Acc: 0.4676
2022-01-21 11:59:28,335 Epoch[246/300], Step[0300/1252], Avg Loss: 3.0714, Avg Acc: 0.4600
2022-01-21 12:01:01,313 Epoch[246/300], Step[0350/1252], Avg Loss: 3.0663, Avg Acc: 0.4593
2022-01-21 12:02:34,926 Epoch[246/300], Step[0400/1252], Avg Loss: 3.0622, Avg Acc: 0.4562
2022-01-21 12:04:09,312 Epoch[246/300], Step[0450/1252], Avg Loss: 3.0627, Avg Acc: 0.4600
2022-01-21 12:05:42,092 Epoch[246/300], Step[0500/1252], Avg Loss: 3.0581, Avg Acc: 0.4599
2022-01-21 12:07:14,894 Epoch[246/300], Step[0550/1252], Avg Loss: 3.0603, Avg Acc: 0.4603
2022-01-21 12:08:48,658 Epoch[246/300], Step[0600/1252], Avg Loss: 3.0588, Avg Acc: 0.4621
2022-01-21 12:10:20,625 Epoch[246/300], Step[0650/1252], Avg Loss: 3.0592, Avg Acc: 0.4624
2022-01-21 12:11:52,429 Epoch[246/300], Step[0700/1252], Avg Loss: 3.0578, Avg Acc: 0.4627
2022-01-21 12:13:26,242 Epoch[246/300], Step[0750/1252], Avg Loss: 3.0530, Avg Acc: 0.4596
2022-01-21 12:14:58,674 Epoch[246/300], Step[0800/1252], Avg Loss: 3.0551, Avg Acc: 0.4601
2022-01-21 12:16:33,339 Epoch[246/300], Step[0850/1252], Avg Loss: 3.0535, Avg Acc: 0.4588
2022-01-21 12:18:07,402 Epoch[246/300], Step[0900/1252], Avg Loss: 3.0527, Avg Acc: 0.4604
2022-01-21 12:19:39,906 Epoch[246/300], Step[0950/1252], Avg Loss: 3.0547, Avg Acc: 0.4599
2022-01-21 12:21:12,062 Epoch[246/300], Step[1000/1252], Avg Loss: 3.0556, Avg Acc: 0.4598
2022-01-21 12:22:46,129 Epoch[246/300], Step[1050/1252], Avg Loss: 3.0509, Avg Acc: 0.4591
2022-01-21 12:24:21,008 Epoch[246/300], Step[1100/1252], Avg Loss: 3.0495, Avg Acc: 0.4598
2022-01-21 12:25:52,886 Epoch[246/300], Step[1150/1252], Avg Loss: 3.0471, Avg Acc: 0.4609
2022-01-21 12:27:26,559 Epoch[246/300], Step[1200/1252], Avg Loss: 3.0472, Avg Acc: 0.4606
2022-01-21 12:28:58,629 Epoch[246/300], Step[1250/1252], Avg Loss: 3.0456, Avg Acc: 0.4603
2022-01-21 12:29:05,072 ----- Epoch[246/300], Train Loss: 3.0456, Train Acc: 0.4603, time: 2451.43, Best Val(epoch242) Acc@1: 0.7651
2022-01-21 12:29:05,072 ----- Validation after Epoch: 246
2022-01-21 12:30:39,186 Val Step[0000/1563], Avg Loss: 0.8684, Avg Acc@1: 0.7812, Avg Acc@5: 1.0000
2022-01-21 12:30:41,479 Val Step[0050/1563], Avg Loss: 1.0423, Avg Acc@1: 0.7739, Avg Acc@5: 0.9326
2022-01-21 12:30:43,631 Val Step[0100/1563], Avg Loss: 1.0611, Avg Acc@1: 0.7738, Avg Acc@5: 0.9332
2022-01-21 12:30:45,736 Val Step[0150/1563], Avg Loss: 1.0628, Avg Acc@1: 0.7759, Avg Acc@5: 0.9303
2022-01-21 12:30:47,873 Val Step[0200/1563], Avg Loss: 1.0642, Avg Acc@1: 0.7758, Avg Acc@5: 0.9321
2022-01-21 12:30:49,992 Val Step[0250/1563], Avg Loss: 1.0557, Avg Acc@1: 0.7763, Avg Acc@5: 0.9336
2022-01-21 12:30:52,099 Val Step[0300/1563], Avg Loss: 1.0591, Avg Acc@1: 0.7740, Avg Acc@5: 0.9324
2022-01-21 12:30:54,273 Val Step[0350/1563], Avg Loss: 1.0660, Avg Acc@1: 0.7731, Avg Acc@5: 0.9322
2022-01-21 12:30:56,564 Val Step[0400/1563], Avg Loss: 1.0644, Avg Acc@1: 0.7730, Avg Acc@5: 0.9325
2022-01-21 12:30:58,780 Val Step[0450/1563], Avg Loss: 1.0681, Avg Acc@1: 0.7707, Avg Acc@5: 0.9331
2022-01-21 12:31:00,928 Val Step[0500/1563], Avg Loss: 1.0692, Avg Acc@1: 0.7703, Avg Acc@5: 0.9334
2022-01-21 12:31:03,080 Val Step[0550/1563], Avg Loss: 1.0712, Avg Acc@1: 0.7689, Avg Acc@5: 0.9333
2022-01-21 12:31:05,151 Val Step[0600/1563], Avg Loss: 1.0705, Avg Acc@1: 0.7689, Avg Acc@5: 0.9335
2022-01-21 12:31:07,327 Val Step[0650/1563], Avg Loss: 1.0708, Avg Acc@1: 0.7688, Avg Acc@5: 0.9337
2022-01-21 12:31:09,485 Val Step[0700/1563], Avg Loss: 1.0694, Avg Acc@1: 0.7694, Avg Acc@5: 0.9341
2022-01-21 12:31:11,680 Val Step[0750/1563], Avg Loss: 1.0750, Avg Acc@1: 0.7681, Avg Acc@5: 0.9335
2022-01-21 12:31:13,753 Val Step[0800/1563], Avg Loss: 1.0758, Avg Acc@1: 0.7687, Avg Acc@5: 0.9333
2022-01-21 12:31:15,765 Val Step[0850/1563], Avg Loss: 1.0774, Avg Acc@1: 0.7681, Avg Acc@5: 0.9330
2022-01-21 12:31:17,880 Val Step[0900/1563], Avg Loss: 1.0749, Avg Acc@1: 0.7683, Avg Acc@5: 0.9333
2022-01-21 12:31:20,011 Val Step[0950/1563], Avg Loss: 1.0747, Avg Acc@1: 0.7687, Avg Acc@5: 0.9332
2022-01-21 12:31:22,191 Val Step[1000/1563], Avg Loss: 1.0758, Avg Acc@1: 0.7687, Avg Acc@5: 0.9332
2022-01-21 12:31:24,337 Val Step[1050/1563], Avg Loss: 1.0766, Avg Acc@1: 0.7681, Avg Acc@5: 0.9328
2022-01-21 12:31:26,503 Val Step[1100/1563], Avg Loss: 1.0773, Avg Acc@1: 0.7676, Avg Acc@5: 0.9327
2022-01-21 12:31:28,824 Val Step[1150/1563], Avg Loss: 1.0757, Avg Acc@1: 0.7676, Avg Acc@5: 0.9329
2022-01-21 12:31:31,195 Val Step[1200/1563], Avg Loss: 1.0750, Avg Acc@1: 0.7677, Avg Acc@5: 0.9330
2022-01-21 12:31:33,497 Val Step[1250/1563], Avg Loss: 1.0743, Avg Acc@1: 0.7674, Avg Acc@5: 0.9335
2022-01-21 12:31:35,784 Val Step[1300/1563], Avg Loss: 1.0774, Avg Acc@1: 0.7671, Avg Acc@5: 0.9330
2022-01-21 12:31:38,163 Val Step[1350/1563], Avg Loss: 1.0787, Avg Acc@1: 0.7663, Avg Acc@5: 0.9328
2022-01-21 12:31:40,428 Val Step[1400/1563], Avg Loss: 1.0784, Avg Acc@1: 0.7662, Avg Acc@5: 0.9328
2022-01-21 12:31:42,652 Val Step[1450/1563], Avg Loss: 1.0775, Avg Acc@1: 0.7665, Avg Acc@5: 0.9329
2022-01-21 12:31:44,887 Val Step[1500/1563], Avg Loss: 1.0770, Avg Acc@1: 0.7667, Avg Acc@5: 0.9332
2022-01-21 12:31:47,057 Val Step[1550/1563], Avg Loss: 1.0769, Avg Acc@1: 0.7667, Avg Acc@5: 0.9331
2022-01-21 12:31:49,497 ----- Epoch[246/300], Validation Loss: 1.0770, Validation Acc@1: 0.7667, Validation Acc@5: 0.9332, time: 164.42
2022-01-21 12:31:50,683 the pre best model acc:0.7651, at epoch 242
2022-01-21 12:31:50,683 current best model acc:0.7667, at epoch 246
2022-01-21 12:31:50,684 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 12:31:50,684 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 12:31:50,684 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 12:31:50,684 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 12:31:50,684 Now training epoch 247. LR=0.000095
2022-01-21 12:33:58,846 Epoch[247/300], Step[0000/1252], Avg Loss: 2.6467, Avg Acc: 0.6914
2022-01-21 12:35:31,412 Epoch[247/300], Step[0050/1252], Avg Loss: 3.0231, Avg Acc: 0.4275
2022-01-21 12:37:02,076 Epoch[247/300], Step[0100/1252], Avg Loss: 3.0151, Avg Acc: 0.4554
2022-01-21 12:38:35,073 Epoch[247/300], Step[0150/1252], Avg Loss: 3.0182, Avg Acc: 0.4571
2022-01-21 12:40:08,132 Epoch[247/300], Step[0200/1252], Avg Loss: 3.0196, Avg Acc: 0.4623
2022-01-21 12:41:40,743 Epoch[247/300], Step[0250/1252], Avg Loss: 3.0303, Avg Acc: 0.4612
2022-01-21 12:43:11,814 Epoch[247/300], Step[0300/1252], Avg Loss: 3.0292, Avg Acc: 0.4627
2022-01-21 12:44:41,809 Epoch[247/300], Step[0350/1252], Avg Loss: 3.0385, Avg Acc: 0.4614
2022-01-21 12:46:11,695 Epoch[247/300], Step[0400/1252], Avg Loss: 3.0397, Avg Acc: 0.4650
2022-01-21 12:47:40,562 Epoch[247/300], Step[0450/1252], Avg Loss: 3.0342, Avg Acc: 0.4638
2022-01-21 12:49:08,709 Epoch[247/300], Step[0500/1252], Avg Loss: 3.0341, Avg Acc: 0.4638
2022-01-21 12:50:38,176 Epoch[247/300], Step[0550/1252], Avg Loss: 3.0355, Avg Acc: 0.4634
2022-01-21 12:52:06,664 Epoch[247/300], Step[0600/1252], Avg Loss: 3.0358, Avg Acc: 0.4650
2022-01-21 12:53:34,702 Epoch[247/300], Step[0650/1252], Avg Loss: 3.0402, Avg Acc: 0.4663
2022-01-21 12:55:02,682 Epoch[247/300], Step[0700/1252], Avg Loss: 3.0333, Avg Acc: 0.4675
2022-01-21 12:56:30,322 Epoch[247/300], Step[0750/1252], Avg Loss: 3.0318, Avg Acc: 0.4701
2022-01-21 12:57:57,485 Epoch[247/300], Step[0800/1252], Avg Loss: 3.0298, Avg Acc: 0.4708
2022-01-21 12:59:24,590 Epoch[247/300], Step[0850/1252], Avg Loss: 3.0320, Avg Acc: 0.4707
2022-01-21 13:00:51,962 Epoch[247/300], Step[0900/1252], Avg Loss: 3.0344, Avg Acc: 0.4700
2022-01-21 13:02:19,901 Epoch[247/300], Step[0950/1252], Avg Loss: 3.0358, Avg Acc: 0.4690
2022-01-21 13:03:47,550 Epoch[247/300], Step[1000/1252], Avg Loss: 3.0378, Avg Acc: 0.4693
2022-01-21 13:05:15,080 Epoch[247/300], Step[1050/1252], Avg Loss: 3.0381, Avg Acc: 0.4696
2022-01-21 13:06:44,583 Epoch[247/300], Step[1100/1252], Avg Loss: 3.0395, Avg Acc: 0.4706
2022-01-21 13:08:14,241 Epoch[247/300], Step[1150/1252], Avg Loss: 3.0405, Avg Acc: 0.4704
2022-01-21 13:09:42,399 Epoch[247/300], Step[1200/1252], Avg Loss: 3.0401, Avg Acc: 0.4723
2022-01-21 13:11:09,850 Epoch[247/300], Step[1250/1252], Avg Loss: 3.0392, Avg Acc: 0.4721
2022-01-21 13:11:17,346 ----- Epoch[247/300], Train Loss: 3.0392, Train Acc: 0.4720, time: 2366.66, Best Val(epoch246) Acc@1: 0.7667
2022-01-21 13:11:17,346 Now training epoch 248. LR=0.000092
2022-01-21 13:13:18,200 Epoch[248/300], Step[0000/1252], Avg Loss: 2.7321, Avg Acc: 0.6738
2022-01-21 13:14:47,473 Epoch[248/300], Step[0050/1252], Avg Loss: 3.0512, Avg Acc: 0.4818
2022-01-21 13:16:16,065 Epoch[248/300], Step[0100/1252], Avg Loss: 3.0611, Avg Acc: 0.4674
2022-01-21 13:17:44,100 Epoch[248/300], Step[0150/1252], Avg Loss: 3.0443, Avg Acc: 0.4602
2022-01-21 13:19:11,964 Epoch[248/300], Step[0200/1252], Avg Loss: 3.0343, Avg Acc: 0.4640
2022-01-21 13:20:40,014 Epoch[248/300], Step[0250/1252], Avg Loss: 3.0358, Avg Acc: 0.4675
2022-01-21 13:22:08,088 Epoch[248/300], Step[0300/1252], Avg Loss: 3.0297, Avg Acc: 0.4699
2022-01-21 13:23:36,608 Epoch[248/300], Step[0350/1252], Avg Loss: 3.0240, Avg Acc: 0.4712
2022-01-21 13:25:03,672 Epoch[248/300], Step[0400/1252], Avg Loss: 3.0259, Avg Acc: 0.4706
2022-01-21 13:26:32,251 Epoch[248/300], Step[0450/1252], Avg Loss: 3.0275, Avg Acc: 0.4664
2022-01-21 13:27:58,627 Epoch[248/300], Step[0500/1252], Avg Loss: 3.0331, Avg Acc: 0.4679
2022-01-21 13:29:26,189 Epoch[248/300], Step[0550/1252], Avg Loss: 3.0324, Avg Acc: 0.4690
2022-01-21 13:30:55,281 Epoch[248/300], Step[0600/1252], Avg Loss: 3.0258, Avg Acc: 0.4677
2022-01-21 13:32:22,872 Epoch[248/300], Step[0650/1252], Avg Loss: 3.0303, Avg Acc: 0.4669
2022-01-21 13:33:50,893 Epoch[248/300], Step[0700/1252], Avg Loss: 3.0319, Avg Acc: 0.4652
2022-01-21 13:35:19,092 Epoch[248/300], Step[0750/1252], Avg Loss: 3.0289, Avg Acc: 0.4684
2022-01-21 13:36:48,090 Epoch[248/300], Step[0800/1252], Avg Loss: 3.0337, Avg Acc: 0.4684
2022-01-21 13:38:16,720 Epoch[248/300], Step[0850/1252], Avg Loss: 3.0323, Avg Acc: 0.4682
2022-01-21 13:39:44,695 Epoch[248/300], Step[0900/1252], Avg Loss: 3.0372, Avg Acc: 0.4674
2022-01-21 13:41:13,382 Epoch[248/300], Step[0950/1252], Avg Loss: 3.0381, Avg Acc: 0.4672
2022-01-21 13:42:42,229 Epoch[248/300], Step[1000/1252], Avg Loss: 3.0368, Avg Acc: 0.4662
2022-01-21 13:44:10,088 Epoch[248/300], Step[1050/1252], Avg Loss: 3.0376, Avg Acc: 0.4649
2022-01-21 13:45:38,633 Epoch[248/300], Step[1100/1252], Avg Loss: 3.0418, Avg Acc: 0.4650
2022-01-21 13:47:06,236 Epoch[248/300], Step[1150/1252], Avg Loss: 3.0396, Avg Acc: 0.4651
2022-01-21 13:48:34,414 Epoch[248/300], Step[1200/1252], Avg Loss: 3.0398, Avg Acc: 0.4642
2022-01-21 13:50:00,905 Epoch[248/300], Step[1250/1252], Avg Loss: 3.0406, Avg Acc: 0.4651
2022-01-21 13:50:08,250 ----- Epoch[248/300], Train Loss: 3.0405, Train Acc: 0.4651, time: 2330.90, Best Val(epoch246) Acc@1: 0.7667
2022-01-21 13:50:08,250 ----- Validation after Epoch: 248
2022-01-21 13:51:44,176 Val Step[0000/1563], Avg Loss: 0.8282, Avg Acc@1: 0.7812, Avg Acc@5: 1.0000
2022-01-21 13:51:46,113 Val Step[0050/1563], Avg Loss: 1.0211, Avg Acc@1: 0.7812, Avg Acc@5: 0.9308
2022-01-21 13:51:47,903 Val Step[0100/1563], Avg Loss: 1.0485, Avg Acc@1: 0.7738, Avg Acc@5: 0.9322
2022-01-21 13:51:49,699 Val Step[0150/1563], Avg Loss: 1.0488, Avg Acc@1: 0.7730, Avg Acc@5: 0.9317
2022-01-21 13:51:51,568 Val Step[0200/1563], Avg Loss: 1.0475, Avg Acc@1: 0.7741, Avg Acc@5: 0.9342
2022-01-21 13:51:53,375 Val Step[0250/1563], Avg Loss: 1.0390, Avg Acc@1: 0.7734, Avg Acc@5: 0.9364
2022-01-21 13:51:55,232 Val Step[0300/1563], Avg Loss: 1.0381, Avg Acc@1: 0.7735, Avg Acc@5: 0.9359
2022-01-21 13:51:57,110 Val Step[0350/1563], Avg Loss: 1.0432, Avg Acc@1: 0.7727, Avg Acc@5: 0.9355
2022-01-21 13:51:58,979 Val Step[0400/1563], Avg Loss: 1.0437, Avg Acc@1: 0.7722, Avg Acc@5: 0.9354
2022-01-21 13:52:00,915 Val Step[0450/1563], Avg Loss: 1.0484, Avg Acc@1: 0.7697, Avg Acc@5: 0.9351
2022-01-21 13:52:02,877 Val Step[0500/1563], Avg Loss: 1.0495, Avg Acc@1: 0.7691, Avg Acc@5: 0.9356
2022-01-21 13:52:04,821 Val Step[0550/1563], Avg Loss: 1.0504, Avg Acc@1: 0.7683, Avg Acc@5: 0.9356
2022-01-21 13:52:06,715 Val Step[0600/1563], Avg Loss: 1.0502, Avg Acc@1: 0.7684, Avg Acc@5: 0.9358
2022-01-21 13:52:08,558 Val Step[0650/1563], Avg Loss: 1.0516, Avg Acc@1: 0.7685, Avg Acc@5: 0.9354
2022-01-21 13:52:10,376 Val Step[0700/1563], Avg Loss: 1.0491, Avg Acc@1: 0.7695, Avg Acc@5: 0.9360
2022-01-21 13:52:12,198 Val Step[0750/1563], Avg Loss: 1.0548, Avg Acc@1: 0.7674, Avg Acc@5: 0.9358
2022-01-21 13:52:14,018 Val Step[0800/1563], Avg Loss: 1.0558, Avg Acc@1: 0.7673, Avg Acc@5: 0.9356
2022-01-21 13:52:15,875 Val Step[0850/1563], Avg Loss: 1.0569, Avg Acc@1: 0.7667, Avg Acc@5: 0.9353
2022-01-21 13:52:17,684 Val Step[0900/1563], Avg Loss: 1.0548, Avg Acc@1: 0.7665, Avg Acc@5: 0.9354
2022-01-21 13:52:19,503 Val Step[0950/1563], Avg Loss: 1.0546, Avg Acc@1: 0.7675, Avg Acc@5: 0.9355
2022-01-21 13:52:21,384 Val Step[1000/1563], Avg Loss: 1.0563, Avg Acc@1: 0.7672, Avg Acc@5: 0.9349
2022-01-21 13:52:23,305 Val Step[1050/1563], Avg Loss: 1.0578, Avg Acc@1: 0.7663, Avg Acc@5: 0.9346
2022-01-21 13:52:25,248 Val Step[1100/1563], Avg Loss: 1.0584, Avg Acc@1: 0.7663, Avg Acc@5: 0.9345
2022-01-21 13:52:27,156 Val Step[1150/1563], Avg Loss: 1.0567, Avg Acc@1: 0.7661, Avg Acc@5: 0.9346
2022-01-21 13:52:29,114 Val Step[1200/1563], Avg Loss: 1.0559, Avg Acc@1: 0.7668, Avg Acc@5: 0.9345
2022-01-21 13:52:30,994 Val Step[1250/1563], Avg Loss: 1.0557, Avg Acc@1: 0.7663, Avg Acc@5: 0.9348
2022-01-21 13:52:32,895 Val Step[1300/1563], Avg Loss: 1.0582, Avg Acc@1: 0.7664, Avg Acc@5: 0.9343
2022-01-21 13:52:34,738 Val Step[1350/1563], Avg Loss: 1.0592, Avg Acc@1: 0.7660, Avg Acc@5: 0.9340
2022-01-21 13:52:36,521 Val Step[1400/1563], Avg Loss: 1.0593, Avg Acc@1: 0.7659, Avg Acc@5: 0.9340
2022-01-21 13:52:38,380 Val Step[1450/1563], Avg Loss: 1.0580, Avg Acc@1: 0.7663, Avg Acc@5: 0.9339
2022-01-21 13:52:40,341 Val Step[1500/1563], Avg Loss: 1.0577, Avg Acc@1: 0.7666, Avg Acc@5: 0.9342
2022-01-21 13:52:42,142 Val Step[1550/1563], Avg Loss: 1.0577, Avg Acc@1: 0.7664, Avg Acc@5: 0.9342
2022-01-21 13:52:44,120 ----- Epoch[248/300], Validation Loss: 1.0574, Validation Acc@1: 0.7665, Validation Acc@5: 0.9342, time: 155.87
2022-01-21 13:52:44,121 Now training epoch 249. LR=0.000089
2022-01-21 13:54:42,750 Epoch[249/300], Step[0000/1252], Avg Loss: 3.2890, Avg Acc: 0.4648
2022-01-21 13:56:12,378 Epoch[249/300], Step[0050/1252], Avg Loss: 3.0713, Avg Acc: 0.4599
2022-01-21 13:57:41,036 Epoch[249/300], Step[0100/1252], Avg Loss: 3.0387, Avg Acc: 0.4543
2022-01-21 13:59:10,010 Epoch[249/300], Step[0150/1252], Avg Loss: 3.0509, Avg Acc: 0.4583
2022-01-21 14:00:37,600 Epoch[249/300], Step[0200/1252], Avg Loss: 3.0541, Avg Acc: 0.4632
2022-01-21 14:02:06,220 Epoch[249/300], Step[0250/1252], Avg Loss: 3.0529, Avg Acc: 0.4625
2022-01-21 14:03:34,040 Epoch[249/300], Step[0300/1252], Avg Loss: 3.0474, Avg Acc: 0.4646
2022-01-21 14:05:01,629 Epoch[249/300], Step[0350/1252], Avg Loss: 3.0449, Avg Acc: 0.4710
2022-01-21 14:06:30,208 Epoch[249/300], Step[0400/1252], Avg Loss: 3.0397, Avg Acc: 0.4718
2022-01-21 14:07:58,679 Epoch[249/300], Step[0450/1252], Avg Loss: 3.0357, Avg Acc: 0.4714
2022-01-21 14:09:28,304 Epoch[249/300], Step[0500/1252], Avg Loss: 3.0374, Avg Acc: 0.4726
2022-01-21 14:10:56,354 Epoch[249/300], Step[0550/1252], Avg Loss: 3.0338, Avg Acc: 0.4720
2022-01-21 14:12:24,029 Epoch[249/300], Step[0600/1252], Avg Loss: 3.0381, Avg Acc: 0.4715
2022-01-21 14:13:53,086 Epoch[249/300], Step[0650/1252], Avg Loss: 3.0387, Avg Acc: 0.4714
2022-01-21 14:15:20,755 Epoch[249/300], Step[0700/1252], Avg Loss: 3.0372, Avg Acc: 0.4714
2022-01-21 14:16:49,333 Epoch[249/300], Step[0750/1252], Avg Loss: 3.0383, Avg Acc: 0.4684
2022-01-21 14:18:17,793 Epoch[249/300], Step[0800/1252], Avg Loss: 3.0385, Avg Acc: 0.4669
2022-01-21 14:19:46,158 Epoch[249/300], Step[0850/1252], Avg Loss: 3.0366, Avg Acc: 0.4672
2022-01-21 14:21:15,091 Epoch[249/300], Step[0900/1252], Avg Loss: 3.0400, Avg Acc: 0.4663
2022-01-21 14:22:43,566 Epoch[249/300], Step[0950/1252], Avg Loss: 3.0405, Avg Acc: 0.4654
2022-01-21 14:24:12,300 Epoch[249/300], Step[1000/1252], Avg Loss: 3.0393, Avg Acc: 0.4660
2022-01-21 14:25:40,931 Epoch[249/300], Step[1050/1252], Avg Loss: 3.0415, Avg Acc: 0.4655
2022-01-21 14:27:09,276 Epoch[249/300], Step[1100/1252], Avg Loss: 3.0392, Avg Acc: 0.4657
2022-01-21 14:28:38,203 Epoch[249/300], Step[1150/1252], Avg Loss: 3.0354, Avg Acc: 0.4667
2022-01-21 14:30:05,651 Epoch[249/300], Step[1200/1252], Avg Loss: 3.0368, Avg Acc: 0.4670
2022-01-21 14:31:33,222 Epoch[249/300], Step[1250/1252], Avg Loss: 3.0355, Avg Acc: 0.4683
2022-01-21 14:31:40,299 ----- Epoch[249/300], Train Loss: 3.0355, Train Acc: 0.4684, time: 2336.17, Best Val(epoch246) Acc@1: 0.7667
2022-01-21 14:31:40,299 Now training epoch 250. LR=0.000086
2022-01-21 14:33:29,997 Epoch[250/300], Step[0000/1252], Avg Loss: 3.0631, Avg Acc: 0.4492
2022-01-21 14:34:57,862 Epoch[250/300], Step[0050/1252], Avg Loss: 3.0292, Avg Acc: 0.4850
2022-01-21 14:36:25,412 Epoch[250/300], Step[0100/1252], Avg Loss: 3.0656, Avg Acc: 0.4639
2022-01-21 14:37:53,290 Epoch[250/300], Step[0150/1252], Avg Loss: 3.0535, Avg Acc: 0.4677
2022-01-21 14:39:21,205 Epoch[250/300], Step[0200/1252], Avg Loss: 3.0379, Avg Acc: 0.4719
2022-01-21 14:40:50,037 Epoch[250/300], Step[0250/1252], Avg Loss: 3.0354, Avg Acc: 0.4674
2022-01-21 14:42:17,439 Epoch[250/300], Step[0300/1252], Avg Loss: 3.0376, Avg Acc: 0.4646
2022-01-21 14:43:46,048 Epoch[250/300], Step[0350/1252], Avg Loss: 3.0349, Avg Acc: 0.4669
2022-01-21 14:45:15,124 Epoch[250/300], Step[0400/1252], Avg Loss: 3.0382, Avg Acc: 0.4673
2022-01-21 14:46:44,614 Epoch[250/300], Step[0450/1252], Avg Loss: 3.0354, Avg Acc: 0.4642
2022-01-21 14:48:12,837 Epoch[250/300], Step[0500/1252], Avg Loss: 3.0329, Avg Acc: 0.4666
2022-01-21 14:49:42,616 Epoch[250/300], Step[0550/1252], Avg Loss: 3.0391, Avg Acc: 0.4660
2022-01-21 14:51:11,619 Epoch[250/300], Step[0600/1252], Avg Loss: 3.0385, Avg Acc: 0.4670
2022-01-21 14:52:40,302 Epoch[250/300], Step[0650/1252], Avg Loss: 3.0342, Avg Acc: 0.4681
2022-01-21 14:54:08,716 Epoch[250/300], Step[0700/1252], Avg Loss: 3.0261, Avg Acc: 0.4685
2022-01-21 14:55:36,863 Epoch[250/300], Step[0750/1252], Avg Loss: 3.0253, Avg Acc: 0.4683
2022-01-21 14:57:04,165 Epoch[250/300], Step[0800/1252], Avg Loss: 3.0241, Avg Acc: 0.4664
2022-01-21 14:58:32,485 Epoch[250/300], Step[0850/1252], Avg Loss: 3.0250, Avg Acc: 0.4664
2022-01-21 15:00:01,294 Epoch[250/300], Step[0900/1252], Avg Loss: 3.0223, Avg Acc: 0.4656
2022-01-21 15:01:29,811 Epoch[250/300], Step[0950/1252], Avg Loss: 3.0221, Avg Acc: 0.4623
2022-01-21 15:02:57,870 Epoch[250/300], Step[1000/1252], Avg Loss: 3.0239, Avg Acc: 0.4611
2022-01-21 15:04:25,763 Epoch[250/300], Step[1050/1252], Avg Loss: 3.0274, Avg Acc: 0.4607
2022-01-21 15:05:54,286 Epoch[250/300], Step[1100/1252], Avg Loss: 3.0291, Avg Acc: 0.4603
2022-01-21 15:07:22,112 Epoch[250/300], Step[1150/1252], Avg Loss: 3.0302, Avg Acc: 0.4607
2022-01-21 15:08:50,133 Epoch[250/300], Step[1200/1252], Avg Loss: 3.0286, Avg Acc: 0.4610
2022-01-21 15:10:16,348 Epoch[250/300], Step[1250/1252], Avg Loss: 3.0289, Avg Acc: 0.4616
2022-01-21 15:10:23,614 ----- Epoch[250/300], Train Loss: 3.0288, Train Acc: 0.4615, time: 2323.31, Best Val(epoch246) Acc@1: 0.7667
2022-01-21 15:10:23,614 ----- Validation after Epoch: 250
2022-01-21 15:11:58,908 Val Step[0000/1563], Avg Loss: 0.8523, Avg Acc@1: 0.7500, Avg Acc@5: 1.0000
2022-01-21 15:12:01,133 Val Step[0050/1563], Avg Loss: 1.0147, Avg Acc@1: 0.7812, Avg Acc@5: 0.9387
2022-01-21 15:12:03,024 Val Step[0100/1563], Avg Loss: 1.0342, Avg Acc@1: 0.7800, Avg Acc@5: 0.9397
2022-01-21 15:12:05,030 Val Step[0150/1563], Avg Loss: 1.0416, Avg Acc@1: 0.7779, Avg Acc@5: 0.9365
2022-01-21 15:12:07,142 Val Step[0200/1563], Avg Loss: 1.0412, Avg Acc@1: 0.7760, Avg Acc@5: 0.9370
2022-01-21 15:12:09,195 Val Step[0250/1563], Avg Loss: 1.0321, Avg Acc@1: 0.7764, Avg Acc@5: 0.9371
2022-01-21 15:12:11,247 Val Step[0300/1563], Avg Loss: 1.0327, Avg Acc@1: 0.7763, Avg Acc@5: 0.9358
2022-01-21 15:12:13,288 Val Step[0350/1563], Avg Loss: 1.0380, Avg Acc@1: 0.7756, Avg Acc@5: 0.9353
2022-01-21 15:12:15,327 Val Step[0400/1563], Avg Loss: 1.0376, Avg Acc@1: 0.7753, Avg Acc@5: 0.9347
2022-01-21 15:12:17,368 Val Step[0450/1563], Avg Loss: 1.0435, Avg Acc@1: 0.7723, Avg Acc@5: 0.9341
2022-01-21 15:12:19,429 Val Step[0500/1563], Avg Loss: 1.0466, Avg Acc@1: 0.7712, Avg Acc@5: 0.9341
2022-01-21 15:12:21,519 Val Step[0550/1563], Avg Loss: 1.0467, Avg Acc@1: 0.7700, Avg Acc@5: 0.9339
2022-01-21 15:12:23,584 Val Step[0600/1563], Avg Loss: 1.0446, Avg Acc@1: 0.7696, Avg Acc@5: 0.9345
2022-01-21 15:12:25,650 Val Step[0650/1563], Avg Loss: 1.0451, Avg Acc@1: 0.7698, Avg Acc@5: 0.9345
2022-01-21 15:12:27,741 Val Step[0700/1563], Avg Loss: 1.0425, Avg Acc@1: 0.7705, Avg Acc@5: 0.9354
2022-01-21 15:12:29,824 Val Step[0750/1563], Avg Loss: 1.0488, Avg Acc@1: 0.7687, Avg Acc@5: 0.9350
2022-01-21 15:12:31,855 Val Step[0800/1563], Avg Loss: 1.0489, Avg Acc@1: 0.7686, Avg Acc@5: 0.9348
2022-01-21 15:12:33,889 Val Step[0850/1563], Avg Loss: 1.0501, Avg Acc@1: 0.7685, Avg Acc@5: 0.9344
2022-01-21 15:12:35,921 Val Step[0900/1563], Avg Loss: 1.0471, Avg Acc@1: 0.7686, Avg Acc@5: 0.9347
2022-01-21 15:12:37,958 Val Step[0950/1563], Avg Loss: 1.0465, Avg Acc@1: 0.7689, Avg Acc@5: 0.9350
2022-01-21 15:12:40,003 Val Step[1000/1563], Avg Loss: 1.0477, Avg Acc@1: 0.7689, Avg Acc@5: 0.9347
2022-01-21 15:12:42,036 Val Step[1050/1563], Avg Loss: 1.0490, Avg Acc@1: 0.7683, Avg Acc@5: 0.9345
2022-01-21 15:12:44,167 Val Step[1100/1563], Avg Loss: 1.0495, Avg Acc@1: 0.7679, Avg Acc@5: 0.9344
2022-01-21 15:12:46,247 Val Step[1150/1563], Avg Loss: 1.0478, Avg Acc@1: 0.7679, Avg Acc@5: 0.9344
2022-01-21 15:12:48,283 Val Step[1200/1563], Avg Loss: 1.0467, Avg Acc@1: 0.7683, Avg Acc@5: 0.9342
2022-01-21 15:12:50,317 Val Step[1250/1563], Avg Loss: 1.0465, Avg Acc@1: 0.7678, Avg Acc@5: 0.9347
2022-01-21 15:12:52,390 Val Step[1300/1563], Avg Loss: 1.0492, Avg Acc@1: 0.7675, Avg Acc@5: 0.9344
2022-01-21 15:12:54,421 Val Step[1350/1563], Avg Loss: 1.0505, Avg Acc@1: 0.7670, Avg Acc@5: 0.9343
2022-01-21 15:12:56,467 Val Step[1400/1563], Avg Loss: 1.0500, Avg Acc@1: 0.7667, Avg Acc@5: 0.9343
2022-01-21 15:12:58,306 Val Step[1450/1563], Avg Loss: 1.0493, Avg Acc@1: 0.7669, Avg Acc@5: 0.9343
2022-01-21 15:13:00,085 Val Step[1500/1563], Avg Loss: 1.0493, Avg Acc@1: 0.7670, Avg Acc@5: 0.9346
2022-01-21 15:13:01,816 Val Step[1550/1563], Avg Loss: 1.0498, Avg Acc@1: 0.7668, Avg Acc@5: 0.9345
2022-01-21 15:13:03,727 ----- Epoch[250/300], Validation Loss: 1.0494, Validation Acc@1: 0.7668, Validation Acc@5: 0.9346, time: 160.11
2022-01-21 15:13:04,863 the pre best model acc:0.7667, at epoch 246
2022-01-21 15:13:04,863 current best model acc:0.7668, at epoch 250
2022-01-21 15:13:04,863 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 15:13:04,863 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 15:13:04,863 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 15:13:04,863 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 15:13:05,423 ----- Save model: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-250-Loss-3.0406310456458296.pdparams
2022-01-21 15:13:05,423 ----- Save optim: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-250-Loss-3.0406310456458296.pdopt
2022-01-21 15:13:05,423 Now training epoch 251. LR=0.000083
2022-01-21 15:15:02,535 Epoch[251/300], Step[0000/1252], Avg Loss: 3.1497, Avg Acc: 0.4717
2022-01-21 15:16:29,539 Epoch[251/300], Step[0050/1252], Avg Loss: 2.9213, Avg Acc: 0.4854
2022-01-21 15:17:55,367 Epoch[251/300], Step[0100/1252], Avg Loss: 2.9700, Avg Acc: 0.4721
2022-01-21 15:19:20,936 Epoch[251/300], Step[0150/1252], Avg Loss: 2.9761, Avg Acc: 0.4792
2022-01-21 15:20:46,302 Epoch[251/300], Step[0200/1252], Avg Loss: 2.9850, Avg Acc: 0.4684
2022-01-21 15:22:13,679 Epoch[251/300], Step[0250/1252], Avg Loss: 3.0027, Avg Acc: 0.4680
2022-01-21 15:23:41,877 Epoch[251/300], Step[0300/1252], Avg Loss: 3.0083, Avg Acc: 0.4692
2022-01-21 15:25:09,719 Epoch[251/300], Step[0350/1252], Avg Loss: 3.0081, Avg Acc: 0.4717
2022-01-21 15:26:38,296 Epoch[251/300], Step[0400/1252], Avg Loss: 3.0203, Avg Acc: 0.4693
2022-01-21 15:28:05,785 Epoch[251/300], Step[0450/1252], Avg Loss: 3.0123, Avg Acc: 0.4716
2022-01-21 15:29:33,448 Epoch[251/300], Step[0500/1252], Avg Loss: 3.0183, Avg Acc: 0.4672
2022-01-21 15:31:00,327 Epoch[251/300], Step[0550/1252], Avg Loss: 3.0124, Avg Acc: 0.4655
2022-01-21 15:32:26,585 Epoch[251/300], Step[0600/1252], Avg Loss: 3.0128, Avg Acc: 0.4647
2022-01-21 15:33:54,692 Epoch[251/300], Step[0650/1252], Avg Loss: 3.0159, Avg Acc: 0.4640
2022-01-21 15:35:20,946 Epoch[251/300], Step[0700/1252], Avg Loss: 3.0152, Avg Acc: 0.4647
2022-01-21 15:36:47,935 Epoch[251/300], Step[0750/1252], Avg Loss: 3.0179, Avg Acc: 0.4653
2022-01-21 15:38:15,314 Epoch[251/300], Step[0800/1252], Avg Loss: 3.0195, Avg Acc: 0.4660
2022-01-21 15:39:43,468 Epoch[251/300], Step[0850/1252], Avg Loss: 3.0198, Avg Acc: 0.4659
2022-01-21 15:41:11,681 Epoch[251/300], Step[0900/1252], Avg Loss: 3.0193, Avg Acc: 0.4666
2022-01-21 15:42:39,736 Epoch[251/300], Step[0950/1252], Avg Loss: 3.0183, Avg Acc: 0.4669
2022-01-21 15:44:06,953 Epoch[251/300], Step[1000/1252], Avg Loss: 3.0186, Avg Acc: 0.4665
2022-01-21 15:45:34,634 Epoch[251/300], Step[1050/1252], Avg Loss: 3.0174, Avg Acc: 0.4665
2022-01-21 15:47:01,454 Epoch[251/300], Step[1100/1252], Avg Loss: 3.0199, Avg Acc: 0.4669
2022-01-21 15:48:28,028 Epoch[251/300], Step[1150/1252], Avg Loss: 3.0194, Avg Acc: 0.4673
2022-01-21 15:49:55,842 Epoch[251/300], Step[1200/1252], Avg Loss: 3.0172, Avg Acc: 0.4667
2022-01-21 15:51:23,023 Epoch[251/300], Step[1250/1252], Avg Loss: 3.0183, Avg Acc: 0.4660
2022-01-21 15:51:30,196 ----- Epoch[251/300], Train Loss: 3.0184, Train Acc: 0.4660, time: 2304.77, Best Val(epoch250) Acc@1: 0.7668
2022-01-21 15:51:30,197 Now training epoch 252. LR=0.000080
2022-01-21 15:53:23,695 Epoch[252/300], Step[0000/1252], Avg Loss: 3.2828, Avg Acc: 0.4092
2022-01-21 15:54:51,345 Epoch[252/300], Step[0050/1252], Avg Loss: 3.0842, Avg Acc: 0.4736
2022-01-21 15:56:19,789 Epoch[252/300], Step[0100/1252], Avg Loss: 3.0701, Avg Acc: 0.4638
2022-01-21 15:57:47,600 Epoch[252/300], Step[0150/1252], Avg Loss: 3.0449, Avg Acc: 0.4624
2022-01-21 15:59:16,073 Epoch[252/300], Step[0200/1252], Avg Loss: 3.0470, Avg Acc: 0.4689
2022-01-21 16:00:43,919 Epoch[252/300], Step[0250/1252], Avg Loss: 3.0407, Avg Acc: 0.4744
2022-01-21 16:02:11,850 Epoch[252/300], Step[0300/1252], Avg Loss: 3.0408, Avg Acc: 0.4742
2022-01-21 16:03:39,741 Epoch[252/300], Step[0350/1252], Avg Loss: 3.0387, Avg Acc: 0.4749
2022-01-21 16:05:07,507 Epoch[252/300], Step[0400/1252], Avg Loss: 3.0347, Avg Acc: 0.4710
2022-01-21 16:06:35,873 Epoch[252/300], Step[0450/1252], Avg Loss: 3.0322, Avg Acc: 0.4683
2022-01-21 16:08:04,146 Epoch[252/300], Step[0500/1252], Avg Loss: 3.0306, Avg Acc: 0.4670
2022-01-21 16:09:32,265 Epoch[252/300], Step[0550/1252], Avg Loss: 3.0285, Avg Acc: 0.4675
2022-01-21 16:10:59,869 Epoch[252/300], Step[0600/1252], Avg Loss: 3.0355, Avg Acc: 0.4662
2022-01-21 16:12:27,706 Epoch[252/300], Step[0650/1252], Avg Loss: 3.0342, Avg Acc: 0.4672
2022-01-21 16:13:55,316 Epoch[252/300], Step[0700/1252], Avg Loss: 3.0276, Avg Acc: 0.4700
2022-01-21 16:15:23,635 Epoch[252/300], Step[0750/1252], Avg Loss: 3.0237, Avg Acc: 0.4711
2022-01-21 16:16:51,442 Epoch[252/300], Step[0800/1252], Avg Loss: 3.0252, Avg Acc: 0.4710
2022-01-21 16:18:19,423 Epoch[252/300], Step[0850/1252], Avg Loss: 3.0287, Avg Acc: 0.4701
2022-01-21 16:19:46,235 Epoch[252/300], Step[0900/1252], Avg Loss: 3.0281, Avg Acc: 0.4699
2022-01-21 16:21:15,052 Epoch[252/300], Step[0950/1252], Avg Loss: 3.0292, Avg Acc: 0.4694
2022-01-21 16:22:44,076 Epoch[252/300], Step[1000/1252], Avg Loss: 3.0281, Avg Acc: 0.4693
2022-01-21 16:24:12,021 Epoch[252/300], Step[1050/1252], Avg Loss: 3.0302, Avg Acc: 0.4687
2022-01-21 16:25:39,482 Epoch[252/300], Step[1100/1252], Avg Loss: 3.0287, Avg Acc: 0.4687
2022-01-21 16:27:07,272 Epoch[252/300], Step[1150/1252], Avg Loss: 3.0295, Avg Acc: 0.4683
2022-01-21 16:28:34,734 Epoch[252/300], Step[1200/1252], Avg Loss: 3.0256, Avg Acc: 0.4690
2022-01-21 16:30:02,953 Epoch[252/300], Step[1250/1252], Avg Loss: 3.0277, Avg Acc: 0.4678
2022-01-21 16:30:10,255 ----- Epoch[252/300], Train Loss: 3.0277, Train Acc: 0.4678, time: 2320.05, Best Val(epoch250) Acc@1: 0.7668
2022-01-21 16:30:10,255 ----- Validation after Epoch: 252
2022-01-21 16:31:39,994 Val Step[0000/1563], Avg Loss: 0.7920, Avg Acc@1: 0.8438, Avg Acc@5: 1.0000
2022-01-21 16:31:41,828 Val Step[0050/1563], Avg Loss: 1.0369, Avg Acc@1: 0.7745, Avg Acc@5: 0.9387
2022-01-21 16:31:43,618 Val Step[0100/1563], Avg Loss: 1.0710, Avg Acc@1: 0.7729, Avg Acc@5: 0.9372
2022-01-21 16:31:45,409 Val Step[0150/1563], Avg Loss: 1.0719, Avg Acc@1: 0.7738, Avg Acc@5: 0.9346
2022-01-21 16:31:47,206 Val Step[0200/1563], Avg Loss: 1.0746, Avg Acc@1: 0.7735, Avg Acc@5: 0.9352
2022-01-21 16:31:49,004 Val Step[0250/1563], Avg Loss: 1.0615, Avg Acc@1: 0.7744, Avg Acc@5: 0.9371
2022-01-21 16:31:50,795 Val Step[0300/1563], Avg Loss: 1.0646, Avg Acc@1: 0.7738, Avg Acc@5: 0.9360
2022-01-21 16:31:52,664 Val Step[0350/1563], Avg Loss: 1.0681, Avg Acc@1: 0.7734, Avg Acc@5: 0.9358
2022-01-21 16:31:54,519 Val Step[0400/1563], Avg Loss: 1.0684, Avg Acc@1: 0.7745, Avg Acc@5: 0.9351
2022-01-21 16:31:56,357 Val Step[0450/1563], Avg Loss: 1.0753, Avg Acc@1: 0.7718, Avg Acc@5: 0.9342
2022-01-21 16:31:58,236 Val Step[0500/1563], Avg Loss: 1.0781, Avg Acc@1: 0.7705, Avg Acc@5: 0.9346
2022-01-21 16:32:00,088 Val Step[0550/1563], Avg Loss: 1.0786, Avg Acc@1: 0.7691, Avg Acc@5: 0.9346
2022-01-21 16:32:01,940 Val Step[0600/1563], Avg Loss: 1.0782, Avg Acc@1: 0.7686, Avg Acc@5: 0.9351
2022-01-21 16:32:03,789 Val Step[0650/1563], Avg Loss: 1.0778, Avg Acc@1: 0.7684, Avg Acc@5: 0.9350
2022-01-21 16:32:05,682 Val Step[0700/1563], Avg Loss: 1.0759, Avg Acc@1: 0.7692, Avg Acc@5: 0.9354
2022-01-21 16:32:07,611 Val Step[0750/1563], Avg Loss: 1.0813, Avg Acc@1: 0.7672, Avg Acc@5: 0.9350
2022-01-21 16:32:09,402 Val Step[0800/1563], Avg Loss: 1.0812, Avg Acc@1: 0.7676, Avg Acc@5: 0.9346
2022-01-21 16:32:11,305 Val Step[0850/1563], Avg Loss: 1.0817, Avg Acc@1: 0.7676, Avg Acc@5: 0.9343
2022-01-21 16:32:13,138 Val Step[0900/1563], Avg Loss: 1.0794, Avg Acc@1: 0.7680, Avg Acc@5: 0.9345
2022-01-21 16:32:15,037 Val Step[0950/1563], Avg Loss: 1.0791, Avg Acc@1: 0.7683, Avg Acc@5: 0.9344
2022-01-21 16:32:16,918 Val Step[1000/1563], Avg Loss: 1.0813, Avg Acc@1: 0.7685, Avg Acc@5: 0.9339
2022-01-21 16:32:18,792 Val Step[1050/1563], Avg Loss: 1.0825, Avg Acc@1: 0.7675, Avg Acc@5: 0.9335
2022-01-21 16:32:20,667 Val Step[1100/1563], Avg Loss: 1.0830, Avg Acc@1: 0.7672, Avg Acc@5: 0.9332
2022-01-21 16:32:22,506 Val Step[1150/1563], Avg Loss: 1.0810, Avg Acc@1: 0.7674, Avg Acc@5: 0.9335
2022-01-21 16:32:24,448 Val Step[1200/1563], Avg Loss: 1.0798, Avg Acc@1: 0.7679, Avg Acc@5: 0.9336
2022-01-21 16:32:26,338 Val Step[1250/1563], Avg Loss: 1.0790, Avg Acc@1: 0.7675, Avg Acc@5: 0.9341
2022-01-21 16:32:28,264 Val Step[1300/1563], Avg Loss: 1.0821, Avg Acc@1: 0.7673, Avg Acc@5: 0.9337
2022-01-21 16:32:30,094 Val Step[1350/1563], Avg Loss: 1.0832, Avg Acc@1: 0.7669, Avg Acc@5: 0.9335
2022-01-21 16:32:32,090 Val Step[1400/1563], Avg Loss: 1.0828, Avg Acc@1: 0.7669, Avg Acc@5: 0.9333
2022-01-21 16:32:34,268 Val Step[1450/1563], Avg Loss: 1.0818, Avg Acc@1: 0.7675, Avg Acc@5: 0.9335
2022-01-21 16:32:36,169 Val Step[1500/1563], Avg Loss: 1.0817, Avg Acc@1: 0.7676, Avg Acc@5: 0.9338
2022-01-21 16:32:37,981 Val Step[1550/1563], Avg Loss: 1.0817, Avg Acc@1: 0.7672, Avg Acc@5: 0.9337
2022-01-21 16:32:40,666 ----- Epoch[252/300], Validation Loss: 1.0816, Validation Acc@1: 0.7672, Validation Acc@5: 0.9337, time: 150.41
2022-01-21 16:32:41,827 the pre best model acc:0.7668, at epoch 250
2022-01-21 16:32:41,827 current best model acc:0.7672, at epoch 252
2022-01-21 16:32:41,827 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 16:32:41,828 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 16:32:41,828 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 16:32:41,828 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 16:32:41,828 Now training epoch 253. LR=0.000077
2022-01-21 16:34:32,567 Epoch[253/300], Step[0000/1252], Avg Loss: 3.1281, Avg Acc: 0.3867
2022-01-21 16:36:02,177 Epoch[253/300], Step[0050/1252], Avg Loss: 3.0194, Avg Acc: 0.4662
2022-01-21 16:37:30,248 Epoch[253/300], Step[0100/1252], Avg Loss: 2.9902, Avg Acc: 0.4580
2022-01-21 16:38:57,429 Epoch[253/300], Step[0150/1252], Avg Loss: 2.9854, Avg Acc: 0.4609
2022-01-21 16:40:25,718 Epoch[253/300], Step[0200/1252], Avg Loss: 2.9803, Avg Acc: 0.4635
2022-01-21 16:41:53,059 Epoch[253/300], Step[0250/1252], Avg Loss: 2.9907, Avg Acc: 0.4674
2022-01-21 16:43:20,423 Epoch[253/300], Step[0300/1252], Avg Loss: 3.0008, Avg Acc: 0.4708
2022-01-21 16:44:47,981 Epoch[253/300], Step[0350/1252], Avg Loss: 2.9954, Avg Acc: 0.4748
2022-01-21 16:46:15,798 Epoch[253/300], Step[0400/1252], Avg Loss: 2.9923, Avg Acc: 0.4748
2022-01-21 16:47:44,158 Epoch[253/300], Step[0450/1252], Avg Loss: 2.9943, Avg Acc: 0.4710
2022-01-21 16:49:13,606 Epoch[253/300], Step[0500/1252], Avg Loss: 2.9989, Avg Acc: 0.4740
2022-01-21 16:50:41,686 Epoch[253/300], Step[0550/1252], Avg Loss: 2.9956, Avg Acc: 0.4761
2022-01-21 16:52:11,110 Epoch[253/300], Step[0600/1252], Avg Loss: 2.9946, Avg Acc: 0.4759
2022-01-21 16:53:40,913 Epoch[253/300], Step[0650/1252], Avg Loss: 2.9994, Avg Acc: 0.4743
2022-01-21 16:55:10,142 Epoch[253/300], Step[0700/1252], Avg Loss: 2.9997, Avg Acc: 0.4730
2022-01-21 16:56:38,625 Epoch[253/300], Step[0750/1252], Avg Loss: 3.0032, Avg Acc: 0.4740
2022-01-21 16:58:06,699 Epoch[253/300], Step[0800/1252], Avg Loss: 3.0089, Avg Acc: 0.4733
2022-01-21 16:59:35,572 Epoch[253/300], Step[0850/1252], Avg Loss: 3.0129, Avg Acc: 0.4730
2022-01-21 17:01:03,590 Epoch[253/300], Step[0900/1252], Avg Loss: 3.0102, Avg Acc: 0.4751
2022-01-21 17:02:31,784 Epoch[253/300], Step[0950/1252], Avg Loss: 3.0154, Avg Acc: 0.4751
2022-01-21 17:04:00,888 Epoch[253/300], Step[1000/1252], Avg Loss: 3.0151, Avg Acc: 0.4751
2022-01-21 17:05:30,056 Epoch[253/300], Step[1050/1252], Avg Loss: 3.0166, Avg Acc: 0.4761
2022-01-21 17:06:58,196 Epoch[253/300], Step[1100/1252], Avg Loss: 3.0155, Avg Acc: 0.4757
2022-01-21 17:08:26,615 Epoch[253/300], Step[1150/1252], Avg Loss: 3.0165, Avg Acc: 0.4758
2022-01-21 17:09:55,820 Epoch[253/300], Step[1200/1252], Avg Loss: 3.0161, Avg Acc: 0.4754
2022-01-21 17:11:24,431 Epoch[253/300], Step[1250/1252], Avg Loss: 3.0161, Avg Acc: 0.4746
2022-01-21 17:11:31,556 ----- Epoch[253/300], Train Loss: 3.0161, Train Acc: 0.4746, time: 2329.72, Best Val(epoch252) Acc@1: 0.7672
2022-01-21 17:11:31,556 Now training epoch 254. LR=0.000074
2022-01-21 17:13:24,481 Epoch[254/300], Step[0000/1252], Avg Loss: 3.1592, Avg Acc: 0.1104
2022-01-21 17:14:51,627 Epoch[254/300], Step[0050/1252], Avg Loss: 3.0566, Avg Acc: 0.4514
2022-01-21 17:16:19,452 Epoch[254/300], Step[0100/1252], Avg Loss: 3.0158, Avg Acc: 0.4619
2022-01-21 17:17:48,058 Epoch[254/300], Step[0150/1252], Avg Loss: 3.0125, Avg Acc: 0.4604
2022-01-21 17:19:14,792 Epoch[254/300], Step[0200/1252], Avg Loss: 3.0048, Avg Acc: 0.4677
2022-01-21 17:20:43,216 Epoch[254/300], Step[0250/1252], Avg Loss: 3.0244, Avg Acc: 0.4654
2022-01-21 17:22:11,811 Epoch[254/300], Step[0300/1252], Avg Loss: 3.0136, Avg Acc: 0.4643
2022-01-21 17:23:39,755 Epoch[254/300], Step[0350/1252], Avg Loss: 3.0094, Avg Acc: 0.4703
2022-01-21 17:25:07,415 Epoch[254/300], Step[0400/1252], Avg Loss: 3.0122, Avg Acc: 0.4689
2022-01-21 17:26:34,917 Epoch[254/300], Step[0450/1252], Avg Loss: 3.0118, Avg Acc: 0.4678
2022-01-21 17:28:01,950 Epoch[254/300], Step[0500/1252], Avg Loss: 3.0117, Avg Acc: 0.4708
2022-01-21 17:29:28,970 Epoch[254/300], Step[0550/1252], Avg Loss: 3.0085, Avg Acc: 0.4724
2022-01-21 17:30:55,938 Epoch[254/300], Step[0600/1252], Avg Loss: 3.0065, Avg Acc: 0.4752
2022-01-21 17:32:22,875 Epoch[254/300], Step[0650/1252], Avg Loss: 3.0074, Avg Acc: 0.4738
2022-01-21 17:33:49,736 Epoch[254/300], Step[0700/1252], Avg Loss: 3.0051, Avg Acc: 0.4733
2022-01-21 17:35:16,610 Epoch[254/300], Step[0750/1252], Avg Loss: 3.0059, Avg Acc: 0.4726
2022-01-21 17:36:44,614 Epoch[254/300], Step[0800/1252], Avg Loss: 3.0069, Avg Acc: 0.4707
2022-01-21 17:38:13,409 Epoch[254/300], Step[0850/1252], Avg Loss: 3.0068, Avg Acc: 0.4709
2022-01-21 17:39:42,238 Epoch[254/300], Step[0900/1252], Avg Loss: 3.0046, Avg Acc: 0.4715
2022-01-21 17:41:11,906 Epoch[254/300], Step[0950/1252], Avg Loss: 3.0030, Avg Acc: 0.4718
2022-01-21 17:42:40,973 Epoch[254/300], Step[1000/1252], Avg Loss: 3.0065, Avg Acc: 0.4707
2022-01-21 17:44:10,482 Epoch[254/300], Step[1050/1252], Avg Loss: 3.0110, Avg Acc: 0.4696
2022-01-21 17:45:40,117 Epoch[254/300], Step[1100/1252], Avg Loss: 3.0101, Avg Acc: 0.4717
2022-01-21 17:47:09,025 Epoch[254/300], Step[1150/1252], Avg Loss: 3.0082, Avg Acc: 0.4714
2022-01-21 17:48:38,617 Epoch[254/300], Step[1200/1252], Avg Loss: 3.0099, Avg Acc: 0.4711
2022-01-21 17:50:07,675 Epoch[254/300], Step[1250/1252], Avg Loss: 3.0091, Avg Acc: 0.4716
2022-01-21 17:50:14,829 ----- Epoch[254/300], Train Loss: 3.0091, Train Acc: 0.4717, time: 2323.27, Best Val(epoch252) Acc@1: 0.7672
2022-01-21 17:50:14,829 ----- Validation after Epoch: 254
2022-01-21 17:51:39,378 Val Step[0000/1563], Avg Loss: 0.8180, Avg Acc@1: 0.8125, Avg Acc@5: 1.0000
2022-01-21 17:51:41,167 Val Step[0050/1563], Avg Loss: 1.0449, Avg Acc@1: 0.7739, Avg Acc@5: 0.9369
2022-01-21 17:51:42,940 Val Step[0100/1563], Avg Loss: 1.0702, Avg Acc@1: 0.7726, Avg Acc@5: 0.9356
2022-01-21 17:51:44,709 Val Step[0150/1563], Avg Loss: 1.0706, Avg Acc@1: 0.7755, Avg Acc@5: 0.9325
2022-01-21 17:51:46,470 Val Step[0200/1563], Avg Loss: 1.0721, Avg Acc@1: 0.7741, Avg Acc@5: 0.9342
2022-01-21 17:51:48,240 Val Step[0250/1563], Avg Loss: 1.0619, Avg Acc@1: 0.7743, Avg Acc@5: 0.9356
2022-01-21 17:51:50,032 Val Step[0300/1563], Avg Loss: 1.0623, Avg Acc@1: 0.7740, Avg Acc@5: 0.9350
2022-01-21 17:51:51,813 Val Step[0350/1563], Avg Loss: 1.0657, Avg Acc@1: 0.7742, Avg Acc@5: 0.9349
2022-01-21 17:51:53,751 Val Step[0400/1563], Avg Loss: 1.0640, Avg Acc@1: 0.7749, Avg Acc@5: 0.9347
2022-01-21 17:51:55,822 Val Step[0450/1563], Avg Loss: 1.0699, Avg Acc@1: 0.7720, Avg Acc@5: 0.9344
2022-01-21 17:51:57,794 Val Step[0500/1563], Avg Loss: 1.0718, Avg Acc@1: 0.7709, Avg Acc@5: 0.9348
2022-01-21 17:51:59,622 Val Step[0550/1563], Avg Loss: 1.0728, Avg Acc@1: 0.7699, Avg Acc@5: 0.9342
2022-01-21 17:52:01,491 Val Step[0600/1563], Avg Loss: 1.0720, Avg Acc@1: 0.7692, Avg Acc@5: 0.9341
2022-01-21 17:52:03,451 Val Step[0650/1563], Avg Loss: 1.0724, Avg Acc@1: 0.7698, Avg Acc@5: 0.9341
2022-01-21 17:52:05,339 Val Step[0700/1563], Avg Loss: 1.0695, Avg Acc@1: 0.7709, Avg Acc@5: 0.9347
2022-01-21 17:52:07,327 Val Step[0750/1563], Avg Loss: 1.0746, Avg Acc@1: 0.7696, Avg Acc@5: 0.9343
2022-01-21 17:52:09,252 Val Step[0800/1563], Avg Loss: 1.0747, Avg Acc@1: 0.7696, Avg Acc@5: 0.9340
2022-01-21 17:52:11,060 Val Step[0850/1563], Avg Loss: 1.0762, Avg Acc@1: 0.7690, Avg Acc@5: 0.9338
2022-01-21 17:52:12,850 Val Step[0900/1563], Avg Loss: 1.0736, Avg Acc@1: 0.7692, Avg Acc@5: 0.9345
2022-01-21 17:52:14,644 Val Step[0950/1563], Avg Loss: 1.0736, Avg Acc@1: 0.7700, Avg Acc@5: 0.9345
2022-01-21 17:52:16,442 Val Step[1000/1563], Avg Loss: 1.0750, Avg Acc@1: 0.7699, Avg Acc@5: 0.9342
2022-01-21 17:52:18,514 Val Step[1050/1563], Avg Loss: 1.0767, Avg Acc@1: 0.7694, Avg Acc@5: 0.9341
2022-01-21 17:52:20,604 Val Step[1100/1563], Avg Loss: 1.0771, Avg Acc@1: 0.7690, Avg Acc@5: 0.9339
2022-01-21 17:52:22,622 Val Step[1150/1563], Avg Loss: 1.0753, Avg Acc@1: 0.7692, Avg Acc@5: 0.9342
2022-01-21 17:52:24,516 Val Step[1200/1563], Avg Loss: 1.0739, Avg Acc@1: 0.7696, Avg Acc@5: 0.9341
2022-01-21 17:52:26,360 Val Step[1250/1563], Avg Loss: 1.0734, Avg Acc@1: 0.7694, Avg Acc@5: 0.9345
2022-01-21 17:52:28,189 Val Step[1300/1563], Avg Loss: 1.0763, Avg Acc@1: 0.7690, Avg Acc@5: 0.9341
2022-01-21 17:52:29,989 Val Step[1350/1563], Avg Loss: 1.0773, Avg Acc@1: 0.7684, Avg Acc@5: 0.9340
2022-01-21 17:52:31,805 Val Step[1400/1563], Avg Loss: 1.0768, Avg Acc@1: 0.7682, Avg Acc@5: 0.9339
2022-01-21 17:52:33,747 Val Step[1450/1563], Avg Loss: 1.0763, Avg Acc@1: 0.7686, Avg Acc@5: 0.9340
2022-01-21 17:52:35,718 Val Step[1500/1563], Avg Loss: 1.0762, Avg Acc@1: 0.7686, Avg Acc@5: 0.9343
2022-01-21 17:52:37,719 Val Step[1550/1563], Avg Loss: 1.0766, Avg Acc@1: 0.7683, Avg Acc@5: 0.9342
2022-01-21 17:52:39,620 ----- Epoch[254/300], Validation Loss: 1.0764, Validation Acc@1: 0.7684, Validation Acc@5: 0.9343, time: 144.79
2022-01-21 17:52:40,757 the pre best model acc:0.7672, at epoch 252
2022-01-21 17:52:40,757 current best model acc:0.7684, at epoch 254
2022-01-21 17:52:40,757 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 17:52:40,757 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 17:52:40,757 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 17:52:40,757 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 17:52:40,758 Now training epoch 255. LR=0.000072
2022-01-21 17:54:37,187 Epoch[255/300], Step[0000/1252], Avg Loss: 2.9708, Avg Acc: 0.5947
2022-01-21 17:56:04,461 Epoch[255/300], Step[0050/1252], Avg Loss: 2.9970, Avg Acc: 0.4681
2022-01-21 17:57:30,843 Epoch[255/300], Step[0100/1252], Avg Loss: 2.9749, Avg Acc: 0.4832
2022-01-21 17:58:59,039 Epoch[255/300], Step[0150/1252], Avg Loss: 2.9785, Avg Acc: 0.4847
2022-01-21 18:00:26,654 Epoch[255/300], Step[0200/1252], Avg Loss: 2.9681, Avg Acc: 0.4867
2022-01-21 18:01:54,128 Epoch[255/300], Step[0250/1252], Avg Loss: 2.9681, Avg Acc: 0.4831
2022-01-21 18:03:22,047 Epoch[255/300], Step[0300/1252], Avg Loss: 2.9860, Avg Acc: 0.4796
2022-01-21 18:04:49,431 Epoch[255/300], Step[0350/1252], Avg Loss: 2.9835, Avg Acc: 0.4792
2022-01-21 18:06:16,558 Epoch[255/300], Step[0400/1252], Avg Loss: 2.9933, Avg Acc: 0.4743
2022-01-21 18:07:44,285 Epoch[255/300], Step[0450/1252], Avg Loss: 2.9924, Avg Acc: 0.4719
2022-01-21 18:09:10,040 Epoch[255/300], Step[0500/1252], Avg Loss: 2.9963, Avg Acc: 0.4729
2022-01-21 18:10:36,560 Epoch[255/300], Step[0550/1252], Avg Loss: 2.9963, Avg Acc: 0.4690
2022-01-21 18:12:04,187 Epoch[255/300], Step[0600/1252], Avg Loss: 2.9973, Avg Acc: 0.4675
2022-01-21 18:13:31,273 Epoch[255/300], Step[0650/1252], Avg Loss: 2.9962, Avg Acc: 0.4677
2022-01-21 18:14:57,821 Epoch[255/300], Step[0700/1252], Avg Loss: 2.9961, Avg Acc: 0.4674
2022-01-21 18:16:24,288 Epoch[255/300], Step[0750/1252], Avg Loss: 2.9948, Avg Acc: 0.4685
2022-01-21 18:17:51,332 Epoch[255/300], Step[0800/1252], Avg Loss: 2.9953, Avg Acc: 0.4689
2022-01-21 18:19:17,048 Epoch[255/300], Step[0850/1252], Avg Loss: 2.9971, Avg Acc: 0.4686
2022-01-21 18:20:44,486 Epoch[255/300], Step[0900/1252], Avg Loss: 2.9981, Avg Acc: 0.4687
2022-01-21 18:22:11,596 Epoch[255/300], Step[0950/1252], Avg Loss: 2.9991, Avg Acc: 0.4686
2022-01-21 18:23:38,500 Epoch[255/300], Step[1000/1252], Avg Loss: 3.0022, Avg Acc: 0.4697
2022-01-21 18:25:05,639 Epoch[255/300], Step[1050/1252], Avg Loss: 3.0027, Avg Acc: 0.4683
2022-01-21 18:26:32,772 Epoch[255/300], Step[1100/1252], Avg Loss: 2.9998, Avg Acc: 0.4679
2022-01-21 18:27:59,937 Epoch[255/300], Step[1150/1252], Avg Loss: 2.9987, Avg Acc: 0.4686
2022-01-21 18:29:27,584 Epoch[255/300], Step[1200/1252], Avg Loss: 2.9973, Avg Acc: 0.4701
2022-01-21 18:30:55,381 Epoch[255/300], Step[1250/1252], Avg Loss: 3.0001, Avg Acc: 0.4700
2022-01-21 18:31:01,481 ----- Epoch[255/300], Train Loss: 3.0001, Train Acc: 0.4700, time: 2300.72, Best Val(epoch254) Acc@1: 0.7684
2022-01-21 18:31:01,481 Now training epoch 256. LR=0.000069
2022-01-21 18:33:07,166 Epoch[256/300], Step[0000/1252], Avg Loss: 3.1184, Avg Acc: 0.3418
2022-01-21 18:34:35,524 Epoch[256/300], Step[0050/1252], Avg Loss: 2.9998, Avg Acc: 0.4725
2022-01-21 18:36:02,180 Epoch[256/300], Step[0100/1252], Avg Loss: 2.9989, Avg Acc: 0.4865
2022-01-21 18:37:30,613 Epoch[256/300], Step[0150/1252], Avg Loss: 2.9971, Avg Acc: 0.4708
2022-01-21 18:38:58,625 Epoch[256/300], Step[0200/1252], Avg Loss: 2.9916, Avg Acc: 0.4786
2022-01-21 18:40:25,819 Epoch[256/300], Step[0250/1252], Avg Loss: 2.9878, Avg Acc: 0.4744
2022-01-21 18:41:53,584 Epoch[256/300], Step[0300/1252], Avg Loss: 2.9856, Avg Acc: 0.4737
2022-01-21 18:43:22,144 Epoch[256/300], Step[0350/1252], Avg Loss: 2.9850, Avg Acc: 0.4738
2022-01-21 18:44:50,082 Epoch[256/300], Step[0400/1252], Avg Loss: 2.9945, Avg Acc: 0.4715
2022-01-21 18:46:17,192 Epoch[256/300], Step[0450/1252], Avg Loss: 2.9972, Avg Acc: 0.4716
2022-01-21 18:47:46,183 Epoch[256/300], Step[0500/1252], Avg Loss: 2.9973, Avg Acc: 0.4708
2022-01-21 18:49:13,317 Epoch[256/300], Step[0550/1252], Avg Loss: 2.9944, Avg Acc: 0.4734
2022-01-21 18:50:41,617 Epoch[256/300], Step[0600/1252], Avg Loss: 2.9968, Avg Acc: 0.4732
2022-01-21 18:52:09,829 Epoch[256/300], Step[0650/1252], Avg Loss: 2.9997, Avg Acc: 0.4726
2022-01-21 18:53:38,986 Epoch[256/300], Step[0700/1252], Avg Loss: 2.9997, Avg Acc: 0.4702
2022-01-21 18:55:07,430 Epoch[256/300], Step[0750/1252], Avg Loss: 3.0000, Avg Acc: 0.4719
2022-01-21 18:56:36,078 Epoch[256/300], Step[0800/1252], Avg Loss: 3.0027, Avg Acc: 0.4708
2022-01-21 18:58:04,426 Epoch[256/300], Step[0850/1252], Avg Loss: 3.0032, Avg Acc: 0.4711
2022-01-21 18:59:32,499 Epoch[256/300], Step[0900/1252], Avg Loss: 3.0028, Avg Acc: 0.4727
2022-01-21 19:01:00,863 Epoch[256/300], Step[0950/1252], Avg Loss: 2.9983, Avg Acc: 0.4725
2022-01-21 19:02:29,462 Epoch[256/300], Step[1000/1252], Avg Loss: 2.9991, Avg Acc: 0.4723
2022-01-21 19:03:58,562 Epoch[256/300], Step[1050/1252], Avg Loss: 3.0001, Avg Acc: 0.4711
2022-01-21 19:05:26,972 Epoch[256/300], Step[1100/1252], Avg Loss: 2.9997, Avg Acc: 0.4702
2022-01-21 19:06:55,852 Epoch[256/300], Step[1150/1252], Avg Loss: 2.9961, Avg Acc: 0.4697
2022-01-21 19:08:23,118 Epoch[256/300], Step[1200/1252], Avg Loss: 2.9954, Avg Acc: 0.4703
2022-01-21 19:09:51,871 Epoch[256/300], Step[1250/1252], Avg Loss: 2.9974, Avg Acc: 0.4695
2022-01-21 19:09:58,374 ----- Epoch[256/300], Train Loss: 2.9975, Train Acc: 0.4695, time: 2336.89, Best Val(epoch254) Acc@1: 0.7684
2022-01-21 19:09:58,374 ----- Validation after Epoch: 256
2022-01-21 19:11:31,510 Val Step[0000/1563], Avg Loss: 0.8404, Avg Acc@1: 0.7812, Avg Acc@5: 1.0000
2022-01-21 19:11:33,804 Val Step[0050/1563], Avg Loss: 1.0418, Avg Acc@1: 0.7819, Avg Acc@5: 0.9338
2022-01-21 19:11:35,935 Val Step[0100/1563], Avg Loss: 1.0563, Avg Acc@1: 0.7847, Avg Acc@5: 0.9341
2022-01-21 19:11:37,751 Val Step[0150/1563], Avg Loss: 1.0578, Avg Acc@1: 0.7831, Avg Acc@5: 0.9329
2022-01-21 19:11:39,561 Val Step[0200/1563], Avg Loss: 1.0588, Avg Acc@1: 0.7819, Avg Acc@5: 0.9352
2022-01-21 19:11:41,435 Val Step[0250/1563], Avg Loss: 1.0494, Avg Acc@1: 0.7801, Avg Acc@5: 0.9373
2022-01-21 19:11:43,336 Val Step[0300/1563], Avg Loss: 1.0510, Avg Acc@1: 0.7790, Avg Acc@5: 0.9367
2022-01-21 19:11:45,233 Val Step[0350/1563], Avg Loss: 1.0546, Avg Acc@1: 0.7780, Avg Acc@5: 0.9363
2022-01-21 19:11:47,044 Val Step[0400/1563], Avg Loss: 1.0544, Avg Acc@1: 0.7773, Avg Acc@5: 0.9363
2022-01-21 19:11:48,839 Val Step[0450/1563], Avg Loss: 1.0606, Avg Acc@1: 0.7745, Avg Acc@5: 0.9359
2022-01-21 19:11:50,633 Val Step[0500/1563], Avg Loss: 1.0628, Avg Acc@1: 0.7735, Avg Acc@5: 0.9358
2022-01-21 19:11:52,493 Val Step[0550/1563], Avg Loss: 1.0637, Avg Acc@1: 0.7720, Avg Acc@5: 0.9357
2022-01-21 19:11:54,292 Val Step[0600/1563], Avg Loss: 1.0621, Avg Acc@1: 0.7719, Avg Acc@5: 0.9358
2022-01-21 19:11:56,095 Val Step[0650/1563], Avg Loss: 1.0627, Avg Acc@1: 0.7721, Avg Acc@5: 0.9359
2022-01-21 19:11:57,950 Val Step[0700/1563], Avg Loss: 1.0606, Avg Acc@1: 0.7728, Avg Acc@5: 0.9367
2022-01-21 19:11:59,773 Val Step[0750/1563], Avg Loss: 1.0657, Avg Acc@1: 0.7708, Avg Acc@5: 0.9363
2022-01-21 19:12:01,699 Val Step[0800/1563], Avg Loss: 1.0657, Avg Acc@1: 0.7711, Avg Acc@5: 0.9364
2022-01-21 19:12:03,525 Val Step[0850/1563], Avg Loss: 1.0672, Avg Acc@1: 0.7700, Avg Acc@5: 0.9359
2022-01-21 19:12:05,451 Val Step[0900/1563], Avg Loss: 1.0642, Avg Acc@1: 0.7704, Avg Acc@5: 0.9362
2022-01-21 19:12:07,322 Val Step[0950/1563], Avg Loss: 1.0639, Avg Acc@1: 0.7707, Avg Acc@5: 0.9362
2022-01-21 19:12:09,139 Val Step[1000/1563], Avg Loss: 1.0659, Avg Acc@1: 0.7704, Avg Acc@5: 0.9357
2022-01-21 19:12:11,009 Val Step[1050/1563], Avg Loss: 1.0674, Avg Acc@1: 0.7701, Avg Acc@5: 0.9354
2022-01-21 19:12:12,843 Val Step[1100/1563], Avg Loss: 1.0671, Avg Acc@1: 0.7700, Avg Acc@5: 0.9354
2022-01-21 19:12:14,735 Val Step[1150/1563], Avg Loss: 1.0652, Avg Acc@1: 0.7703, Avg Acc@5: 0.9357
2022-01-21 19:12:16,573 Val Step[1200/1563], Avg Loss: 1.0638, Avg Acc@1: 0.7709, Avg Acc@5: 0.9358
2022-01-21 19:12:18,450 Val Step[1250/1563], Avg Loss: 1.0633, Avg Acc@1: 0.7704, Avg Acc@5: 0.9361
2022-01-21 19:12:20,285 Val Step[1300/1563], Avg Loss: 1.0660, Avg Acc@1: 0.7704, Avg Acc@5: 0.9358
2022-01-21 19:12:22,101 Val Step[1350/1563], Avg Loss: 1.0670, Avg Acc@1: 0.7699, Avg Acc@5: 0.9357
2022-01-21 19:12:23,901 Val Step[1400/1563], Avg Loss: 1.0665, Avg Acc@1: 0.7699, Avg Acc@5: 0.9357
2022-01-21 19:12:25,774 Val Step[1450/1563], Avg Loss: 1.0657, Avg Acc@1: 0.7701, Avg Acc@5: 0.9358
2022-01-21 19:12:27,828 Val Step[1500/1563], Avg Loss: 1.0658, Avg Acc@1: 0.7705, Avg Acc@5: 0.9359
2022-01-21 19:12:29,847 Val Step[1550/1563], Avg Loss: 1.0658, Avg Acc@1: 0.7704, Avg Acc@5: 0.9359
2022-01-21 19:12:32,030 ----- Epoch[256/300], Validation Loss: 1.0657, Validation Acc@1: 0.7704, Validation Acc@5: 0.9360, time: 153.65
2022-01-21 19:12:33,189 the pre best model acc:0.7684, at epoch 254
2022-01-21 19:12:33,189 current best model acc:0.7704, at epoch 256
2022-01-21 19:12:33,189 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 19:12:33,189 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 19:12:33,189 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 19:12:33,190 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 19:12:33,190 Now training epoch 257. LR=0.000067
2022-01-21 19:14:28,183 Epoch[257/300], Step[0000/1252], Avg Loss: 2.4662, Avg Acc: 0.5820
2022-01-21 19:15:53,618 Epoch[257/300], Step[0050/1252], Avg Loss: 2.9872, Avg Acc: 0.4436
2022-01-21 19:17:21,139 Epoch[257/300], Step[0100/1252], Avg Loss: 2.9838, Avg Acc: 0.4593
2022-01-21 19:18:46,738 Epoch[257/300], Step[0150/1252], Avg Loss: 2.9898, Avg Acc: 0.4685
2022-01-21 19:20:14,089 Epoch[257/300], Step[0200/1252], Avg Loss: 2.9835, Avg Acc: 0.4659
2022-01-21 19:21:41,261 Epoch[257/300], Step[0250/1252], Avg Loss: 2.9667, Avg Acc: 0.4721
2022-01-21 19:23:07,778 Epoch[257/300], Step[0300/1252], Avg Loss: 2.9617, Avg Acc: 0.4731
2022-01-21 19:24:34,819 Epoch[257/300], Step[0350/1252], Avg Loss: 2.9689, Avg Acc: 0.4680
2022-01-21 19:26:02,190 Epoch[257/300], Step[0400/1252], Avg Loss: 2.9751, Avg Acc: 0.4678
2022-01-21 19:27:27,628 Epoch[257/300], Step[0450/1252], Avg Loss: 2.9826, Avg Acc: 0.4665
2022-01-21 19:28:55,283 Epoch[257/300], Step[0500/1252], Avg Loss: 2.9872, Avg Acc: 0.4692
2022-01-21 19:30:22,735 Epoch[257/300], Step[0550/1252], Avg Loss: 2.9860, Avg Acc: 0.4677
2022-01-21 19:31:50,133 Epoch[257/300], Step[0600/1252], Avg Loss: 2.9843, Avg Acc: 0.4691
2022-01-21 19:33:18,572 Epoch[257/300], Step[0650/1252], Avg Loss: 2.9904, Avg Acc: 0.4680
2022-01-21 19:34:47,783 Epoch[257/300], Step[0700/1252], Avg Loss: 2.9899, Avg Acc: 0.4683
2022-01-21 19:36:16,663 Epoch[257/300], Step[0750/1252], Avg Loss: 2.9856, Avg Acc: 0.4691
2022-01-21 19:37:44,968 Epoch[257/300], Step[0800/1252], Avg Loss: 2.9834, Avg Acc: 0.4707
2022-01-21 19:39:14,156 Epoch[257/300], Step[0850/1252], Avg Loss: 2.9874, Avg Acc: 0.4707
2022-01-21 19:40:43,375 Epoch[257/300], Step[0900/1252], Avg Loss: 2.9898, Avg Acc: 0.4692
2022-01-21 19:42:11,369 Epoch[257/300], Step[0950/1252], Avg Loss: 2.9899, Avg Acc: 0.4698
2022-01-21 19:43:39,483 Epoch[257/300], Step[1000/1252], Avg Loss: 2.9898, Avg Acc: 0.4719
2022-01-21 19:45:08,550 Epoch[257/300], Step[1050/1252], Avg Loss: 2.9889, Avg Acc: 0.4717
2022-01-21 19:46:37,493 Epoch[257/300], Step[1100/1252], Avg Loss: 2.9926, Avg Acc: 0.4721
2022-01-21 19:48:05,691 Epoch[257/300], Step[1150/1252], Avg Loss: 2.9946, Avg Acc: 0.4702
2022-01-21 19:49:34,595 Epoch[257/300], Step[1200/1252], Avg Loss: 2.9982, Avg Acc: 0.4700
2022-01-21 19:51:04,444 Epoch[257/300], Step[1250/1252], Avg Loss: 2.9980, Avg Acc: 0.4698
2022-01-21 19:51:11,095 ----- Epoch[257/300], Train Loss: 2.9980, Train Acc: 0.4699, time: 2317.90, Best Val(epoch256) Acc@1: 0.7704
2022-01-21 19:51:11,095 Now training epoch 258. LR=0.000064
2022-01-21 19:53:03,190 Epoch[258/300], Step[0000/1252], Avg Loss: 2.4883, Avg Acc: 0.5625
2022-01-21 19:54:31,677 Epoch[258/300], Step[0050/1252], Avg Loss: 3.0052, Avg Acc: 0.4822
2022-01-21 19:55:59,817 Epoch[258/300], Step[0100/1252], Avg Loss: 2.9866, Avg Acc: 0.4815
2022-01-21 19:57:27,267 Epoch[258/300], Step[0150/1252], Avg Loss: 3.0044, Avg Acc: 0.4771
2022-01-21 19:58:54,755 Epoch[258/300], Step[0200/1252], Avg Loss: 2.9949, Avg Acc: 0.4830
2022-01-21 20:00:22,679 Epoch[258/300], Step[0250/1252], Avg Loss: 2.9978, Avg Acc: 0.4779
2022-01-21 20:01:50,752 Epoch[258/300], Step[0300/1252], Avg Loss: 2.9910, Avg Acc: 0.4798
2022-01-21 20:03:18,951 Epoch[258/300], Step[0350/1252], Avg Loss: 2.9922, Avg Acc: 0.4765
2022-01-21 20:04:47,440 Epoch[258/300], Step[0400/1252], Avg Loss: 2.9948, Avg Acc: 0.4760
2022-01-21 20:06:15,995 Epoch[258/300], Step[0450/1252], Avg Loss: 2.9985, Avg Acc: 0.4744
2022-01-21 20:07:43,888 Epoch[258/300], Step[0500/1252], Avg Loss: 2.9979, Avg Acc: 0.4731
2022-01-21 20:09:10,874 Epoch[258/300], Step[0550/1252], Avg Loss: 2.9961, Avg Acc: 0.4747
2022-01-21 20:10:38,849 Epoch[258/300], Step[0600/1252], Avg Loss: 3.0007, Avg Acc: 0.4725
2022-01-21 20:12:05,413 Epoch[258/300], Step[0650/1252], Avg Loss: 2.9949, Avg Acc: 0.4729
2022-01-21 20:13:33,596 Epoch[258/300], Step[0700/1252], Avg Loss: 2.9941, Avg Acc: 0.4751
2022-01-21 20:15:00,776 Epoch[258/300], Step[0750/1252], Avg Loss: 2.9941, Avg Acc: 0.4734
2022-01-21 20:16:30,037 Epoch[258/300], Step[0800/1252], Avg Loss: 2.9930, Avg Acc: 0.4726
2022-01-21 20:17:58,992 Epoch[258/300], Step[0850/1252], Avg Loss: 2.9901, Avg Acc: 0.4707
2022-01-21 20:19:26,600 Epoch[258/300], Step[0900/1252], Avg Loss: 2.9890, Avg Acc: 0.4708
2022-01-21 20:20:55,323 Epoch[258/300], Step[0950/1252], Avg Loss: 2.9852, Avg Acc: 0.4697
2022-01-21 20:22:23,572 Epoch[258/300], Step[1000/1252], Avg Loss: 2.9857, Avg Acc: 0.4698
2022-01-21 20:23:52,583 Epoch[258/300], Step[1050/1252], Avg Loss: 2.9882, Avg Acc: 0.4683
2022-01-21 20:25:21,567 Epoch[258/300], Step[1100/1252], Avg Loss: 2.9865, Avg Acc: 0.4693
2022-01-21 20:26:49,487 Epoch[258/300], Step[1150/1252], Avg Loss: 2.9875, Avg Acc: 0.4691
2022-01-21 20:28:17,466 Epoch[258/300], Step[1200/1252], Avg Loss: 2.9897, Avg Acc: 0.4701
2022-01-21 20:29:46,628 Epoch[258/300], Step[1250/1252], Avg Loss: 2.9899, Avg Acc: 0.4699
2022-01-21 20:29:53,189 ----- Epoch[258/300], Train Loss: 2.9899, Train Acc: 0.4700, time: 2322.09, Best Val(epoch256) Acc@1: 0.7704
2022-01-21 20:29:53,189 ----- Validation after Epoch: 258
2022-01-21 20:31:32,622 Val Step[0000/1563], Avg Loss: 0.7963, Avg Acc@1: 0.8125, Avg Acc@5: 1.0000
2022-01-21 20:31:34,459 Val Step[0050/1563], Avg Loss: 1.0345, Avg Acc@1: 0.7763, Avg Acc@5: 0.9320
2022-01-21 20:31:36,338 Val Step[0100/1563], Avg Loss: 1.0537, Avg Acc@1: 0.7772, Avg Acc@5: 0.9335
2022-01-21 20:31:38,241 Val Step[0150/1563], Avg Loss: 1.0569, Avg Acc@1: 0.7767, Avg Acc@5: 0.9317
2022-01-21 20:31:40,093 Val Step[0200/1563], Avg Loss: 1.0563, Avg Acc@1: 0.7780, Avg Acc@5: 0.9330
2022-01-21 20:31:41,905 Val Step[0250/1563], Avg Loss: 1.0492, Avg Acc@1: 0.7771, Avg Acc@5: 0.9349
2022-01-21 20:31:43,755 Val Step[0300/1563], Avg Loss: 1.0509, Avg Acc@1: 0.7765, Avg Acc@5: 0.9342
2022-01-21 20:31:45,573 Val Step[0350/1563], Avg Loss: 1.0543, Avg Acc@1: 0.7766, Avg Acc@5: 0.9338
2022-01-21 20:31:47,507 Val Step[0400/1563], Avg Loss: 1.0548, Avg Acc@1: 0.7763, Avg Acc@5: 0.9333
2022-01-21 20:31:49,381 Val Step[0450/1563], Avg Loss: 1.0606, Avg Acc@1: 0.7731, Avg Acc@5: 0.9336
2022-01-21 20:31:51,179 Val Step[0500/1563], Avg Loss: 1.0625, Avg Acc@1: 0.7730, Avg Acc@5: 0.9339
2022-01-21 20:31:52,976 Val Step[0550/1563], Avg Loss: 1.0623, Avg Acc@1: 0.7721, Avg Acc@5: 0.9344
2022-01-21 20:31:54,896 Val Step[0600/1563], Avg Loss: 1.0605, Avg Acc@1: 0.7714, Avg Acc@5: 0.9345
2022-01-21 20:31:56,931 Val Step[0650/1563], Avg Loss: 1.0620, Avg Acc@1: 0.7713, Avg Acc@5: 0.9345
2022-01-21 20:31:58,862 Val Step[0700/1563], Avg Loss: 1.0596, Avg Acc@1: 0.7719, Avg Acc@5: 0.9352
2022-01-21 20:32:00,701 Val Step[0750/1563], Avg Loss: 1.0647, Avg Acc@1: 0.7702, Avg Acc@5: 0.9349
2022-01-21 20:32:02,551 Val Step[0800/1563], Avg Loss: 1.0642, Avg Acc@1: 0.7706, Avg Acc@5: 0.9344
2022-01-21 20:32:04,394 Val Step[0850/1563], Avg Loss: 1.0659, Avg Acc@1: 0.7703, Avg Acc@5: 0.9339
2022-01-21 20:32:06,222 Val Step[0900/1563], Avg Loss: 1.0634, Avg Acc@1: 0.7711, Avg Acc@5: 0.9343
2022-01-21 20:32:08,043 Val Step[0950/1563], Avg Loss: 1.0628, Avg Acc@1: 0.7716, Avg Acc@5: 0.9344
2022-01-21 20:32:09,896 Val Step[1000/1563], Avg Loss: 1.0642, Avg Acc@1: 0.7712, Avg Acc@5: 0.9342
2022-01-21 20:32:11,788 Val Step[1050/1563], Avg Loss: 1.0653, Avg Acc@1: 0.7703, Avg Acc@5: 0.9339
2022-01-21 20:32:13,681 Val Step[1100/1563], Avg Loss: 1.0652, Avg Acc@1: 0.7703, Avg Acc@5: 0.9340
2022-01-21 20:32:15,515 Val Step[1150/1563], Avg Loss: 1.0632, Avg Acc@1: 0.7707, Avg Acc@5: 0.9343
2022-01-21 20:32:17,325 Val Step[1200/1563], Avg Loss: 1.0623, Avg Acc@1: 0.7714, Avg Acc@5: 0.9343
2022-01-21 20:32:19,135 Val Step[1250/1563], Avg Loss: 1.0622, Avg Acc@1: 0.7710, Avg Acc@5: 0.9345
2022-01-21 20:32:20,941 Val Step[1300/1563], Avg Loss: 1.0652, Avg Acc@1: 0.7705, Avg Acc@5: 0.9341
2022-01-21 20:32:22,764 Val Step[1350/1563], Avg Loss: 1.0664, Avg Acc@1: 0.7701, Avg Acc@5: 0.9340
2022-01-21 20:32:24,591 Val Step[1400/1563], Avg Loss: 1.0659, Avg Acc@1: 0.7699, Avg Acc@5: 0.9340
2022-01-21 20:32:26,395 Val Step[1450/1563], Avg Loss: 1.0648, Avg Acc@1: 0.7702, Avg Acc@5: 0.9342
2022-01-21 20:32:28,197 Val Step[1500/1563], Avg Loss: 1.0647, Avg Acc@1: 0.7701, Avg Acc@5: 0.9345
2022-01-21 20:32:29,960 Val Step[1550/1563], Avg Loss: 1.0653, Avg Acc@1: 0.7698, Avg Acc@5: 0.9344
2022-01-21 20:32:31,672 ----- Epoch[258/300], Validation Loss: 1.0653, Validation Acc@1: 0.7698, Validation Acc@5: 0.9344, time: 158.48
2022-01-21 20:32:31,672 Now training epoch 259. LR=0.000061
2022-01-21 20:34:29,724 Epoch[259/300], Step[0000/1252], Avg Loss: 2.8151, Avg Acc: 0.5332
2022-01-21 20:35:56,960 Epoch[259/300], Step[0050/1252], Avg Loss: 3.0026, Avg Acc: 0.4849
2022-01-21 20:37:24,196 Epoch[259/300], Step[0100/1252], Avg Loss: 2.9873, Avg Acc: 0.4839
2022-01-21 20:38:52,731 Epoch[259/300], Step[0150/1252], Avg Loss: 2.9831, Avg Acc: 0.4841
2022-01-21 20:40:20,440 Epoch[259/300], Step[0200/1252], Avg Loss: 2.9955, Avg Acc: 0.4791
2022-01-21 20:41:48,654 Epoch[259/300], Step[0250/1252], Avg Loss: 2.9812, Avg Acc: 0.4832
2022-01-21 20:43:17,415 Epoch[259/300], Step[0300/1252], Avg Loss: 2.9867, Avg Acc: 0.4838
2022-01-21 20:44:46,744 Epoch[259/300], Step[0350/1252], Avg Loss: 2.9858, Avg Acc: 0.4758
2022-01-21 20:46:15,533 Epoch[259/300], Step[0400/1252], Avg Loss: 2.9808, Avg Acc: 0.4769
2022-01-21 20:47:43,901 Epoch[259/300], Step[0450/1252], Avg Loss: 2.9825, Avg Acc: 0.4775
2022-01-21 20:49:12,156 Epoch[259/300], Step[0500/1252], Avg Loss: 2.9844, Avg Acc: 0.4760
2022-01-21 20:50:40,349 Epoch[259/300], Step[0550/1252], Avg Loss: 2.9892, Avg Acc: 0.4737
2022-01-21 20:52:07,601 Epoch[259/300], Step[0600/1252], Avg Loss: 2.9925, Avg Acc: 0.4734
2022-01-21 20:53:35,453 Epoch[259/300], Step[0650/1252], Avg Loss: 2.9906, Avg Acc: 0.4720
2022-01-21 20:55:03,454 Epoch[259/300], Step[0700/1252], Avg Loss: 2.9919, Avg Acc: 0.4720
2022-01-21 20:56:31,151 Epoch[259/300], Step[0750/1252], Avg Loss: 2.9954, Avg Acc: 0.4711
2022-01-21 20:57:58,977 Epoch[259/300], Step[0800/1252], Avg Loss: 2.9936, Avg Acc: 0.4724
2022-01-21 20:59:27,049 Epoch[259/300], Step[0850/1252], Avg Loss: 2.9922, Avg Acc: 0.4718
2022-01-21 21:00:54,990 Epoch[259/300], Step[0900/1252], Avg Loss: 2.9954, Avg Acc: 0.4721
2022-01-21 21:02:22,409 Epoch[259/300], Step[0950/1252], Avg Loss: 2.9982, Avg Acc: 0.4720
2022-01-21 21:03:50,690 Epoch[259/300], Step[1000/1252], Avg Loss: 2.9977, Avg Acc: 0.4702
2022-01-21 21:05:17,689 Epoch[259/300], Step[1050/1252], Avg Loss: 2.9977, Avg Acc: 0.4714
2022-01-21 21:06:46,022 Epoch[259/300], Step[1100/1252], Avg Loss: 2.9953, Avg Acc: 0.4714
2022-01-21 21:08:14,060 Epoch[259/300], Step[1150/1252], Avg Loss: 2.9958, Avg Acc: 0.4705
2022-01-21 21:09:42,441 Epoch[259/300], Step[1200/1252], Avg Loss: 2.9969, Avg Acc: 0.4702
2022-01-21 21:11:11,291 Epoch[259/300], Step[1250/1252], Avg Loss: 2.9989, Avg Acc: 0.4683
2022-01-21 21:11:17,769 ----- Epoch[259/300], Train Loss: 2.9989, Train Acc: 0.4683, time: 2326.09, Best Val(epoch256) Acc@1: 0.7704
2022-01-21 21:11:17,770 Now training epoch 260. LR=0.000059
2022-01-21 21:13:19,937 Epoch[260/300], Step[0000/1252], Avg Loss: 2.4404, Avg Acc: 0.7334
2022-01-21 21:14:46,144 Epoch[260/300], Step[0050/1252], Avg Loss: 2.8738, Avg Acc: 0.4905
2022-01-21 21:16:12,898 Epoch[260/300], Step[0100/1252], Avg Loss: 2.8927, Avg Acc: 0.4985
2022-01-21 21:17:40,623 Epoch[260/300], Step[0150/1252], Avg Loss: 2.9008, Avg Acc: 0.4857
2022-01-21 21:19:06,825 Epoch[260/300], Step[0200/1252], Avg Loss: 2.9106, Avg Acc: 0.4872
2022-01-21 21:20:33,887 Epoch[260/300], Step[0250/1252], Avg Loss: 2.9270, Avg Acc: 0.4820
2022-01-21 21:22:01,434 Epoch[260/300], Step[0300/1252], Avg Loss: 2.9285, Avg Acc: 0.4804
2022-01-21 21:23:29,273 Epoch[260/300], Step[0350/1252], Avg Loss: 2.9446, Avg Acc: 0.4828
2022-01-21 21:24:56,582 Epoch[260/300], Step[0400/1252], Avg Loss: 2.9494, Avg Acc: 0.4847
2022-01-21 21:26:25,285 Epoch[260/300], Step[0450/1252], Avg Loss: 2.9512, Avg Acc: 0.4813
2022-01-21 21:27:52,680 Epoch[260/300], Step[0500/1252], Avg Loss: 2.9543, Avg Acc: 0.4780
2022-01-21 21:29:21,328 Epoch[260/300], Step[0550/1252], Avg Loss: 2.9573, Avg Acc: 0.4790
2022-01-21 21:30:49,967 Epoch[260/300], Step[0600/1252], Avg Loss: 2.9627, Avg Acc: 0.4775
2022-01-21 21:32:16,354 Epoch[260/300], Step[0650/1252], Avg Loss: 2.9619, Avg Acc: 0.4787
2022-01-21 21:33:44,483 Epoch[260/300], Step[0700/1252], Avg Loss: 2.9643, Avg Acc: 0.4767
2022-01-21 21:35:12,994 Epoch[260/300], Step[0750/1252], Avg Loss: 2.9613, Avg Acc: 0.4767
2022-01-21 21:36:41,031 Epoch[260/300], Step[0800/1252], Avg Loss: 2.9655, Avg Acc: 0.4765
2022-01-21 21:38:08,862 Epoch[260/300], Step[0850/1252], Avg Loss: 2.9684, Avg Acc: 0.4746
2022-01-21 21:39:37,323 Epoch[260/300], Step[0900/1252], Avg Loss: 2.9674, Avg Acc: 0.4738
2022-01-21 21:41:04,983 Epoch[260/300], Step[0950/1252], Avg Loss: 2.9679, Avg Acc: 0.4750
2022-01-21 21:42:32,721 Epoch[260/300], Step[1000/1252], Avg Loss: 2.9699, Avg Acc: 0.4756
2022-01-21 21:44:00,774 Epoch[260/300], Step[1050/1252], Avg Loss: 2.9714, Avg Acc: 0.4742
2022-01-21 21:45:27,189 Epoch[260/300], Step[1100/1252], Avg Loss: 2.9710, Avg Acc: 0.4756
2022-01-21 21:46:53,602 Epoch[260/300], Step[1150/1252], Avg Loss: 2.9719, Avg Acc: 0.4748
2022-01-21 21:48:21,582 Epoch[260/300], Step[1200/1252], Avg Loss: 2.9738, Avg Acc: 0.4748
2022-01-21 21:49:49,580 Epoch[260/300], Step[1250/1252], Avg Loss: 2.9750, Avg Acc: 0.4730
2022-01-21 21:49:55,624 ----- Epoch[260/300], Train Loss: 2.9749, Train Acc: 0.4730, time: 2317.85, Best Val(epoch256) Acc@1: 0.7704
2022-01-21 21:49:55,624 ----- Validation after Epoch: 260
2022-01-21 21:51:37,651 Val Step[0000/1563], Avg Loss: 0.8103, Avg Acc@1: 0.7812, Avg Acc@5: 1.0000
2022-01-21 21:51:39,513 Val Step[0050/1563], Avg Loss: 1.0116, Avg Acc@1: 0.7831, Avg Acc@5: 0.9369
2022-01-21 21:51:41,355 Val Step[0100/1563], Avg Loss: 1.0345, Avg Acc@1: 0.7806, Avg Acc@5: 0.9366
2022-01-21 21:51:43,190 Val Step[0150/1563], Avg Loss: 1.0383, Avg Acc@1: 0.7784, Avg Acc@5: 0.9350
2022-01-21 21:51:45,061 Val Step[0200/1563], Avg Loss: 1.0360, Avg Acc@1: 0.7795, Avg Acc@5: 0.9359
2022-01-21 21:51:46,894 Val Step[0250/1563], Avg Loss: 1.0244, Avg Acc@1: 0.7801, Avg Acc@5: 0.9375
2022-01-21 21:51:48,735 Val Step[0300/1563], Avg Loss: 1.0260, Avg Acc@1: 0.7789, Avg Acc@5: 0.9357
2022-01-21 21:51:50,671 Val Step[0350/1563], Avg Loss: 1.0314, Avg Acc@1: 0.7776, Avg Acc@5: 0.9351
2022-01-21 21:51:52,809 Val Step[0400/1563], Avg Loss: 1.0302, Avg Acc@1: 0.7776, Avg Acc@5: 0.9354
2022-01-21 21:51:54,903 Val Step[0450/1563], Avg Loss: 1.0351, Avg Acc@1: 0.7745, Avg Acc@5: 0.9349
2022-01-21 21:51:56,965 Val Step[0500/1563], Avg Loss: 1.0366, Avg Acc@1: 0.7740, Avg Acc@5: 0.9354
2022-01-21 21:51:59,033 Val Step[0550/1563], Avg Loss: 1.0370, Avg Acc@1: 0.7727, Avg Acc@5: 0.9356
2022-01-21 21:52:01,113 Val Step[0600/1563], Avg Loss: 1.0366, Avg Acc@1: 0.7722, Avg Acc@5: 0.9357
2022-01-21 21:52:03,187 Val Step[0650/1563], Avg Loss: 1.0374, Avg Acc@1: 0.7721, Avg Acc@5: 0.9358
2022-01-21 21:52:05,272 Val Step[0700/1563], Avg Loss: 1.0345, Avg Acc@1: 0.7730, Avg Acc@5: 0.9367
2022-01-21 21:52:07,414 Val Step[0750/1563], Avg Loss: 1.0406, Avg Acc@1: 0.7708, Avg Acc@5: 0.9365
2022-01-21 21:52:09,515 Val Step[0800/1563], Avg Loss: 1.0403, Avg Acc@1: 0.7707, Avg Acc@5: 0.9363
2022-01-21 21:52:11,477 Val Step[0850/1563], Avg Loss: 1.0416, Avg Acc@1: 0.7703, Avg Acc@5: 0.9360
2022-01-21 21:52:13,353 Val Step[0900/1563], Avg Loss: 1.0389, Avg Acc@1: 0.7710, Avg Acc@5: 0.9363
2022-01-21 21:52:15,173 Val Step[0950/1563], Avg Loss: 1.0379, Avg Acc@1: 0.7719, Avg Acc@5: 0.9362
2022-01-21 21:52:16,975 Val Step[1000/1563], Avg Loss: 1.0398, Avg Acc@1: 0.7719, Avg Acc@5: 0.9358
2022-01-21 21:52:18,771 Val Step[1050/1563], Avg Loss: 1.0407, Avg Acc@1: 0.7713, Avg Acc@5: 0.9355
2022-01-21 21:52:20,595 Val Step[1100/1563], Avg Loss: 1.0407, Avg Acc@1: 0.7711, Avg Acc@5: 0.9355
2022-01-21 21:52:22,561 Val Step[1150/1563], Avg Loss: 1.0385, Avg Acc@1: 0.7714, Avg Acc@5: 0.9357
2022-01-21 21:52:24,447 Val Step[1200/1563], Avg Loss: 1.0376, Avg Acc@1: 0.7719, Avg Acc@5: 0.9355
2022-01-21 21:52:26,319 Val Step[1250/1563], Avg Loss: 1.0370, Avg Acc@1: 0.7715, Avg Acc@5: 0.9360
2022-01-21 21:52:28,184 Val Step[1300/1563], Avg Loss: 1.0402, Avg Acc@1: 0.7713, Avg Acc@5: 0.9356
2022-01-21 21:52:29,997 Val Step[1350/1563], Avg Loss: 1.0418, Avg Acc@1: 0.7707, Avg Acc@5: 0.9354
2022-01-21 21:52:31,856 Val Step[1400/1563], Avg Loss: 1.0411, Avg Acc@1: 0.7704, Avg Acc@5: 0.9354
2022-01-21 21:52:33,809 Val Step[1450/1563], Avg Loss: 1.0401, Avg Acc@1: 0.7709, Avg Acc@5: 0.9354
2022-01-21 21:52:35,764 Val Step[1500/1563], Avg Loss: 1.0399, Avg Acc@1: 0.7708, Avg Acc@5: 0.9355
2022-01-21 21:52:37,541 Val Step[1550/1563], Avg Loss: 1.0401, Avg Acc@1: 0.7707, Avg Acc@5: 0.9354
2022-01-21 21:52:39,241 ----- Epoch[260/300], Validation Loss: 1.0399, Validation Acc@1: 0.7706, Validation Acc@5: 0.9355, time: 163.61
2022-01-21 21:52:40,468 the pre best model acc:0.7704, at epoch 256
2022-01-21 21:52:40,469 current best model acc:0.7706, at epoch 260
2022-01-21 21:52:40,469 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 21:52:40,469 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 21:52:40,469 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-21 21:52:40,469 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-21 21:52:40,469 Now training epoch 261. LR=0.000057
2022-01-21 21:54:41,235 Epoch[261/300], Step[0000/1252], Avg Loss: 2.9267, Avg Acc: 0.3691
2022-01-21 21:56:09,259 Epoch[261/300], Step[0050/1252], Avg Loss: 3.0320, Avg Acc: 0.4568
2022-01-21 21:57:35,939 Epoch[261/300], Step[0100/1252], Avg Loss: 3.0234, Avg Acc: 0.4709
2022-01-21 21:59:02,053 Epoch[261/300], Step[0150/1252], Avg Loss: 2.9969, Avg Acc: 0.4689
2022-01-21 22:00:29,955 Epoch[261/300], Step[0200/1252], Avg Loss: 2.9860, Avg Acc: 0.4738
2022-01-21 22:01:56,533 Epoch[261/300], Step[0250/1252], Avg Loss: 2.9812, Avg Acc: 0.4849
2022-01-21 22:03:24,850 Epoch[261/300], Step[0300/1252], Avg Loss: 2.9913, Avg Acc: 0.4746
2022-01-21 22:04:51,649 Epoch[261/300], Step[0350/1252], Avg Loss: 2.9831, Avg Acc: 0.4770
2022-01-21 22:06:19,010 Epoch[261/300], Step[0400/1252], Avg Loss: 2.9930, Avg Acc: 0.4752
2022-01-21 22:07:46,208 Epoch[261/300], Step[0450/1252], Avg Loss: 2.9970, Avg Acc: 0.4723
2022-01-21 22:09:13,580 Epoch[261/300], Step[0500/1252], Avg Loss: 2.9962, Avg Acc: 0.4732
2022-01-21 22:10:42,070 Epoch[261/300], Step[0550/1252], Avg Loss: 2.9949, Avg Acc: 0.4705
2022-01-21 22:12:10,378 Epoch[261/300], Step[0600/1252], Avg Loss: 2.9923, Avg Acc: 0.4705
2022-01-21 22:13:39,996 Epoch[261/300], Step[0650/1252], Avg Loss: 2.9912, Avg Acc: 0.4696
2022-01-21 22:15:09,269 Epoch[261/300], Step[0700/1252], Avg Loss: 2.9898, Avg Acc: 0.4678
2022-01-21 22:16:38,751 Epoch[261/300], Step[0750/1252], Avg Loss: 2.9934, Avg Acc: 0.4666
2022-01-21 22:18:06,683 Epoch[261/300], Step[0800/1252], Avg Loss: 2.9938, Avg Acc: 0.4698
2022-01-21 22:19:35,392 Epoch[261/300], Step[0850/1252], Avg Loss: 2.9941, Avg Acc: 0.4705
2022-01-21 22:21:03,731 Epoch[261/300], Step[0900/1252], Avg Loss: 2.9942, Avg Acc: 0.4693
2022-01-21 22:22:31,933 Epoch[261/300], Step[0950/1252], Avg Loss: 2.9955, Avg Acc: 0.4688
2022-01-21 22:24:00,724 Epoch[261/300], Step[1000/1252], Avg Loss: 2.9920, Avg Acc: 0.4691
2022-01-21 22:25:28,619 Epoch[261/300], Step[1050/1252], Avg Loss: 2.9928, Avg Acc: 0.4698
2022-01-21 22:26:56,863 Epoch[261/300], Step[1100/1252], Avg Loss: 2.9941, Avg Acc: 0.4684
2022-01-21 22:28:24,543 Epoch[261/300], Step[1150/1252], Avg Loss: 2.9933, Avg Acc: 0.4694
2022-01-21 22:29:52,723 Epoch[261/300], Step[1200/1252], Avg Loss: 2.9964, Avg Acc: 0.4698
2022-01-21 22:31:21,283 Epoch[261/300], Step[1250/1252], Avg Loss: 2.9926, Avg Acc: 0.4710
2022-01-21 22:31:27,559 ----- Epoch[261/300], Train Loss: 2.9926, Train Acc: 0.4710, time: 2327.09, Best Val(epoch260) Acc@1: 0.7706
2022-01-21 22:31:27,559 Now training epoch 262. LR=0.000054
2022-01-21 22:33:27,918 Epoch[262/300], Step[0000/1252], Avg Loss: 3.2057, Avg Acc: 0.2754
2022-01-21 22:34:55,676 Epoch[262/300], Step[0050/1252], Avg Loss: 2.9420, Avg Acc: 0.4659
2022-01-21 22:36:22,022 Epoch[262/300], Step[0100/1252], Avg Loss: 2.9566, Avg Acc: 0.4849
2022-01-21 22:37:50,521 Epoch[262/300], Step[0150/1252], Avg Loss: 2.9682, Avg Acc: 0.4681
2022-01-21 22:39:18,751 Epoch[262/300], Step[0200/1252], Avg Loss: 2.9587, Avg Acc: 0.4622
2022-01-21 22:40:46,728 Epoch[262/300], Step[0250/1252], Avg Loss: 2.9656, Avg Acc: 0.4679
2022-01-21 22:42:13,266 Epoch[262/300], Step[0300/1252], Avg Loss: 2.9751, Avg Acc: 0.4716
2022-01-21 22:43:40,593 Epoch[262/300], Step[0350/1252], Avg Loss: 2.9660, Avg Acc: 0.4789
2022-01-21 22:45:08,586 Epoch[262/300], Step[0400/1252], Avg Loss: 2.9689, Avg Acc: 0.4823
2022-01-21 22:46:36,286 Epoch[262/300], Step[0450/1252], Avg Loss: 2.9636, Avg Acc: 0.4836
2022-01-21 22:48:04,521 Epoch[262/300], Step[0500/1252], Avg Loss: 2.9665, Avg Acc: 0.4827
2022-01-21 22:49:33,057 Epoch[262/300], Step[0550/1252], Avg Loss: 2.9689, Avg Acc: 0.4786
2022-01-21 22:51:01,765 Epoch[262/300], Step[0600/1252], Avg Loss: 2.9697, Avg Acc: 0.4779
2022-01-21 22:52:30,741 Epoch[262/300], Step[0650/1252], Avg Loss: 2.9687, Avg Acc: 0.4789
2022-01-21 22:53:58,632 Epoch[262/300], Step[0700/1252], Avg Loss: 2.9622, Avg Acc: 0.4809
2022-01-21 22:55:26,538 Epoch[262/300], Step[0750/1252], Avg Loss: 2.9626, Avg Acc: 0.4802
2022-01-21 22:56:54,766 Epoch[262/300], Step[0800/1252], Avg Loss: 2.9627, Avg Acc: 0.4793
2022-01-21 22:58:22,038 Epoch[262/300], Step[0850/1252], Avg Loss: 2.9663, Avg Acc: 0.4770
2022-01-21 22:59:50,313 Epoch[262/300], Step[0900/1252], Avg Loss: 2.9658, Avg Acc: 0.4752
2022-01-21 23:01:18,318 Epoch[262/300], Step[0950/1252], Avg Loss: 2.9650, Avg Acc: 0.4754
2022-01-21 23:02:46,658 Epoch[262/300], Step[1000/1252], Avg Loss: 2.9632, Avg Acc: 0.4770
2022-01-21 23:04:13,974 Epoch[262/300], Step[1050/1252], Avg Loss: 2.9673, Avg Acc: 0.4768
2022-01-21 23:05:41,696 Epoch[262/300], Step[1100/1252], Avg Loss: 2.9697, Avg Acc: 0.4754
2022-01-21 23:07:09,438 Epoch[262/300], Step[1150/1252], Avg Loss: 2.9689, Avg Acc: 0.4770
2022-01-21 23:08:36,981 Epoch[262/300], Step[1200/1252], Avg Loss: 2.9693, Avg Acc: 0.4752
2022-01-21 23:10:04,987 Epoch[262/300], Step[1250/1252], Avg Loss: 2.9723, Avg Acc: 0.4745
2022-01-21 23:10:11,509 ----- Epoch[262/300], Train Loss: 2.9724, Train Acc: 0.4745, time: 2323.95, Best Val(epoch260) Acc@1: 0.7706
2022-01-21 23:10:11,510 ----- Validation after Epoch: 262
