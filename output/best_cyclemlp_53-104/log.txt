2022-01-14 19:32:20,394 
AMP: False
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RE_COUNT: 1
  RE_MODE: pixel
  RE_PROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 256
  BATCH_SIZE_EVAL: 8
  CROP_PCT: 0.9
  DATASET: imagenet2012
  DATA_PATH: /root/paddlejob/workspace/train_data/datasets/Light_ILSVRC2012
  IMAGE_SIZE: 224
  NUM_WORKERS: 16
EVAL: False
LOCAL_RANK: 0
MODEL:
  MIXER:
    EMBED_DIMS: [64, 128, 320, 512]
    LAYERS: [2, 2, 4, 2]
    MLP_RATIOS: [4, 4, 4, 4]
    TRANSITIONS: [True, True, True, True]
  NAME: cyclemlp_b1
  NUM_CLASSES: 1000
  PRETRAINED: None
  RESUME: Best_CycleMLP
  TYPE: CycleMLP
NGPUS: 4
REPORT_FREQ: 50
SAVE: /root/paddlejob/workspace/output//train
SAVE_FREQ: 10
SEED: 0
TAG: default
TRAIN:
  ACCUM_ITER: 1
  AUTO_AUGMENT: True
  BASE_LR: 0.0005
  COLOR_JITTER: 0.4
  CUTMIX_ALPHA: 1.0
  CUTMIX_MINMAX: None
  END_LR: 5e-06
  GRAD_CLIP: 5.0
  LAST_EPOCH: 52
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    MILESTONES: 30, 60, 90
    NAME: warmupcosine
  MIXUP_ALPHA: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  NUM_EPOCHS: 300
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: AdamW
  RANDOM_ERASE_COUNT: 1
  RANDOM_ERASE_MODE: pixel
  RANDOM_ERASE_PROB: 0.25
  RANDOM_ERASE_SPLIT: False
  SMOOTHING: 0.1
  WARMUP_EPOCHS: 20
  WARMUP_START_LR: 5e-07
  WEIGHT_DECAY: 0.05
VALIDATE_FREQ: 2
VALIDATION:
  REQUIREMENTS: 0.789
2022-01-14 19:32:20,394 ----- world_size = 4, local_rank = 0
2022-01-14 19:32:21,149 ----- Total # of train batch (single gpu): 1252
2022-01-14 19:32:21,149 ----- Total # of val batch (single gpu): 1563
2022-01-14 19:32:23,423 ----- Resume Training: Load model and optmizer from Best_CycleMLP
2022-01-14 19:32:23,423 Start training from epoch 53.
2022-01-14 19:32:23,423 Now training epoch 53. LR=0.000966
2022-01-14 19:34:22,272 Epoch[053/300], Step[0000/1252], Avg Loss: 3.8299, Avg Acc: 0.2090
2022-01-14 19:35:50,184 Epoch[053/300], Step[0050/1252], Avg Loss: 3.8147, Avg Acc: 0.3330
2022-01-14 19:37:17,243 Epoch[053/300], Step[0100/1252], Avg Loss: 3.8039, Avg Acc: 0.3434
2022-01-14 19:38:44,161 Epoch[053/300], Step[0150/1252], Avg Loss: 3.7865, Avg Acc: 0.3439
2022-01-14 19:40:11,622 Epoch[053/300], Step[0200/1252], Avg Loss: 3.7627, Avg Acc: 0.3454
2022-01-14 19:41:39,508 Epoch[053/300], Step[0250/1252], Avg Loss: 3.7496, Avg Acc: 0.3485
2022-01-14 19:43:06,770 Epoch[053/300], Step[0300/1252], Avg Loss: 3.7355, Avg Acc: 0.3538
2022-01-14 19:44:35,499 Epoch[053/300], Step[0350/1252], Avg Loss: 3.7558, Avg Acc: 0.3546
2022-01-14 19:46:03,998 Epoch[053/300], Step[0400/1252], Avg Loss: 3.7526, Avg Acc: 0.3532
2022-01-14 19:47:32,272 Epoch[053/300], Step[0450/1252], Avg Loss: 3.7591, Avg Acc: 0.3512
2022-01-14 19:49:00,470 Epoch[053/300], Step[0500/1252], Avg Loss: 3.7570, Avg Acc: 0.3525
2022-01-14 19:50:27,813 Epoch[053/300], Step[0550/1252], Avg Loss: 3.7513, Avg Acc: 0.3536
2022-01-14 19:51:56,703 Epoch[053/300], Step[0600/1252], Avg Loss: 3.7547, Avg Acc: 0.3545
2022-01-14 19:53:26,061 Epoch[053/300], Step[0650/1252], Avg Loss: 3.7571, Avg Acc: 0.3528
2022-01-14 19:54:54,918 Epoch[053/300], Step[0700/1252], Avg Loss: 3.7561, Avg Acc: 0.3540
2022-01-14 19:56:24,572 Epoch[053/300], Step[0750/1252], Avg Loss: 3.7560, Avg Acc: 0.3555
2022-01-14 19:57:52,822 Epoch[053/300], Step[0800/1252], Avg Loss: 3.7567, Avg Acc: 0.3550
2022-01-14 19:59:21,928 Epoch[053/300], Step[0850/1252], Avg Loss: 3.7529, Avg Acc: 0.3542
2022-01-14 20:00:51,086 Epoch[053/300], Step[0900/1252], Avg Loss: 3.7539, Avg Acc: 0.3547
2022-01-14 20:02:20,947 Epoch[053/300], Step[0950/1252], Avg Loss: 3.7524, Avg Acc: 0.3554
2022-01-14 20:03:50,245 Epoch[053/300], Step[1000/1252], Avg Loss: 3.7526, Avg Acc: 0.3544
2022-01-14 20:05:18,554 Epoch[053/300], Step[1050/1252], Avg Loss: 3.7528, Avg Acc: 0.3526
2022-01-14 20:06:46,730 Epoch[053/300], Step[1100/1252], Avg Loss: 3.7531, Avg Acc: 0.3520
2022-01-14 20:08:14,434 Epoch[053/300], Step[1150/1252], Avg Loss: 3.7549, Avg Acc: 0.3514
2022-01-14 20:09:43,651 Epoch[053/300], Step[1200/1252], Avg Loss: 3.7554, Avg Acc: 0.3511
2022-01-14 20:11:11,702 Epoch[053/300], Step[1250/1252], Avg Loss: 3.7559, Avg Acc: 0.3505
2022-01-14 20:11:18,156 ----- Epoch[053/300], Train Loss: 3.7558, Train Acc: 0.3505, time: 2334.73
2022-01-14 20:11:18,156 Now training epoch 54. LR=0.000964
2022-01-14 20:13:26,876 Epoch[054/300], Step[0000/1252], Avg Loss: 3.8899, Avg Acc: 0.2119
2022-01-14 20:14:53,041 Epoch[054/300], Step[0050/1252], Avg Loss: 3.8284, Avg Acc: 0.3464
2022-01-14 20:16:18,943 Epoch[054/300], Step[0100/1252], Avg Loss: 3.7833, Avg Acc: 0.3437
2022-01-14 20:17:44,546 Epoch[054/300], Step[0150/1252], Avg Loss: 3.7884, Avg Acc: 0.3464
2022-01-14 20:19:10,987 Epoch[054/300], Step[0200/1252], Avg Loss: 3.7763, Avg Acc: 0.3508
2022-01-14 20:20:38,580 Epoch[054/300], Step[0250/1252], Avg Loss: 3.7663, Avg Acc: 0.3547
2022-01-14 20:22:04,077 Epoch[054/300], Step[0300/1252], Avg Loss: 3.7664, Avg Acc: 0.3572
2022-01-14 20:23:31,890 Epoch[054/300], Step[0350/1252], Avg Loss: 3.7691, Avg Acc: 0.3552
2022-01-14 20:24:59,301 Epoch[054/300], Step[0400/1252], Avg Loss: 3.7640, Avg Acc: 0.3555
2022-01-14 20:26:26,861 Epoch[054/300], Step[0450/1252], Avg Loss: 3.7646, Avg Acc: 0.3540
2022-01-14 20:27:54,140 Epoch[054/300], Step[0500/1252], Avg Loss: 3.7615, Avg Acc: 0.3521
2022-01-14 20:29:20,739 Epoch[054/300], Step[0550/1252], Avg Loss: 3.7596, Avg Acc: 0.3524
2022-01-14 20:30:47,625 Epoch[054/300], Step[0600/1252], Avg Loss: 3.7567, Avg Acc: 0.3547
2022-01-14 20:32:15,934 Epoch[054/300], Step[0650/1252], Avg Loss: 3.7517, Avg Acc: 0.3531
2022-01-14 20:33:41,976 Epoch[054/300], Step[0700/1252], Avg Loss: 3.7425, Avg Acc: 0.3545
2022-01-14 20:35:09,290 Epoch[054/300], Step[0750/1252], Avg Loss: 3.7404, Avg Acc: 0.3560
2022-01-14 20:36:35,991 Epoch[054/300], Step[0800/1252], Avg Loss: 3.7442, Avg Acc: 0.3540
2022-01-14 20:38:04,169 Epoch[054/300], Step[0850/1252], Avg Loss: 3.7444, Avg Acc: 0.3527
2022-01-14 20:39:31,333 Epoch[054/300], Step[0900/1252], Avg Loss: 3.7481, Avg Acc: 0.3526
2022-01-14 20:40:57,985 Epoch[054/300], Step[0950/1252], Avg Loss: 3.7512, Avg Acc: 0.3524
2022-01-14 20:42:25,391 Epoch[054/300], Step[1000/1252], Avg Loss: 3.7522, Avg Acc: 0.3516
2022-01-14 20:43:53,789 Epoch[054/300], Step[1050/1252], Avg Loss: 3.7515, Avg Acc: 0.3502
2022-01-14 20:45:21,923 Epoch[054/300], Step[1100/1252], Avg Loss: 3.7544, Avg Acc: 0.3501
2022-01-14 20:46:49,494 Epoch[054/300], Step[1150/1252], Avg Loss: 3.7542, Avg Acc: 0.3497
2022-01-14 20:48:17,645 Epoch[054/300], Step[1200/1252], Avg Loss: 3.7552, Avg Acc: 0.3509
2022-01-14 20:49:44,695 Epoch[054/300], Step[1250/1252], Avg Loss: 3.7562, Avg Acc: 0.3515
2022-01-14 20:49:51,752 ----- Epoch[054/300], Train Loss: 3.7562, Train Acc: 0.3515, time: 2313.59
2022-01-14 20:49:51,753 ----- Validation after Epoch: 54
2022-01-14 20:51:22,510 Val Step[0000/1563], Avg Loss: 1.3154, Avg Acc@1: 0.5938, Avg Acc@5: 0.9062
2022-01-14 20:51:24,428 Val Step[0050/1563], Avg Loss: 1.4632, Avg Acc@1: 0.6808, Avg Acc@5: 0.8811
2022-01-14 20:51:26,280 Val Step[0100/1563], Avg Loss: 1.4701, Avg Acc@1: 0.6801, Avg Acc@5: 0.8800
2022-01-14 20:51:28,162 Val Step[0150/1563], Avg Loss: 1.4562, Avg Acc@1: 0.6838, Avg Acc@5: 0.8802
2022-01-14 20:51:29,993 Val Step[0200/1563], Avg Loss: 1.4604, Avg Acc@1: 0.6821, Avg Acc@5: 0.8783
2022-01-14 20:51:31,875 Val Step[0250/1563], Avg Loss: 1.4420, Avg Acc@1: 0.6864, Avg Acc@5: 0.8794
2022-01-14 20:51:33,849 Val Step[0300/1563], Avg Loss: 1.4431, Avg Acc@1: 0.6857, Avg Acc@5: 0.8797
2022-01-14 20:51:35,853 Val Step[0350/1563], Avg Loss: 1.4485, Avg Acc@1: 0.6844, Avg Acc@5: 0.8797
2022-01-14 20:51:37,891 Val Step[0400/1563], Avg Loss: 1.4448, Avg Acc@1: 0.6851, Avg Acc@5: 0.8808
2022-01-14 20:51:39,986 Val Step[0450/1563], Avg Loss: 1.4526, Avg Acc@1: 0.6813, Avg Acc@5: 0.8803
2022-01-14 20:51:42,059 Val Step[0500/1563], Avg Loss: 1.4559, Avg Acc@1: 0.6801, Avg Acc@5: 0.8809
2022-01-14 20:51:44,111 Val Step[0550/1563], Avg Loss: 1.4586, Avg Acc@1: 0.6784, Avg Acc@5: 0.8808
2022-01-14 20:51:46,106 Val Step[0600/1563], Avg Loss: 1.4600, Avg Acc@1: 0.6777, Avg Acc@5: 0.8807
2022-01-14 20:51:48,064 Val Step[0650/1563], Avg Loss: 1.4633, Avg Acc@1: 0.6773, Avg Acc@5: 0.8806
2022-01-14 20:51:49,925 Val Step[0700/1563], Avg Loss: 1.4612, Avg Acc@1: 0.6767, Avg Acc@5: 0.8814
2022-01-14 20:51:51,805 Val Step[0750/1563], Avg Loss: 1.4662, Avg Acc@1: 0.6755, Avg Acc@5: 0.8810
2022-01-14 20:51:53,668 Val Step[0800/1563], Avg Loss: 1.4649, Avg Acc@1: 0.6765, Avg Acc@5: 0.8814
2022-01-14 20:51:55,549 Val Step[0850/1563], Avg Loss: 1.4674, Avg Acc@1: 0.6769, Avg Acc@5: 0.8809
2022-01-14 20:51:57,459 Val Step[0900/1563], Avg Loss: 1.4641, Avg Acc@1: 0.6774, Avg Acc@5: 0.8817
2022-01-14 20:51:59,379 Val Step[0950/1563], Avg Loss: 1.4642, Avg Acc@1: 0.6769, Avg Acc@5: 0.8820
2022-01-14 20:52:01,286 Val Step[1000/1563], Avg Loss: 1.4633, Avg Acc@1: 0.6773, Avg Acc@5: 0.8819
2022-01-14 20:52:03,201 Val Step[1050/1563], Avg Loss: 1.4671, Avg Acc@1: 0.6761, Avg Acc@5: 0.8812
2022-01-14 20:52:05,103 Val Step[1100/1563], Avg Loss: 1.4670, Avg Acc@1: 0.6756, Avg Acc@5: 0.8814
2022-01-14 20:52:07,031 Val Step[1150/1563], Avg Loss: 1.4661, Avg Acc@1: 0.6755, Avg Acc@5: 0.8811
2022-01-14 20:52:09,004 Val Step[1200/1563], Avg Loss: 1.4648, Avg Acc@1: 0.6756, Avg Acc@5: 0.8813
2022-01-14 20:52:10,960 Val Step[1250/1563], Avg Loss: 1.4632, Avg Acc@1: 0.6754, Avg Acc@5: 0.8812
2022-01-14 20:52:12,791 Val Step[1300/1563], Avg Loss: 1.4675, Avg Acc@1: 0.6750, Avg Acc@5: 0.8805
2022-01-14 20:52:14,668 Val Step[1350/1563], Avg Loss: 1.4674, Avg Acc@1: 0.6746, Avg Acc@5: 0.8806
2022-01-14 20:52:16,493 Val Step[1400/1563], Avg Loss: 1.4664, Avg Acc@1: 0.6743, Avg Acc@5: 0.8807
2022-01-14 20:52:18,353 Val Step[1450/1563], Avg Loss: 1.4665, Avg Acc@1: 0.6740, Avg Acc@5: 0.8808
2022-01-14 20:52:20,174 Val Step[1500/1563], Avg Loss: 1.4661, Avg Acc@1: 0.6742, Avg Acc@5: 0.8808
2022-01-14 20:52:22,085 Val Step[1550/1563], Avg Loss: 1.4668, Avg Acc@1: 0.6741, Avg Acc@5: 0.8808
2022-01-14 20:52:24,028 ----- Epoch[054/300], Validation Loss: 1.4668, Validation Acc@1: 0.6743, Validation Acc@5: 0.8808, time: 152.27
2022-01-14 20:52:24,496 the pre best model acc:0.0000, at epoch 0
2022-01-14 20:52:24,496 current best model acc:0.6743, at epoch 54
2022-01-14 20:52:24,496 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-14 20:52:24,496 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-14 20:52:24,496 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-14 20:52:24,496 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-14 20:52:24,496 Now training epoch 55. LR=0.000962
2022-01-14 20:54:28,604 Epoch[055/300], Step[0000/1252], Avg Loss: 3.5402, Avg Acc: 0.4600
2022-01-14 20:55:54,229 Epoch[055/300], Step[0050/1252], Avg Loss: 3.7183, Avg Acc: 0.3774
2022-01-14 20:57:19,037 Epoch[055/300], Step[0100/1252], Avg Loss: 3.7185, Avg Acc: 0.3798
2022-01-14 20:58:43,926 Epoch[055/300], Step[0150/1252], Avg Loss: 3.7225, Avg Acc: 0.3668
2022-01-14 21:00:09,726 Epoch[055/300], Step[0200/1252], Avg Loss: 3.7341, Avg Acc: 0.3647
2022-01-14 21:01:37,116 Epoch[055/300], Step[0250/1252], Avg Loss: 3.7353, Avg Acc: 0.3663
2022-01-14 21:03:04,563 Epoch[055/300], Step[0300/1252], Avg Loss: 3.7286, Avg Acc: 0.3661
2022-01-14 21:04:31,585 Epoch[055/300], Step[0350/1252], Avg Loss: 3.7330, Avg Acc: 0.3650
2022-01-14 21:05:57,422 Epoch[055/300], Step[0400/1252], Avg Loss: 3.7339, Avg Acc: 0.3653
2022-01-14 21:07:24,923 Epoch[055/300], Step[0450/1252], Avg Loss: 3.7312, Avg Acc: 0.3632
2022-01-14 21:08:51,842 Epoch[055/300], Step[0500/1252], Avg Loss: 3.7303, Avg Acc: 0.3609
2022-01-14 21:10:18,576 Epoch[055/300], Step[0550/1252], Avg Loss: 3.7371, Avg Acc: 0.3601
2022-01-14 21:11:45,885 Epoch[055/300], Step[0600/1252], Avg Loss: 3.7377, Avg Acc: 0.3607
2022-01-14 21:13:13,339 Epoch[055/300], Step[0650/1252], Avg Loss: 3.7381, Avg Acc: 0.3606
2022-01-14 21:14:39,940 Epoch[055/300], Step[0700/1252], Avg Loss: 3.7402, Avg Acc: 0.3583
2022-01-14 21:16:07,358 Epoch[055/300], Step[0750/1252], Avg Loss: 3.7452, Avg Acc: 0.3567
2022-01-14 21:17:34,910 Epoch[055/300], Step[0800/1252], Avg Loss: 3.7475, Avg Acc: 0.3576
2022-01-14 21:19:01,331 Epoch[055/300], Step[0850/1252], Avg Loss: 3.7479, Avg Acc: 0.3569
2022-01-14 21:20:28,136 Epoch[055/300], Step[0900/1252], Avg Loss: 3.7498, Avg Acc: 0.3544
2022-01-14 21:21:54,779 Epoch[055/300], Step[0950/1252], Avg Loss: 3.7476, Avg Acc: 0.3546
2022-01-14 21:23:21,621 Epoch[055/300], Step[1000/1252], Avg Loss: 3.7474, Avg Acc: 0.3551
2022-01-14 21:24:49,263 Epoch[055/300], Step[1050/1252], Avg Loss: 3.7468, Avg Acc: 0.3542
2022-01-14 21:26:16,721 Epoch[055/300], Step[1100/1252], Avg Loss: 3.7472, Avg Acc: 0.3539
2022-01-14 21:27:43,451 Epoch[055/300], Step[1150/1252], Avg Loss: 3.7503, Avg Acc: 0.3538
2022-01-14 21:29:09,745 Epoch[055/300], Step[1200/1252], Avg Loss: 3.7481, Avg Acc: 0.3538
2022-01-14 21:30:36,416 Epoch[055/300], Step[1250/1252], Avg Loss: 3.7517, Avg Acc: 0.3525
2022-01-14 21:30:42,605 ----- Epoch[055/300], Train Loss: 3.7517, Train Acc: 0.3525, time: 2298.10, Best Val(epoch54) Acc@1: 0.6743
2022-01-14 21:30:42,605 Now training epoch 56. LR=0.000960
2022-01-14 21:35:44,896 Epoch[056/300], Step[0000/1252], Avg Loss: 2.9291, Avg Acc: 0.4932
2022-01-14 21:37:13,347 Epoch[056/300], Step[0050/1252], Avg Loss: 3.7651, Avg Acc: 0.3621
2022-01-14 21:38:40,106 Epoch[056/300], Step[0100/1252], Avg Loss: 3.7606, Avg Acc: 0.3604
2022-01-14 21:40:07,660 Epoch[056/300], Step[0150/1252], Avg Loss: 3.7411, Avg Acc: 0.3721
2022-01-14 21:41:35,602 Epoch[056/300], Step[0200/1252], Avg Loss: 3.7416, Avg Acc: 0.3633
2022-01-14 21:43:03,544 Epoch[056/300], Step[0250/1252], Avg Loss: 3.7318, Avg Acc: 0.3569
2022-01-14 21:44:33,698 Epoch[056/300], Step[0300/1252], Avg Loss: 3.7399, Avg Acc: 0.3598
2022-01-14 21:46:02,526 Epoch[056/300], Step[0350/1252], Avg Loss: 3.7307, Avg Acc: 0.3613
2022-01-14 21:47:31,741 Epoch[056/300], Step[0400/1252], Avg Loss: 3.7274, Avg Acc: 0.3617
2022-01-14 21:49:00,612 Epoch[056/300], Step[0450/1252], Avg Loss: 3.7325, Avg Acc: 0.3603
2022-01-14 21:50:29,768 Epoch[056/300], Step[0500/1252], Avg Loss: 3.7390, Avg Acc: 0.3598
2022-01-14 21:51:58,776 Epoch[056/300], Step[0550/1252], Avg Loss: 3.7356, Avg Acc: 0.3610
2022-01-14 21:53:29,163 Epoch[056/300], Step[0600/1252], Avg Loss: 3.7376, Avg Acc: 0.3607
2022-01-14 21:54:56,936 Epoch[056/300], Step[0650/1252], Avg Loss: 3.7394, Avg Acc: 0.3594
2022-01-14 21:56:25,513 Epoch[056/300], Step[0700/1252], Avg Loss: 3.7440, Avg Acc: 0.3580
2022-01-14 21:57:54,996 Epoch[056/300], Step[0750/1252], Avg Loss: 3.7475, Avg Acc: 0.3567
2022-01-14 21:59:24,824 Epoch[056/300], Step[0800/1252], Avg Loss: 3.7513, Avg Acc: 0.3548
2022-01-14 22:00:53,819 Epoch[056/300], Step[0850/1252], Avg Loss: 3.7553, Avg Acc: 0.3534
2022-01-14 22:02:24,154 Epoch[056/300], Step[0900/1252], Avg Loss: 3.7567, Avg Acc: 0.3535
2022-01-14 22:03:54,831 Epoch[056/300], Step[0950/1252], Avg Loss: 3.7587, Avg Acc: 0.3532
2022-01-14 22:05:23,490 Epoch[056/300], Step[1000/1252], Avg Loss: 3.7559, Avg Acc: 0.3533
2022-01-14 22:06:52,937 Epoch[056/300], Step[1050/1252], Avg Loss: 3.7561, Avg Acc: 0.3543
2022-01-14 22:08:23,056 Epoch[056/300], Step[1100/1252], Avg Loss: 3.7547, Avg Acc: 0.3546
2022-01-14 22:09:51,143 Epoch[056/300], Step[1150/1252], Avg Loss: 3.7512, Avg Acc: 0.3554
2022-01-14 22:11:21,585 Epoch[056/300], Step[1200/1252], Avg Loss: 3.7514, Avg Acc: 0.3559
2022-01-14 22:12:51,623 Epoch[056/300], Step[1250/1252], Avg Loss: 3.7528, Avg Acc: 0.3551
2022-01-14 22:12:58,862 ----- Epoch[056/300], Train Loss: 3.7528, Train Acc: 0.3551, time: 2536.25, Best Val(epoch54) Acc@1: 0.6743
2022-01-14 22:12:58,862 ----- Validation after Epoch: 56
2022-01-14 22:14:20,765 Val Step[0000/1563], Avg Loss: 1.3190, Avg Acc@1: 0.6250, Avg Acc@5: 0.9062
2022-01-14 22:14:22,659 Val Step[0050/1563], Avg Loss: 1.4556, Avg Acc@1: 0.6771, Avg Acc@5: 0.8836
2022-01-14 22:14:24,435 Val Step[0100/1563], Avg Loss: 1.4709, Avg Acc@1: 0.6714, Avg Acc@5: 0.8837
2022-01-14 22:14:26,232 Val Step[0150/1563], Avg Loss: 1.4750, Avg Acc@1: 0.6749, Avg Acc@5: 0.8798
2022-01-14 22:14:28,136 Val Step[0200/1563], Avg Loss: 1.4759, Avg Acc@1: 0.6768, Avg Acc@5: 0.8790
2022-01-14 22:14:29,948 Val Step[0250/1563], Avg Loss: 1.4563, Avg Acc@1: 0.6804, Avg Acc@5: 0.8817
2022-01-14 22:14:31,832 Val Step[0300/1563], Avg Loss: 1.4542, Avg Acc@1: 0.6811, Avg Acc@5: 0.8822
2022-01-14 22:14:33,698 Val Step[0350/1563], Avg Loss: 1.4599, Avg Acc@1: 0.6812, Avg Acc@5: 0.8810
2022-01-14 22:14:35,820 Val Step[0400/1563], Avg Loss: 1.4599, Avg Acc@1: 0.6817, Avg Acc@5: 0.8800
2022-01-14 22:14:37,827 Val Step[0450/1563], Avg Loss: 1.4664, Avg Acc@1: 0.6790, Avg Acc@5: 0.8790
2022-01-14 22:14:39,649 Val Step[0500/1563], Avg Loss: 1.4703, Avg Acc@1: 0.6774, Avg Acc@5: 0.8785
2022-01-14 22:14:41,525 Val Step[0550/1563], Avg Loss: 1.4711, Avg Acc@1: 0.6770, Avg Acc@5: 0.8789
2022-01-14 22:14:43,403 Val Step[0600/1563], Avg Loss: 1.4699, Avg Acc@1: 0.6768, Avg Acc@5: 0.8799
2022-01-14 22:14:45,178 Val Step[0650/1563], Avg Loss: 1.4718, Avg Acc@1: 0.6764, Avg Acc@5: 0.8803
2022-01-14 22:14:47,062 Val Step[0700/1563], Avg Loss: 1.4691, Avg Acc@1: 0.6771, Avg Acc@5: 0.8812
2022-01-14 22:14:49,146 Val Step[0750/1563], Avg Loss: 1.4747, Avg Acc@1: 0.6757, Avg Acc@5: 0.8805
2022-01-14 22:14:51,207 Val Step[0800/1563], Avg Loss: 1.4730, Avg Acc@1: 0.6765, Avg Acc@5: 0.8811
2022-01-14 22:14:52,977 Val Step[0850/1563], Avg Loss: 1.4757, Avg Acc@1: 0.6754, Avg Acc@5: 0.8805
2022-01-14 22:14:54,830 Val Step[0900/1563], Avg Loss: 1.4712, Avg Acc@1: 0.6765, Avg Acc@5: 0.8812
2022-01-14 22:14:56,635 Val Step[0950/1563], Avg Loss: 1.4707, Avg Acc@1: 0.6765, Avg Acc@5: 0.8813
2022-01-14 22:14:58,438 Val Step[1000/1563], Avg Loss: 1.4693, Avg Acc@1: 0.6770, Avg Acc@5: 0.8810
2022-01-14 22:15:00,263 Val Step[1050/1563], Avg Loss: 1.4720, Avg Acc@1: 0.6762, Avg Acc@5: 0.8805
2022-01-14 22:15:02,125 Val Step[1100/1563], Avg Loss: 1.4718, Avg Acc@1: 0.6759, Avg Acc@5: 0.8804
2022-01-14 22:15:03,983 Val Step[1150/1563], Avg Loss: 1.4702, Avg Acc@1: 0.6760, Avg Acc@5: 0.8805
2022-01-14 22:15:05,845 Val Step[1200/1563], Avg Loss: 1.4694, Avg Acc@1: 0.6762, Avg Acc@5: 0.8805
2022-01-14 22:15:07,655 Val Step[1250/1563], Avg Loss: 1.4690, Avg Acc@1: 0.6757, Avg Acc@5: 0.8807
2022-01-14 22:15:09,486 Val Step[1300/1563], Avg Loss: 1.4724, Avg Acc@1: 0.6752, Avg Acc@5: 0.8801
2022-01-14 22:15:11,270 Val Step[1350/1563], Avg Loss: 1.4713, Avg Acc@1: 0.6751, Avg Acc@5: 0.8800
2022-01-14 22:15:13,047 Val Step[1400/1563], Avg Loss: 1.4705, Avg Acc@1: 0.6751, Avg Acc@5: 0.8798
2022-01-14 22:15:14,821 Val Step[1450/1563], Avg Loss: 1.4703, Avg Acc@1: 0.6753, Avg Acc@5: 0.8798
2022-01-14 22:15:16,687 Val Step[1500/1563], Avg Loss: 1.4695, Avg Acc@1: 0.6759, Avg Acc@5: 0.8801
2022-01-14 22:15:18,471 Val Step[1550/1563], Avg Loss: 1.4705, Avg Acc@1: 0.6758, Avg Acc@5: 0.8800
2022-01-14 22:15:20,371 ----- Epoch[056/300], Validation Loss: 1.4710, Validation Acc@1: 0.6757, Validation Acc@5: 0.8801, time: 141.51
2022-01-14 22:15:21,496 the pre best model acc:0.6743, at epoch 54
2022-01-14 22:15:21,497 current best model acc:0.6757, at epoch 56
2022-01-14 22:15:21,497 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-14 22:15:21,497 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-14 22:15:21,497 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-14 22:15:21,497 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-14 22:15:21,497 Now training epoch 57. LR=0.000958
2022-01-14 22:17:14,020 Epoch[057/300], Step[0000/1252], Avg Loss: 3.1841, Avg Acc: 0.5684
2022-01-14 22:18:41,456 Epoch[057/300], Step[0050/1252], Avg Loss: 3.7674, Avg Acc: 0.3402
2022-01-14 22:20:07,199 Epoch[057/300], Step[0100/1252], Avg Loss: 3.7414, Avg Acc: 0.3720
2022-01-14 22:21:33,922 Epoch[057/300], Step[0150/1252], Avg Loss: 3.7157, Avg Acc: 0.3778
2022-01-14 22:23:00,255 Epoch[057/300], Step[0200/1252], Avg Loss: 3.7191, Avg Acc: 0.3687
2022-01-14 22:24:28,060 Epoch[057/300], Step[0250/1252], Avg Loss: 3.7296, Avg Acc: 0.3669
2022-01-14 22:25:56,702 Epoch[057/300], Step[0300/1252], Avg Loss: 3.7337, Avg Acc: 0.3644
2022-01-14 22:27:24,847 Epoch[057/300], Step[0350/1252], Avg Loss: 3.7322, Avg Acc: 0.3627
2022-01-14 22:28:53,351 Epoch[057/300], Step[0400/1252], Avg Loss: 3.7383, Avg Acc: 0.3645
2022-01-14 22:30:21,986 Epoch[057/300], Step[0450/1252], Avg Loss: 3.7415, Avg Acc: 0.3632
2022-01-14 22:31:48,959 Epoch[057/300], Step[0500/1252], Avg Loss: 3.7374, Avg Acc: 0.3631
2022-01-14 22:33:15,998 Epoch[057/300], Step[0550/1252], Avg Loss: 3.7341, Avg Acc: 0.3622
2022-01-14 22:34:44,949 Epoch[057/300], Step[0600/1252], Avg Loss: 3.7385, Avg Acc: 0.3602
2022-01-14 22:36:11,861 Epoch[057/300], Step[0650/1252], Avg Loss: 3.7388, Avg Acc: 0.3572
2022-01-14 22:37:40,584 Epoch[057/300], Step[0700/1252], Avg Loss: 3.7403, Avg Acc: 0.3565
2022-01-14 22:39:08,510 Epoch[057/300], Step[0750/1252], Avg Loss: 3.7376, Avg Acc: 0.3573
2022-01-14 22:40:36,575 Epoch[057/300], Step[0800/1252], Avg Loss: 3.7348, Avg Acc: 0.3575
2022-01-14 22:42:05,072 Epoch[057/300], Step[0850/1252], Avg Loss: 3.7359, Avg Acc: 0.3576
2022-01-14 22:43:33,077 Epoch[057/300], Step[0900/1252], Avg Loss: 3.7391, Avg Acc: 0.3573
2022-01-14 22:45:00,811 Epoch[057/300], Step[0950/1252], Avg Loss: 3.7396, Avg Acc: 0.3578
2022-01-14 22:46:28,572 Epoch[057/300], Step[1000/1252], Avg Loss: 3.7428, Avg Acc: 0.3571
2022-01-14 22:47:56,832 Epoch[057/300], Step[1050/1252], Avg Loss: 3.7390, Avg Acc: 0.3578
2022-01-14 22:49:25,304 Epoch[057/300], Step[1100/1252], Avg Loss: 3.7398, Avg Acc: 0.3581
2022-01-14 22:50:53,254 Epoch[057/300], Step[1150/1252], Avg Loss: 3.7395, Avg Acc: 0.3579
2022-01-14 22:52:21,224 Epoch[057/300], Step[1200/1252], Avg Loss: 3.7395, Avg Acc: 0.3576
2022-01-14 22:53:50,269 Epoch[057/300], Step[1250/1252], Avg Loss: 3.7421, Avg Acc: 0.3567
2022-01-14 22:53:57,342 ----- Epoch[057/300], Train Loss: 3.7421, Train Acc: 0.3567, time: 2315.84, Best Val(epoch56) Acc@1: 0.6757
2022-01-14 22:53:57,342 Now training epoch 58. LR=0.000956
2022-01-14 22:55:46,684 Epoch[058/300], Step[0000/1252], Avg Loss: 4.0470, Avg Acc: 0.1152
2022-01-14 22:57:15,280 Epoch[058/300], Step[0050/1252], Avg Loss: 3.8292, Avg Acc: 0.3458
2022-01-14 22:58:43,422 Epoch[058/300], Step[0100/1252], Avg Loss: 3.7897, Avg Acc: 0.3550
2022-01-14 23:00:12,339 Epoch[058/300], Step[0150/1252], Avg Loss: 3.7892, Avg Acc: 0.3562
2022-01-14 23:01:40,692 Epoch[058/300], Step[0200/1252], Avg Loss: 3.7815, Avg Acc: 0.3478
2022-01-14 23:03:09,508 Epoch[058/300], Step[0250/1252], Avg Loss: 3.7682, Avg Acc: 0.3505
2022-01-14 23:04:37,017 Epoch[058/300], Step[0300/1252], Avg Loss: 3.7757, Avg Acc: 0.3516
2022-01-14 23:06:03,869 Epoch[058/300], Step[0350/1252], Avg Loss: 3.7673, Avg Acc: 0.3519
2022-01-14 23:07:30,538 Epoch[058/300], Step[0400/1252], Avg Loss: 3.7653, Avg Acc: 0.3528
2022-01-14 23:08:57,625 Epoch[058/300], Step[0450/1252], Avg Loss: 3.7541, Avg Acc: 0.3574
2022-01-14 23:10:25,126 Epoch[058/300], Step[0500/1252], Avg Loss: 3.7535, Avg Acc: 0.3558
2022-01-14 23:11:50,766 Epoch[058/300], Step[0550/1252], Avg Loss: 3.7539, Avg Acc: 0.3551
2022-01-14 23:13:17,503 Epoch[058/300], Step[0600/1252], Avg Loss: 3.7476, Avg Acc: 0.3540
2022-01-14 23:14:44,021 Epoch[058/300], Step[0650/1252], Avg Loss: 3.7468, Avg Acc: 0.3535
2022-01-14 23:16:11,987 Epoch[058/300], Step[0700/1252], Avg Loss: 3.7475, Avg Acc: 0.3541
2022-01-14 23:17:39,618 Epoch[058/300], Step[0750/1252], Avg Loss: 3.7441, Avg Acc: 0.3546
2022-01-14 23:19:04,689 Epoch[058/300], Step[0800/1252], Avg Loss: 3.7437, Avg Acc: 0.3562
2022-01-14 23:20:31,852 Epoch[058/300], Step[0850/1252], Avg Loss: 3.7441, Avg Acc: 0.3565
2022-01-14 23:22:00,117 Epoch[058/300], Step[0900/1252], Avg Loss: 3.7466, Avg Acc: 0.3563
2022-01-14 23:23:28,792 Epoch[058/300], Step[0950/1252], Avg Loss: 3.7426, Avg Acc: 0.3559
2022-01-14 23:24:56,947 Epoch[058/300], Step[1000/1252], Avg Loss: 3.7431, Avg Acc: 0.3548
2022-01-14 23:26:25,207 Epoch[058/300], Step[1050/1252], Avg Loss: 3.7429, Avg Acc: 0.3530
2022-01-14 23:27:53,479 Epoch[058/300], Step[1100/1252], Avg Loss: 3.7413, Avg Acc: 0.3532
2022-01-14 23:29:20,591 Epoch[058/300], Step[1150/1252], Avg Loss: 3.7404, Avg Acc: 0.3532
2022-01-14 23:30:49,129 Epoch[058/300], Step[1200/1252], Avg Loss: 3.7399, Avg Acc: 0.3537
2022-01-14 23:32:17,581 Epoch[058/300], Step[1250/1252], Avg Loss: 3.7430, Avg Acc: 0.3549
2022-01-14 23:32:24,876 ----- Epoch[058/300], Train Loss: 3.7430, Train Acc: 0.3549, time: 2307.53, Best Val(epoch56) Acc@1: 0.6757
2022-01-14 23:32:24,876 ----- Validation after Epoch: 58
2022-01-14 23:33:43,339 Val Step[0000/1563], Avg Loss: 1.5195, Avg Acc@1: 0.6562, Avg Acc@5: 0.9062
2022-01-14 23:33:45,171 Val Step[0050/1563], Avg Loss: 1.5282, Avg Acc@1: 0.6710, Avg Acc@5: 0.8738
2022-01-14 23:33:46,948 Val Step[0100/1563], Avg Loss: 1.5604, Avg Acc@1: 0.6652, Avg Acc@5: 0.8735
2022-01-14 23:33:48,738 Val Step[0150/1563], Avg Loss: 1.5415, Avg Acc@1: 0.6678, Avg Acc@5: 0.8760
2022-01-14 23:33:50,518 Val Step[0200/1563], Avg Loss: 1.5433, Avg Acc@1: 0.6681, Avg Acc@5: 0.8753
2022-01-14 23:33:52,310 Val Step[0250/1563], Avg Loss: 1.5253, Avg Acc@1: 0.6749, Avg Acc@5: 0.8786
2022-01-14 23:33:54,234 Val Step[0300/1563], Avg Loss: 1.5232, Avg Acc@1: 0.6758, Avg Acc@5: 0.8786
2022-01-14 23:33:56,021 Val Step[0350/1563], Avg Loss: 1.5273, Avg Acc@1: 0.6742, Avg Acc@5: 0.8791
2022-01-14 23:33:57,851 Val Step[0400/1563], Avg Loss: 1.5253, Avg Acc@1: 0.6750, Avg Acc@5: 0.8792
2022-01-14 23:33:59,655 Val Step[0450/1563], Avg Loss: 1.5335, Avg Acc@1: 0.6730, Avg Acc@5: 0.8780
2022-01-14 23:34:01,559 Val Step[0500/1563], Avg Loss: 1.5338, Avg Acc@1: 0.6734, Avg Acc@5: 0.8768
2022-01-14 23:34:03,374 Val Step[0550/1563], Avg Loss: 1.5350, Avg Acc@1: 0.6728, Avg Acc@5: 0.8764
2022-01-14 23:34:05,179 Val Step[0600/1563], Avg Loss: 1.5350, Avg Acc@1: 0.6718, Avg Acc@5: 0.8766
2022-01-14 23:34:06,978 Val Step[0650/1563], Avg Loss: 1.5360, Avg Acc@1: 0.6712, Avg Acc@5: 0.8769
2022-01-14 23:34:08,998 Val Step[0700/1563], Avg Loss: 1.5351, Avg Acc@1: 0.6711, Avg Acc@5: 0.8769
2022-01-14 23:34:11,042 Val Step[0750/1563], Avg Loss: 1.5397, Avg Acc@1: 0.6705, Avg Acc@5: 0.8762
2022-01-14 23:34:13,088 Val Step[0800/1563], Avg Loss: 1.5378, Avg Acc@1: 0.6709, Avg Acc@5: 0.8767
2022-01-14 23:34:15,066 Val Step[0850/1563], Avg Loss: 1.5384, Avg Acc@1: 0.6709, Avg Acc@5: 0.8768
2022-01-14 23:34:16,975 Val Step[0900/1563], Avg Loss: 1.5356, Avg Acc@1: 0.6714, Avg Acc@5: 0.8770
2022-01-14 23:34:19,060 Val Step[0950/1563], Avg Loss: 1.5351, Avg Acc@1: 0.6713, Avg Acc@5: 0.8778
2022-01-14 23:34:21,152 Val Step[1000/1563], Avg Loss: 1.5349, Avg Acc@1: 0.6713, Avg Acc@5: 0.8775
2022-01-14 23:34:22,986 Val Step[1050/1563], Avg Loss: 1.5386, Avg Acc@1: 0.6701, Avg Acc@5: 0.8770
2022-01-14 23:34:24,840 Val Step[1100/1563], Avg Loss: 1.5378, Avg Acc@1: 0.6698, Avg Acc@5: 0.8773
2022-01-14 23:34:26,617 Val Step[1150/1563], Avg Loss: 1.5370, Avg Acc@1: 0.6694, Avg Acc@5: 0.8773
2022-01-14 23:34:28,449 Val Step[1200/1563], Avg Loss: 1.5368, Avg Acc@1: 0.6695, Avg Acc@5: 0.8775
2022-01-14 23:34:30,260 Val Step[1250/1563], Avg Loss: 1.5358, Avg Acc@1: 0.6694, Avg Acc@5: 0.8777
2022-01-14 23:34:32,047 Val Step[1300/1563], Avg Loss: 1.5396, Avg Acc@1: 0.6686, Avg Acc@5: 0.8773
2022-01-14 23:34:33,871 Val Step[1350/1563], Avg Loss: 1.5390, Avg Acc@1: 0.6683, Avg Acc@5: 0.8773
2022-01-14 23:34:35,812 Val Step[1400/1563], Avg Loss: 1.5371, Avg Acc@1: 0.6686, Avg Acc@5: 0.8775
2022-01-14 23:34:37,698 Val Step[1450/1563], Avg Loss: 1.5381, Avg Acc@1: 0.6683, Avg Acc@5: 0.8775
2022-01-14 23:34:39,501 Val Step[1500/1563], Avg Loss: 1.5372, Avg Acc@1: 0.6687, Avg Acc@5: 0.8779
2022-01-14 23:34:41,248 Val Step[1550/1563], Avg Loss: 1.5376, Avg Acc@1: 0.6684, Avg Acc@5: 0.8779
2022-01-14 23:34:43,173 ----- Epoch[058/300], Validation Loss: 1.5374, Validation Acc@1: 0.6684, Validation Acc@5: 0.8781, time: 138.29
2022-01-14 23:34:43,173 Now training epoch 59. LR=0.000953
2022-01-14 23:36:32,772 Epoch[059/300], Step[0000/1252], Avg Loss: 3.1501, Avg Acc: 0.2959
2022-01-14 23:38:00,485 Epoch[059/300], Step[0050/1252], Avg Loss: 3.7538, Avg Acc: 0.3566
2022-01-14 23:39:28,666 Epoch[059/300], Step[0100/1252], Avg Loss: 3.7346, Avg Acc: 0.3534
2022-01-14 23:40:55,474 Epoch[059/300], Step[0150/1252], Avg Loss: 3.7259, Avg Acc: 0.3519
2022-01-14 23:42:22,641 Epoch[059/300], Step[0200/1252], Avg Loss: 3.7170, Avg Acc: 0.3587
2022-01-14 23:43:50,268 Epoch[059/300], Step[0250/1252], Avg Loss: 3.7180, Avg Acc: 0.3595
2022-01-14 23:45:19,582 Epoch[059/300], Step[0300/1252], Avg Loss: 3.7176, Avg Acc: 0.3579
2022-01-14 23:46:48,649 Epoch[059/300], Step[0350/1252], Avg Loss: 3.7158, Avg Acc: 0.3580
2022-01-14 23:48:16,750 Epoch[059/300], Step[0400/1252], Avg Loss: 3.7293, Avg Acc: 0.3571
2022-01-14 23:49:45,959 Epoch[059/300], Step[0450/1252], Avg Loss: 3.7282, Avg Acc: 0.3578
2022-01-14 23:51:15,477 Epoch[059/300], Step[0500/1252], Avg Loss: 3.7247, Avg Acc: 0.3593
2022-01-14 23:52:42,868 Epoch[059/300], Step[0550/1252], Avg Loss: 3.7229, Avg Acc: 0.3590
2022-01-14 23:54:13,204 Epoch[059/300], Step[0600/1252], Avg Loss: 3.7242, Avg Acc: 0.3601
2022-01-14 23:55:44,404 Epoch[059/300], Step[0650/1252], Avg Loss: 3.7201, Avg Acc: 0.3618
2022-01-14 23:57:14,828 Epoch[059/300], Step[0700/1252], Avg Loss: 3.7212, Avg Acc: 0.3610
2022-01-14 23:58:45,948 Epoch[059/300], Step[0750/1252], Avg Loss: 3.7231, Avg Acc: 0.3611
2022-01-15 00:00:17,379 Epoch[059/300], Step[0800/1252], Avg Loss: 3.7259, Avg Acc: 0.3607
2022-01-15 00:01:48,490 Epoch[059/300], Step[0850/1252], Avg Loss: 3.7290, Avg Acc: 0.3608
2022-01-15 00:03:18,609 Epoch[059/300], Step[0900/1252], Avg Loss: 3.7275, Avg Acc: 0.3601
2022-01-15 00:04:50,783 Epoch[059/300], Step[0950/1252], Avg Loss: 3.7268, Avg Acc: 0.3601
2022-01-15 00:06:20,115 Epoch[059/300], Step[1000/1252], Avg Loss: 3.7262, Avg Acc: 0.3609
2022-01-15 00:07:50,209 Epoch[059/300], Step[1050/1252], Avg Loss: 3.7272, Avg Acc: 0.3610
2022-01-15 00:09:20,136 Epoch[059/300], Step[1100/1252], Avg Loss: 3.7307, Avg Acc: 0.3603
2022-01-15 00:10:51,056 Epoch[059/300], Step[1150/1252], Avg Loss: 3.7340, Avg Acc: 0.3598
2022-01-15 00:12:20,575 Epoch[059/300], Step[1200/1252], Avg Loss: 3.7328, Avg Acc: 0.3602
2022-01-15 00:13:52,155 Epoch[059/300], Step[1250/1252], Avg Loss: 3.7335, Avg Acc: 0.3602
2022-01-15 00:13:58,793 ----- Epoch[059/300], Train Loss: 3.7335, Train Acc: 0.3603, time: 2355.62, Best Val(epoch56) Acc@1: 0.6757
2022-01-15 00:13:58,794 Now training epoch 60. LR=0.000951
2022-01-15 00:16:00,467 Epoch[060/300], Step[0000/1252], Avg Loss: 3.4758, Avg Acc: 0.3135
2022-01-15 00:17:28,067 Epoch[060/300], Step[0050/1252], Avg Loss: 3.7003, Avg Acc: 0.3985
2022-01-15 00:18:57,206 Epoch[060/300], Step[0100/1252], Avg Loss: 3.7366, Avg Acc: 0.3659
2022-01-15 00:20:26,062 Epoch[060/300], Step[0150/1252], Avg Loss: 3.7276, Avg Acc: 0.3582
2022-01-15 00:21:56,515 Epoch[060/300], Step[0200/1252], Avg Loss: 3.7194, Avg Acc: 0.3567
2022-01-15 00:23:27,143 Epoch[060/300], Step[0250/1252], Avg Loss: 3.7087, Avg Acc: 0.3582
2022-01-15 00:24:58,982 Epoch[060/300], Step[0300/1252], Avg Loss: 3.7039, Avg Acc: 0.3612
2022-01-15 00:26:30,976 Epoch[060/300], Step[0350/1252], Avg Loss: 3.7039, Avg Acc: 0.3613
2022-01-15 00:28:02,766 Epoch[060/300], Step[0400/1252], Avg Loss: 3.7088, Avg Acc: 0.3637
2022-01-15 00:29:33,447 Epoch[060/300], Step[0450/1252], Avg Loss: 3.7083, Avg Acc: 0.3663
2022-01-15 00:31:03,970 Epoch[060/300], Step[0500/1252], Avg Loss: 3.7081, Avg Acc: 0.3665
2022-01-15 00:32:35,216 Epoch[060/300], Step[0550/1252], Avg Loss: 3.7108, Avg Acc: 0.3622
2022-01-15 00:34:04,633 Epoch[060/300], Step[0600/1252], Avg Loss: 3.7124, Avg Acc: 0.3649
2022-01-15 00:35:34,785 Epoch[060/300], Step[0650/1252], Avg Loss: 3.7180, Avg Acc: 0.3654
2022-01-15 00:37:06,401 Epoch[060/300], Step[0700/1252], Avg Loss: 3.7207, Avg Acc: 0.3652
2022-01-15 00:38:37,842 Epoch[060/300], Step[0750/1252], Avg Loss: 3.7214, Avg Acc: 0.3653
2022-01-15 00:40:07,845 Epoch[060/300], Step[0800/1252], Avg Loss: 3.7179, Avg Acc: 0.3657
2022-01-15 00:41:38,531 Epoch[060/300], Step[0850/1252], Avg Loss: 3.7182, Avg Acc: 0.3659
2022-01-15 00:43:09,957 Epoch[060/300], Step[0900/1252], Avg Loss: 3.7187, Avg Acc: 0.3651
2022-01-15 00:44:42,635 Epoch[060/300], Step[0950/1252], Avg Loss: 3.7212, Avg Acc: 0.3638
2022-01-15 00:46:13,092 Epoch[060/300], Step[1000/1252], Avg Loss: 3.7234, Avg Acc: 0.3630
2022-01-15 00:47:44,998 Epoch[060/300], Step[1050/1252], Avg Loss: 3.7244, Avg Acc: 0.3617
2022-01-15 00:49:16,311 Epoch[060/300], Step[1100/1252], Avg Loss: 3.7234, Avg Acc: 0.3632
2022-01-15 00:50:46,865 Epoch[060/300], Step[1150/1252], Avg Loss: 3.7233, Avg Acc: 0.3627
2022-01-15 00:52:17,830 Epoch[060/300], Step[1200/1252], Avg Loss: 3.7214, Avg Acc: 0.3621
2022-01-15 00:53:48,776 Epoch[060/300], Step[1250/1252], Avg Loss: 3.7191, Avg Acc: 0.3631
2022-01-15 00:53:55,252 ----- Epoch[060/300], Train Loss: 3.7191, Train Acc: 0.3631, time: 2396.45, Best Val(epoch56) Acc@1: 0.6757
2022-01-15 00:53:55,253 ----- Validation after Epoch: 60
2022-01-15 00:55:13,379 Val Step[0000/1563], Avg Loss: 1.3393, Avg Acc@1: 0.6875, Avg Acc@5: 0.8750
2022-01-15 00:55:15,499 Val Step[0050/1563], Avg Loss: 1.4661, Avg Acc@1: 0.6728, Avg Acc@5: 0.8799
2022-01-15 00:55:17,518 Val Step[0100/1563], Avg Loss: 1.4619, Avg Acc@1: 0.6770, Avg Acc@5: 0.8846
2022-01-15 00:55:19,599 Val Step[0150/1563], Avg Loss: 1.4566, Avg Acc@1: 0.6751, Avg Acc@5: 0.8827
2022-01-15 00:55:21,634 Val Step[0200/1563], Avg Loss: 1.4601, Avg Acc@1: 0.6762, Avg Acc@5: 0.8798
2022-01-15 00:55:23,567 Val Step[0250/1563], Avg Loss: 1.4429, Avg Acc@1: 0.6793, Avg Acc@5: 0.8807
2022-01-15 00:55:25,620 Val Step[0300/1563], Avg Loss: 1.4443, Avg Acc@1: 0.6795, Avg Acc@5: 0.8794
2022-01-15 00:55:27,568 Val Step[0350/1563], Avg Loss: 1.4489, Avg Acc@1: 0.6784, Avg Acc@5: 0.8795
2022-01-15 00:55:29,491 Val Step[0400/1563], Avg Loss: 1.4472, Avg Acc@1: 0.6792, Avg Acc@5: 0.8802
2022-01-15 00:55:31,518 Val Step[0450/1563], Avg Loss: 1.4578, Avg Acc@1: 0.6761, Avg Acc@5: 0.8793
2022-01-15 00:55:33,509 Val Step[0500/1563], Avg Loss: 1.4599, Avg Acc@1: 0.6758, Avg Acc@5: 0.8796
2022-01-15 00:55:35,492 Val Step[0550/1563], Avg Loss: 1.4608, Avg Acc@1: 0.6758, Avg Acc@5: 0.8801
2022-01-15 00:55:37,493 Val Step[0600/1563], Avg Loss: 1.4596, Avg Acc@1: 0.6756, Avg Acc@5: 0.8811
2022-01-15 00:55:39,592 Val Step[0650/1563], Avg Loss: 1.4612, Avg Acc@1: 0.6753, Avg Acc@5: 0.8812
2022-01-15 00:55:41,593 Val Step[0700/1563], Avg Loss: 1.4606, Avg Acc@1: 0.6750, Avg Acc@5: 0.8816
2022-01-15 00:55:43,615 Val Step[0750/1563], Avg Loss: 1.4673, Avg Acc@1: 0.6736, Avg Acc@5: 0.8805
2022-01-15 00:55:45,635 Val Step[0800/1563], Avg Loss: 1.4651, Avg Acc@1: 0.6741, Avg Acc@5: 0.8813
2022-01-15 00:55:47,649 Val Step[0850/1563], Avg Loss: 1.4661, Avg Acc@1: 0.6730, Avg Acc@5: 0.8811
2022-01-15 00:55:49,693 Val Step[0900/1563], Avg Loss: 1.4619, Avg Acc@1: 0.6741, Avg Acc@5: 0.8817
2022-01-15 00:55:51,674 Val Step[0950/1563], Avg Loss: 1.4620, Avg Acc@1: 0.6743, Avg Acc@5: 0.8818
2022-01-15 00:55:53,729 Val Step[1000/1563], Avg Loss: 1.4613, Avg Acc@1: 0.6744, Avg Acc@5: 0.8817
2022-01-15 00:55:55,948 Val Step[1050/1563], Avg Loss: 1.4647, Avg Acc@1: 0.6738, Avg Acc@5: 0.8811
2022-01-15 00:55:57,908 Val Step[1100/1563], Avg Loss: 1.4637, Avg Acc@1: 0.6734, Avg Acc@5: 0.8811
2022-01-15 00:55:59,909 Val Step[1150/1563], Avg Loss: 1.4617, Avg Acc@1: 0.6743, Avg Acc@5: 0.8812
2022-01-15 00:56:02,006 Val Step[1200/1563], Avg Loss: 1.4606, Avg Acc@1: 0.6746, Avg Acc@5: 0.8811
2022-01-15 00:56:04,064 Val Step[1250/1563], Avg Loss: 1.4595, Avg Acc@1: 0.6748, Avg Acc@5: 0.8813
2022-01-15 00:56:06,241 Val Step[1300/1563], Avg Loss: 1.4637, Avg Acc@1: 0.6738, Avg Acc@5: 0.8808
2022-01-15 00:56:08,405 Val Step[1350/1563], Avg Loss: 1.4644, Avg Acc@1: 0.6736, Avg Acc@5: 0.8807
2022-01-15 00:56:10,672 Val Step[1400/1563], Avg Loss: 1.4629, Avg Acc@1: 0.6738, Avg Acc@5: 0.8809
2022-01-15 00:56:12,890 Val Step[1450/1563], Avg Loss: 1.4627, Avg Acc@1: 0.6738, Avg Acc@5: 0.8808
2022-01-15 00:56:14,850 Val Step[1500/1563], Avg Loss: 1.4624, Avg Acc@1: 0.6741, Avg Acc@5: 0.8810
2022-01-15 00:56:16,917 Val Step[1550/1563], Avg Loss: 1.4629, Avg Acc@1: 0.6739, Avg Acc@5: 0.8810
2022-01-15 00:56:18,834 ----- Epoch[060/300], Validation Loss: 1.4633, Validation Acc@1: 0.6738, Validation Acc@5: 0.8810, time: 143.58
2022-01-15 00:56:19,317 ----- Save model: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-60-Loss-3.727763408273073.pdparams
2022-01-15 00:56:19,318 ----- Save optim: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-60-Loss-3.727763408273073.pdopt
2022-01-15 00:56:19,318 Now training epoch 61. LR=0.000949
2022-01-15 00:58:08,410 Epoch[061/300], Step[0000/1252], Avg Loss: 3.9827, Avg Acc: 0.2295
2022-01-15 00:59:38,067 Epoch[061/300], Step[0050/1252], Avg Loss: 3.7824, Avg Acc: 0.3593
2022-01-15 01:01:07,461 Epoch[061/300], Step[0100/1252], Avg Loss: 3.7543, Avg Acc: 0.3639
2022-01-15 01:02:38,033 Epoch[061/300], Step[0150/1252], Avg Loss: 3.7314, Avg Acc: 0.3669
2022-01-15 01:04:05,808 Epoch[061/300], Step[0200/1252], Avg Loss: 3.7272, Avg Acc: 0.3636
2022-01-15 01:05:36,116 Epoch[061/300], Step[0250/1252], Avg Loss: 3.7207, Avg Acc: 0.3620
2022-01-15 01:07:06,114 Epoch[061/300], Step[0300/1252], Avg Loss: 3.7207, Avg Acc: 0.3645
2022-01-15 01:08:38,189 Epoch[061/300], Step[0350/1252], Avg Loss: 3.7268, Avg Acc: 0.3631
2022-01-15 01:10:09,020 Epoch[061/300], Step[0400/1252], Avg Loss: 3.7309, Avg Acc: 0.3611
2022-01-15 01:11:38,024 Epoch[061/300], Step[0450/1252], Avg Loss: 3.7246, Avg Acc: 0.3614
2022-01-15 01:13:05,829 Epoch[061/300], Step[0500/1252], Avg Loss: 3.7235, Avg Acc: 0.3602
2022-01-15 01:14:34,854 Epoch[061/300], Step[0550/1252], Avg Loss: 3.7197, Avg Acc: 0.3617
2022-01-15 01:16:03,197 Epoch[061/300], Step[0600/1252], Avg Loss: 3.7213, Avg Acc: 0.3639
2022-01-15 01:17:32,322 Epoch[061/300], Step[0650/1252], Avg Loss: 3.7237, Avg Acc: 0.3609
2022-01-15 01:18:59,521 Epoch[061/300], Step[0700/1252], Avg Loss: 3.7268, Avg Acc: 0.3596
2022-01-15 01:20:30,008 Epoch[061/300], Step[0750/1252], Avg Loss: 3.7270, Avg Acc: 0.3613
2022-01-15 01:21:58,592 Epoch[061/300], Step[0800/1252], Avg Loss: 3.7277, Avg Acc: 0.3634
2022-01-15 01:23:28,355 Epoch[061/300], Step[0850/1252], Avg Loss: 3.7307, Avg Acc: 0.3634
2022-01-15 01:24:58,652 Epoch[061/300], Step[0900/1252], Avg Loss: 3.7294, Avg Acc: 0.3648
2022-01-15 01:26:30,368 Epoch[061/300], Step[0950/1252], Avg Loss: 3.7272, Avg Acc: 0.3654
2022-01-15 01:28:01,675 Epoch[061/300], Step[1000/1252], Avg Loss: 3.7296, Avg Acc: 0.3644
2022-01-15 01:29:32,278 Epoch[061/300], Step[1050/1252], Avg Loss: 3.7278, Avg Acc: 0.3652
2022-01-15 01:31:03,108 Epoch[061/300], Step[1100/1252], Avg Loss: 3.7269, Avg Acc: 0.3650
2022-01-15 01:32:34,011 Epoch[061/300], Step[1150/1252], Avg Loss: 3.7267, Avg Acc: 0.3645
2022-01-15 01:34:03,487 Epoch[061/300], Step[1200/1252], Avg Loss: 3.7258, Avg Acc: 0.3641
2022-01-15 01:35:35,265 Epoch[061/300], Step[1250/1252], Avg Loss: 3.7240, Avg Acc: 0.3651
2022-01-15 01:35:42,077 ----- Epoch[061/300], Train Loss: 3.7240, Train Acc: 0.3651, time: 2362.75, Best Val(epoch56) Acc@1: 0.6757
2022-01-15 01:35:42,077 Now training epoch 62. LR=0.000946
2022-01-15 01:37:34,134 Epoch[062/300], Step[0000/1252], Avg Loss: 4.0497, Avg Acc: 0.4805
2022-01-15 01:39:03,362 Epoch[062/300], Step[0050/1252], Avg Loss: 3.7997, Avg Acc: 0.3393
2022-01-15 01:40:33,589 Epoch[062/300], Step[0100/1252], Avg Loss: 3.7729, Avg Acc: 0.3535
2022-01-15 01:42:05,055 Epoch[062/300], Step[0150/1252], Avg Loss: 3.7536, Avg Acc: 0.3548
2022-01-15 01:43:37,306 Epoch[062/300], Step[0200/1252], Avg Loss: 3.7637, Avg Acc: 0.3485
2022-01-15 01:45:07,923 Epoch[062/300], Step[0250/1252], Avg Loss: 3.7563, Avg Acc: 0.3531
2022-01-15 01:46:38,529 Epoch[062/300], Step[0300/1252], Avg Loss: 3.7507, Avg Acc: 0.3519
2022-01-15 01:48:10,199 Epoch[062/300], Step[0350/1252], Avg Loss: 3.7474, Avg Acc: 0.3546
2022-01-15 01:49:42,001 Epoch[062/300], Step[0400/1252], Avg Loss: 3.7445, Avg Acc: 0.3544
2022-01-15 01:51:13,649 Epoch[062/300], Step[0450/1252], Avg Loss: 3.7426, Avg Acc: 0.3570
2022-01-15 01:52:45,049 Epoch[062/300], Step[0500/1252], Avg Loss: 3.7386, Avg Acc: 0.3597
2022-01-15 01:54:15,498 Epoch[062/300], Step[0550/1252], Avg Loss: 3.7376, Avg Acc: 0.3614
2022-01-15 01:55:46,944 Epoch[062/300], Step[0600/1252], Avg Loss: 3.7382, Avg Acc: 0.3610
2022-01-15 01:57:18,272 Epoch[062/300], Step[0650/1252], Avg Loss: 3.7426, Avg Acc: 0.3593
2022-01-15 01:58:49,608 Epoch[062/300], Step[0700/1252], Avg Loss: 3.7447, Avg Acc: 0.3589
2022-01-15 02:00:21,098 Epoch[062/300], Step[0750/1252], Avg Loss: 3.7391, Avg Acc: 0.3570
2022-01-15 02:01:51,501 Epoch[062/300], Step[0800/1252], Avg Loss: 3.7383, Avg Acc: 0.3587
2022-01-15 02:03:21,235 Epoch[062/300], Step[0850/1252], Avg Loss: 3.7352, Avg Acc: 0.3595
2022-01-15 02:04:52,927 Epoch[062/300], Step[0900/1252], Avg Loss: 3.7339, Avg Acc: 0.3609
2022-01-15 02:06:25,128 Epoch[062/300], Step[0950/1252], Avg Loss: 3.7325, Avg Acc: 0.3604
2022-01-15 02:07:57,360 Epoch[062/300], Step[1000/1252], Avg Loss: 3.7332, Avg Acc: 0.3582
2022-01-15 02:09:28,744 Epoch[062/300], Step[1050/1252], Avg Loss: 3.7335, Avg Acc: 0.3574
2022-01-15 02:11:00,030 Epoch[062/300], Step[1100/1252], Avg Loss: 3.7330, Avg Acc: 0.3573
2022-01-15 02:12:31,350 Epoch[062/300], Step[1150/1252], Avg Loss: 3.7317, Avg Acc: 0.3574
2022-01-15 02:14:02,105 Epoch[062/300], Step[1200/1252], Avg Loss: 3.7329, Avg Acc: 0.3582
2022-01-15 02:15:34,077 Epoch[062/300], Step[1250/1252], Avg Loss: 3.7340, Avg Acc: 0.3582
2022-01-15 02:15:41,136 ----- Epoch[062/300], Train Loss: 3.7340, Train Acc: 0.3582, time: 2399.05, Best Val(epoch56) Acc@1: 0.6757
2022-01-15 02:15:41,136 ----- Validation after Epoch: 62
2022-01-15 02:17:02,455 Val Step[0000/1563], Avg Loss: 1.3541, Avg Acc@1: 0.6875, Avg Acc@5: 0.9375
2022-01-15 02:17:04,853 Val Step[0050/1563], Avg Loss: 1.4641, Avg Acc@1: 0.6844, Avg Acc@5: 0.8842
2022-01-15 02:17:06,861 Val Step[0100/1563], Avg Loss: 1.4766, Avg Acc@1: 0.6751, Avg Acc@5: 0.8880
2022-01-15 02:17:08,764 Val Step[0150/1563], Avg Loss: 1.4760, Avg Acc@1: 0.6763, Avg Acc@5: 0.8851
2022-01-15 02:17:10,677 Val Step[0200/1563], Avg Loss: 1.4818, Avg Acc@1: 0.6775, Avg Acc@5: 0.8806
2022-01-15 02:17:12,529 Val Step[0250/1563], Avg Loss: 1.4667, Avg Acc@1: 0.6824, Avg Acc@5: 0.8816
2022-01-15 02:17:14,527 Val Step[0300/1563], Avg Loss: 1.4693, Avg Acc@1: 0.6832, Avg Acc@5: 0.8814
2022-01-15 02:17:16,706 Val Step[0350/1563], Avg Loss: 1.4754, Avg Acc@1: 0.6822, Avg Acc@5: 0.8818
2022-01-15 02:17:18,712 Val Step[0400/1563], Avg Loss: 1.4707, Avg Acc@1: 0.6827, Avg Acc@5: 0.8823
2022-01-15 02:17:20,747 Val Step[0450/1563], Avg Loss: 1.4767, Avg Acc@1: 0.6804, Avg Acc@5: 0.8815
2022-01-15 02:17:22,934 Val Step[0500/1563], Avg Loss: 1.4773, Avg Acc@1: 0.6806, Avg Acc@5: 0.8818
2022-01-15 02:17:25,044 Val Step[0550/1563], Avg Loss: 1.4770, Avg Acc@1: 0.6794, Avg Acc@5: 0.8819
2022-01-15 02:17:27,017 Val Step[0600/1563], Avg Loss: 1.4773, Avg Acc@1: 0.6785, Avg Acc@5: 0.8826
2022-01-15 02:17:29,150 Val Step[0650/1563], Avg Loss: 1.4784, Avg Acc@1: 0.6779, Avg Acc@5: 0.8828
2022-01-15 02:17:31,102 Val Step[0700/1563], Avg Loss: 1.4769, Avg Acc@1: 0.6779, Avg Acc@5: 0.8829
2022-01-15 02:17:33,102 Val Step[0750/1563], Avg Loss: 1.4819, Avg Acc@1: 0.6767, Avg Acc@5: 0.8821
2022-01-15 02:17:35,246 Val Step[0800/1563], Avg Loss: 1.4803, Avg Acc@1: 0.6768, Avg Acc@5: 0.8822
2022-01-15 02:17:37,437 Val Step[0850/1563], Avg Loss: 1.4829, Avg Acc@1: 0.6756, Avg Acc@5: 0.8814
2022-01-15 02:17:39,401 Val Step[0900/1563], Avg Loss: 1.4800, Avg Acc@1: 0.6762, Avg Acc@5: 0.8818
2022-01-15 02:17:41,361 Val Step[0950/1563], Avg Loss: 1.4803, Avg Acc@1: 0.6761, Avg Acc@5: 0.8821
2022-01-15 02:17:43,519 Val Step[1000/1563], Avg Loss: 1.4794, Avg Acc@1: 0.6760, Avg Acc@5: 0.8821
2022-01-15 02:17:45,577 Val Step[1050/1563], Avg Loss: 1.4828, Avg Acc@1: 0.6755, Avg Acc@5: 0.8815
2022-01-15 02:17:47,487 Val Step[1100/1563], Avg Loss: 1.4829, Avg Acc@1: 0.6754, Avg Acc@5: 0.8817
2022-01-15 02:17:49,519 Val Step[1150/1563], Avg Loss: 1.4814, Avg Acc@1: 0.6760, Avg Acc@5: 0.8819
2022-01-15 02:17:51,481 Val Step[1200/1563], Avg Loss: 1.4797, Avg Acc@1: 0.6766, Avg Acc@5: 0.8822
2022-01-15 02:17:53,578 Val Step[1250/1563], Avg Loss: 1.4792, Avg Acc@1: 0.6768, Avg Acc@5: 0.8826
2022-01-15 02:17:55,534 Val Step[1300/1563], Avg Loss: 1.4822, Avg Acc@1: 0.6764, Avg Acc@5: 0.8824
2022-01-15 02:17:57,455 Val Step[1350/1563], Avg Loss: 1.4815, Avg Acc@1: 0.6766, Avg Acc@5: 0.8824
2022-01-15 02:17:59,480 Val Step[1400/1563], Avg Loss: 1.4808, Avg Acc@1: 0.6760, Avg Acc@5: 0.8826
2022-01-15 02:18:01,501 Val Step[1450/1563], Avg Loss: 1.4812, Avg Acc@1: 0.6757, Avg Acc@5: 0.8823
2022-01-15 02:18:03,433 Val Step[1500/1563], Avg Loss: 1.4807, Avg Acc@1: 0.6759, Avg Acc@5: 0.8826
2022-01-15 02:18:05,535 Val Step[1550/1563], Avg Loss: 1.4818, Avg Acc@1: 0.6757, Avg Acc@5: 0.8823
2022-01-15 02:18:07,345 ----- Epoch[062/300], Validation Loss: 1.4819, Validation Acc@1: 0.6756, Validation Acc@5: 0.8823, time: 146.21
2022-01-15 02:18:07,345 Now training epoch 63. LR=0.000943
2022-01-15 02:20:06,142 Epoch[063/300], Step[0000/1252], Avg Loss: 3.9379, Avg Acc: 0.4482
2022-01-15 02:21:36,376 Epoch[063/300], Step[0050/1252], Avg Loss: 3.7305, Avg Acc: 0.3655
2022-01-15 02:23:06,669 Epoch[063/300], Step[0100/1252], Avg Loss: 3.7358, Avg Acc: 0.3512
2022-01-15 02:24:36,969 Epoch[063/300], Step[0150/1252], Avg Loss: 3.7371, Avg Acc: 0.3479
2022-01-15 02:26:07,325 Epoch[063/300], Step[0200/1252], Avg Loss: 3.7483, Avg Acc: 0.3534
2022-01-15 02:27:37,982 Epoch[063/300], Step[0250/1252], Avg Loss: 3.7348, Avg Acc: 0.3530
2022-01-15 02:29:08,136 Epoch[063/300], Step[0300/1252], Avg Loss: 3.7296, Avg Acc: 0.3534
2022-01-15 02:30:38,006 Epoch[063/300], Step[0350/1252], Avg Loss: 3.7201, Avg Acc: 0.3554
2022-01-15 02:32:07,916 Epoch[063/300], Step[0400/1252], Avg Loss: 3.7274, Avg Acc: 0.3560
2022-01-15 02:33:38,524 Epoch[063/300], Step[0450/1252], Avg Loss: 3.7216, Avg Acc: 0.3597
2022-01-15 02:35:10,206 Epoch[063/300], Step[0500/1252], Avg Loss: 3.7193, Avg Acc: 0.3602
2022-01-15 02:36:40,558 Epoch[063/300], Step[0550/1252], Avg Loss: 3.7185, Avg Acc: 0.3615
2022-01-15 02:38:13,057 Epoch[063/300], Step[0600/1252], Avg Loss: 3.7201, Avg Acc: 0.3609
2022-01-15 02:39:44,033 Epoch[063/300], Step[0650/1252], Avg Loss: 3.7185, Avg Acc: 0.3598
2022-01-15 02:41:14,578 Epoch[063/300], Step[0700/1252], Avg Loss: 3.7197, Avg Acc: 0.3608
2022-01-15 02:42:45,768 Epoch[063/300], Step[0750/1252], Avg Loss: 3.7258, Avg Acc: 0.3594
2022-01-15 02:44:16,225 Epoch[063/300], Step[0800/1252], Avg Loss: 3.7219, Avg Acc: 0.3607
2022-01-15 02:45:47,569 Epoch[063/300], Step[0850/1252], Avg Loss: 3.7230, Avg Acc: 0.3603
2022-01-15 02:47:17,257 Epoch[063/300], Step[0900/1252], Avg Loss: 3.7220, Avg Acc: 0.3604
2022-01-15 02:48:46,652 Epoch[063/300], Step[0950/1252], Avg Loss: 3.7251, Avg Acc: 0.3588
2022-01-15 02:50:15,434 Epoch[063/300], Step[1000/1252], Avg Loss: 3.7228, Avg Acc: 0.3601
2022-01-15 02:51:43,883 Epoch[063/300], Step[1050/1252], Avg Loss: 3.7188, Avg Acc: 0.3610
2022-01-15 02:53:13,188 Epoch[063/300], Step[1100/1252], Avg Loss: 3.7153, Avg Acc: 0.3624
2022-01-15 02:54:40,371 Epoch[063/300], Step[1150/1252], Avg Loss: 3.7132, Avg Acc: 0.3633
2022-01-15 02:56:10,998 Epoch[063/300], Step[1200/1252], Avg Loss: 3.7143, Avg Acc: 0.3631
2022-01-15 02:57:42,046 Epoch[063/300], Step[1250/1252], Avg Loss: 3.7158, Avg Acc: 0.3641
2022-01-15 02:57:48,821 ----- Epoch[063/300], Train Loss: 3.7158, Train Acc: 0.3641, time: 2381.47, Best Val(epoch56) Acc@1: 0.6757
2022-01-15 02:57:48,821 Now training epoch 64. LR=0.000941
2022-01-15 02:59:46,041 Epoch[064/300], Step[0000/1252], Avg Loss: 3.4502, Avg Acc: 0.2246
2022-01-15 03:01:16,651 Epoch[064/300], Step[0050/1252], Avg Loss: 3.6713, Avg Acc: 0.3653
2022-01-15 03:02:47,545 Epoch[064/300], Step[0100/1252], Avg Loss: 3.6737, Avg Acc: 0.3767
2022-01-15 03:04:19,581 Epoch[064/300], Step[0150/1252], Avg Loss: 3.6718, Avg Acc: 0.3706
2022-01-15 03:05:51,953 Epoch[064/300], Step[0200/1252], Avg Loss: 3.6671, Avg Acc: 0.3693
2022-01-15 03:07:22,947 Epoch[064/300], Step[0250/1252], Avg Loss: 3.6697, Avg Acc: 0.3684
2022-01-15 03:08:53,314 Epoch[064/300], Step[0300/1252], Avg Loss: 3.6691, Avg Acc: 0.3699
2022-01-15 03:10:25,421 Epoch[064/300], Step[0350/1252], Avg Loss: 3.6778, Avg Acc: 0.3684
2022-01-15 03:11:57,131 Epoch[064/300], Step[0400/1252], Avg Loss: 3.6838, Avg Acc: 0.3673
2022-01-15 03:13:29,148 Epoch[064/300], Step[0450/1252], Avg Loss: 3.6893, Avg Acc: 0.3658
2022-01-15 03:15:00,464 Epoch[064/300], Step[0500/1252], Avg Loss: 3.6882, Avg Acc: 0.3651
2022-01-15 03:16:31,298 Epoch[064/300], Step[0550/1252], Avg Loss: 3.6838, Avg Acc: 0.3656
2022-01-15 03:18:01,007 Epoch[064/300], Step[0600/1252], Avg Loss: 3.6870, Avg Acc: 0.3669
2022-01-15 03:19:30,035 Epoch[064/300], Step[0650/1252], Avg Loss: 3.6895, Avg Acc: 0.3679
2022-01-15 03:21:01,420 Epoch[064/300], Step[0700/1252], Avg Loss: 3.6948, Avg Acc: 0.3665
2022-01-15 03:22:32,846 Epoch[064/300], Step[0750/1252], Avg Loss: 3.6994, Avg Acc: 0.3646
2022-01-15 03:24:04,176 Epoch[064/300], Step[0800/1252], Avg Loss: 3.7006, Avg Acc: 0.3637
2022-01-15 03:25:34,808 Epoch[064/300], Step[0850/1252], Avg Loss: 3.7032, Avg Acc: 0.3630
2022-01-15 03:27:05,587 Epoch[064/300], Step[0900/1252], Avg Loss: 3.7026, Avg Acc: 0.3634
2022-01-15 03:28:37,154 Epoch[064/300], Step[0950/1252], Avg Loss: 3.6985, Avg Acc: 0.3636
2022-01-15 03:30:09,245 Epoch[064/300], Step[1000/1252], Avg Loss: 3.7014, Avg Acc: 0.3638
2022-01-15 03:31:40,495 Epoch[064/300], Step[1050/1252], Avg Loss: 3.7005, Avg Acc: 0.3641
2022-01-15 03:33:12,593 Epoch[064/300], Step[1100/1252], Avg Loss: 3.7037, Avg Acc: 0.3632
2022-01-15 03:34:43,879 Epoch[064/300], Step[1150/1252], Avg Loss: 3.7047, Avg Acc: 0.3624
2022-01-15 03:36:14,254 Epoch[064/300], Step[1200/1252], Avg Loss: 3.7018, Avg Acc: 0.3629
2022-01-15 03:37:46,289 Epoch[064/300], Step[1250/1252], Avg Loss: 3.7041, Avg Acc: 0.3631
2022-01-15 03:37:53,439 ----- Epoch[064/300], Train Loss: 3.7041, Train Acc: 0.3631, time: 2404.61, Best Val(epoch56) Acc@1: 0.6757
2022-01-15 03:37:53,439 ----- Validation after Epoch: 64
2022-01-15 03:39:09,373 Val Step[0000/1563], Avg Loss: 1.4726, Avg Acc@1: 0.6562, Avg Acc@5: 0.9375
2022-01-15 03:39:11,368 Val Step[0050/1563], Avg Loss: 1.5036, Avg Acc@1: 0.6875, Avg Acc@5: 0.8805
2022-01-15 03:39:13,403 Val Step[0100/1563], Avg Loss: 1.5337, Avg Acc@1: 0.6748, Avg Acc@5: 0.8787
2022-01-15 03:39:15,303 Val Step[0150/1563], Avg Loss: 1.5295, Avg Acc@1: 0.6734, Avg Acc@5: 0.8787
2022-01-15 03:39:17,254 Val Step[0200/1563], Avg Loss: 1.5321, Avg Acc@1: 0.6735, Avg Acc@5: 0.8781
2022-01-15 03:39:19,185 Val Step[0250/1563], Avg Loss: 1.5144, Avg Acc@1: 0.6789, Avg Acc@5: 0.8794
2022-01-15 03:39:21,238 Val Step[0300/1563], Avg Loss: 1.5077, Avg Acc@1: 0.6796, Avg Acc@5: 0.8807
2022-01-15 03:39:23,233 Val Step[0350/1563], Avg Loss: 1.5112, Avg Acc@1: 0.6786, Avg Acc@5: 0.8807
2022-01-15 03:39:25,223 Val Step[0400/1563], Avg Loss: 1.5092, Avg Acc@1: 0.6798, Avg Acc@5: 0.8816
2022-01-15 03:39:27,304 Val Step[0450/1563], Avg Loss: 1.5159, Avg Acc@1: 0.6776, Avg Acc@5: 0.8812
2022-01-15 03:39:29,404 Val Step[0500/1563], Avg Loss: 1.5170, Avg Acc@1: 0.6763, Avg Acc@5: 0.8813
2022-01-15 03:39:31,472 Val Step[0550/1563], Avg Loss: 1.5134, Avg Acc@1: 0.6771, Avg Acc@5: 0.8820
2022-01-15 03:39:33,467 Val Step[0600/1563], Avg Loss: 1.5136, Avg Acc@1: 0.6769, Avg Acc@5: 0.8823
2022-01-15 03:39:35,604 Val Step[0650/1563], Avg Loss: 1.5131, Avg Acc@1: 0.6773, Avg Acc@5: 0.8826
2022-01-15 03:39:37,564 Val Step[0700/1563], Avg Loss: 1.5109, Avg Acc@1: 0.6772, Avg Acc@5: 0.8833
2022-01-15 03:39:39,493 Val Step[0750/1563], Avg Loss: 1.5152, Avg Acc@1: 0.6761, Avg Acc@5: 0.8829
2022-01-15 03:39:41,543 Val Step[0800/1563], Avg Loss: 1.5154, Avg Acc@1: 0.6768, Avg Acc@5: 0.8833
2022-01-15 03:39:43,649 Val Step[0850/1563], Avg Loss: 1.5170, Avg Acc@1: 0.6764, Avg Acc@5: 0.8828
2022-01-15 03:39:45,570 Val Step[0900/1563], Avg Loss: 1.5130, Avg Acc@1: 0.6767, Avg Acc@5: 0.8836
2022-01-15 03:39:47,549 Val Step[0950/1563], Avg Loss: 1.5126, Avg Acc@1: 0.6770, Avg Acc@5: 0.8836
2022-01-15 03:39:49,550 Val Step[1000/1563], Avg Loss: 1.5133, Avg Acc@1: 0.6764, Avg Acc@5: 0.8831
2022-01-15 03:39:51,456 Val Step[1050/1563], Avg Loss: 1.5166, Avg Acc@1: 0.6756, Avg Acc@5: 0.8825
2022-01-15 03:39:53,430 Val Step[1100/1563], Avg Loss: 1.5168, Avg Acc@1: 0.6750, Avg Acc@5: 0.8825
2022-01-15 03:39:55,407 Val Step[1150/1563], Avg Loss: 1.5158, Avg Acc@1: 0.6748, Avg Acc@5: 0.8823
2022-01-15 03:39:57,411 Val Step[1200/1563], Avg Loss: 1.5148, Avg Acc@1: 0.6752, Avg Acc@5: 0.8824
2022-01-15 03:39:59,368 Val Step[1250/1563], Avg Loss: 1.5144, Avg Acc@1: 0.6753, Avg Acc@5: 0.8824
2022-01-15 03:40:01,423 Val Step[1300/1563], Avg Loss: 1.5173, Avg Acc@1: 0.6752, Avg Acc@5: 0.8821
2022-01-15 03:40:03,406 Val Step[1350/1563], Avg Loss: 1.5172, Avg Acc@1: 0.6750, Avg Acc@5: 0.8821
2022-01-15 03:40:05,389 Val Step[1400/1563], Avg Loss: 1.5155, Avg Acc@1: 0.6749, Avg Acc@5: 0.8823
2022-01-15 03:40:07,428 Val Step[1450/1563], Avg Loss: 1.5165, Avg Acc@1: 0.6745, Avg Acc@5: 0.8822
2022-01-15 03:40:09,400 Val Step[1500/1563], Avg Loss: 1.5163, Avg Acc@1: 0.6747, Avg Acc@5: 0.8823
2022-01-15 03:40:11,292 Val Step[1550/1563], Avg Loss: 1.5170, Avg Acc@1: 0.6747, Avg Acc@5: 0.8823
2022-01-15 03:40:13,264 ----- Epoch[064/300], Validation Loss: 1.5174, Validation Acc@1: 0.6746, Validation Acc@5: 0.8825, time: 139.82
2022-01-15 03:40:13,265 Now training epoch 65. LR=0.000938
2022-01-15 03:42:06,825 Epoch[065/300], Step[0000/1252], Avg Loss: 3.7348, Avg Acc: 0.3535
2022-01-15 03:43:37,528 Epoch[065/300], Step[0050/1252], Avg Loss: 3.6734, Avg Acc: 0.3959
2022-01-15 03:45:08,764 Epoch[065/300], Step[0100/1252], Avg Loss: 3.7167, Avg Acc: 0.3529
2022-01-15 03:46:39,927 Epoch[065/300], Step[0150/1252], Avg Loss: 3.7159, Avg Acc: 0.3584
2022-01-15 03:48:10,122 Epoch[065/300], Step[0200/1252], Avg Loss: 3.7088, Avg Acc: 0.3616
2022-01-15 03:49:40,568 Epoch[065/300], Step[0250/1252], Avg Loss: 3.7211, Avg Acc: 0.3573
2022-01-15 03:51:12,964 Epoch[065/300], Step[0300/1252], Avg Loss: 3.7221, Avg Acc: 0.3550
2022-01-15 03:52:43,911 Epoch[065/300], Step[0350/1252], Avg Loss: 3.7278, Avg Acc: 0.3557
2022-01-15 03:54:14,938 Epoch[065/300], Step[0400/1252], Avg Loss: 3.7205, Avg Acc: 0.3583
2022-01-15 03:55:46,813 Epoch[065/300], Step[0450/1252], Avg Loss: 3.7106, Avg Acc: 0.3601
2022-01-15 03:57:17,715 Epoch[065/300], Step[0500/1252], Avg Loss: 3.7072, Avg Acc: 0.3629
2022-01-15 03:58:48,928 Epoch[065/300], Step[0550/1252], Avg Loss: 3.7163, Avg Acc: 0.3598
2022-01-15 04:00:20,159 Epoch[065/300], Step[0600/1252], Avg Loss: 3.7163, Avg Acc: 0.3575
2022-01-15 04:01:51,042 Epoch[065/300], Step[0650/1252], Avg Loss: 3.7183, Avg Acc: 0.3563
2022-01-15 04:03:21,736 Epoch[065/300], Step[0700/1252], Avg Loss: 3.7174, Avg Acc: 0.3565
2022-01-15 04:04:50,764 Epoch[065/300], Step[0750/1252], Avg Loss: 3.7213, Avg Acc: 0.3575
2022-01-15 04:06:21,670 Epoch[065/300], Step[0800/1252], Avg Loss: 3.7179, Avg Acc: 0.3591
2022-01-15 04:07:53,196 Epoch[065/300], Step[0850/1252], Avg Loss: 3.7167, Avg Acc: 0.3594
2022-01-15 04:09:24,915 Epoch[065/300], Step[0900/1252], Avg Loss: 3.7178, Avg Acc: 0.3581
2022-01-15 04:10:56,629 Epoch[065/300], Step[0950/1252], Avg Loss: 3.7164, Avg Acc: 0.3574
2022-01-15 04:12:27,236 Epoch[065/300], Step[1000/1252], Avg Loss: 3.7163, Avg Acc: 0.3586
2022-01-15 04:13:58,424 Epoch[065/300], Step[1050/1252], Avg Loss: 3.7179, Avg Acc: 0.3578
2022-01-15 04:15:28,378 Epoch[065/300], Step[1100/1252], Avg Loss: 3.7189, Avg Acc: 0.3573
2022-01-15 04:16:59,894 Epoch[065/300], Step[1150/1252], Avg Loss: 3.7199, Avg Acc: 0.3578
2022-01-15 04:18:32,253 Epoch[065/300], Step[1200/1252], Avg Loss: 3.7209, Avg Acc: 0.3575
2022-01-15 04:20:02,203 Epoch[065/300], Step[1250/1252], Avg Loss: 3.7189, Avg Acc: 0.3576
2022-01-15 04:20:08,990 ----- Epoch[065/300], Train Loss: 3.7188, Train Acc: 0.3576, time: 2395.72, Best Val(epoch56) Acc@1: 0.6757
2022-01-15 04:20:08,991 Now training epoch 66. LR=0.000936
2022-01-15 04:22:09,840 Epoch[066/300], Step[0000/1252], Avg Loss: 3.9089, Avg Acc: 0.3613
2022-01-15 04:23:38,618 Epoch[066/300], Step[0050/1252], Avg Loss: 3.6749, Avg Acc: 0.3661
2022-01-15 04:25:07,133 Epoch[066/300], Step[0100/1252], Avg Loss: 3.6705, Avg Acc: 0.3630
2022-01-15 04:26:35,221 Epoch[066/300], Step[0150/1252], Avg Loss: 3.6568, Avg Acc: 0.3545
2022-01-15 04:28:03,238 Epoch[066/300], Step[0200/1252], Avg Loss: 3.6695, Avg Acc: 0.3623
2022-01-15 04:29:31,516 Epoch[066/300], Step[0250/1252], Avg Loss: 3.6750, Avg Acc: 0.3610
2022-01-15 04:30:58,695 Epoch[066/300], Step[0300/1252], Avg Loss: 3.6773, Avg Acc: 0.3591
2022-01-15 04:32:27,499 Epoch[066/300], Step[0350/1252], Avg Loss: 3.6824, Avg Acc: 0.3584
2022-01-15 04:33:56,707 Epoch[066/300], Step[0400/1252], Avg Loss: 3.6838, Avg Acc: 0.3628
2022-01-15 04:35:25,434 Epoch[066/300], Step[0450/1252], Avg Loss: 3.6900, Avg Acc: 0.3625
2022-01-15 04:36:54,847 Epoch[066/300], Step[0500/1252], Avg Loss: 3.6932, Avg Acc: 0.3630
2022-01-15 04:38:26,100 Epoch[066/300], Step[0550/1252], Avg Loss: 3.6937, Avg Acc: 0.3646
2022-01-15 04:39:57,040 Epoch[066/300], Step[0600/1252], Avg Loss: 3.7016, Avg Acc: 0.3644
2022-01-15 04:41:28,125 Epoch[066/300], Step[0650/1252], Avg Loss: 3.6965, Avg Acc: 0.3642
2022-01-15 04:43:00,423 Epoch[066/300], Step[0700/1252], Avg Loss: 3.7006, Avg Acc: 0.3633
2022-01-15 04:44:30,681 Epoch[066/300], Step[0750/1252], Avg Loss: 3.7005, Avg Acc: 0.3642
2022-01-15 04:46:01,600 Epoch[066/300], Step[0800/1252], Avg Loss: 3.7038, Avg Acc: 0.3624
2022-01-15 04:47:33,113 Epoch[066/300], Step[0850/1252], Avg Loss: 3.6997, Avg Acc: 0.3611
2022-01-15 04:49:04,797 Epoch[066/300], Step[0900/1252], Avg Loss: 3.7026, Avg Acc: 0.3601
2022-01-15 04:50:37,007 Epoch[066/300], Step[0950/1252], Avg Loss: 3.7007, Avg Acc: 0.3595
2022-01-15 04:52:09,701 Epoch[066/300], Step[1000/1252], Avg Loss: 3.7002, Avg Acc: 0.3596
2022-01-15 04:53:41,256 Epoch[066/300], Step[1050/1252], Avg Loss: 3.7014, Avg Acc: 0.3593
2022-01-15 04:55:11,308 Epoch[066/300], Step[1100/1252], Avg Loss: 3.6998, Avg Acc: 0.3592
2022-01-15 04:56:43,059 Epoch[066/300], Step[1150/1252], Avg Loss: 3.6978, Avg Acc: 0.3602
2022-01-15 04:58:14,492 Epoch[066/300], Step[1200/1252], Avg Loss: 3.6967, Avg Acc: 0.3604
2022-01-15 04:59:46,209 Epoch[066/300], Step[1250/1252], Avg Loss: 3.6949, Avg Acc: 0.3612
2022-01-15 04:59:53,150 ----- Epoch[066/300], Train Loss: 3.6949, Train Acc: 0.3611, time: 2384.15, Best Val(epoch56) Acc@1: 0.6757
2022-01-15 04:59:53,150 ----- Validation after Epoch: 66
2022-01-15 05:01:12,722 Val Step[0000/1563], Avg Loss: 1.2379, Avg Acc@1: 0.7500, Avg Acc@5: 0.9375
2022-01-15 05:01:14,708 Val Step[0050/1563], Avg Loss: 1.4319, Avg Acc@1: 0.6863, Avg Acc@5: 0.8824
2022-01-15 05:01:16,623 Val Step[0100/1563], Avg Loss: 1.4453, Avg Acc@1: 0.6798, Avg Acc@5: 0.8843
2022-01-15 05:01:18,718 Val Step[0150/1563], Avg Loss: 1.4451, Avg Acc@1: 0.6767, Avg Acc@5: 0.8800
2022-01-15 05:01:20,712 Val Step[0200/1563], Avg Loss: 1.4447, Avg Acc@1: 0.6769, Avg Acc@5: 0.8800
2022-01-15 05:01:22,777 Val Step[0250/1563], Avg Loss: 1.4273, Avg Acc@1: 0.6819, Avg Acc@5: 0.8821
2022-01-15 05:01:24,711 Val Step[0300/1563], Avg Loss: 1.4287, Avg Acc@1: 0.6827, Avg Acc@5: 0.8810
2022-01-15 05:01:26,678 Val Step[0350/1563], Avg Loss: 1.4361, Avg Acc@1: 0.6809, Avg Acc@5: 0.8809
2022-01-15 05:01:28,631 Val Step[0400/1563], Avg Loss: 1.4326, Avg Acc@1: 0.6810, Avg Acc@5: 0.8809
2022-01-15 05:01:30,533 Val Step[0450/1563], Avg Loss: 1.4391, Avg Acc@1: 0.6761, Avg Acc@5: 0.8806
2022-01-15 05:01:32,497 Val Step[0500/1563], Avg Loss: 1.4431, Avg Acc@1: 0.6747, Avg Acc@5: 0.8801
2022-01-15 05:01:34,456 Val Step[0550/1563], Avg Loss: 1.4441, Avg Acc@1: 0.6741, Avg Acc@5: 0.8795
2022-01-15 05:01:36,399 Val Step[0600/1563], Avg Loss: 1.4428, Avg Acc@1: 0.6739, Avg Acc@5: 0.8801
2022-01-15 05:01:38,471 Val Step[0650/1563], Avg Loss: 1.4426, Avg Acc@1: 0.6743, Avg Acc@5: 0.8799
2022-01-15 05:01:40,647 Val Step[0700/1563], Avg Loss: 1.4397, Avg Acc@1: 0.6747, Avg Acc@5: 0.8803
2022-01-15 05:01:42,694 Val Step[0750/1563], Avg Loss: 1.4457, Avg Acc@1: 0.6733, Avg Acc@5: 0.8799
2022-01-15 05:01:44,719 Val Step[0800/1563], Avg Loss: 1.4439, Avg Acc@1: 0.6746, Avg Acc@5: 0.8803
2022-01-15 05:01:46,691 Val Step[0850/1563], Avg Loss: 1.4464, Avg Acc@1: 0.6735, Avg Acc@5: 0.8801
2022-01-15 05:01:48,603 Val Step[0900/1563], Avg Loss: 1.4430, Avg Acc@1: 0.6737, Avg Acc@5: 0.8810
2022-01-15 05:01:50,531 Val Step[0950/1563], Avg Loss: 1.4429, Avg Acc@1: 0.6734, Avg Acc@5: 0.8812
2022-01-15 05:01:52,458 Val Step[1000/1563], Avg Loss: 1.4432, Avg Acc@1: 0.6734, Avg Acc@5: 0.8811
2022-01-15 05:01:54,335 Val Step[1050/1563], Avg Loss: 1.4473, Avg Acc@1: 0.6725, Avg Acc@5: 0.8802
2022-01-15 05:01:56,223 Val Step[1100/1563], Avg Loss: 1.4471, Avg Acc@1: 0.6723, Avg Acc@5: 0.8801
2022-01-15 05:01:58,220 Val Step[1150/1563], Avg Loss: 1.4461, Avg Acc@1: 0.6724, Avg Acc@5: 0.8798
2022-01-15 05:02:00,093 Val Step[1200/1563], Avg Loss: 1.4450, Avg Acc@1: 0.6729, Avg Acc@5: 0.8800
2022-01-15 05:02:02,138 Val Step[1250/1563], Avg Loss: 1.4439, Avg Acc@1: 0.6730, Avg Acc@5: 0.8807
2022-01-15 05:02:04,180 Val Step[1300/1563], Avg Loss: 1.4466, Avg Acc@1: 0.6726, Avg Acc@5: 0.8803
2022-01-15 05:02:06,069 Val Step[1350/1563], Avg Loss: 1.4461, Avg Acc@1: 0.6728, Avg Acc@5: 0.8803
2022-01-15 05:02:08,078 Val Step[1400/1563], Avg Loss: 1.4438, Avg Acc@1: 0.6732, Avg Acc@5: 0.8804
2022-01-15 05:02:10,050 Val Step[1450/1563], Avg Loss: 1.4443, Avg Acc@1: 0.6731, Avg Acc@5: 0.8801
2022-01-15 05:02:12,050 Val Step[1500/1563], Avg Loss: 1.4438, Avg Acc@1: 0.6731, Avg Acc@5: 0.8805
2022-01-15 05:02:13,937 Val Step[1550/1563], Avg Loss: 1.4450, Avg Acc@1: 0.6727, Avg Acc@5: 0.8804
2022-01-15 05:02:15,908 ----- Epoch[066/300], Validation Loss: 1.4447, Validation Acc@1: 0.6728, Validation Acc@5: 0.8805, time: 142.76
2022-01-15 05:02:15,908 Now training epoch 67. LR=0.000933
2022-01-15 05:04:17,230 Epoch[067/300], Step[0000/1252], Avg Loss: 3.2099, Avg Acc: 0.5459
2022-01-15 05:05:47,314 Epoch[067/300], Step[0050/1252], Avg Loss: 3.6642, Avg Acc: 0.3606
2022-01-15 05:07:17,684 Epoch[067/300], Step[0100/1252], Avg Loss: 3.6368, Avg Acc: 0.3578
2022-01-15 05:08:48,072 Epoch[067/300], Step[0150/1252], Avg Loss: 3.6509, Avg Acc: 0.3618
2022-01-15 05:10:19,324 Epoch[067/300], Step[0200/1252], Avg Loss: 3.6561, Avg Acc: 0.3631
2022-01-15 05:11:50,749 Epoch[067/300], Step[0250/1252], Avg Loss: 3.6646, Avg Acc: 0.3621
2022-01-15 05:13:21,701 Epoch[067/300], Step[0300/1252], Avg Loss: 3.6612, Avg Acc: 0.3649
2022-01-15 05:14:52,290 Epoch[067/300], Step[0350/1252], Avg Loss: 3.6696, Avg Acc: 0.3676
2022-01-15 05:16:23,583 Epoch[067/300], Step[0400/1252], Avg Loss: 3.6753, Avg Acc: 0.3671
2022-01-15 05:17:52,499 Epoch[067/300], Step[0450/1252], Avg Loss: 3.6760, Avg Acc: 0.3671
2022-01-15 05:19:21,738 Epoch[067/300], Step[0500/1252], Avg Loss: 3.6696, Avg Acc: 0.3675
2022-01-15 05:20:52,701 Epoch[067/300], Step[0550/1252], Avg Loss: 3.6800, Avg Acc: 0.3645
2022-01-15 05:22:24,381 Epoch[067/300], Step[0600/1252], Avg Loss: 3.6851, Avg Acc: 0.3644
2022-01-15 05:23:56,304 Epoch[067/300], Step[0650/1252], Avg Loss: 3.6888, Avg Acc: 0.3649
2022-01-15 05:25:26,315 Epoch[067/300], Step[0700/1252], Avg Loss: 3.6935, Avg Acc: 0.3640
2022-01-15 05:26:56,606 Epoch[067/300], Step[0750/1252], Avg Loss: 3.6950, Avg Acc: 0.3629
2022-01-15 05:28:26,954 Epoch[067/300], Step[0800/1252], Avg Loss: 3.6959, Avg Acc: 0.3627
2022-01-15 05:29:57,987 Epoch[067/300], Step[0850/1252], Avg Loss: 3.6970, Avg Acc: 0.3627
2022-01-15 05:31:27,756 Epoch[067/300], Step[0900/1252], Avg Loss: 3.6995, Avg Acc: 0.3639
2022-01-15 05:32:59,779 Epoch[067/300], Step[0950/1252], Avg Loss: 3.7013, Avg Acc: 0.3641
2022-01-15 05:34:31,431 Epoch[067/300], Step[1000/1252], Avg Loss: 3.6994, Avg Acc: 0.3647
2022-01-15 05:36:03,213 Epoch[067/300], Step[1050/1252], Avg Loss: 3.6998, Avg Acc: 0.3648
2022-01-15 05:37:34,713 Epoch[067/300], Step[1100/1252], Avg Loss: 3.6980, Avg Acc: 0.3649
2022-01-15 05:39:06,450 Epoch[067/300], Step[1150/1252], Avg Loss: 3.6966, Avg Acc: 0.3651
2022-01-15 05:40:37,276 Epoch[067/300], Step[1200/1252], Avg Loss: 3.6971, Avg Acc: 0.3661
2022-01-15 05:42:04,951 Epoch[067/300], Step[1250/1252], Avg Loss: 3.6997, Avg Acc: 0.3650
2022-01-15 05:42:11,895 ----- Epoch[067/300], Train Loss: 3.6996, Train Acc: 0.3650, time: 2395.98, Best Val(epoch56) Acc@1: 0.6757
2022-01-15 05:42:11,895 Now training epoch 68. LR=0.000930
2022-01-15 05:44:10,336 Epoch[068/300], Step[0000/1252], Avg Loss: 3.9605, Avg Acc: 0.1992
2022-01-15 05:45:40,730 Epoch[068/300], Step[0050/1252], Avg Loss: 3.6314, Avg Acc: 0.3889
2022-01-15 05:47:10,959 Epoch[068/300], Step[0100/1252], Avg Loss: 3.6252, Avg Acc: 0.3886
2022-01-15 05:48:42,407 Epoch[068/300], Step[0150/1252], Avg Loss: 3.6586, Avg Acc: 0.3814
2022-01-15 05:50:13,302 Epoch[068/300], Step[0200/1252], Avg Loss: 3.6627, Avg Acc: 0.3777
2022-01-15 05:51:43,833 Epoch[068/300], Step[0250/1252], Avg Loss: 3.6727, Avg Acc: 0.3769
2022-01-15 05:53:15,747 Epoch[068/300], Step[0300/1252], Avg Loss: 3.6693, Avg Acc: 0.3712
2022-01-15 05:54:47,289 Epoch[068/300], Step[0350/1252], Avg Loss: 3.6725, Avg Acc: 0.3720
2022-01-15 05:56:19,731 Epoch[068/300], Step[0400/1252], Avg Loss: 3.6694, Avg Acc: 0.3667
2022-01-15 05:57:49,721 Epoch[068/300], Step[0450/1252], Avg Loss: 3.6784, Avg Acc: 0.3662
2022-01-15 05:59:20,799 Epoch[068/300], Step[0500/1252], Avg Loss: 3.6800, Avg Acc: 0.3673
2022-01-15 06:00:50,894 Epoch[068/300], Step[0550/1252], Avg Loss: 3.6797, Avg Acc: 0.3659
2022-01-15 06:02:20,590 Epoch[068/300], Step[0600/1252], Avg Loss: 3.6821, Avg Acc: 0.3674
2022-01-15 06:03:49,507 Epoch[068/300], Step[0650/1252], Avg Loss: 3.6851, Avg Acc: 0.3679
2022-01-15 06:05:18,714 Epoch[068/300], Step[0700/1252], Avg Loss: 3.6872, Avg Acc: 0.3665
2022-01-15 06:06:48,946 Epoch[068/300], Step[0750/1252], Avg Loss: 3.6876, Avg Acc: 0.3695
2022-01-15 06:08:18,660 Epoch[068/300], Step[0800/1252], Avg Loss: 3.6885, Avg Acc: 0.3692
2022-01-15 06:09:51,114 Epoch[068/300], Step[0850/1252], Avg Loss: 3.6887, Avg Acc: 0.3690
2022-01-15 06:11:22,917 Epoch[068/300], Step[0900/1252], Avg Loss: 3.6843, Avg Acc: 0.3689
2022-01-15 06:12:54,073 Epoch[068/300], Step[0950/1252], Avg Loss: 3.6869, Avg Acc: 0.3685
2022-01-15 06:14:25,781 Epoch[068/300], Step[1000/1252], Avg Loss: 3.6847, Avg Acc: 0.3699
2022-01-15 06:15:56,236 Epoch[068/300], Step[1050/1252], Avg Loss: 3.6844, Avg Acc: 0.3685
2022-01-15 06:17:25,354 Epoch[068/300], Step[1100/1252], Avg Loss: 3.6842, Avg Acc: 0.3683
2022-01-15 06:18:56,964 Epoch[068/300], Step[1150/1252], Avg Loss: 3.6851, Avg Acc: 0.3691
2022-01-15 06:20:26,762 Epoch[068/300], Step[1200/1252], Avg Loss: 3.6858, Avg Acc: 0.3694
2022-01-15 06:21:57,262 Epoch[068/300], Step[1250/1252], Avg Loss: 3.6869, Avg Acc: 0.3688
2022-01-15 06:22:04,300 ----- Epoch[068/300], Train Loss: 3.6870, Train Acc: 0.3687, time: 2392.40, Best Val(epoch56) Acc@1: 0.6757
2022-01-15 06:22:04,301 ----- Validation after Epoch: 68
2022-01-15 06:23:20,714 Val Step[0000/1563], Avg Loss: 1.2267, Avg Acc@1: 0.7188, Avg Acc@5: 0.9375
2022-01-15 06:23:23,379 Val Step[0050/1563], Avg Loss: 1.5087, Avg Acc@1: 0.6722, Avg Acc@5: 0.8952
2022-01-15 06:23:25,373 Val Step[0100/1563], Avg Loss: 1.5220, Avg Acc@1: 0.6671, Avg Acc@5: 0.8883
2022-01-15 06:23:27,333 Val Step[0150/1563], Avg Loss: 1.5253, Avg Acc@1: 0.6683, Avg Acc@5: 0.8837
2022-01-15 06:23:29,304 Val Step[0200/1563], Avg Loss: 1.5292, Avg Acc@1: 0.6685, Avg Acc@5: 0.8817
2022-01-15 06:23:31,514 Val Step[0250/1563], Avg Loss: 1.5094, Avg Acc@1: 0.6760, Avg Acc@5: 0.8836
2022-01-15 06:23:33,532 Val Step[0300/1563], Avg Loss: 1.5104, Avg Acc@1: 0.6775, Avg Acc@5: 0.8822
2022-01-15 06:23:35,509 Val Step[0350/1563], Avg Loss: 1.5199, Avg Acc@1: 0.6757, Avg Acc@5: 0.8814
2022-01-15 06:23:37,552 Val Step[0400/1563], Avg Loss: 1.5134, Avg Acc@1: 0.6774, Avg Acc@5: 0.8817
2022-01-15 06:23:39,661 Val Step[0450/1563], Avg Loss: 1.5197, Avg Acc@1: 0.6757, Avg Acc@5: 0.8810
2022-01-15 06:23:41,640 Val Step[0500/1563], Avg Loss: 1.5221, Avg Acc@1: 0.6751, Avg Acc@5: 0.8809
2022-01-15 06:23:43,623 Val Step[0550/1563], Avg Loss: 1.5220, Avg Acc@1: 0.6750, Avg Acc@5: 0.8811
2022-01-15 06:23:45,572 Val Step[0600/1563], Avg Loss: 1.5215, Avg Acc@1: 0.6746, Avg Acc@5: 0.8816
2022-01-15 06:23:47,762 Val Step[0650/1563], Avg Loss: 1.5229, Avg Acc@1: 0.6735, Avg Acc@5: 0.8818
2022-01-15 06:23:49,886 Val Step[0700/1563], Avg Loss: 1.5209, Avg Acc@1: 0.6741, Avg Acc@5: 0.8826
2022-01-15 06:23:52,066 Val Step[0750/1563], Avg Loss: 1.5246, Avg Acc@1: 0.6733, Avg Acc@5: 0.8823
2022-01-15 06:23:54,131 Val Step[0800/1563], Avg Loss: 1.5223, Avg Acc@1: 0.6744, Avg Acc@5: 0.8825
2022-01-15 06:23:56,178 Val Step[0850/1563], Avg Loss: 1.5247, Avg Acc@1: 0.6739, Avg Acc@5: 0.8816
2022-01-15 06:23:58,215 Val Step[0900/1563], Avg Loss: 1.5223, Avg Acc@1: 0.6745, Avg Acc@5: 0.8821
2022-01-15 06:24:00,197 Val Step[0950/1563], Avg Loss: 1.5210, Avg Acc@1: 0.6751, Avg Acc@5: 0.8825
2022-01-15 06:24:02,217 Val Step[1000/1563], Avg Loss: 1.5195, Avg Acc@1: 0.6759, Avg Acc@5: 0.8827
2022-01-15 06:24:04,328 Val Step[1050/1563], Avg Loss: 1.5216, Avg Acc@1: 0.6752, Avg Acc@5: 0.8824
2022-01-15 06:24:06,301 Val Step[1100/1563], Avg Loss: 1.5212, Avg Acc@1: 0.6751, Avg Acc@5: 0.8826
2022-01-15 06:24:08,333 Val Step[1150/1563], Avg Loss: 1.5197, Avg Acc@1: 0.6748, Avg Acc@5: 0.8833
2022-01-15 06:24:10,247 Val Step[1200/1563], Avg Loss: 1.5192, Avg Acc@1: 0.6753, Avg Acc@5: 0.8833
2022-01-15 06:24:12,248 Val Step[1250/1563], Avg Loss: 1.5184, Avg Acc@1: 0.6755, Avg Acc@5: 0.8835
2022-01-15 06:24:14,308 Val Step[1300/1563], Avg Loss: 1.5219, Avg Acc@1: 0.6751, Avg Acc@5: 0.8828
2022-01-15 06:24:16,272 Val Step[1350/1563], Avg Loss: 1.5205, Avg Acc@1: 0.6751, Avg Acc@5: 0.8828
2022-01-15 06:24:18,277 Val Step[1400/1563], Avg Loss: 1.5199, Avg Acc@1: 0.6747, Avg Acc@5: 0.8828
2022-01-15 06:24:20,262 Val Step[1450/1563], Avg Loss: 1.5203, Avg Acc@1: 0.6748, Avg Acc@5: 0.8827
2022-01-15 06:24:22,468 Val Step[1500/1563], Avg Loss: 1.5190, Avg Acc@1: 0.6753, Avg Acc@5: 0.8829
2022-01-15 06:24:24,412 Val Step[1550/1563], Avg Loss: 1.5201, Avg Acc@1: 0.6752, Avg Acc@5: 0.8827
2022-01-15 06:24:26,124 ----- Epoch[068/300], Validation Loss: 1.5203, Validation Acc@1: 0.6753, Validation Acc@5: 0.8827, time: 141.82
2022-01-15 06:24:26,124 Now training epoch 69. LR=0.000927
2022-01-15 06:26:18,533 Epoch[069/300], Step[0000/1252], Avg Loss: 3.9050, Avg Acc: 0.3203
2022-01-15 06:27:49,062 Epoch[069/300], Step[0050/1252], Avg Loss: 3.7308, Avg Acc: 0.3861
2022-01-15 06:29:19,532 Epoch[069/300], Step[0100/1252], Avg Loss: 3.6934, Avg Acc: 0.3898
2022-01-15 06:30:49,351 Epoch[069/300], Step[0150/1252], Avg Loss: 3.6399, Avg Acc: 0.3919
2022-01-15 06:32:21,779 Epoch[069/300], Step[0200/1252], Avg Loss: 3.6420, Avg Acc: 0.3861
2022-01-15 06:33:54,314 Epoch[069/300], Step[0250/1252], Avg Loss: 3.6516, Avg Acc: 0.3840
2022-01-15 06:35:24,272 Epoch[069/300], Step[0300/1252], Avg Loss: 3.6495, Avg Acc: 0.3873
2022-01-15 06:36:55,150 Epoch[069/300], Step[0350/1252], Avg Loss: 3.6554, Avg Acc: 0.3855
2022-01-15 06:38:26,940 Epoch[069/300], Step[0400/1252], Avg Loss: 3.6553, Avg Acc: 0.3842
2022-01-15 06:39:57,253 Epoch[069/300], Step[0450/1252], Avg Loss: 3.6679, Avg Acc: 0.3809
2022-01-15 06:41:28,555 Epoch[069/300], Step[0500/1252], Avg Loss: 3.6735, Avg Acc: 0.3802
2022-01-15 06:43:00,414 Epoch[069/300], Step[0550/1252], Avg Loss: 3.6690, Avg Acc: 0.3815
2022-01-15 06:44:30,705 Epoch[069/300], Step[0600/1252], Avg Loss: 3.6695, Avg Acc: 0.3792
2022-01-15 06:46:02,076 Epoch[069/300], Step[0650/1252], Avg Loss: 3.6764, Avg Acc: 0.3771
2022-01-15 06:47:33,168 Epoch[069/300], Step[0700/1252], Avg Loss: 3.6772, Avg Acc: 0.3746
2022-01-15 06:49:03,975 Epoch[069/300], Step[0750/1252], Avg Loss: 3.6805, Avg Acc: 0.3730
2022-01-15 06:50:36,275 Epoch[069/300], Step[0800/1252], Avg Loss: 3.6810, Avg Acc: 0.3728
2022-01-15 06:52:07,519 Epoch[069/300], Step[0850/1252], Avg Loss: 3.6797, Avg Acc: 0.3715
2022-01-15 06:53:39,277 Epoch[069/300], Step[0900/1252], Avg Loss: 3.6855, Avg Acc: 0.3711
2022-01-15 06:55:09,634 Epoch[069/300], Step[0950/1252], Avg Loss: 3.6822, Avg Acc: 0.3711
2022-01-15 06:56:41,449 Epoch[069/300], Step[1000/1252], Avg Loss: 3.6836, Avg Acc: 0.3707
2022-01-15 06:58:13,188 Epoch[069/300], Step[1050/1252], Avg Loss: 3.6818, Avg Acc: 0.3698
2022-01-15 06:59:43,755 Epoch[069/300], Step[1100/1252], Avg Loss: 3.6826, Avg Acc: 0.3693
2022-01-15 07:01:13,565 Epoch[069/300], Step[1150/1252], Avg Loss: 3.6829, Avg Acc: 0.3702
2022-01-15 07:02:44,294 Epoch[069/300], Step[1200/1252], Avg Loss: 3.6846, Avg Acc: 0.3702
2022-01-15 07:04:13,840 Epoch[069/300], Step[1250/1252], Avg Loss: 3.6873, Avg Acc: 0.3699
2022-01-15 07:04:20,921 ----- Epoch[069/300], Train Loss: 3.6873, Train Acc: 0.3699, time: 2394.79, Best Val(epoch56) Acc@1: 0.6757
2022-01-15 07:04:20,921 Now training epoch 70. LR=0.000924
2022-01-15 07:06:13,116 Epoch[070/300], Step[0000/1252], Avg Loss: 3.2008, Avg Acc: 0.4961
2022-01-15 07:07:44,027 Epoch[070/300], Step[0050/1252], Avg Loss: 3.6631, Avg Acc: 0.3238
2022-01-15 07:09:14,815 Epoch[070/300], Step[0100/1252], Avg Loss: 3.6800, Avg Acc: 0.3460
2022-01-15 07:10:46,224 Epoch[070/300], Step[0150/1252], Avg Loss: 3.6739, Avg Acc: 0.3555
2022-01-15 07:12:18,341 Epoch[070/300], Step[0200/1252], Avg Loss: 3.6723, Avg Acc: 0.3585
2022-01-15 07:13:49,713 Epoch[070/300], Step[0250/1252], Avg Loss: 3.6721, Avg Acc: 0.3654
2022-01-15 07:15:20,478 Epoch[070/300], Step[0300/1252], Avg Loss: 3.6673, Avg Acc: 0.3674
2022-01-15 07:16:50,193 Epoch[070/300], Step[0350/1252], Avg Loss: 3.6753, Avg Acc: 0.3658
2022-01-15 07:18:21,259 Epoch[070/300], Step[0400/1252], Avg Loss: 3.6736, Avg Acc: 0.3650
2022-01-15 07:19:51,430 Epoch[070/300], Step[0450/1252], Avg Loss: 3.6756, Avg Acc: 0.3651
2022-01-15 07:21:23,281 Epoch[070/300], Step[0500/1252], Avg Loss: 3.6755, Avg Acc: 0.3643
2022-01-15 07:22:54,364 Epoch[070/300], Step[0550/1252], Avg Loss: 3.6780, Avg Acc: 0.3650
2022-01-15 07:24:26,461 Epoch[070/300], Step[0600/1252], Avg Loss: 3.6767, Avg Acc: 0.3650
2022-01-15 07:25:57,470 Epoch[070/300], Step[0650/1252], Avg Loss: 3.6846, Avg Acc: 0.3645
2022-01-15 07:27:27,846 Epoch[070/300], Step[0700/1252], Avg Loss: 3.6815, Avg Acc: 0.3663
2022-01-15 07:28:58,477 Epoch[070/300], Step[0750/1252], Avg Loss: 3.6817, Avg Acc: 0.3656
2022-01-15 07:30:30,291 Epoch[070/300], Step[0800/1252], Avg Loss: 3.6830, Avg Acc: 0.3650
2022-01-15 07:32:00,898 Epoch[070/300], Step[0850/1252], Avg Loss: 3.6835, Avg Acc: 0.3642
2022-01-15 07:33:32,453 Epoch[070/300], Step[0900/1252], Avg Loss: 3.6861, Avg Acc: 0.3628
2022-01-15 07:35:02,631 Epoch[070/300], Step[0950/1252], Avg Loss: 3.6912, Avg Acc: 0.3616
2022-01-15 07:36:32,770 Epoch[070/300], Step[1000/1252], Avg Loss: 3.6945, Avg Acc: 0.3613
2022-01-15 07:38:01,101 Epoch[070/300], Step[1050/1252], Avg Loss: 3.6960, Avg Acc: 0.3606
2022-01-15 07:39:29,973 Epoch[070/300], Step[1100/1252], Avg Loss: 3.6955, Avg Acc: 0.3609
2022-01-15 07:40:59,833 Epoch[070/300], Step[1150/1252], Avg Loss: 3.6945, Avg Acc: 0.3604
2022-01-15 07:42:28,888 Epoch[070/300], Step[1200/1252], Avg Loss: 3.6960, Avg Acc: 0.3609
2022-01-15 07:43:57,669 Epoch[070/300], Step[1250/1252], Avg Loss: 3.6961, Avg Acc: 0.3615
2022-01-15 07:44:04,217 ----- Epoch[070/300], Train Loss: 3.6961, Train Acc: 0.3615, time: 2383.29, Best Val(epoch56) Acc@1: 0.6757
2022-01-15 07:44:04,217 ----- Validation after Epoch: 70
2022-01-15 07:45:15,394 Val Step[0000/1563], Avg Loss: 1.2826, Avg Acc@1: 0.6250, Avg Acc@5: 0.9375
2022-01-15 07:45:17,429 Val Step[0050/1563], Avg Loss: 1.4693, Avg Acc@1: 0.6752, Avg Acc@5: 0.8805
2022-01-15 07:45:19,515 Val Step[0100/1563], Avg Loss: 1.4831, Avg Acc@1: 0.6705, Avg Acc@5: 0.8809
2022-01-15 07:45:21,622 Val Step[0150/1563], Avg Loss: 1.4728, Avg Acc@1: 0.6776, Avg Acc@5: 0.8781
2022-01-15 07:45:23,661 Val Step[0200/1563], Avg Loss: 1.4798, Avg Acc@1: 0.6763, Avg Acc@5: 0.8758
2022-01-15 07:45:25,708 Val Step[0250/1563], Avg Loss: 1.4608, Avg Acc@1: 0.6810, Avg Acc@5: 0.8792
2022-01-15 07:45:27,841 Val Step[0300/1563], Avg Loss: 1.4571, Avg Acc@1: 0.6823, Avg Acc@5: 0.8795
2022-01-15 07:45:30,004 Val Step[0350/1563], Avg Loss: 1.4647, Avg Acc@1: 0.6813, Avg Acc@5: 0.8780
2022-01-15 07:45:32,170 Val Step[0400/1563], Avg Loss: 1.4655, Avg Acc@1: 0.6799, Avg Acc@5: 0.8788
2022-01-15 07:45:34,351 Val Step[0450/1563], Avg Loss: 1.4707, Avg Acc@1: 0.6771, Avg Acc@5: 0.8778
2022-01-15 07:45:36,518 Val Step[0500/1563], Avg Loss: 1.4743, Avg Acc@1: 0.6766, Avg Acc@5: 0.8772
2022-01-15 07:45:38,602 Val Step[0550/1563], Avg Loss: 1.4750, Avg Acc@1: 0.6763, Avg Acc@5: 0.8766
2022-01-15 07:45:40,725 Val Step[0600/1563], Avg Loss: 1.4746, Avg Acc@1: 0.6754, Avg Acc@5: 0.8772
2022-01-15 07:45:42,841 Val Step[0650/1563], Avg Loss: 1.4759, Avg Acc@1: 0.6751, Avg Acc@5: 0.8778
2022-01-15 07:45:44,885 Val Step[0700/1563], Avg Loss: 1.4717, Avg Acc@1: 0.6750, Avg Acc@5: 0.8789
2022-01-15 07:45:46,884 Val Step[0750/1563], Avg Loss: 1.4767, Avg Acc@1: 0.6746, Avg Acc@5: 0.8782
2022-01-15 07:45:48,864 Val Step[0800/1563], Avg Loss: 1.4739, Avg Acc@1: 0.6756, Avg Acc@5: 0.8787
2022-01-15 07:45:50,924 Val Step[0850/1563], Avg Loss: 1.4757, Avg Acc@1: 0.6752, Avg Acc@5: 0.8784
2022-01-15 07:45:52,904 Val Step[0900/1563], Avg Loss: 1.4720, Avg Acc@1: 0.6759, Avg Acc@5: 0.8793
2022-01-15 07:45:54,833 Val Step[0950/1563], Avg Loss: 1.4721, Avg Acc@1: 0.6753, Avg Acc@5: 0.8797
2022-01-15 07:45:56,799 Val Step[1000/1563], Avg Loss: 1.4719, Avg Acc@1: 0.6753, Avg Acc@5: 0.8800
2022-01-15 07:45:58,744 Val Step[1050/1563], Avg Loss: 1.4744, Avg Acc@1: 0.6747, Avg Acc@5: 0.8798
2022-01-15 07:46:00,683 Val Step[1100/1563], Avg Loss: 1.4739, Avg Acc@1: 0.6748, Avg Acc@5: 0.8799
2022-01-15 07:46:02,608 Val Step[1150/1563], Avg Loss: 1.4732, Avg Acc@1: 0.6752, Avg Acc@5: 0.8801
2022-01-15 07:46:04,490 Val Step[1200/1563], Avg Loss: 1.4727, Avg Acc@1: 0.6754, Avg Acc@5: 0.8799
2022-01-15 07:46:06,421 Val Step[1250/1563], Avg Loss: 1.4713, Avg Acc@1: 0.6756, Avg Acc@5: 0.8805
2022-01-15 07:46:08,530 Val Step[1300/1563], Avg Loss: 1.4735, Avg Acc@1: 0.6752, Avg Acc@5: 0.8803
2022-01-15 07:46:10,791 Val Step[1350/1563], Avg Loss: 1.4730, Avg Acc@1: 0.6751, Avg Acc@5: 0.8805
2022-01-15 07:46:12,803 Val Step[1400/1563], Avg Loss: 1.4726, Avg Acc@1: 0.6749, Avg Acc@5: 0.8806
2022-01-15 07:46:14,734 Val Step[1450/1563], Avg Loss: 1.4730, Avg Acc@1: 0.6752, Avg Acc@5: 0.8806
2022-01-15 07:46:16,717 Val Step[1500/1563], Avg Loss: 1.4732, Avg Acc@1: 0.6754, Avg Acc@5: 0.8806
2022-01-15 07:46:18,583 Val Step[1550/1563], Avg Loss: 1.4742, Avg Acc@1: 0.6748, Avg Acc@5: 0.8806
2022-01-15 07:46:20,537 ----- Epoch[070/300], Validation Loss: 1.4745, Validation Acc@1: 0.6748, Validation Acc@5: 0.8806, time: 136.32
2022-01-15 07:46:21,034 ----- Save model: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-70-Loss-3.691598867992805.pdparams
2022-01-15 07:46:21,034 ----- Save optim: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-70-Loss-3.691598867992805.pdopt
2022-01-15 07:46:21,034 Now training epoch 71. LR=0.000921
2022-01-15 07:48:11,219 Epoch[071/300], Step[0000/1252], Avg Loss: 3.7628, Avg Acc: 0.2744
2022-01-15 07:49:40,957 Epoch[071/300], Step[0050/1252], Avg Loss: 3.7162, Avg Acc: 0.3776
2022-01-15 07:51:11,379 Epoch[071/300], Step[0100/1252], Avg Loss: 3.6571, Avg Acc: 0.3726
2022-01-15 07:52:44,113 Epoch[071/300], Step[0150/1252], Avg Loss: 3.6419, Avg Acc: 0.3748
2022-01-15 07:54:14,278 Epoch[071/300], Step[0200/1252], Avg Loss: 3.6565, Avg Acc: 0.3798
2022-01-15 07:55:46,255 Epoch[071/300], Step[0250/1252], Avg Loss: 3.6637, Avg Acc: 0.3742
2022-01-15 07:57:16,633 Epoch[071/300], Step[0300/1252], Avg Loss: 3.6628, Avg Acc: 0.3751
2022-01-15 07:58:47,958 Epoch[071/300], Step[0350/1252], Avg Loss: 3.6564, Avg Acc: 0.3743
2022-01-15 08:00:17,267 Epoch[071/300], Step[0400/1252], Avg Loss: 3.6604, Avg Acc: 0.3737
2022-01-15 08:01:48,104 Epoch[071/300], Step[0450/1252], Avg Loss: 3.6605, Avg Acc: 0.3719
2022-01-15 08:03:17,846 Epoch[071/300], Step[0500/1252], Avg Loss: 3.6675, Avg Acc: 0.3736
2022-01-15 08:04:49,633 Epoch[071/300], Step[0550/1252], Avg Loss: 3.6664, Avg Acc: 0.3736
2022-01-15 08:06:21,003 Epoch[071/300], Step[0600/1252], Avg Loss: 3.6663, Avg Acc: 0.3724
2022-01-15 08:07:51,896 Epoch[071/300], Step[0650/1252], Avg Loss: 3.6694, Avg Acc: 0.3722
2022-01-15 08:09:22,536 Epoch[071/300], Step[0700/1252], Avg Loss: 3.6715, Avg Acc: 0.3716
2022-01-15 08:10:52,059 Epoch[071/300], Step[0750/1252], Avg Loss: 3.6739, Avg Acc: 0.3714
2022-01-15 08:12:24,194 Epoch[071/300], Step[0800/1252], Avg Loss: 3.6760, Avg Acc: 0.3696
2022-01-15 08:13:55,668 Epoch[071/300], Step[0850/1252], Avg Loss: 3.6723, Avg Acc: 0.3684
2022-01-15 08:15:28,118 Epoch[071/300], Step[0900/1252], Avg Loss: 3.6709, Avg Acc: 0.3681
2022-01-15 08:16:59,765 Epoch[071/300], Step[0950/1252], Avg Loss: 3.6712, Avg Acc: 0.3677
2022-01-15 08:18:31,962 Epoch[071/300], Step[1000/1252], Avg Loss: 3.6730, Avg Acc: 0.3664
2022-01-15 08:20:03,054 Epoch[071/300], Step[1050/1252], Avg Loss: 3.6746, Avg Acc: 0.3668
2022-01-15 08:21:34,988 Epoch[071/300], Step[1100/1252], Avg Loss: 3.6797, Avg Acc: 0.3663
2022-01-15 08:23:06,744 Epoch[071/300], Step[1150/1252], Avg Loss: 3.6784, Avg Acc: 0.3665
2022-01-15 08:24:37,498 Epoch[071/300], Step[1200/1252], Avg Loss: 3.6801, Avg Acc: 0.3664
2022-01-15 08:26:08,990 Epoch[071/300], Step[1250/1252], Avg Loss: 3.6810, Avg Acc: 0.3661
2022-01-15 08:26:15,434 ----- Epoch[071/300], Train Loss: 3.6811, Train Acc: 0.3661, time: 2394.40, Best Val(epoch56) Acc@1: 0.6757
2022-01-15 08:26:15,434 Now training epoch 72. LR=0.000918
2022-01-15 08:28:08,428 Epoch[072/300], Step[0000/1252], Avg Loss: 3.6987, Avg Acc: 0.3057
2022-01-15 08:29:38,142 Epoch[072/300], Step[0050/1252], Avg Loss: 3.6913, Avg Acc: 0.3647
2022-01-15 08:31:09,471 Epoch[072/300], Step[0100/1252], Avg Loss: 3.6955, Avg Acc: 0.3595
2022-01-15 08:32:39,581 Epoch[072/300], Step[0150/1252], Avg Loss: 3.6916, Avg Acc: 0.3640
2022-01-15 08:34:11,012 Epoch[072/300], Step[0200/1252], Avg Loss: 3.6912, Avg Acc: 0.3705
2022-01-15 08:35:42,393 Epoch[072/300], Step[0250/1252], Avg Loss: 3.6756, Avg Acc: 0.3727
2022-01-15 08:37:12,552 Epoch[072/300], Step[0300/1252], Avg Loss: 3.6719, Avg Acc: 0.3726
2022-01-15 08:38:42,232 Epoch[072/300], Step[0350/1252], Avg Loss: 3.6774, Avg Acc: 0.3689
2022-01-15 08:40:13,307 Epoch[072/300], Step[0400/1252], Avg Loss: 3.6844, Avg Acc: 0.3677
2022-01-15 08:41:46,148 Epoch[072/300], Step[0450/1252], Avg Loss: 3.6800, Avg Acc: 0.3669
2022-01-15 08:43:18,874 Epoch[072/300], Step[0500/1252], Avg Loss: 3.6867, Avg Acc: 0.3657
2022-01-15 08:44:49,369 Epoch[072/300], Step[0550/1252], Avg Loss: 3.6878, Avg Acc: 0.3664
2022-01-15 08:46:21,158 Epoch[072/300], Step[0600/1252], Avg Loss: 3.6871, Avg Acc: 0.3665
2022-01-15 08:47:54,066 Epoch[072/300], Step[0650/1252], Avg Loss: 3.6851, Avg Acc: 0.3649
2022-01-15 08:49:24,925 Epoch[072/300], Step[0700/1252], Avg Loss: 3.6798, Avg Acc: 0.3647
2022-01-15 08:50:56,602 Epoch[072/300], Step[0750/1252], Avg Loss: 3.6806, Avg Acc: 0.3623
2022-01-15 08:52:27,303 Epoch[072/300], Step[0800/1252], Avg Loss: 3.6799, Avg Acc: 0.3627
2022-01-15 08:53:58,695 Epoch[072/300], Step[0850/1252], Avg Loss: 3.6820, Avg Acc: 0.3635
2022-01-15 08:55:28,971 Epoch[072/300], Step[0900/1252], Avg Loss: 3.6805, Avg Acc: 0.3639
2022-01-15 08:56:57,824 Epoch[072/300], Step[0950/1252], Avg Loss: 3.6841, Avg Acc: 0.3622
2022-01-15 08:58:28,483 Epoch[072/300], Step[1000/1252], Avg Loss: 3.6829, Avg Acc: 0.3629
2022-01-15 08:59:59,021 Epoch[072/300], Step[1050/1252], Avg Loss: 3.6838, Avg Acc: 0.3623
2022-01-15 09:01:28,990 Epoch[072/300], Step[1100/1252], Avg Loss: 3.6821, Avg Acc: 0.3628
2022-01-15 09:03:00,047 Epoch[072/300], Step[1150/1252], Avg Loss: 3.6809, Avg Acc: 0.3642
2022-01-15 09:04:31,687 Epoch[072/300], Step[1200/1252], Avg Loss: 3.6823, Avg Acc: 0.3638
2022-01-15 09:06:02,412 Epoch[072/300], Step[1250/1252], Avg Loss: 3.6820, Avg Acc: 0.3639
2022-01-15 09:06:09,170 ----- Epoch[072/300], Train Loss: 3.6820, Train Acc: 0.3639, time: 2393.73, Best Val(epoch56) Acc@1: 0.6757
2022-01-15 09:06:09,170 ----- Validation after Epoch: 72
2022-01-15 09:07:27,798 Val Step[0000/1563], Avg Loss: 1.2240, Avg Acc@1: 0.6875, Avg Acc@5: 0.9062
2022-01-15 09:07:29,826 Val Step[0050/1563], Avg Loss: 1.3962, Avg Acc@1: 0.6875, Avg Acc@5: 0.8909
2022-01-15 09:07:31,790 Val Step[0100/1563], Avg Loss: 1.4185, Avg Acc@1: 0.6782, Avg Acc@5: 0.8883
2022-01-15 09:07:33,771 Val Step[0150/1563], Avg Loss: 1.4180, Avg Acc@1: 0.6776, Avg Acc@5: 0.8885
2022-01-15 09:07:35,731 Val Step[0200/1563], Avg Loss: 1.4248, Avg Acc@1: 0.6811, Avg Acc@5: 0.8856
2022-01-15 09:07:37,780 Val Step[0250/1563], Avg Loss: 1.4077, Avg Acc@1: 0.6859, Avg Acc@5: 0.8876
2022-01-15 09:07:39,742 Val Step[0300/1563], Avg Loss: 1.4054, Avg Acc@1: 0.6882, Avg Acc@5: 0.8875
2022-01-15 09:07:41,708 Val Step[0350/1563], Avg Loss: 1.4139, Avg Acc@1: 0.6878, Avg Acc@5: 0.8860
2022-01-15 09:07:43,646 Val Step[0400/1563], Avg Loss: 1.4133, Avg Acc@1: 0.6874, Avg Acc@5: 0.8866
2022-01-15 09:07:45,679 Val Step[0450/1563], Avg Loss: 1.4206, Avg Acc@1: 0.6855, Avg Acc@5: 0.8857
2022-01-15 09:07:47,690 Val Step[0500/1563], Avg Loss: 1.4229, Avg Acc@1: 0.6841, Avg Acc@5: 0.8859
2022-01-15 09:07:49,614 Val Step[0550/1563], Avg Loss: 1.4227, Avg Acc@1: 0.6835, Avg Acc@5: 0.8863
2022-01-15 09:07:51,610 Val Step[0600/1563], Avg Loss: 1.4215, Avg Acc@1: 0.6828, Avg Acc@5: 0.8868
2022-01-15 09:07:53,517 Val Step[0650/1563], Avg Loss: 1.4221, Avg Acc@1: 0.6837, Avg Acc@5: 0.8867
2022-01-15 09:07:55,526 Val Step[0700/1563], Avg Loss: 1.4210, Avg Acc@1: 0.6838, Avg Acc@5: 0.8870
2022-01-15 09:07:57,520 Val Step[0750/1563], Avg Loss: 1.4257, Avg Acc@1: 0.6835, Avg Acc@5: 0.8864
2022-01-15 09:07:59,558 Val Step[0800/1563], Avg Loss: 1.4246, Avg Acc@1: 0.6839, Avg Acc@5: 0.8868
2022-01-15 09:08:01,588 Val Step[0850/1563], Avg Loss: 1.4266, Avg Acc@1: 0.6835, Avg Acc@5: 0.8862
2022-01-15 09:08:03,579 Val Step[0900/1563], Avg Loss: 1.4223, Avg Acc@1: 0.6841, Avg Acc@5: 0.8868
2022-01-15 09:08:05,540 Val Step[0950/1563], Avg Loss: 1.4222, Avg Acc@1: 0.6842, Avg Acc@5: 0.8869
2022-01-15 09:08:07,643 Val Step[1000/1563], Avg Loss: 1.4213, Avg Acc@1: 0.6848, Avg Acc@5: 0.8869
2022-01-15 09:08:09,750 Val Step[1050/1563], Avg Loss: 1.4230, Avg Acc@1: 0.6844, Avg Acc@5: 0.8868
2022-01-15 09:08:11,717 Val Step[1100/1563], Avg Loss: 1.4230, Avg Acc@1: 0.6840, Avg Acc@5: 0.8867
2022-01-15 09:08:13,679 Val Step[1150/1563], Avg Loss: 1.4210, Avg Acc@1: 0.6845, Avg Acc@5: 0.8867
2022-01-15 09:08:15,668 Val Step[1200/1563], Avg Loss: 1.4199, Avg Acc@1: 0.6845, Avg Acc@5: 0.8869
2022-01-15 09:08:17,600 Val Step[1250/1563], Avg Loss: 1.4186, Avg Acc@1: 0.6847, Avg Acc@5: 0.8871
2022-01-15 09:08:19,529 Val Step[1300/1563], Avg Loss: 1.4218, Avg Acc@1: 0.6840, Avg Acc@5: 0.8867
2022-01-15 09:08:21,586 Val Step[1350/1563], Avg Loss: 1.4207, Avg Acc@1: 0.6841, Avg Acc@5: 0.8870
2022-01-15 09:08:23,525 Val Step[1400/1563], Avg Loss: 1.4199, Avg Acc@1: 0.6841, Avg Acc@5: 0.8872
2022-01-15 09:08:25,488 Val Step[1450/1563], Avg Loss: 1.4207, Avg Acc@1: 0.6841, Avg Acc@5: 0.8870
2022-01-15 09:08:27,475 Val Step[1500/1563], Avg Loss: 1.4208, Avg Acc@1: 0.6843, Avg Acc@5: 0.8870
2022-01-15 09:08:29,377 Val Step[1550/1563], Avg Loss: 1.4222, Avg Acc@1: 0.6840, Avg Acc@5: 0.8870
2022-01-15 09:08:31,249 ----- Epoch[072/300], Validation Loss: 1.4222, Validation Acc@1: 0.6840, Validation Acc@5: 0.8870, time: 142.08
2022-01-15 09:08:32,408 the pre best model acc:0.6757, at epoch 56
2022-01-15 09:08:32,706 current best model acc:0.6840, at epoch 72
2022-01-15 09:08:32,706 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-15 09:08:32,707 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-15 09:08:32,707 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-15 09:08:32,707 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-15 09:08:32,707 Now training epoch 73. LR=0.000915
2022-01-15 09:10:23,072 Epoch[073/300], Step[0000/1252], Avg Loss: 3.5992, Avg Acc: 0.5283
2022-01-15 09:11:52,736 Epoch[073/300], Step[0050/1252], Avg Loss: 3.6710, Avg Acc: 0.3517
2022-01-15 09:13:21,997 Epoch[073/300], Step[0100/1252], Avg Loss: 3.6528, Avg Acc: 0.3803
2022-01-15 09:14:50,993 Epoch[073/300], Step[0150/1252], Avg Loss: 3.6522, Avg Acc: 0.3770
2022-01-15 09:16:19,444 Epoch[073/300], Step[0200/1252], Avg Loss: 3.6853, Avg Acc: 0.3731
2022-01-15 09:17:49,125 Epoch[073/300], Step[0250/1252], Avg Loss: 3.6833, Avg Acc: 0.3774
2022-01-15 09:19:18,144 Epoch[073/300], Step[0300/1252], Avg Loss: 3.6874, Avg Acc: 0.3793
2022-01-15 09:20:46,333 Epoch[073/300], Step[0350/1252], Avg Loss: 3.6790, Avg Acc: 0.3794
2022-01-15 09:22:15,612 Epoch[073/300], Step[0400/1252], Avg Loss: 3.6827, Avg Acc: 0.3811
2022-01-15 09:23:46,251 Epoch[073/300], Step[0450/1252], Avg Loss: 3.6804, Avg Acc: 0.3791
2022-01-15 09:25:16,327 Epoch[073/300], Step[0500/1252], Avg Loss: 3.6726, Avg Acc: 0.3779
2022-01-15 09:26:46,697 Epoch[073/300], Step[0550/1252], Avg Loss: 3.6718, Avg Acc: 0.3785
2022-01-15 09:28:17,864 Epoch[073/300], Step[0600/1252], Avg Loss: 3.6677, Avg Acc: 0.3776
2022-01-15 09:29:49,169 Epoch[073/300], Step[0650/1252], Avg Loss: 3.6715, Avg Acc: 0.3762
2022-01-15 09:31:20,033 Epoch[073/300], Step[0700/1252], Avg Loss: 3.6709, Avg Acc: 0.3754
2022-01-15 09:32:51,102 Epoch[073/300], Step[0750/1252], Avg Loss: 3.6737, Avg Acc: 0.3755
2022-01-15 09:34:22,052 Epoch[073/300], Step[0800/1252], Avg Loss: 3.6745, Avg Acc: 0.3760
2022-01-15 09:35:52,817 Epoch[073/300], Step[0850/1252], Avg Loss: 3.6721, Avg Acc: 0.3765
2022-01-15 09:37:23,708 Epoch[073/300], Step[0900/1252], Avg Loss: 3.6715, Avg Acc: 0.3754
2022-01-15 09:38:52,063 Epoch[073/300], Step[0950/1252], Avg Loss: 3.6712, Avg Acc: 0.3756
2022-01-15 09:40:22,165 Epoch[073/300], Step[1000/1252], Avg Loss: 3.6713, Avg Acc: 0.3745
2022-01-15 09:41:52,237 Epoch[073/300], Step[1050/1252], Avg Loss: 3.6734, Avg Acc: 0.3743
2022-01-15 09:43:24,351 Epoch[073/300], Step[1100/1252], Avg Loss: 3.6747, Avg Acc: 0.3737
2022-01-15 09:44:55,612 Epoch[073/300], Step[1150/1252], Avg Loss: 3.6766, Avg Acc: 0.3730
2022-01-15 09:46:26,977 Epoch[073/300], Step[1200/1252], Avg Loss: 3.6734, Avg Acc: 0.3727
2022-01-15 09:47:57,661 Epoch[073/300], Step[1250/1252], Avg Loss: 3.6721, Avg Acc: 0.3718
2022-01-15 09:48:04,913 ----- Epoch[073/300], Train Loss: 3.6721, Train Acc: 0.3718, time: 2372.20, Best Val(epoch72) Acc@1: 0.6840
2022-01-15 09:48:04,913 Now training epoch 74. LR=0.000912
2022-01-15 09:49:55,807 Epoch[074/300], Step[0000/1252], Avg Loss: 3.3057, Avg Acc: 0.4912
2022-01-15 09:51:25,413 Epoch[074/300], Step[0050/1252], Avg Loss: 3.6130, Avg Acc: 0.3799
2022-01-15 09:52:56,293 Epoch[074/300], Step[0100/1252], Avg Loss: 3.6426, Avg Acc: 0.3896
2022-01-15 09:54:28,169 Epoch[074/300], Step[0150/1252], Avg Loss: 3.6388, Avg Acc: 0.3778
2022-01-15 09:55:59,860 Epoch[074/300], Step[0200/1252], Avg Loss: 3.6417, Avg Acc: 0.3791
2022-01-15 09:57:30,670 Epoch[074/300], Step[0250/1252], Avg Loss: 3.6557, Avg Acc: 0.3782
2022-01-15 09:58:59,896 Epoch[074/300], Step[0300/1252], Avg Loss: 3.6588, Avg Acc: 0.3807
2022-01-15 10:00:30,446 Epoch[074/300], Step[0350/1252], Avg Loss: 3.6564, Avg Acc: 0.3790
2022-01-15 10:02:01,042 Epoch[074/300], Step[0400/1252], Avg Loss: 3.6696, Avg Acc: 0.3760
2022-01-15 10:03:31,695 Epoch[074/300], Step[0450/1252], Avg Loss: 3.6727, Avg Acc: 0.3758
2022-01-15 10:05:03,035 Epoch[074/300], Step[0500/1252], Avg Loss: 3.6699, Avg Acc: 0.3754
2022-01-15 10:06:33,560 Epoch[074/300], Step[0550/1252], Avg Loss: 3.6764, Avg Acc: 0.3749
2022-01-15 10:08:02,184 Epoch[074/300], Step[0600/1252], Avg Loss: 3.6749, Avg Acc: 0.3729
2022-01-15 10:09:33,857 Epoch[074/300], Step[0650/1252], Avg Loss: 3.6745, Avg Acc: 0.3732
2022-01-15 10:11:05,490 Epoch[074/300], Step[0700/1252], Avg Loss: 3.6684, Avg Acc: 0.3727
2022-01-15 10:12:33,882 Epoch[074/300], Step[0750/1252], Avg Loss: 3.6689, Avg Acc: 0.3720
2022-01-15 10:14:04,530 Epoch[074/300], Step[0800/1252], Avg Loss: 3.6659, Avg Acc: 0.3720
2022-01-15 10:15:36,361 Epoch[074/300], Step[0850/1252], Avg Loss: 3.6691, Avg Acc: 0.3712
2022-01-15 10:17:07,065 Epoch[074/300], Step[0900/1252], Avg Loss: 3.6717, Avg Acc: 0.3704
2022-01-15 10:18:38,050 Epoch[074/300], Step[0950/1252], Avg Loss: 3.6704, Avg Acc: 0.3705
2022-01-15 10:20:08,647 Epoch[074/300], Step[1000/1252], Avg Loss: 3.6719, Avg Acc: 0.3692
2022-01-15 10:21:39,436 Epoch[074/300], Step[1050/1252], Avg Loss: 3.6728, Avg Acc: 0.3694
2022-01-15 10:23:11,143 Epoch[074/300], Step[1100/1252], Avg Loss: 3.6726, Avg Acc: 0.3704
2022-01-15 10:24:41,615 Epoch[074/300], Step[1150/1252], Avg Loss: 3.6704, Avg Acc: 0.3709
2022-01-15 10:26:12,489 Epoch[074/300], Step[1200/1252], Avg Loss: 3.6700, Avg Acc: 0.3710
2022-01-15 10:27:43,832 Epoch[074/300], Step[1250/1252], Avg Loss: 3.6733, Avg Acc: 0.3705
2022-01-15 10:27:50,833 ----- Epoch[074/300], Train Loss: 3.6733, Train Acc: 0.3705, time: 2385.91, Best Val(epoch72) Acc@1: 0.6840
2022-01-15 10:27:50,834 ----- Validation after Epoch: 74
2022-01-15 10:29:09,952 Val Step[0000/1563], Avg Loss: 1.0827, Avg Acc@1: 0.6875, Avg Acc@5: 0.9688
2022-01-15 10:29:12,157 Val Step[0050/1563], Avg Loss: 1.4300, Avg Acc@1: 0.6893, Avg Acc@5: 0.8866
2022-01-15 10:29:14,560 Val Step[0100/1563], Avg Loss: 1.4486, Avg Acc@1: 0.6835, Avg Acc@5: 0.8864
2022-01-15 10:29:16,566 Val Step[0150/1563], Avg Loss: 1.4394, Avg Acc@1: 0.6844, Avg Acc@5: 0.8887
2022-01-15 10:29:18,575 Val Step[0200/1563], Avg Loss: 1.4396, Avg Acc@1: 0.6839, Avg Acc@5: 0.8876
2022-01-15 10:29:20,516 Val Step[0250/1563], Avg Loss: 1.4239, Avg Acc@1: 0.6886, Avg Acc@5: 0.8889
2022-01-15 10:29:22,673 Val Step[0300/1563], Avg Loss: 1.4242, Avg Acc@1: 0.6876, Avg Acc@5: 0.8893
2022-01-15 10:29:24,651 Val Step[0350/1563], Avg Loss: 1.4300, Avg Acc@1: 0.6855, Avg Acc@5: 0.8894
2022-01-15 10:29:26,847 Val Step[0400/1563], Avg Loss: 1.4298, Avg Acc@1: 0.6853, Avg Acc@5: 0.8896
2022-01-15 10:29:28,839 Val Step[0450/1563], Avg Loss: 1.4360, Avg Acc@1: 0.6842, Avg Acc@5: 0.8889
2022-01-15 10:29:30,848 Val Step[0500/1563], Avg Loss: 1.4404, Avg Acc@1: 0.6827, Avg Acc@5: 0.8885
2022-01-15 10:29:32,847 Val Step[0550/1563], Avg Loss: 1.4408, Avg Acc@1: 0.6826, Avg Acc@5: 0.8890
2022-01-15 10:29:34,810 Val Step[0600/1563], Avg Loss: 1.4397, Avg Acc@1: 0.6821, Avg Acc@5: 0.8894
2022-01-15 10:29:36,828 Val Step[0650/1563], Avg Loss: 1.4401, Avg Acc@1: 0.6825, Avg Acc@5: 0.8894
2022-01-15 10:29:38,746 Val Step[0700/1563], Avg Loss: 1.4382, Avg Acc@1: 0.6826, Avg Acc@5: 0.8900
2022-01-15 10:29:40,696 Val Step[0750/1563], Avg Loss: 1.4439, Avg Acc@1: 0.6811, Avg Acc@5: 0.8891
2022-01-15 10:29:42,666 Val Step[0800/1563], Avg Loss: 1.4420, Avg Acc@1: 0.6822, Avg Acc@5: 0.8896
2022-01-15 10:29:44,660 Val Step[0850/1563], Avg Loss: 1.4427, Avg Acc@1: 0.6820, Avg Acc@5: 0.8891
2022-01-15 10:29:46,682 Val Step[0900/1563], Avg Loss: 1.4382, Avg Acc@1: 0.6827, Avg Acc@5: 0.8895
2022-01-15 10:29:48,786 Val Step[0950/1563], Avg Loss: 1.4372, Avg Acc@1: 0.6830, Avg Acc@5: 0.8896
2022-01-15 10:29:50,828 Val Step[1000/1563], Avg Loss: 1.4374, Avg Acc@1: 0.6827, Avg Acc@5: 0.8894
2022-01-15 10:29:52,745 Val Step[1050/1563], Avg Loss: 1.4409, Avg Acc@1: 0.6819, Avg Acc@5: 0.8889
2022-01-15 10:29:54,668 Val Step[1100/1563], Avg Loss: 1.4408, Avg Acc@1: 0.6819, Avg Acc@5: 0.8890
2022-01-15 10:29:56,674 Val Step[1150/1563], Avg Loss: 1.4385, Avg Acc@1: 0.6824, Avg Acc@5: 0.8892
2022-01-15 10:29:58,841 Val Step[1200/1563], Avg Loss: 1.4371, Avg Acc@1: 0.6827, Avg Acc@5: 0.8895
2022-01-15 10:30:00,881 Val Step[1250/1563], Avg Loss: 1.4359, Avg Acc@1: 0.6827, Avg Acc@5: 0.8896
2022-01-15 10:30:02,969 Val Step[1300/1563], Avg Loss: 1.4392, Avg Acc@1: 0.6826, Avg Acc@5: 0.8891
2022-01-15 10:30:04,946 Val Step[1350/1563], Avg Loss: 1.4387, Avg Acc@1: 0.6827, Avg Acc@5: 0.8891
2022-01-15 10:30:07,121 Val Step[1400/1563], Avg Loss: 1.4388, Avg Acc@1: 0.6821, Avg Acc@5: 0.8890
2022-01-15 10:30:09,383 Val Step[1450/1563], Avg Loss: 1.4395, Avg Acc@1: 0.6819, Avg Acc@5: 0.8890
2022-01-15 10:30:11,662 Val Step[1500/1563], Avg Loss: 1.4391, Avg Acc@1: 0.6827, Avg Acc@5: 0.8889
2022-01-15 10:30:13,589 Val Step[1550/1563], Avg Loss: 1.4399, Avg Acc@1: 0.6827, Avg Acc@5: 0.8886
2022-01-15 10:30:15,392 ----- Epoch[074/300], Validation Loss: 1.4398, Validation Acc@1: 0.6828, Validation Acc@5: 0.8886, time: 144.55
2022-01-15 10:30:15,392 Now training epoch 75. LR=0.000909
2022-01-15 10:32:07,863 Epoch[075/300], Step[0000/1252], Avg Loss: 3.3996, Avg Acc: 0.4941
2022-01-15 10:33:37,866 Epoch[075/300], Step[0050/1252], Avg Loss: 3.6281, Avg Acc: 0.3631
2022-01-15 10:35:08,370 Epoch[075/300], Step[0100/1252], Avg Loss: 3.6615, Avg Acc: 0.3613
2022-01-15 10:36:39,668 Epoch[075/300], Step[0150/1252], Avg Loss: 3.6437, Avg Acc: 0.3648
2022-01-15 10:38:10,589 Epoch[075/300], Step[0200/1252], Avg Loss: 3.6526, Avg Acc: 0.3656
2022-01-15 10:39:40,734 Epoch[075/300], Step[0250/1252], Avg Loss: 3.6516, Avg Acc: 0.3641
2022-01-15 10:41:09,718 Epoch[075/300], Step[0300/1252], Avg Loss: 3.6632, Avg Acc: 0.3655
2022-01-15 10:42:41,989 Epoch[075/300], Step[0350/1252], Avg Loss: 3.6577, Avg Acc: 0.3651
2022-01-15 10:44:13,443 Epoch[075/300], Step[0400/1252], Avg Loss: 3.6512, Avg Acc: 0.3662
2022-01-15 10:45:45,205 Epoch[075/300], Step[0450/1252], Avg Loss: 3.6533, Avg Acc: 0.3656
2022-01-15 10:47:16,717 Epoch[075/300], Step[0500/1252], Avg Loss: 3.6592, Avg Acc: 0.3681
2022-01-15 10:48:47,014 Epoch[075/300], Step[0550/1252], Avg Loss: 3.6611, Avg Acc: 0.3702
2022-01-15 10:50:16,809 Epoch[075/300], Step[0600/1252], Avg Loss: 3.6574, Avg Acc: 0.3683
2022-01-15 10:51:46,849 Epoch[075/300], Step[0650/1252], Avg Loss: 3.6603, Avg Acc: 0.3672
2022-01-15 10:53:16,367 Epoch[075/300], Step[0700/1252], Avg Loss: 3.6588, Avg Acc: 0.3690
2022-01-15 10:54:44,293 Epoch[075/300], Step[0750/1252], Avg Loss: 3.6574, Avg Acc: 0.3702
2022-01-15 10:56:11,989 Epoch[075/300], Step[0800/1252], Avg Loss: 3.6582, Avg Acc: 0.3704
2022-01-15 10:57:38,815 Epoch[075/300], Step[0850/1252], Avg Loss: 3.6568, Avg Acc: 0.3715
2022-01-15 10:59:06,678 Epoch[075/300], Step[0900/1252], Avg Loss: 3.6528, Avg Acc: 0.3729
2022-01-15 11:00:38,030 Epoch[075/300], Step[0950/1252], Avg Loss: 3.6529, Avg Acc: 0.3723
2022-01-15 11:02:10,425 Epoch[075/300], Step[1000/1252], Avg Loss: 3.6534, Avg Acc: 0.3717
2022-01-15 11:03:42,073 Epoch[075/300], Step[1050/1252], Avg Loss: 3.6559, Avg Acc: 0.3700
2022-01-15 11:05:12,778 Epoch[075/300], Step[1100/1252], Avg Loss: 3.6590, Avg Acc: 0.3682
2022-01-15 11:06:43,675 Epoch[075/300], Step[1150/1252], Avg Loss: 3.6630, Avg Acc: 0.3679
2022-01-15 11:08:15,277 Epoch[075/300], Step[1200/1252], Avg Loss: 3.6645, Avg Acc: 0.3678
2022-01-15 11:09:46,784 Epoch[075/300], Step[1250/1252], Avg Loss: 3.6665, Avg Acc: 0.3673
2022-01-15 11:09:53,812 ----- Epoch[075/300], Train Loss: 3.6666, Train Acc: 0.3673, time: 2378.42, Best Val(epoch72) Acc@1: 0.6840
2022-01-15 11:09:53,812 Now training epoch 76. LR=0.000905
2022-01-15 11:11:47,551 Epoch[076/300], Step[0000/1252], Avg Loss: 3.1308, Avg Acc: 0.1172
2022-01-15 11:13:18,217 Epoch[076/300], Step[0050/1252], Avg Loss: 3.6767, Avg Acc: 0.3773
2022-01-15 11:14:48,893 Epoch[076/300], Step[0100/1252], Avg Loss: 3.6556, Avg Acc: 0.3643
2022-01-15 11:16:20,433 Epoch[076/300], Step[0150/1252], Avg Loss: 3.6424, Avg Acc: 0.3770
2022-01-15 11:17:51,217 Epoch[076/300], Step[0200/1252], Avg Loss: 3.6346, Avg Acc: 0.3771
2022-01-15 11:19:22,645 Epoch[076/300], Step[0250/1252], Avg Loss: 3.6318, Avg Acc: 0.3679
2022-01-15 11:20:54,480 Epoch[076/300], Step[0300/1252], Avg Loss: 3.6438, Avg Acc: 0.3667
2022-01-15 11:22:25,350 Epoch[076/300], Step[0350/1252], Avg Loss: 3.6376, Avg Acc: 0.3718
2022-01-15 11:23:58,087 Epoch[076/300], Step[0400/1252], Avg Loss: 3.6365, Avg Acc: 0.3750
2022-01-15 11:25:30,453 Epoch[076/300], Step[0450/1252], Avg Loss: 3.6412, Avg Acc: 0.3738
2022-01-15 11:27:01,310 Epoch[076/300], Step[0500/1252], Avg Loss: 3.6472, Avg Acc: 0.3725
2022-01-15 11:28:33,323 Epoch[076/300], Step[0550/1252], Avg Loss: 3.6499, Avg Acc: 0.3728
2022-01-15 11:30:04,736 Epoch[076/300], Step[0600/1252], Avg Loss: 3.6487, Avg Acc: 0.3736
2022-01-15 11:31:36,432 Epoch[076/300], Step[0650/1252], Avg Loss: 3.6459, Avg Acc: 0.3727
2022-01-15 11:33:08,123 Epoch[076/300], Step[0700/1252], Avg Loss: 3.6486, Avg Acc: 0.3736
2022-01-15 11:34:40,067 Epoch[076/300], Step[0750/1252], Avg Loss: 3.6459, Avg Acc: 0.3737
2022-01-15 11:36:10,557 Epoch[076/300], Step[0800/1252], Avg Loss: 3.6447, Avg Acc: 0.3720
2022-01-15 11:37:41,528 Epoch[076/300], Step[0850/1252], Avg Loss: 3.6449, Avg Acc: 0.3724
2022-01-15 11:39:12,391 Epoch[076/300], Step[0900/1252], Avg Loss: 3.6479, Avg Acc: 0.3699
2022-01-15 11:40:43,599 Epoch[076/300], Step[0950/1252], Avg Loss: 3.6499, Avg Acc: 0.3700
2022-01-15 11:42:15,545 Epoch[076/300], Step[1000/1252], Avg Loss: 3.6471, Avg Acc: 0.3701
2022-01-15 11:43:45,554 Epoch[076/300], Step[1050/1252], Avg Loss: 3.6507, Avg Acc: 0.3681
2022-01-15 11:45:17,603 Epoch[076/300], Step[1100/1252], Avg Loss: 3.6492, Avg Acc: 0.3705
2022-01-15 11:46:46,998 Epoch[076/300], Step[1150/1252], Avg Loss: 3.6508, Avg Acc: 0.3715
2022-01-15 11:48:17,977 Epoch[076/300], Step[1200/1252], Avg Loss: 3.6541, Avg Acc: 0.3703
2022-01-15 11:49:49,917 Epoch[076/300], Step[1250/1252], Avg Loss: 3.6570, Avg Acc: 0.3705
2022-01-15 11:49:56,612 ----- Epoch[076/300], Train Loss: 3.6570, Train Acc: 0.3704, time: 2402.80, Best Val(epoch72) Acc@1: 0.6840
2022-01-15 11:49:56,612 ----- Validation after Epoch: 76
2022-01-15 11:51:19,769 Val Step[0000/1563], Avg Loss: 1.4521, Avg Acc@1: 0.6875, Avg Acc@5: 0.9062
2022-01-15 11:51:21,922 Val Step[0050/1563], Avg Loss: 1.4660, Avg Acc@1: 0.6844, Avg Acc@5: 0.8897
2022-01-15 11:51:24,117 Val Step[0100/1563], Avg Loss: 1.4899, Avg Acc@1: 0.6785, Avg Acc@5: 0.8908
2022-01-15 11:51:26,355 Val Step[0150/1563], Avg Loss: 1.4935, Avg Acc@1: 0.6794, Avg Acc@5: 0.8901
2022-01-15 11:51:28,676 Val Step[0200/1563], Avg Loss: 1.4947, Avg Acc@1: 0.6811, Avg Acc@5: 0.8895
2022-01-15 11:51:30,920 Val Step[0250/1563], Avg Loss: 1.4797, Avg Acc@1: 0.6846, Avg Acc@5: 0.8903
2022-01-15 11:51:33,082 Val Step[0300/1563], Avg Loss: 1.4745, Avg Acc@1: 0.6873, Avg Acc@5: 0.8913
2022-01-15 11:51:35,223 Val Step[0350/1563], Avg Loss: 1.4788, Avg Acc@1: 0.6871, Avg Acc@5: 0.8900
2022-01-15 11:51:37,364 Val Step[0400/1563], Avg Loss: 1.4744, Avg Acc@1: 0.6884, Avg Acc@5: 0.8907
2022-01-15 11:51:39,327 Val Step[0450/1563], Avg Loss: 1.4812, Avg Acc@1: 0.6856, Avg Acc@5: 0.8895
2022-01-15 11:51:41,216 Val Step[0500/1563], Avg Loss: 1.4825, Avg Acc@1: 0.6847, Avg Acc@5: 0.8898
2022-01-15 11:51:43,162 Val Step[0550/1563], Avg Loss: 1.4834, Avg Acc@1: 0.6841, Avg Acc@5: 0.8903
2022-01-15 11:51:45,131 Val Step[0600/1563], Avg Loss: 1.4830, Avg Acc@1: 0.6837, Avg Acc@5: 0.8908
2022-01-15 11:51:47,084 Val Step[0650/1563], Avg Loss: 1.4836, Avg Acc@1: 0.6839, Avg Acc@5: 0.8913
2022-01-15 11:51:49,048 Val Step[0700/1563], Avg Loss: 1.4799, Avg Acc@1: 0.6839, Avg Acc@5: 0.8919
2022-01-15 11:51:51,027 Val Step[0750/1563], Avg Loss: 1.4873, Avg Acc@1: 0.6824, Avg Acc@5: 0.8909
2022-01-15 11:51:53,070 Val Step[0800/1563], Avg Loss: 1.4864, Avg Acc@1: 0.6827, Avg Acc@5: 0.8912
2022-01-15 11:51:55,001 Val Step[0850/1563], Avg Loss: 1.4884, Avg Acc@1: 0.6822, Avg Acc@5: 0.8908
2022-01-15 11:51:56,921 Val Step[0900/1563], Avg Loss: 1.4849, Avg Acc@1: 0.6831, Avg Acc@5: 0.8914
2022-01-15 11:51:58,877 Val Step[0950/1563], Avg Loss: 1.4846, Avg Acc@1: 0.6828, Avg Acc@5: 0.8918
2022-01-15 11:52:00,888 Val Step[1000/1563], Avg Loss: 1.4848, Avg Acc@1: 0.6823, Avg Acc@5: 0.8917
2022-01-15 11:52:02,854 Val Step[1050/1563], Avg Loss: 1.4879, Avg Acc@1: 0.6815, Avg Acc@5: 0.8912
2022-01-15 11:52:04,842 Val Step[1100/1563], Avg Loss: 1.4867, Avg Acc@1: 0.6817, Avg Acc@5: 0.8913
2022-01-15 11:52:06,764 Val Step[1150/1563], Avg Loss: 1.4845, Avg Acc@1: 0.6821, Avg Acc@5: 0.8913
2022-01-15 11:52:08,718 Val Step[1200/1563], Avg Loss: 1.4844, Avg Acc@1: 0.6823, Avg Acc@5: 0.8913
2022-01-15 11:52:10,686 Val Step[1250/1563], Avg Loss: 1.4831, Avg Acc@1: 0.6825, Avg Acc@5: 0.8914
2022-01-15 11:52:12,606 Val Step[1300/1563], Avg Loss: 1.4859, Avg Acc@1: 0.6821, Avg Acc@5: 0.8911
2022-01-15 11:52:14,681 Val Step[1350/1563], Avg Loss: 1.4849, Avg Acc@1: 0.6827, Avg Acc@5: 0.8911
2022-01-15 11:52:16,816 Val Step[1400/1563], Avg Loss: 1.4848, Avg Acc@1: 0.6819, Avg Acc@5: 0.8909
2022-01-15 11:52:18,983 Val Step[1450/1563], Avg Loss: 1.4852, Avg Acc@1: 0.6820, Avg Acc@5: 0.8907
2022-01-15 11:52:21,178 Val Step[1500/1563], Avg Loss: 1.4843, Avg Acc@1: 0.6825, Avg Acc@5: 0.8911
2022-01-15 11:52:23,387 Val Step[1550/1563], Avg Loss: 1.4850, Avg Acc@1: 0.6824, Avg Acc@5: 0.8908
2022-01-15 11:52:25,321 ----- Epoch[076/300], Validation Loss: 1.4857, Validation Acc@1: 0.6822, Validation Acc@5: 0.8907, time: 148.70
2022-01-15 11:52:25,321 Now training epoch 77. LR=0.000902
2022-01-15 11:54:22,809 Epoch[077/300], Step[0000/1252], Avg Loss: 3.7593, Avg Acc: 0.3740
2022-01-15 11:55:52,849 Epoch[077/300], Step[0050/1252], Avg Loss: 3.6407, Avg Acc: 0.3623
2022-01-15 11:57:22,635 Epoch[077/300], Step[0100/1252], Avg Loss: 3.6808, Avg Acc: 0.3609
2022-01-15 11:58:54,765 Epoch[077/300], Step[0150/1252], Avg Loss: 3.6849, Avg Acc: 0.3650
2022-01-15 12:00:23,822 Epoch[077/300], Step[0200/1252], Avg Loss: 3.6823, Avg Acc: 0.3675
2022-01-15 12:01:55,097 Epoch[077/300], Step[0250/1252], Avg Loss: 3.6832, Avg Acc: 0.3658
2022-01-15 12:03:27,187 Epoch[077/300], Step[0300/1252], Avg Loss: 3.6779, Avg Acc: 0.3652
2022-01-15 12:04:57,715 Epoch[077/300], Step[0350/1252], Avg Loss: 3.6789, Avg Acc: 0.3655
2022-01-15 12:06:28,811 Epoch[077/300], Step[0400/1252], Avg Loss: 3.6820, Avg Acc: 0.3683
2022-01-15 12:07:59,737 Epoch[077/300], Step[0450/1252], Avg Loss: 3.6860, Avg Acc: 0.3701
2022-01-15 12:09:31,609 Epoch[077/300], Step[0500/1252], Avg Loss: 3.6799, Avg Acc: 0.3683
2022-01-15 12:11:02,909 Epoch[077/300], Step[0550/1252], Avg Loss: 3.6710, Avg Acc: 0.3696
2022-01-15 12:12:33,137 Epoch[077/300], Step[0600/1252], Avg Loss: 3.6686, Avg Acc: 0.3696
2022-01-15 12:14:03,792 Epoch[077/300], Step[0650/1252], Avg Loss: 3.6664, Avg Acc: 0.3690
2022-01-15 12:15:33,617 Epoch[077/300], Step[0700/1252], Avg Loss: 3.6664, Avg Acc: 0.3672
2022-01-15 12:17:04,373 Epoch[077/300], Step[0750/1252], Avg Loss: 3.6655, Avg Acc: 0.3669
2022-01-15 12:18:34,309 Epoch[077/300], Step[0800/1252], Avg Loss: 3.6715, Avg Acc: 0.3666
2022-01-15 12:20:04,140 Epoch[077/300], Step[0850/1252], Avg Loss: 3.6754, Avg Acc: 0.3661
2022-01-15 12:21:34,130 Epoch[077/300], Step[0900/1252], Avg Loss: 3.6732, Avg Acc: 0.3667
2022-01-15 12:23:05,847 Epoch[077/300], Step[0950/1252], Avg Loss: 3.6711, Avg Acc: 0.3668
2022-01-15 12:24:37,147 Epoch[077/300], Step[1000/1252], Avg Loss: 3.6698, Avg Acc: 0.3673
2022-01-15 12:26:06,722 Epoch[077/300], Step[1050/1252], Avg Loss: 3.6687, Avg Acc: 0.3673
2022-01-15 12:27:36,394 Epoch[077/300], Step[1100/1252], Avg Loss: 3.6733, Avg Acc: 0.3664
2022-01-15 12:29:05,226 Epoch[077/300], Step[1150/1252], Avg Loss: 3.6747, Avg Acc: 0.3670
2022-01-15 12:30:35,419 Epoch[077/300], Step[1200/1252], Avg Loss: 3.6737, Avg Acc: 0.3669
2022-01-15 12:32:06,013 Epoch[077/300], Step[1250/1252], Avg Loss: 3.6764, Avg Acc: 0.3670
2022-01-15 12:32:12,328 ----- Epoch[077/300], Train Loss: 3.6763, Train Acc: 0.3670, time: 2387.00, Best Val(epoch72) Acc@1: 0.6840
2022-01-15 12:32:12,329 Now training epoch 78. LR=0.000899
2022-01-15 12:34:02,403 Epoch[078/300], Step[0000/1252], Avg Loss: 4.1274, Avg Acc: 0.3896
2022-01-15 12:35:31,253 Epoch[078/300], Step[0050/1252], Avg Loss: 3.6882, Avg Acc: 0.3753
2022-01-15 12:37:00,059 Epoch[078/300], Step[0100/1252], Avg Loss: 3.6515, Avg Acc: 0.3779
2022-01-15 12:38:29,217 Epoch[078/300], Step[0150/1252], Avg Loss: 3.6521, Avg Acc: 0.3673
2022-01-15 12:39:59,952 Epoch[078/300], Step[0200/1252], Avg Loss: 3.6482, Avg Acc: 0.3664
2022-01-15 12:41:29,930 Epoch[078/300], Step[0250/1252], Avg Loss: 3.6359, Avg Acc: 0.3691
2022-01-15 12:43:01,714 Epoch[078/300], Step[0300/1252], Avg Loss: 3.6361, Avg Acc: 0.3699
2022-01-15 12:44:32,384 Epoch[078/300], Step[0350/1252], Avg Loss: 3.6350, Avg Acc: 0.3718
2022-01-15 12:46:03,418 Epoch[078/300], Step[0400/1252], Avg Loss: 3.6467, Avg Acc: 0.3697
2022-01-15 12:47:33,973 Epoch[078/300], Step[0450/1252], Avg Loss: 3.6509, Avg Acc: 0.3675
2022-01-15 12:49:05,889 Epoch[078/300], Step[0500/1252], Avg Loss: 3.6564, Avg Acc: 0.3641
2022-01-15 12:50:35,545 Epoch[078/300], Step[0550/1252], Avg Loss: 3.6561, Avg Acc: 0.3659
2022-01-15 12:52:06,804 Epoch[078/300], Step[0600/1252], Avg Loss: 3.6577, Avg Acc: 0.3642
2022-01-15 12:53:37,936 Epoch[078/300], Step[0650/1252], Avg Loss: 3.6563, Avg Acc: 0.3631
2022-01-15 12:55:08,781 Epoch[078/300], Step[0700/1252], Avg Loss: 3.6564, Avg Acc: 0.3638
2022-01-15 12:56:40,318 Epoch[078/300], Step[0750/1252], Avg Loss: 3.6551, Avg Acc: 0.3652
2022-01-15 12:58:10,809 Epoch[078/300], Step[0800/1252], Avg Loss: 3.6584, Avg Acc: 0.3662
2022-01-15 12:59:42,437 Epoch[078/300], Step[0850/1252], Avg Loss: 3.6601, Avg Acc: 0.3654
2022-01-15 13:01:14,021 Epoch[078/300], Step[0900/1252], Avg Loss: 3.6657, Avg Acc: 0.3652
2022-01-15 13:02:45,889 Epoch[078/300], Step[0950/1252], Avg Loss: 3.6647, Avg Acc: 0.3639
2022-01-15 13:04:17,053 Epoch[078/300], Step[1000/1252], Avg Loss: 3.6658, Avg Acc: 0.3632
2022-01-15 13:05:49,248 Epoch[078/300], Step[1050/1252], Avg Loss: 3.6668, Avg Acc: 0.3629
2022-01-15 13:07:21,231 Epoch[078/300], Step[1100/1252], Avg Loss: 3.6679, Avg Acc: 0.3629
2022-01-15 13:08:52,663 Epoch[078/300], Step[1150/1252], Avg Loss: 3.6671, Avg Acc: 0.3638
2022-01-15 13:10:23,054 Epoch[078/300], Step[1200/1252], Avg Loss: 3.6639, Avg Acc: 0.3651
2022-01-15 13:11:54,428 Epoch[078/300], Step[1250/1252], Avg Loss: 3.6636, Avg Acc: 0.3660
2022-01-15 13:12:01,413 ----- Epoch[078/300], Train Loss: 3.6635, Train Acc: 0.3660, time: 2389.08, Best Val(epoch72) Acc@1: 0.6840
2022-01-15 13:12:01,413 ----- Validation after Epoch: 78
2022-01-15 13:13:17,573 Val Step[0000/1563], Avg Loss: 1.1630, Avg Acc@1: 0.6875, Avg Acc@5: 0.9062
2022-01-15 13:13:19,593 Val Step[0050/1563], Avg Loss: 1.3884, Avg Acc@1: 0.6924, Avg Acc@5: 0.8897
2022-01-15 13:13:21,919 Val Step[0100/1563], Avg Loss: 1.3992, Avg Acc@1: 0.6881, Avg Acc@5: 0.8939
2022-01-15 13:13:24,109 Val Step[0150/1563], Avg Loss: 1.4001, Avg Acc@1: 0.6887, Avg Acc@5: 0.8922
2022-01-15 13:13:26,262 Val Step[0200/1563], Avg Loss: 1.4037, Avg Acc@1: 0.6889, Avg Acc@5: 0.8899
2022-01-15 13:13:28,475 Val Step[0250/1563], Avg Loss: 1.3936, Avg Acc@1: 0.6904, Avg Acc@5: 0.8891
2022-01-15 13:13:30,573 Val Step[0300/1563], Avg Loss: 1.3937, Avg Acc@1: 0.6911, Avg Acc@5: 0.8894
2022-01-15 13:13:32,538 Val Step[0350/1563], Avg Loss: 1.4007, Avg Acc@1: 0.6905, Avg Acc@5: 0.8884
2022-01-15 13:13:34,556 Val Step[0400/1563], Avg Loss: 1.3987, Avg Acc@1: 0.6908, Avg Acc@5: 0.8890
2022-01-15 13:13:36,450 Val Step[0450/1563], Avg Loss: 1.4029, Avg Acc@1: 0.6892, Avg Acc@5: 0.8880
2022-01-15 13:13:38,492 Val Step[0500/1563], Avg Loss: 1.4041, Avg Acc@1: 0.6881, Avg Acc@5: 0.8887
2022-01-15 13:13:40,547 Val Step[0550/1563], Avg Loss: 1.4015, Avg Acc@1: 0.6881, Avg Acc@5: 0.8893
2022-01-15 13:13:42,548 Val Step[0600/1563], Avg Loss: 1.4009, Avg Acc@1: 0.6881, Avg Acc@5: 0.8897
2022-01-15 13:13:44,582 Val Step[0650/1563], Avg Loss: 1.4019, Avg Acc@1: 0.6885, Avg Acc@5: 0.8897
2022-01-15 13:13:46,531 Val Step[0700/1563], Avg Loss: 1.4026, Avg Acc@1: 0.6880, Avg Acc@5: 0.8905
2022-01-15 13:13:48,455 Val Step[0750/1563], Avg Loss: 1.4086, Avg Acc@1: 0.6864, Avg Acc@5: 0.8899
2022-01-15 13:13:50,483 Val Step[0800/1563], Avg Loss: 1.4073, Avg Acc@1: 0.6869, Avg Acc@5: 0.8902
2022-01-15 13:13:52,393 Val Step[0850/1563], Avg Loss: 1.4089, Avg Acc@1: 0.6865, Avg Acc@5: 0.8898
2022-01-15 13:13:54,370 Val Step[0900/1563], Avg Loss: 1.4064, Avg Acc@1: 0.6869, Avg Acc@5: 0.8903
2022-01-15 13:13:56,302 Val Step[0950/1563], Avg Loss: 1.4058, Avg Acc@1: 0.6869, Avg Acc@5: 0.8903
2022-01-15 13:13:58,385 Val Step[1000/1563], Avg Loss: 1.4066, Avg Acc@1: 0.6862, Avg Acc@5: 0.8905
2022-01-15 13:14:00,350 Val Step[1050/1563], Avg Loss: 1.4102, Avg Acc@1: 0.6855, Avg Acc@5: 0.8896
2022-01-15 13:14:02,346 Val Step[1100/1563], Avg Loss: 1.4097, Avg Acc@1: 0.6853, Avg Acc@5: 0.8894
2022-01-15 13:14:04,296 Val Step[1150/1563], Avg Loss: 1.4081, Avg Acc@1: 0.6855, Avg Acc@5: 0.8896
2022-01-15 13:14:06,266 Val Step[1200/1563], Avg Loss: 1.4074, Avg Acc@1: 0.6858, Avg Acc@5: 0.8898
2022-01-15 13:14:08,161 Val Step[1250/1563], Avg Loss: 1.4067, Avg Acc@1: 0.6857, Avg Acc@5: 0.8901
2022-01-15 13:14:10,182 Val Step[1300/1563], Avg Loss: 1.4100, Avg Acc@1: 0.6851, Avg Acc@5: 0.8898
2022-01-15 13:14:12,122 Val Step[1350/1563], Avg Loss: 1.4106, Avg Acc@1: 0.6850, Avg Acc@5: 0.8896
2022-01-15 13:14:14,138 Val Step[1400/1563], Avg Loss: 1.4102, Avg Acc@1: 0.6847, Avg Acc@5: 0.8897
2022-01-15 13:14:16,038 Val Step[1450/1563], Avg Loss: 1.4100, Avg Acc@1: 0.6846, Avg Acc@5: 0.8897
2022-01-15 13:14:18,086 Val Step[1500/1563], Avg Loss: 1.4093, Avg Acc@1: 0.6851, Avg Acc@5: 0.8899
2022-01-15 13:14:20,003 Val Step[1550/1563], Avg Loss: 1.4105, Avg Acc@1: 0.6848, Avg Acc@5: 0.8898
2022-01-15 13:14:22,044 ----- Epoch[078/300], Validation Loss: 1.4108, Validation Acc@1: 0.6846, Validation Acc@5: 0.8899, time: 140.63
2022-01-15 13:14:23,321 the pre best model acc:0.6840, at epoch 72
2022-01-15 13:14:23,495 current best model acc:0.6846, at epoch 78
2022-01-15 13:14:23,496 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-15 13:14:23,496 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-15 13:14:23,496 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-15 13:14:23,496 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-15 13:14:23,496 Now training epoch 79. LR=0.000895
2022-01-15 13:16:18,124 Epoch[079/300], Step[0000/1252], Avg Loss: 4.1121, Avg Acc: 0.3545
2022-01-15 13:17:49,478 Epoch[079/300], Step[0050/1252], Avg Loss: 3.7151, Avg Acc: 0.3798
2022-01-15 13:19:19,694 Epoch[079/300], Step[0100/1252], Avg Loss: 3.6890, Avg Acc: 0.3866
2022-01-15 13:20:50,661 Epoch[079/300], Step[0150/1252], Avg Loss: 3.6706, Avg Acc: 0.3815
2022-01-15 13:22:21,491 Epoch[079/300], Step[0200/1252], Avg Loss: 3.6613, Avg Acc: 0.3740
2022-01-15 13:23:52,857 Epoch[079/300], Step[0250/1252], Avg Loss: 3.6583, Avg Acc: 0.3747
2022-01-15 13:25:23,754 Epoch[079/300], Step[0300/1252], Avg Loss: 3.6658, Avg Acc: 0.3748
2022-01-15 13:26:55,303 Epoch[079/300], Step[0350/1252], Avg Loss: 3.6588, Avg Acc: 0.3747
2022-01-15 13:28:26,204 Epoch[079/300], Step[0400/1252], Avg Loss: 3.6584, Avg Acc: 0.3723
2022-01-15 13:29:58,343 Epoch[079/300], Step[0450/1252], Avg Loss: 3.6513, Avg Acc: 0.3726
2022-01-15 13:31:30,320 Epoch[079/300], Step[0500/1252], Avg Loss: 3.6604, Avg Acc: 0.3709
2022-01-15 13:33:02,007 Epoch[079/300], Step[0550/1252], Avg Loss: 3.6613, Avg Acc: 0.3703
2022-01-15 13:34:33,704 Epoch[079/300], Step[0600/1252], Avg Loss: 3.6604, Avg Acc: 0.3689
2022-01-15 13:36:05,174 Epoch[079/300], Step[0650/1252], Avg Loss: 3.6633, Avg Acc: 0.3682
2022-01-15 13:37:35,917 Epoch[079/300], Step[0700/1252], Avg Loss: 3.6640, Avg Acc: 0.3698
2022-01-15 13:39:06,325 Epoch[079/300], Step[0750/1252], Avg Loss: 3.6654, Avg Acc: 0.3680
2022-01-15 13:40:36,908 Epoch[079/300], Step[0800/1252], Avg Loss: 3.6678, Avg Acc: 0.3659
2022-01-15 13:42:07,135 Epoch[079/300], Step[0850/1252], Avg Loss: 3.6664, Avg Acc: 0.3674
2022-01-15 13:43:39,374 Epoch[079/300], Step[0900/1252], Avg Loss: 3.6645, Avg Acc: 0.3682
2022-01-15 13:45:10,267 Epoch[079/300], Step[0950/1252], Avg Loss: 3.6662, Avg Acc: 0.3686
2022-01-15 13:46:40,315 Epoch[079/300], Step[1000/1252], Avg Loss: 3.6675, Avg Acc: 0.3691
2022-01-15 13:48:12,057 Epoch[079/300], Step[1050/1252], Avg Loss: 3.6664, Avg Acc: 0.3687
2022-01-15 13:49:43,124 Epoch[079/300], Step[1100/1252], Avg Loss: 3.6655, Avg Acc: 0.3696
2022-01-15 13:51:14,153 Epoch[079/300], Step[1150/1252], Avg Loss: 3.6637, Avg Acc: 0.3703
2022-01-15 13:52:44,774 Epoch[079/300], Step[1200/1252], Avg Loss: 3.6654, Avg Acc: 0.3704
2022-01-15 13:54:17,172 Epoch[079/300], Step[1250/1252], Avg Loss: 3.6629, Avg Acc: 0.3707
2022-01-15 13:54:24,280 ----- Epoch[079/300], Train Loss: 3.6629, Train Acc: 0.3707, time: 2400.78, Best Val(epoch78) Acc@1: 0.6846
2022-01-15 13:54:24,281 Now training epoch 80. LR=0.000892
2022-01-15 13:56:15,266 Epoch[080/300], Step[0000/1252], Avg Loss: 3.9301, Avg Acc: 0.3643
2022-01-15 13:57:45,086 Epoch[080/300], Step[0050/1252], Avg Loss: 3.6974, Avg Acc: 0.3662
2022-01-15 13:59:13,929 Epoch[080/300], Step[0100/1252], Avg Loss: 3.6696, Avg Acc: 0.3571
2022-01-15 14:00:43,759 Epoch[080/300], Step[0150/1252], Avg Loss: 3.6596, Avg Acc: 0.3573
2022-01-15 14:02:13,859 Epoch[080/300], Step[0200/1252], Avg Loss: 3.6549, Avg Acc: 0.3649
2022-01-15 14:03:43,225 Epoch[080/300], Step[0250/1252], Avg Loss: 3.6479, Avg Acc: 0.3667
2022-01-15 14:05:13,827 Epoch[080/300], Step[0300/1252], Avg Loss: 3.6480, Avg Acc: 0.3656
2022-01-15 14:06:43,596 Epoch[080/300], Step[0350/1252], Avg Loss: 3.6439, Avg Acc: 0.3684
2022-01-15 14:08:12,942 Epoch[080/300], Step[0400/1252], Avg Loss: 3.6368, Avg Acc: 0.3693
2022-01-15 14:09:42,943 Epoch[080/300], Step[0450/1252], Avg Loss: 3.6385, Avg Acc: 0.3685
2022-01-15 14:11:13,433 Epoch[080/300], Step[0500/1252], Avg Loss: 3.6357, Avg Acc: 0.3703
2022-01-15 14:12:44,082 Epoch[080/300], Step[0550/1252], Avg Loss: 3.6390, Avg Acc: 0.3723
2022-01-15 14:14:14,346 Epoch[080/300], Step[0600/1252], Avg Loss: 3.6469, Avg Acc: 0.3720
2022-01-15 14:15:45,951 Epoch[080/300], Step[0650/1252], Avg Loss: 3.6425, Avg Acc: 0.3721
2022-01-15 14:17:17,338 Epoch[080/300], Step[0700/1252], Avg Loss: 3.6433, Avg Acc: 0.3707
2022-01-15 14:18:49,293 Epoch[080/300], Step[0750/1252], Avg Loss: 3.6451, Avg Acc: 0.3714
2022-01-15 14:20:18,582 Epoch[080/300], Step[0800/1252], Avg Loss: 3.6450, Avg Acc: 0.3712
2022-01-15 14:21:49,904 Epoch[080/300], Step[0850/1252], Avg Loss: 3.6413, Avg Acc: 0.3717
2022-01-15 14:23:21,291 Epoch[080/300], Step[0900/1252], Avg Loss: 3.6454, Avg Acc: 0.3712
2022-01-15 14:24:51,998 Epoch[080/300], Step[0950/1252], Avg Loss: 3.6476, Avg Acc: 0.3711
2022-01-15 14:26:24,340 Epoch[080/300], Step[1000/1252], Avg Loss: 3.6497, Avg Acc: 0.3713
2022-01-15 14:27:55,067 Epoch[080/300], Step[1050/1252], Avg Loss: 3.6504, Avg Acc: 0.3712
2022-01-15 14:29:25,980 Epoch[080/300], Step[1100/1252], Avg Loss: 3.6493, Avg Acc: 0.3705
2022-01-15 14:30:55,849 Epoch[080/300], Step[1150/1252], Avg Loss: 3.6520, Avg Acc: 0.3697
2022-01-15 14:32:27,964 Epoch[080/300], Step[1200/1252], Avg Loss: 3.6507, Avg Acc: 0.3713
2022-01-15 14:33:58,614 Epoch[080/300], Step[1250/1252], Avg Loss: 3.6489, Avg Acc: 0.3713
2022-01-15 14:34:05,701 ----- Epoch[080/300], Train Loss: 3.6489, Train Acc: 0.3713, time: 2381.42, Best Val(epoch78) Acc@1: 0.6846
2022-01-15 14:34:05,701 ----- Validation after Epoch: 80
2022-01-15 14:35:30,881 Val Step[0000/1563], Avg Loss: 1.1670, Avg Acc@1: 0.7500, Avg Acc@5: 0.9375
2022-01-15 14:35:33,118 Val Step[0050/1563], Avg Loss: 1.4444, Avg Acc@1: 0.6942, Avg Acc@5: 0.8848
2022-01-15 14:35:35,496 Val Step[0100/1563], Avg Loss: 1.4645, Avg Acc@1: 0.6825, Avg Acc@5: 0.8855
2022-01-15 14:35:37,613 Val Step[0150/1563], Avg Loss: 1.4697, Avg Acc@1: 0.6819, Avg Acc@5: 0.8853
2022-01-15 14:35:39,721 Val Step[0200/1563], Avg Loss: 1.4683, Avg Acc@1: 0.6835, Avg Acc@5: 0.8851
2022-01-15 14:35:41,852 Val Step[0250/1563], Avg Loss: 1.4510, Avg Acc@1: 0.6869, Avg Acc@5: 0.8857
2022-01-15 14:35:44,005 Val Step[0300/1563], Avg Loss: 1.4556, Avg Acc@1: 0.6864, Avg Acc@5: 0.8852
2022-01-15 14:35:46,202 Val Step[0350/1563], Avg Loss: 1.4583, Avg Acc@1: 0.6868, Avg Acc@5: 0.8856
2022-01-15 14:35:48,394 Val Step[0400/1563], Avg Loss: 1.4574, Avg Acc@1: 0.6866, Avg Acc@5: 0.8861
2022-01-15 14:35:50,487 Val Step[0450/1563], Avg Loss: 1.4625, Avg Acc@1: 0.6838, Avg Acc@5: 0.8857
2022-01-15 14:35:52,660 Val Step[0500/1563], Avg Loss: 1.4657, Avg Acc@1: 0.6819, Avg Acc@5: 0.8852
2022-01-15 14:35:54,805 Val Step[0550/1563], Avg Loss: 1.4645, Avg Acc@1: 0.6819, Avg Acc@5: 0.8860
2022-01-15 14:35:56,911 Val Step[0600/1563], Avg Loss: 1.4633, Avg Acc@1: 0.6812, Avg Acc@5: 0.8858
2022-01-15 14:35:59,007 Val Step[0650/1563], Avg Loss: 1.4636, Avg Acc@1: 0.6821, Avg Acc@5: 0.8859
2022-01-15 14:36:01,186 Val Step[0700/1563], Avg Loss: 1.4625, Avg Acc@1: 0.6822, Avg Acc@5: 0.8862
2022-01-15 14:36:03,261 Val Step[0750/1563], Avg Loss: 1.4694, Avg Acc@1: 0.6800, Avg Acc@5: 0.8856
2022-01-15 14:36:05,285 Val Step[0800/1563], Avg Loss: 1.4672, Avg Acc@1: 0.6804, Avg Acc@5: 0.8863
2022-01-15 14:36:07,412 Val Step[0850/1563], Avg Loss: 1.4694, Avg Acc@1: 0.6799, Avg Acc@5: 0.8855
2022-01-15 14:36:09,414 Val Step[0900/1563], Avg Loss: 1.4658, Avg Acc@1: 0.6806, Avg Acc@5: 0.8860
2022-01-15 14:36:11,426 Val Step[0950/1563], Avg Loss: 1.4656, Avg Acc@1: 0.6805, Avg Acc@5: 0.8864
2022-01-15 14:36:13,466 Val Step[1000/1563], Avg Loss: 1.4639, Avg Acc@1: 0.6806, Avg Acc@5: 0.8864
2022-01-15 14:36:15,446 Val Step[1050/1563], Avg Loss: 1.4662, Avg Acc@1: 0.6799, Avg Acc@5: 0.8859
2022-01-15 14:36:17,437 Val Step[1100/1563], Avg Loss: 1.4662, Avg Acc@1: 0.6794, Avg Acc@5: 0.8861
2022-01-15 14:36:19,375 Val Step[1150/1563], Avg Loss: 1.4645, Avg Acc@1: 0.6800, Avg Acc@5: 0.8862
2022-01-15 14:36:21,541 Val Step[1200/1563], Avg Loss: 1.4646, Avg Acc@1: 0.6801, Avg Acc@5: 0.8863
2022-01-15 14:36:23,698 Val Step[1250/1563], Avg Loss: 1.4619, Avg Acc@1: 0.6804, Avg Acc@5: 0.8869
2022-01-15 14:36:25,906 Val Step[1300/1563], Avg Loss: 1.4650, Avg Acc@1: 0.6800, Avg Acc@5: 0.8866
2022-01-15 14:36:28,015 Val Step[1350/1563], Avg Loss: 1.4652, Avg Acc@1: 0.6797, Avg Acc@5: 0.8867
2022-01-15 14:36:30,041 Val Step[1400/1563], Avg Loss: 1.4653, Avg Acc@1: 0.6794, Avg Acc@5: 0.8865
2022-01-15 14:36:32,123 Val Step[1450/1563], Avg Loss: 1.4653, Avg Acc@1: 0.6798, Avg Acc@5: 0.8865
2022-01-15 14:36:34,169 Val Step[1500/1563], Avg Loss: 1.4650, Avg Acc@1: 0.6800, Avg Acc@5: 0.8869
2022-01-15 14:36:35,989 Val Step[1550/1563], Avg Loss: 1.4657, Avg Acc@1: 0.6800, Avg Acc@5: 0.8867
2022-01-15 14:36:37,848 ----- Epoch[080/300], Validation Loss: 1.4659, Validation Acc@1: 0.6799, Validation Acc@5: 0.8867, time: 152.14
2022-01-15 14:36:38,328 ----- Save model: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-80-Loss-3.649470347313523.pdparams
2022-01-15 14:36:38,328 ----- Save optim: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-80-Loss-3.649470347313523.pdopt
2022-01-15 14:36:38,329 Now training epoch 81. LR=0.000889
2022-01-15 14:38:33,893 Epoch[081/300], Step[0000/1252], Avg Loss: 3.7674, Avg Acc: 0.2568
2022-01-15 14:40:02,416 Epoch[081/300], Step[0050/1252], Avg Loss: 3.6536, Avg Acc: 0.3925
2022-01-15 14:41:31,541 Epoch[081/300], Step[0100/1252], Avg Loss: 3.7116, Avg Acc: 0.3690
2022-01-15 14:43:01,140 Epoch[081/300], Step[0150/1252], Avg Loss: 3.6984, Avg Acc: 0.3668
2022-01-15 14:44:30,231 Epoch[081/300], Step[0200/1252], Avg Loss: 3.6934, Avg Acc: 0.3705
2022-01-15 14:46:00,781 Epoch[081/300], Step[0250/1252], Avg Loss: 3.6800, Avg Acc: 0.3639
2022-01-15 14:47:30,620 Epoch[081/300], Step[0300/1252], Avg Loss: 3.6719, Avg Acc: 0.3638
2022-01-15 14:49:01,182 Epoch[081/300], Step[0350/1252], Avg Loss: 3.6664, Avg Acc: 0.3642
2022-01-15 14:50:32,314 Epoch[081/300], Step[0400/1252], Avg Loss: 3.6638, Avg Acc: 0.3646
2022-01-15 14:52:03,264 Epoch[081/300], Step[0450/1252], Avg Loss: 3.6588, Avg Acc: 0.3640
2022-01-15 14:53:34,808 Epoch[081/300], Step[0500/1252], Avg Loss: 3.6559, Avg Acc: 0.3636
2022-01-15 14:55:04,913 Epoch[081/300], Step[0550/1252], Avg Loss: 3.6556, Avg Acc: 0.3641
2022-01-15 14:56:34,979 Epoch[081/300], Step[0600/1252], Avg Loss: 3.6489, Avg Acc: 0.3639
2022-01-15 14:58:05,094 Epoch[081/300], Step[0650/1252], Avg Loss: 3.6502, Avg Acc: 0.3641
2022-01-15 14:59:35,243 Epoch[081/300], Step[0700/1252], Avg Loss: 3.6457, Avg Acc: 0.3651
2022-01-15 15:01:05,103 Epoch[081/300], Step[0750/1252], Avg Loss: 3.6444, Avg Acc: 0.3637
2022-01-15 15:02:33,362 Epoch[081/300], Step[0800/1252], Avg Loss: 3.6468, Avg Acc: 0.3654
2022-01-15 15:04:01,959 Epoch[081/300], Step[0850/1252], Avg Loss: 3.6425, Avg Acc: 0.3665
2022-01-15 15:05:32,643 Epoch[081/300], Step[0900/1252], Avg Loss: 3.6420, Avg Acc: 0.3660
2022-01-15 15:07:03,893 Epoch[081/300], Step[0950/1252], Avg Loss: 3.6416, Avg Acc: 0.3672
2022-01-15 15:08:35,730 Epoch[081/300], Step[1000/1252], Avg Loss: 3.6427, Avg Acc: 0.3679
2022-01-15 15:10:06,485 Epoch[081/300], Step[1050/1252], Avg Loss: 3.6437, Avg Acc: 0.3675
2022-01-15 15:11:37,913 Epoch[081/300], Step[1100/1252], Avg Loss: 3.6431, Avg Acc: 0.3665
2022-01-15 15:13:08,197 Epoch[081/300], Step[1150/1252], Avg Loss: 3.6425, Avg Acc: 0.3665
2022-01-15 15:14:39,125 Epoch[081/300], Step[1200/1252], Avg Loss: 3.6424, Avg Acc: 0.3674
2022-01-15 15:16:10,147 Epoch[081/300], Step[1250/1252], Avg Loss: 3.6429, Avg Acc: 0.3668
2022-01-15 15:16:16,976 ----- Epoch[081/300], Train Loss: 3.6429, Train Acc: 0.3668, time: 2378.64, Best Val(epoch78) Acc@1: 0.6846
2022-01-15 15:16:16,976 Now training epoch 82. LR=0.000885
2022-01-15 15:18:10,501 Epoch[082/300], Step[0000/1252], Avg Loss: 3.7593, Avg Acc: 0.2930
2022-01-15 15:19:40,059 Epoch[082/300], Step[0050/1252], Avg Loss: 3.6067, Avg Acc: 0.3748
2022-01-15 15:21:11,131 Epoch[082/300], Step[0100/1252], Avg Loss: 3.6480, Avg Acc: 0.3684
2022-01-15 15:22:42,185 Epoch[082/300], Step[0150/1252], Avg Loss: 3.6364, Avg Acc: 0.3659
2022-01-15 15:24:13,516 Epoch[082/300], Step[0200/1252], Avg Loss: 3.6178, Avg Acc: 0.3706
2022-01-15 15:25:44,335 Epoch[082/300], Step[0250/1252], Avg Loss: 3.6275, Avg Acc: 0.3713
2022-01-15 15:27:14,696 Epoch[082/300], Step[0300/1252], Avg Loss: 3.6334, Avg Acc: 0.3732
2022-01-15 15:28:46,249 Epoch[082/300], Step[0350/1252], Avg Loss: 3.6329, Avg Acc: 0.3758
2022-01-15 15:30:18,111 Epoch[082/300], Step[0400/1252], Avg Loss: 3.6346, Avg Acc: 0.3739
2022-01-15 15:31:49,187 Epoch[082/300], Step[0450/1252], Avg Loss: 3.6370, Avg Acc: 0.3744
2022-01-15 15:33:21,245 Epoch[082/300], Step[0500/1252], Avg Loss: 3.6354, Avg Acc: 0.3737
2022-01-15 15:34:52,936 Epoch[082/300], Step[0550/1252], Avg Loss: 3.6323, Avg Acc: 0.3708
2022-01-15 15:36:24,940 Epoch[082/300], Step[0600/1252], Avg Loss: 3.6365, Avg Acc: 0.3695
2022-01-15 15:37:53,363 Epoch[082/300], Step[0650/1252], Avg Loss: 3.6372, Avg Acc: 0.3703
2022-01-15 15:39:24,136 Epoch[082/300], Step[0700/1252], Avg Loss: 3.6423, Avg Acc: 0.3684
2022-01-15 15:40:54,132 Epoch[082/300], Step[0750/1252], Avg Loss: 3.6420, Avg Acc: 0.3673
2022-01-15 15:42:24,648 Epoch[082/300], Step[0800/1252], Avg Loss: 3.6438, Avg Acc: 0.3674
2022-01-15 15:43:54,676 Epoch[082/300], Step[0850/1252], Avg Loss: 3.6448, Avg Acc: 0.3666
2022-01-15 15:45:23,585 Epoch[082/300], Step[0900/1252], Avg Loss: 3.6427, Avg Acc: 0.3674
2022-01-15 15:46:52,049 Epoch[082/300], Step[0950/1252], Avg Loss: 3.6427, Avg Acc: 0.3689
2022-01-15 15:48:23,792 Epoch[082/300], Step[1000/1252], Avg Loss: 3.6446, Avg Acc: 0.3685
2022-01-15 15:49:55,108 Epoch[082/300], Step[1050/1252], Avg Loss: 3.6413, Avg Acc: 0.3691
2022-01-15 15:51:27,073 Epoch[082/300], Step[1100/1252], Avg Loss: 3.6429, Avg Acc: 0.3685
2022-01-15 15:52:58,412 Epoch[082/300], Step[1150/1252], Avg Loss: 3.6431, Avg Acc: 0.3681
2022-01-15 15:54:28,412 Epoch[082/300], Step[1200/1252], Avg Loss: 3.6425, Avg Acc: 0.3690
2022-01-15 15:56:00,542 Epoch[082/300], Step[1250/1252], Avg Loss: 3.6428, Avg Acc: 0.3682
2022-01-15 15:56:07,616 ----- Epoch[082/300], Train Loss: 3.6428, Train Acc: 0.3682, time: 2390.64, Best Val(epoch78) Acc@1: 0.6846
2022-01-15 15:56:07,617 ----- Validation after Epoch: 82
2022-01-15 15:57:24,454 Val Step[0000/1563], Avg Loss: 1.2145, Avg Acc@1: 0.6875, Avg Acc@5: 0.9375
2022-01-15 15:57:26,642 Val Step[0050/1563], Avg Loss: 1.4432, Avg Acc@1: 0.6783, Avg Acc@5: 0.8848
2022-01-15 15:57:28,617 Val Step[0100/1563], Avg Loss: 1.4441, Avg Acc@1: 0.6785, Avg Acc@5: 0.8868
2022-01-15 15:57:30,549 Val Step[0150/1563], Avg Loss: 1.4300, Avg Acc@1: 0.6846, Avg Acc@5: 0.8858
2022-01-15 15:57:32,554 Val Step[0200/1563], Avg Loss: 1.4245, Avg Acc@1: 0.6870, Avg Acc@5: 0.8862
2022-01-15 15:57:34,540 Val Step[0250/1563], Avg Loss: 1.4095, Avg Acc@1: 0.6917, Avg Acc@5: 0.8887
2022-01-15 15:57:36,530 Val Step[0300/1563], Avg Loss: 1.4110, Avg Acc@1: 0.6906, Avg Acc@5: 0.8892
2022-01-15 15:57:38,598 Val Step[0350/1563], Avg Loss: 1.4164, Avg Acc@1: 0.6903, Avg Acc@5: 0.8884
2022-01-15 15:57:40,517 Val Step[0400/1563], Avg Loss: 1.4119, Avg Acc@1: 0.6910, Avg Acc@5: 0.8886
2022-01-15 15:57:42,664 Val Step[0450/1563], Avg Loss: 1.4162, Avg Acc@1: 0.6881, Avg Acc@5: 0.8892
2022-01-15 15:57:44,753 Val Step[0500/1563], Avg Loss: 1.4194, Avg Acc@1: 0.6873, Avg Acc@5: 0.8893
2022-01-15 15:57:46,971 Val Step[0550/1563], Avg Loss: 1.4185, Avg Acc@1: 0.6872, Avg Acc@5: 0.8893
2022-01-15 15:57:49,031 Val Step[0600/1563], Avg Loss: 1.4167, Avg Acc@1: 0.6872, Avg Acc@5: 0.8898
2022-01-15 15:57:51,047 Val Step[0650/1563], Avg Loss: 1.4177, Avg Acc@1: 0.6873, Avg Acc@5: 0.8898
2022-01-15 15:57:52,981 Val Step[0700/1563], Avg Loss: 1.4159, Avg Acc@1: 0.6874, Avg Acc@5: 0.8903
2022-01-15 15:57:54,946 Val Step[0750/1563], Avg Loss: 1.4208, Avg Acc@1: 0.6861, Avg Acc@5: 0.8898
2022-01-15 15:57:56,956 Val Step[0800/1563], Avg Loss: 1.4192, Avg Acc@1: 0.6876, Avg Acc@5: 0.8902
2022-01-15 15:57:58,958 Val Step[0850/1563], Avg Loss: 1.4204, Avg Acc@1: 0.6870, Avg Acc@5: 0.8901
2022-01-15 15:58:01,027 Val Step[0900/1563], Avg Loss: 1.4168, Avg Acc@1: 0.6874, Avg Acc@5: 0.8904
2022-01-15 15:58:02,919 Val Step[0950/1563], Avg Loss: 1.4177, Avg Acc@1: 0.6870, Avg Acc@5: 0.8903
2022-01-15 15:58:04,912 Val Step[1000/1563], Avg Loss: 1.4173, Avg Acc@1: 0.6873, Avg Acc@5: 0.8901
2022-01-15 15:58:06,900 Val Step[1050/1563], Avg Loss: 1.4214, Avg Acc@1: 0.6862, Avg Acc@5: 0.8894
2022-01-15 15:58:08,807 Val Step[1100/1563], Avg Loss: 1.4216, Avg Acc@1: 0.6863, Avg Acc@5: 0.8891
2022-01-15 15:58:10,741 Val Step[1150/1563], Avg Loss: 1.4204, Avg Acc@1: 0.6865, Avg Acc@5: 0.8891
2022-01-15 15:58:12,654 Val Step[1200/1563], Avg Loss: 1.4198, Avg Acc@1: 0.6865, Avg Acc@5: 0.8893
2022-01-15 15:58:14,628 Val Step[1250/1563], Avg Loss: 1.4189, Avg Acc@1: 0.6859, Avg Acc@5: 0.8895
2022-01-15 15:58:16,566 Val Step[1300/1563], Avg Loss: 1.4217, Avg Acc@1: 0.6857, Avg Acc@5: 0.8890
2022-01-15 15:58:18,656 Val Step[1350/1563], Avg Loss: 1.4223, Avg Acc@1: 0.6859, Avg Acc@5: 0.8886
2022-01-15 15:58:20,770 Val Step[1400/1563], Avg Loss: 1.4214, Avg Acc@1: 0.6857, Avg Acc@5: 0.8888
2022-01-15 15:58:22,818 Val Step[1450/1563], Avg Loss: 1.4209, Avg Acc@1: 0.6858, Avg Acc@5: 0.8890
2022-01-15 15:58:24,876 Val Step[1500/1563], Avg Loss: 1.4213, Avg Acc@1: 0.6860, Avg Acc@5: 0.8890
2022-01-15 15:58:26,812 Val Step[1550/1563], Avg Loss: 1.4223, Avg Acc@1: 0.6858, Avg Acc@5: 0.8887
2022-01-15 15:58:28,912 ----- Epoch[082/300], Validation Loss: 1.4224, Validation Acc@1: 0.6856, Validation Acc@5: 0.8888, time: 141.29
2022-01-15 15:58:30,060 the pre best model acc:0.6846, at epoch 78
2022-01-15 15:58:30,060 current best model acc:0.6856, at epoch 82
2022-01-15 15:58:30,060 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-15 15:58:30,061 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-15 15:58:30,061 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-15 15:58:30,061 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-15 15:58:30,061 Now training epoch 83. LR=0.000881
2022-01-15 16:00:20,113 Epoch[083/300], Step[0000/1252], Avg Loss: 4.0326, Avg Acc: 0.4346
2022-01-15 16:01:51,661 Epoch[083/300], Step[0050/1252], Avg Loss: 3.6185, Avg Acc: 0.3914
2022-01-15 16:03:21,939 Epoch[083/300], Step[0100/1252], Avg Loss: 3.6537, Avg Acc: 0.3783
2022-01-15 16:04:51,792 Epoch[083/300], Step[0150/1252], Avg Loss: 3.6430, Avg Acc: 0.3840
2022-01-15 16:06:22,511 Epoch[083/300], Step[0200/1252], Avg Loss: 3.6242, Avg Acc: 0.3869
2022-01-15 16:07:54,237 Epoch[083/300], Step[0250/1252], Avg Loss: 3.6257, Avg Acc: 0.3807
2022-01-15 16:09:25,621 Epoch[083/300], Step[0300/1252], Avg Loss: 3.6271, Avg Acc: 0.3828
2022-01-15 16:10:55,856 Epoch[083/300], Step[0350/1252], Avg Loss: 3.6236, Avg Acc: 0.3822
2022-01-15 16:12:26,279 Epoch[083/300], Step[0400/1252], Avg Loss: 3.6143, Avg Acc: 0.3819
2022-01-15 16:13:57,169 Epoch[083/300], Step[0450/1252], Avg Loss: 3.6065, Avg Acc: 0.3820
2022-01-15 16:15:28,086 Epoch[083/300], Step[0500/1252], Avg Loss: 3.6036, Avg Acc: 0.3817
2022-01-15 16:16:58,735 Epoch[083/300], Step[0550/1252], Avg Loss: 3.6066, Avg Acc: 0.3815
2022-01-15 16:18:30,257 Epoch[083/300], Step[0600/1252], Avg Loss: 3.6134, Avg Acc: 0.3781
2022-01-15 16:20:01,079 Epoch[083/300], Step[0650/1252], Avg Loss: 3.6194, Avg Acc: 0.3800
2022-01-15 16:21:31,988 Epoch[083/300], Step[0700/1252], Avg Loss: 3.6281, Avg Acc: 0.3796
2022-01-15 16:23:03,171 Epoch[083/300], Step[0750/1252], Avg Loss: 3.6314, Avg Acc: 0.3802
2022-01-15 16:24:34,607 Epoch[083/300], Step[0800/1252], Avg Loss: 3.6329, Avg Acc: 0.3799
2022-01-15 16:26:06,021 Epoch[083/300], Step[0850/1252], Avg Loss: 3.6329, Avg Acc: 0.3793
2022-01-15 16:27:36,129 Epoch[083/300], Step[0900/1252], Avg Loss: 3.6320, Avg Acc: 0.3809
2022-01-15 16:29:06,323 Epoch[083/300], Step[0950/1252], Avg Loss: 3.6348, Avg Acc: 0.3821
2022-01-15 16:30:37,157 Epoch[083/300], Step[1000/1252], Avg Loss: 3.6350, Avg Acc: 0.3807
2022-01-15 16:32:08,309 Epoch[083/300], Step[1050/1252], Avg Loss: 3.6344, Avg Acc: 0.3807
2022-01-15 16:33:40,023 Epoch[083/300], Step[1100/1252], Avg Loss: 3.6323, Avg Acc: 0.3803
2022-01-15 16:35:11,186 Epoch[083/300], Step[1150/1252], Avg Loss: 3.6331, Avg Acc: 0.3804
2022-01-15 16:36:42,838 Epoch[083/300], Step[1200/1252], Avg Loss: 3.6339, Avg Acc: 0.3808
2022-01-15 16:38:15,283 Epoch[083/300], Step[1250/1252], Avg Loss: 3.6324, Avg Acc: 0.3801
2022-01-15 16:38:22,608 ----- Epoch[083/300], Train Loss: 3.6324, Train Acc: 0.3801, time: 2392.54, Best Val(epoch82) Acc@1: 0.6856
2022-01-15 16:38:22,608 Now training epoch 84. LR=0.000878
2022-01-15 16:40:28,187 Epoch[084/300], Step[0000/1252], Avg Loss: 3.5849, Avg Acc: 0.3828
2022-01-15 16:41:58,055 Epoch[084/300], Step[0050/1252], Avg Loss: 3.5889, Avg Acc: 0.3933
2022-01-15 16:43:27,814 Epoch[084/300], Step[0100/1252], Avg Loss: 3.6105, Avg Acc: 0.3892
2022-01-15 16:44:57,767 Epoch[084/300], Step[0150/1252], Avg Loss: 3.6164, Avg Acc: 0.3764
2022-01-15 16:46:27,747 Epoch[084/300], Step[0200/1252], Avg Loss: 3.6180, Avg Acc: 0.3762
2022-01-15 16:47:57,421 Epoch[084/300], Step[0250/1252], Avg Loss: 3.6221, Avg Acc: 0.3759
2022-01-15 16:49:28,712 Epoch[084/300], Step[0300/1252], Avg Loss: 3.6272, Avg Acc: 0.3766
2022-01-15 16:50:59,066 Epoch[084/300], Step[0350/1252], Avg Loss: 3.6345, Avg Acc: 0.3749
2022-01-15 16:52:31,020 Epoch[084/300], Step[0400/1252], Avg Loss: 3.6369, Avg Acc: 0.3758
2022-01-15 16:54:01,312 Epoch[084/300], Step[0450/1252], Avg Loss: 3.6412, Avg Acc: 0.3772
2022-01-15 16:55:31,469 Epoch[084/300], Step[0500/1252], Avg Loss: 3.6412, Avg Acc: 0.3785
2022-01-15 16:57:03,666 Epoch[084/300], Step[0550/1252], Avg Loss: 3.6428, Avg Acc: 0.3753
2022-01-15 16:58:35,964 Epoch[084/300], Step[0600/1252], Avg Loss: 3.6451, Avg Acc: 0.3744
2022-01-15 17:00:07,642 Epoch[084/300], Step[0650/1252], Avg Loss: 3.6448, Avg Acc: 0.3758
2022-01-15 17:01:39,920 Epoch[084/300], Step[0700/1252], Avg Loss: 3.6473, Avg Acc: 0.3749
2022-01-15 17:03:10,901 Epoch[084/300], Step[0750/1252], Avg Loss: 3.6449, Avg Acc: 0.3751
2022-01-15 17:04:40,856 Epoch[084/300], Step[0800/1252], Avg Loss: 3.6511, Avg Acc: 0.3754
2022-01-15 17:06:11,281 Epoch[084/300], Step[0850/1252], Avg Loss: 3.6527, Avg Acc: 0.3756
2022-01-15 17:07:41,298 Epoch[084/300], Step[0900/1252], Avg Loss: 3.6507, Avg Acc: 0.3764
2022-01-15 17:09:13,542 Epoch[084/300], Step[0950/1252], Avg Loss: 3.6511, Avg Acc: 0.3761
2022-01-15 17:10:44,104 Epoch[084/300], Step[1000/1252], Avg Loss: 3.6483, Avg Acc: 0.3766
2022-01-15 17:12:14,788 Epoch[084/300], Step[1050/1252], Avg Loss: 3.6466, Avg Acc: 0.3776
2022-01-15 17:13:45,207 Epoch[084/300], Step[1100/1252], Avg Loss: 3.6444, Avg Acc: 0.3776
2022-01-15 17:15:16,206 Epoch[084/300], Step[1150/1252], Avg Loss: 3.6440, Avg Acc: 0.3777
2022-01-15 17:16:46,782 Epoch[084/300], Step[1200/1252], Avg Loss: 3.6446, Avg Acc: 0.3780
2022-01-15 17:18:16,076 Epoch[084/300], Step[1250/1252], Avg Loss: 3.6418, Avg Acc: 0.3792
2022-01-15 17:18:22,915 ----- Epoch[084/300], Train Loss: 3.6417, Train Acc: 0.3791, time: 2400.30, Best Val(epoch82) Acc@1: 0.6856
2022-01-15 17:18:22,915 ----- Validation after Epoch: 84
2022-01-15 17:19:39,120 Val Step[0000/1563], Avg Loss: 1.2565, Avg Acc@1: 0.6562, Avg Acc@5: 0.9688
2022-01-15 17:19:41,054 Val Step[0050/1563], Avg Loss: 1.3821, Avg Acc@1: 0.6918, Avg Acc@5: 0.8830
2022-01-15 17:19:42,921 Val Step[0100/1563], Avg Loss: 1.3832, Avg Acc@1: 0.6900, Avg Acc@5: 0.8877
2022-01-15 17:19:44,880 Val Step[0150/1563], Avg Loss: 1.3821, Avg Acc@1: 0.6889, Avg Acc@5: 0.8872
2022-01-15 17:19:46,866 Val Step[0200/1563], Avg Loss: 1.3826, Avg Acc@1: 0.6866, Avg Acc@5: 0.8881
2022-01-15 17:19:48,722 Val Step[0250/1563], Avg Loss: 1.3678, Avg Acc@1: 0.6930, Avg Acc@5: 0.8893
2022-01-15 17:19:50,858 Val Step[0300/1563], Avg Loss: 1.3677, Avg Acc@1: 0.6933, Avg Acc@5: 0.8903
2022-01-15 17:19:53,052 Val Step[0350/1563], Avg Loss: 1.3733, Avg Acc@1: 0.6926, Avg Acc@5: 0.8903
2022-01-15 17:19:55,111 Val Step[0400/1563], Avg Loss: 1.3720, Avg Acc@1: 0.6940, Avg Acc@5: 0.8906
2022-01-15 17:19:57,178 Val Step[0450/1563], Avg Loss: 1.3751, Avg Acc@1: 0.6916, Avg Acc@5: 0.8905
2022-01-15 17:19:59,332 Val Step[0500/1563], Avg Loss: 1.3798, Avg Acc@1: 0.6904, Avg Acc@5: 0.8902
2022-01-15 17:20:01,582 Val Step[0550/1563], Avg Loss: 1.3810, Avg Acc@1: 0.6898, Avg Acc@5: 0.8903
2022-01-15 17:20:03,686 Val Step[0600/1563], Avg Loss: 1.3830, Avg Acc@1: 0.6883, Avg Acc@5: 0.8902
2022-01-15 17:20:05,740 Val Step[0650/1563], Avg Loss: 1.3840, Avg Acc@1: 0.6884, Avg Acc@5: 0.8899
2022-01-15 17:20:07,572 Val Step[0700/1563], Avg Loss: 1.3814, Avg Acc@1: 0.6883, Avg Acc@5: 0.8911
2022-01-15 17:20:09,459 Val Step[0750/1563], Avg Loss: 1.3876, Avg Acc@1: 0.6867, Avg Acc@5: 0.8904
2022-01-15 17:20:11,400 Val Step[0800/1563], Avg Loss: 1.3873, Avg Acc@1: 0.6871, Avg Acc@5: 0.8904
2022-01-15 17:20:13,333 Val Step[0850/1563], Avg Loss: 1.3872, Avg Acc@1: 0.6874, Avg Acc@5: 0.8899
2022-01-15 17:20:15,171 Val Step[0900/1563], Avg Loss: 1.3838, Avg Acc@1: 0.6878, Avg Acc@5: 0.8898
2022-01-15 17:20:16,984 Val Step[0950/1563], Avg Loss: 1.3848, Avg Acc@1: 0.6875, Avg Acc@5: 0.8901
2022-01-15 17:20:18,800 Val Step[1000/1563], Avg Loss: 1.3841, Avg Acc@1: 0.6877, Avg Acc@5: 0.8902
2022-01-15 17:20:20,655 Val Step[1050/1563], Avg Loss: 1.3864, Avg Acc@1: 0.6873, Avg Acc@5: 0.8895
2022-01-15 17:20:22,543 Val Step[1100/1563], Avg Loss: 1.3861, Avg Acc@1: 0.6873, Avg Acc@5: 0.8897
2022-01-15 17:20:24,474 Val Step[1150/1563], Avg Loss: 1.3845, Avg Acc@1: 0.6871, Avg Acc@5: 0.8899
2022-01-15 17:20:26,338 Val Step[1200/1563], Avg Loss: 1.3837, Avg Acc@1: 0.6870, Avg Acc@5: 0.8902
2022-01-15 17:20:28,147 Val Step[1250/1563], Avg Loss: 1.3823, Avg Acc@1: 0.6871, Avg Acc@5: 0.8905
2022-01-15 17:20:29,990 Val Step[1300/1563], Avg Loss: 1.3859, Avg Acc@1: 0.6866, Avg Acc@5: 0.8899
2022-01-15 17:20:31,822 Val Step[1350/1563], Avg Loss: 1.3865, Avg Acc@1: 0.6862, Avg Acc@5: 0.8899
2022-01-15 17:20:33,658 Val Step[1400/1563], Avg Loss: 1.3860, Avg Acc@1: 0.6857, Avg Acc@5: 0.8899
2022-01-15 17:20:35,522 Val Step[1450/1563], Avg Loss: 1.3861, Avg Acc@1: 0.6855, Avg Acc@5: 0.8899
2022-01-15 17:20:37,437 Val Step[1500/1563], Avg Loss: 1.3859, Avg Acc@1: 0.6854, Avg Acc@5: 0.8901
2022-01-15 17:20:39,246 Val Step[1550/1563], Avg Loss: 1.3864, Avg Acc@1: 0.6855, Avg Acc@5: 0.8900
2022-01-15 17:20:40,954 ----- Epoch[084/300], Validation Loss: 1.3866, Validation Acc@1: 0.6855, Validation Acc@5: 0.8900, time: 138.04
2022-01-15 17:20:40,954 Now training epoch 85. LR=0.000874
2022-01-15 17:22:31,549 Epoch[085/300], Step[0000/1252], Avg Loss: 3.5688, Avg Acc: 0.5859
2022-01-15 17:24:02,459 Epoch[085/300], Step[0050/1252], Avg Loss: 3.6688, Avg Acc: 0.3747
2022-01-15 17:25:32,754 Epoch[085/300], Step[0100/1252], Avg Loss: 3.6634, Avg Acc: 0.3794
2022-01-15 17:27:04,025 Epoch[085/300], Step[0150/1252], Avg Loss: 3.6470, Avg Acc: 0.3792
2022-01-15 17:28:34,651 Epoch[085/300], Step[0200/1252], Avg Loss: 3.6379, Avg Acc: 0.3714
2022-01-15 17:30:03,889 Epoch[085/300], Step[0250/1252], Avg Loss: 3.6396, Avg Acc: 0.3691
2022-01-15 17:31:34,057 Epoch[085/300], Step[0300/1252], Avg Loss: 3.6343, Avg Acc: 0.3734
2022-01-15 17:33:04,869 Epoch[085/300], Step[0350/1252], Avg Loss: 3.6313, Avg Acc: 0.3749
2022-01-15 17:34:36,525 Epoch[085/300], Step[0400/1252], Avg Loss: 3.6378, Avg Acc: 0.3722
2022-01-15 17:36:07,731 Epoch[085/300], Step[0450/1252], Avg Loss: 3.6500, Avg Acc: 0.3713
2022-01-15 17:37:39,156 Epoch[085/300], Step[0500/1252], Avg Loss: 3.6453, Avg Acc: 0.3729
2022-01-15 17:39:09,295 Epoch[085/300], Step[0550/1252], Avg Loss: 3.6446, Avg Acc: 0.3743
2022-01-15 17:40:39,186 Epoch[085/300], Step[0600/1252], Avg Loss: 3.6437, Avg Acc: 0.3768
2022-01-15 17:42:11,282 Epoch[085/300], Step[0650/1252], Avg Loss: 3.6439, Avg Acc: 0.3758
2022-01-15 17:43:43,493 Epoch[085/300], Step[0700/1252], Avg Loss: 3.6415, Avg Acc: 0.3767
2022-01-15 17:45:13,513 Epoch[085/300], Step[0750/1252], Avg Loss: 3.6422, Avg Acc: 0.3760
2022-01-15 17:46:45,036 Epoch[085/300], Step[0800/1252], Avg Loss: 3.6387, Avg Acc: 0.3746
2022-01-15 17:48:17,041 Epoch[085/300], Step[0850/1252], Avg Loss: 3.6400, Avg Acc: 0.3745
2022-01-15 17:49:48,900 Epoch[085/300], Step[0900/1252], Avg Loss: 3.6406, Avg Acc: 0.3744
2022-01-15 17:51:19,783 Epoch[085/300], Step[0950/1252], Avg Loss: 3.6403, Avg Acc: 0.3739
2022-01-15 17:52:50,533 Epoch[085/300], Step[1000/1252], Avg Loss: 3.6398, Avg Acc: 0.3744
2022-01-15 17:54:21,783 Epoch[085/300], Step[1050/1252], Avg Loss: 3.6427, Avg Acc: 0.3739
2022-01-15 17:55:54,241 Epoch[085/300], Step[1100/1252], Avg Loss: 3.6427, Avg Acc: 0.3731
2022-01-15 17:57:24,151 Epoch[085/300], Step[1150/1252], Avg Loss: 3.6446, Avg Acc: 0.3732
2022-01-15 17:58:55,103 Epoch[085/300], Step[1200/1252], Avg Loss: 3.6446, Avg Acc: 0.3740
2022-01-15 18:00:26,115 Epoch[085/300], Step[1250/1252], Avg Loss: 3.6473, Avg Acc: 0.3737
2022-01-15 18:00:32,977 ----- Epoch[085/300], Train Loss: 3.6473, Train Acc: 0.3737, time: 2392.02, Best Val(epoch82) Acc@1: 0.6856
2022-01-15 18:00:32,977 Now training epoch 86. LR=0.000870
2022-01-15 18:02:30,312 Epoch[086/300], Step[0000/1252], Avg Loss: 3.3837, Avg Acc: 0.3086
2022-01-15 18:03:59,622 Epoch[086/300], Step[0050/1252], Avg Loss: 3.5818, Avg Acc: 0.3657
2022-01-15 18:05:30,739 Epoch[086/300], Step[0100/1252], Avg Loss: 3.6028, Avg Acc: 0.3670
2022-01-15 18:07:00,564 Epoch[086/300], Step[0150/1252], Avg Loss: 3.6139, Avg Acc: 0.3736
2022-01-15 18:08:32,293 Epoch[086/300], Step[0200/1252], Avg Loss: 3.6041, Avg Acc: 0.3791
2022-01-15 18:10:00,559 Epoch[086/300], Step[0250/1252], Avg Loss: 3.5974, Avg Acc: 0.3846
2022-01-15 18:11:32,041 Epoch[086/300], Step[0300/1252], Avg Loss: 3.5969, Avg Acc: 0.3841
2022-01-15 18:13:02,610 Epoch[086/300], Step[0350/1252], Avg Loss: 3.5898, Avg Acc: 0.3860
2022-01-15 18:14:33,202 Epoch[086/300], Step[0400/1252], Avg Loss: 3.5900, Avg Acc: 0.3837
2022-01-15 18:16:03,661 Epoch[086/300], Step[0450/1252], Avg Loss: 3.5911, Avg Acc: 0.3814
2022-01-15 18:17:35,363 Epoch[086/300], Step[0500/1252], Avg Loss: 3.5962, Avg Acc: 0.3810
2022-01-15 18:19:06,159 Epoch[086/300], Step[0550/1252], Avg Loss: 3.5984, Avg Acc: 0.3819
2022-01-15 18:20:37,528 Epoch[086/300], Step[0600/1252], Avg Loss: 3.6035, Avg Acc: 0.3805
2022-01-15 18:22:07,895 Epoch[086/300], Step[0650/1252], Avg Loss: 3.6063, Avg Acc: 0.3802
2022-01-15 18:23:38,933 Epoch[086/300], Step[0700/1252], Avg Loss: 3.6140, Avg Acc: 0.3790
2022-01-15 18:25:10,127 Epoch[086/300], Step[0750/1252], Avg Loss: 3.6145, Avg Acc: 0.3784
2022-01-15 18:26:41,405 Epoch[086/300], Step[0800/1252], Avg Loss: 3.6122, Avg Acc: 0.3788
2022-01-15 18:28:10,011 Epoch[086/300], Step[0850/1252], Avg Loss: 3.6132, Avg Acc: 0.3790
2022-01-15 18:29:39,907 Epoch[086/300], Step[0900/1252], Avg Loss: 3.6165, Avg Acc: 0.3791
2022-01-15 18:31:08,658 Epoch[086/300], Step[0950/1252], Avg Loss: 3.6165, Avg Acc: 0.3793
2022-01-15 18:32:38,388 Epoch[086/300], Step[1000/1252], Avg Loss: 3.6134, Avg Acc: 0.3796
2022-01-15 18:34:08,468 Epoch[086/300], Step[1050/1252], Avg Loss: 3.6145, Avg Acc: 0.3791
2022-01-15 18:35:38,004 Epoch[086/300], Step[1100/1252], Avg Loss: 3.6147, Avg Acc: 0.3786
2022-01-15 18:37:09,660 Epoch[086/300], Step[1150/1252], Avg Loss: 3.6177, Avg Acc: 0.3775
2022-01-15 18:38:40,795 Epoch[086/300], Step[1200/1252], Avg Loss: 3.6196, Avg Acc: 0.3772
2022-01-15 18:40:11,391 Epoch[086/300], Step[1250/1252], Avg Loss: 3.6175, Avg Acc: 0.3776
2022-01-15 18:40:18,489 ----- Epoch[086/300], Train Loss: 3.6175, Train Acc: 0.3776, time: 2385.51, Best Val(epoch82) Acc@1: 0.6856
2022-01-15 18:40:18,489 ----- Validation after Epoch: 86
2022-01-15 18:41:40,597 Val Step[0000/1563], Avg Loss: 1.2049, Avg Acc@1: 0.7500, Avg Acc@5: 0.9375
2022-01-15 18:41:42,886 Val Step[0050/1563], Avg Loss: 1.3822, Avg Acc@1: 0.6893, Avg Acc@5: 0.8885
2022-01-15 18:41:45,199 Val Step[0100/1563], Avg Loss: 1.3975, Avg Acc@1: 0.6890, Avg Acc@5: 0.8895
2022-01-15 18:41:47,219 Val Step[0150/1563], Avg Loss: 1.4025, Avg Acc@1: 0.6892, Avg Acc@5: 0.8889
2022-01-15 18:41:49,225 Val Step[0200/1563], Avg Loss: 1.4034, Avg Acc@1: 0.6881, Avg Acc@5: 0.8887
2022-01-15 18:41:51,161 Val Step[0250/1563], Avg Loss: 1.3917, Avg Acc@1: 0.6897, Avg Acc@5: 0.8908
2022-01-15 18:41:53,137 Val Step[0300/1563], Avg Loss: 1.3895, Avg Acc@1: 0.6896, Avg Acc@5: 0.8907
2022-01-15 18:41:55,157 Val Step[0350/1563], Avg Loss: 1.3955, Avg Acc@1: 0.6900, Avg Acc@5: 0.8896
2022-01-15 18:41:57,108 Val Step[0400/1563], Avg Loss: 1.3948, Avg Acc@1: 0.6892, Avg Acc@5: 0.8897
2022-01-15 18:41:59,137 Val Step[0450/1563], Avg Loss: 1.4001, Avg Acc@1: 0.6869, Avg Acc@5: 0.8893
2022-01-15 18:42:01,353 Val Step[0500/1563], Avg Loss: 1.4012, Avg Acc@1: 0.6863, Avg Acc@5: 0.8897
2022-01-15 18:42:03,548 Val Step[0550/1563], Avg Loss: 1.4019, Avg Acc@1: 0.6853, Avg Acc@5: 0.8904
2022-01-15 18:42:05,678 Val Step[0600/1563], Avg Loss: 1.4016, Avg Acc@1: 0.6858, Avg Acc@5: 0.8908
2022-01-15 18:42:07,811 Val Step[0650/1563], Avg Loss: 1.4016, Avg Acc@1: 0.6855, Avg Acc@5: 0.8913
2022-01-15 18:42:09,930 Val Step[0700/1563], Avg Loss: 1.3989, Avg Acc@1: 0.6857, Avg Acc@5: 0.8919
2022-01-15 18:42:12,098 Val Step[0750/1563], Avg Loss: 1.4045, Avg Acc@1: 0.6850, Avg Acc@5: 0.8914
2022-01-15 18:42:14,310 Val Step[0800/1563], Avg Loss: 1.4039, Avg Acc@1: 0.6858, Avg Acc@5: 0.8917
2022-01-15 18:42:16,474 Val Step[0850/1563], Avg Loss: 1.4061, Avg Acc@1: 0.6853, Avg Acc@5: 0.8915
2022-01-15 18:42:18,611 Val Step[0900/1563], Avg Loss: 1.4039, Avg Acc@1: 0.6858, Avg Acc@5: 0.8919
2022-01-15 18:42:20,730 Val Step[0950/1563], Avg Loss: 1.4028, Avg Acc@1: 0.6860, Avg Acc@5: 0.8925
2022-01-15 18:42:23,057 Val Step[1000/1563], Avg Loss: 1.4037, Avg Acc@1: 0.6857, Avg Acc@5: 0.8921
2022-01-15 18:42:25,376 Val Step[1050/1563], Avg Loss: 1.4056, Avg Acc@1: 0.6856, Avg Acc@5: 0.8916
2022-01-15 18:42:27,700 Val Step[1100/1563], Avg Loss: 1.4052, Avg Acc@1: 0.6855, Avg Acc@5: 0.8913
2022-01-15 18:42:29,797 Val Step[1150/1563], Avg Loss: 1.4045, Avg Acc@1: 0.6854, Avg Acc@5: 0.8915
2022-01-15 18:42:31,907 Val Step[1200/1563], Avg Loss: 1.4037, Avg Acc@1: 0.6858, Avg Acc@5: 0.8917
2022-01-15 18:42:34,040 Val Step[1250/1563], Avg Loss: 1.4021, Avg Acc@1: 0.6864, Avg Acc@5: 0.8919
2022-01-15 18:42:36,131 Val Step[1300/1563], Avg Loss: 1.4049, Avg Acc@1: 0.6857, Avg Acc@5: 0.8912
2022-01-15 18:42:38,300 Val Step[1350/1563], Avg Loss: 1.4056, Avg Acc@1: 0.6855, Avg Acc@5: 0.8910
2022-01-15 18:42:40,381 Val Step[1400/1563], Avg Loss: 1.4054, Avg Acc@1: 0.6854, Avg Acc@5: 0.8910
2022-01-15 18:42:42,530 Val Step[1450/1563], Avg Loss: 1.4064, Avg Acc@1: 0.6851, Avg Acc@5: 0.8906
2022-01-15 18:42:44,637 Val Step[1500/1563], Avg Loss: 1.4066, Avg Acc@1: 0.6851, Avg Acc@5: 0.8905
2022-01-15 18:42:46,694 Val Step[1550/1563], Avg Loss: 1.4080, Avg Acc@1: 0.6852, Avg Acc@5: 0.8901
2022-01-15 18:42:48,771 ----- Epoch[086/300], Validation Loss: 1.4082, Validation Acc@1: 0.6853, Validation Acc@5: 0.8901, time: 150.28
2022-01-15 18:42:48,771 Now training epoch 87. LR=0.000867
2022-01-15 18:44:44,189 Epoch[087/300], Step[0000/1252], Avg Loss: 4.2973, Avg Acc: 0.3340
2022-01-15 18:46:15,469 Epoch[087/300], Step[0050/1252], Avg Loss: 3.5609, Avg Acc: 0.3774
2022-01-15 18:47:45,816 Epoch[087/300], Step[0100/1252], Avg Loss: 3.6168, Avg Acc: 0.3802
2022-01-15 18:49:16,758 Epoch[087/300], Step[0150/1252], Avg Loss: 3.6031, Avg Acc: 0.3839
2022-01-15 18:50:45,406 Epoch[087/300], Step[0200/1252], Avg Loss: 3.6097, Avg Acc: 0.3811
2022-01-15 18:52:15,540 Epoch[087/300], Step[0250/1252], Avg Loss: 3.6098, Avg Acc: 0.3761
2022-01-15 18:53:45,158 Epoch[087/300], Step[0300/1252], Avg Loss: 3.6094, Avg Acc: 0.3738
2022-01-15 18:55:14,752 Epoch[087/300], Step[0350/1252], Avg Loss: 3.6081, Avg Acc: 0.3783
2022-01-15 18:56:44,704 Epoch[087/300], Step[0400/1252], Avg Loss: 3.6095, Avg Acc: 0.3772
2022-01-15 18:58:14,452 Epoch[087/300], Step[0450/1252], Avg Loss: 3.6133, Avg Acc: 0.3757
2022-01-15 18:59:45,345 Epoch[087/300], Step[0500/1252], Avg Loss: 3.6160, Avg Acc: 0.3747
2022-01-15 19:01:15,466 Epoch[087/300], Step[0550/1252], Avg Loss: 3.6207, Avg Acc: 0.3750
2022-01-15 19:02:44,530 Epoch[087/300], Step[0600/1252], Avg Loss: 3.6191, Avg Acc: 0.3771
2022-01-15 19:04:14,219 Epoch[087/300], Step[0650/1252], Avg Loss: 3.6237, Avg Acc: 0.3753
2022-01-15 19:05:44,709 Epoch[087/300], Step[0700/1252], Avg Loss: 3.6277, Avg Acc: 0.3753
2022-01-15 19:07:13,846 Epoch[087/300], Step[0750/1252], Avg Loss: 3.6315, Avg Acc: 0.3742
2022-01-15 19:08:43,037 Epoch[087/300], Step[0800/1252], Avg Loss: 3.6278, Avg Acc: 0.3759
2022-01-15 19:10:13,001 Epoch[087/300], Step[0850/1252], Avg Loss: 3.6276, Avg Acc: 0.3762
2022-01-15 19:11:42,837 Epoch[087/300], Step[0900/1252], Avg Loss: 3.6271, Avg Acc: 0.3761
2022-01-15 19:13:12,639 Epoch[087/300], Step[0950/1252], Avg Loss: 3.6281, Avg Acc: 0.3755
2022-01-15 19:14:40,666 Epoch[087/300], Step[1000/1252], Avg Loss: 3.6296, Avg Acc: 0.3743
2022-01-15 19:16:11,085 Epoch[087/300], Step[1050/1252], Avg Loss: 3.6313, Avg Acc: 0.3746
2022-01-15 19:17:41,513 Epoch[087/300], Step[1100/1252], Avg Loss: 3.6306, Avg Acc: 0.3742
2022-01-15 19:19:11,369 Epoch[087/300], Step[1150/1252], Avg Loss: 3.6333, Avg Acc: 0.3741
2022-01-15 19:20:40,332 Epoch[087/300], Step[1200/1252], Avg Loss: 3.6340, Avg Acc: 0.3742
2022-01-15 19:22:09,647 Epoch[087/300], Step[1250/1252], Avg Loss: 3.6326, Avg Acc: 0.3740
2022-01-15 19:22:17,382 ----- Epoch[087/300], Train Loss: 3.6327, Train Acc: 0.3740, time: 2368.61, Best Val(epoch82) Acc@1: 0.6856
2022-01-15 19:22:17,382 Now training epoch 88. LR=0.000863
2022-01-15 19:24:06,696 Epoch[088/300], Step[0000/1252], Avg Loss: 4.1001, Avg Acc: 0.4033
2022-01-15 19:25:35,021 Epoch[088/300], Step[0050/1252], Avg Loss: 3.7187, Avg Acc: 0.3724
2022-01-15 19:27:02,547 Epoch[088/300], Step[0100/1252], Avg Loss: 3.6586, Avg Acc: 0.3796
2022-01-15 19:28:29,865 Epoch[088/300], Step[0150/1252], Avg Loss: 3.6542, Avg Acc: 0.3788
2022-01-15 19:29:57,539 Epoch[088/300], Step[0200/1252], Avg Loss: 3.6512, Avg Acc: 0.3748
2022-01-15 19:31:24,699 Epoch[088/300], Step[0250/1252], Avg Loss: 3.6592, Avg Acc: 0.3780
2022-01-15 19:32:51,117 Epoch[088/300], Step[0300/1252], Avg Loss: 3.6623, Avg Acc: 0.3715
2022-01-15 19:34:18,876 Epoch[088/300], Step[0350/1252], Avg Loss: 3.6557, Avg Acc: 0.3714
2022-01-15 19:35:47,301 Epoch[088/300], Step[0400/1252], Avg Loss: 3.6513, Avg Acc: 0.3689
2022-01-15 19:37:14,910 Epoch[088/300], Step[0450/1252], Avg Loss: 3.6529, Avg Acc: 0.3722
2022-01-15 19:38:41,909 Epoch[088/300], Step[0500/1252], Avg Loss: 3.6475, Avg Acc: 0.3721
2022-01-15 19:40:09,702 Epoch[088/300], Step[0550/1252], Avg Loss: 3.6500, Avg Acc: 0.3723
2022-01-15 19:41:38,083 Epoch[088/300], Step[0600/1252], Avg Loss: 3.6480, Avg Acc: 0.3736
2022-01-15 19:43:04,847 Epoch[088/300], Step[0650/1252], Avg Loss: 3.6483, Avg Acc: 0.3739
2022-01-15 19:44:31,673 Epoch[088/300], Step[0700/1252], Avg Loss: 3.6465, Avg Acc: 0.3740
2022-01-15 19:46:00,038 Epoch[088/300], Step[0750/1252], Avg Loss: 3.6423, Avg Acc: 0.3711
2022-01-15 19:47:26,526 Epoch[088/300], Step[0800/1252], Avg Loss: 3.6442, Avg Acc: 0.3715
2022-01-15 19:48:54,802 Epoch[088/300], Step[0850/1252], Avg Loss: 3.6422, Avg Acc: 0.3703
2022-01-15 19:50:22,379 Epoch[088/300], Step[0900/1252], Avg Loss: 3.6408, Avg Acc: 0.3716
2022-01-15 19:51:47,968 Epoch[088/300], Step[0950/1252], Avg Loss: 3.6425, Avg Acc: 0.3713
2022-01-15 19:53:13,987 Epoch[088/300], Step[1000/1252], Avg Loss: 3.6438, Avg Acc: 0.3711
2022-01-15 19:54:41,051 Epoch[088/300], Step[1050/1252], Avg Loss: 3.6382, Avg Acc: 0.3707
2022-01-15 19:56:08,684 Epoch[088/300], Step[1100/1252], Avg Loss: 3.6365, Avg Acc: 0.3715
2022-01-15 19:57:35,225 Epoch[088/300], Step[1150/1252], Avg Loss: 3.6341, Avg Acc: 0.3724
2022-01-15 19:59:02,983 Epoch[088/300], Step[1200/1252], Avg Loss: 3.6349, Avg Acc: 0.3720
2022-01-15 20:00:30,594 Epoch[088/300], Step[1250/1252], Avg Loss: 3.6331, Avg Acc: 0.3717
2022-01-15 20:00:37,681 ----- Epoch[088/300], Train Loss: 3.6332, Train Acc: 0.3717, time: 2300.30, Best Val(epoch82) Acc@1: 0.6856
2022-01-15 20:00:37,681 ----- Validation after Epoch: 88
2022-01-15 20:02:12,925 Val Step[0000/1563], Avg Loss: 1.1306, Avg Acc@1: 0.7812, Avg Acc@5: 0.9375
2022-01-15 20:02:14,848 Val Step[0050/1563], Avg Loss: 1.3680, Avg Acc@1: 0.6973, Avg Acc@5: 0.8915
2022-01-15 20:02:16,736 Val Step[0100/1563], Avg Loss: 1.3561, Avg Acc@1: 0.7020, Avg Acc@5: 0.8951
2022-01-15 20:02:18,782 Val Step[0150/1563], Avg Loss: 1.3621, Avg Acc@1: 0.6983, Avg Acc@5: 0.8922
2022-01-15 20:02:20,929 Val Step[0200/1563], Avg Loss: 1.3616, Avg Acc@1: 0.6985, Avg Acc@5: 0.8935
2022-01-15 20:02:22,883 Val Step[0250/1563], Avg Loss: 1.3521, Avg Acc@1: 0.6978, Avg Acc@5: 0.8933
2022-01-15 20:02:24,735 Val Step[0300/1563], Avg Loss: 1.3518, Avg Acc@1: 0.6979, Avg Acc@5: 0.8935
2022-01-15 20:02:26,634 Val Step[0350/1563], Avg Loss: 1.3589, Avg Acc@1: 0.6967, Avg Acc@5: 0.8937
2022-01-15 20:02:28,574 Val Step[0400/1563], Avg Loss: 1.3579, Avg Acc@1: 0.6963, Avg Acc@5: 0.8944
2022-01-15 20:02:30,476 Val Step[0450/1563], Avg Loss: 1.3636, Avg Acc@1: 0.6940, Avg Acc@5: 0.8932
2022-01-15 20:02:32,380 Val Step[0500/1563], Avg Loss: 1.3660, Avg Acc@1: 0.6933, Avg Acc@5: 0.8933
2022-01-15 20:02:34,370 Val Step[0550/1563], Avg Loss: 1.3685, Avg Acc@1: 0.6924, Avg Acc@5: 0.8934
2022-01-15 20:02:36,197 Val Step[0600/1563], Avg Loss: 1.3661, Avg Acc@1: 0.6925, Avg Acc@5: 0.8941
2022-01-15 20:02:38,049 Val Step[0650/1563], Avg Loss: 1.3664, Avg Acc@1: 0.6928, Avg Acc@5: 0.8947
2022-01-15 20:02:40,005 Val Step[0700/1563], Avg Loss: 1.3621, Avg Acc@1: 0.6937, Avg Acc@5: 0.8951
2022-01-15 20:02:41,850 Val Step[0750/1563], Avg Loss: 1.3684, Avg Acc@1: 0.6928, Avg Acc@5: 0.8941
2022-01-15 20:02:43,648 Val Step[0800/1563], Avg Loss: 1.3678, Avg Acc@1: 0.6935, Avg Acc@5: 0.8940
2022-01-15 20:02:45,482 Val Step[0850/1563], Avg Loss: 1.3707, Avg Acc@1: 0.6922, Avg Acc@5: 0.8933
2022-01-15 20:02:47,317 Val Step[0900/1563], Avg Loss: 1.3683, Avg Acc@1: 0.6922, Avg Acc@5: 0.8937
2022-01-15 20:02:49,226 Val Step[0950/1563], Avg Loss: 1.3682, Avg Acc@1: 0.6925, Avg Acc@5: 0.8939
2022-01-15 20:02:51,136 Val Step[1000/1563], Avg Loss: 1.3679, Avg Acc@1: 0.6929, Avg Acc@5: 0.8937
2022-01-15 20:02:53,073 Val Step[1050/1563], Avg Loss: 1.3714, Avg Acc@1: 0.6918, Avg Acc@5: 0.8931
2022-01-15 20:02:55,043 Val Step[1100/1563], Avg Loss: 1.3717, Avg Acc@1: 0.6913, Avg Acc@5: 0.8930
2022-01-15 20:02:56,839 Val Step[1150/1563], Avg Loss: 1.3713, Avg Acc@1: 0.6913, Avg Acc@5: 0.8926
2022-01-15 20:02:58,660 Val Step[1200/1563], Avg Loss: 1.3709, Avg Acc@1: 0.6911, Avg Acc@5: 0.8926
2022-01-15 20:03:00,477 Val Step[1250/1563], Avg Loss: 1.3698, Avg Acc@1: 0.6912, Avg Acc@5: 0.8929
2022-01-15 20:03:02,312 Val Step[1300/1563], Avg Loss: 1.3728, Avg Acc@1: 0.6908, Avg Acc@5: 0.8924
2022-01-15 20:03:04,219 Val Step[1350/1563], Avg Loss: 1.3726, Avg Acc@1: 0.6910, Avg Acc@5: 0.8921
2022-01-15 20:03:06,175 Val Step[1400/1563], Avg Loss: 1.3722, Avg Acc@1: 0.6909, Avg Acc@5: 0.8922
2022-01-15 20:03:08,141 Val Step[1450/1563], Avg Loss: 1.3727, Avg Acc@1: 0.6909, Avg Acc@5: 0.8921
2022-01-15 20:03:10,165 Val Step[1500/1563], Avg Loss: 1.3722, Avg Acc@1: 0.6910, Avg Acc@5: 0.8922
2022-01-15 20:03:12,178 Val Step[1550/1563], Avg Loss: 1.3739, Avg Acc@1: 0.6906, Avg Acc@5: 0.8920
2022-01-15 20:03:14,091 ----- Epoch[088/300], Validation Loss: 1.3738, Validation Acc@1: 0.6906, Validation Acc@5: 0.8921, time: 156.41
2022-01-15 20:03:15,241 the pre best model acc:0.6856, at epoch 82
2022-01-15 20:03:15,241 current best model acc:0.6906, at epoch 88
2022-01-15 20:03:15,242 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-15 20:03:15,242 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-15 20:03:15,242 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-15 20:03:15,242 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-15 20:03:15,242 Now training epoch 89. LR=0.000859
2022-01-15 20:05:11,686 Epoch[089/300], Step[0000/1252], Avg Loss: 3.1775, Avg Acc: 0.5576
2022-01-15 20:06:39,849 Epoch[089/300], Step[0050/1252], Avg Loss: 3.6083, Avg Acc: 0.3462
2022-01-15 20:08:05,516 Epoch[089/300], Step[0100/1252], Avg Loss: 3.5943, Avg Acc: 0.3717
2022-01-15 20:09:32,842 Epoch[089/300], Step[0150/1252], Avg Loss: 3.5949, Avg Acc: 0.3721
2022-01-15 20:11:00,150 Epoch[089/300], Step[0200/1252], Avg Loss: 3.5978, Avg Acc: 0.3761
2022-01-15 20:12:27,840 Epoch[089/300], Step[0250/1252], Avg Loss: 3.6102, Avg Acc: 0.3746
2022-01-15 20:13:54,020 Epoch[089/300], Step[0300/1252], Avg Loss: 3.6086, Avg Acc: 0.3753
2022-01-15 20:15:21,989 Epoch[089/300], Step[0350/1252], Avg Loss: 3.6207, Avg Acc: 0.3738
2022-01-15 20:16:50,366 Epoch[089/300], Step[0400/1252], Avg Loss: 3.6220, Avg Acc: 0.3762
2022-01-15 20:18:18,913 Epoch[089/300], Step[0450/1252], Avg Loss: 3.6169, Avg Acc: 0.3751
2022-01-15 20:19:47,294 Epoch[089/300], Step[0500/1252], Avg Loss: 3.6169, Avg Acc: 0.3750
2022-01-15 20:21:16,906 Epoch[089/300], Step[0550/1252], Avg Loss: 3.6186, Avg Acc: 0.3746
2022-01-15 20:22:45,572 Epoch[089/300], Step[0600/1252], Avg Loss: 3.6203, Avg Acc: 0.3756
2022-01-15 20:24:13,702 Epoch[089/300], Step[0650/1252], Avg Loss: 3.6251, Avg Acc: 0.3767
2022-01-15 20:25:41,663 Epoch[089/300], Step[0700/1252], Avg Loss: 3.6188, Avg Acc: 0.3775
2022-01-15 20:27:09,228 Epoch[089/300], Step[0750/1252], Avg Loss: 3.6177, Avg Acc: 0.3795
2022-01-15 20:28:36,715 Epoch[089/300], Step[0800/1252], Avg Loss: 3.6166, Avg Acc: 0.3800
2022-01-15 20:30:04,987 Epoch[089/300], Step[0850/1252], Avg Loss: 3.6198, Avg Acc: 0.3796
2022-01-15 20:31:33,668 Epoch[089/300], Step[0900/1252], Avg Loss: 3.6226, Avg Acc: 0.3791
2022-01-15 20:33:02,618 Epoch[089/300], Step[0950/1252], Avg Loss: 3.6244, Avg Acc: 0.3782
2022-01-15 20:34:31,057 Epoch[089/300], Step[1000/1252], Avg Loss: 3.6262, Avg Acc: 0.3784
2022-01-15 20:35:59,630 Epoch[089/300], Step[1050/1252], Avg Loss: 3.6265, Avg Acc: 0.3788
2022-01-15 20:37:28,853 Epoch[089/300], Step[1100/1252], Avg Loss: 3.6279, Avg Acc: 0.3797
2022-01-15 20:38:57,743 Epoch[089/300], Step[1150/1252], Avg Loss: 3.6286, Avg Acc: 0.3795
2022-01-15 20:40:25,378 Epoch[089/300], Step[1200/1252], Avg Loss: 3.6282, Avg Acc: 0.3811
2022-01-15 20:41:52,775 Epoch[089/300], Step[1250/1252], Avg Loss: 3.6279, Avg Acc: 0.3808
2022-01-15 20:41:59,794 ----- Epoch[089/300], Train Loss: 3.6279, Train Acc: 0.3808, time: 2324.55, Best Val(epoch88) Acc@1: 0.6906
2022-01-15 20:41:59,794 Now training epoch 90. LR=0.000855
2022-01-15 20:43:52,006 Epoch[090/300], Step[0000/1252], Avg Loss: 3.2975, Avg Acc: 0.5732
2022-01-15 20:45:20,775 Epoch[090/300], Step[0050/1252], Avg Loss: 3.6505, Avg Acc: 0.3868
2022-01-15 20:46:49,245 Epoch[090/300], Step[0100/1252], Avg Loss: 3.6504, Avg Acc: 0.3762
2022-01-15 20:48:16,487 Epoch[090/300], Step[0150/1252], Avg Loss: 3.6308, Avg Acc: 0.3715
2022-01-15 20:49:44,018 Epoch[090/300], Step[0200/1252], Avg Loss: 3.6210, Avg Acc: 0.3751
2022-01-15 20:51:11,770 Epoch[090/300], Step[0250/1252], Avg Loss: 3.6224, Avg Acc: 0.3778
2022-01-15 20:52:39,927 Epoch[090/300], Step[0300/1252], Avg Loss: 3.6149, Avg Acc: 0.3806
2022-01-15 20:54:08,618 Epoch[090/300], Step[0350/1252], Avg Loss: 3.6097, Avg Acc: 0.3815
2022-01-15 20:55:35,634 Epoch[090/300], Step[0400/1252], Avg Loss: 3.6123, Avg Acc: 0.3812
2022-01-15 20:57:03,714 Epoch[090/300], Step[0450/1252], Avg Loss: 3.6129, Avg Acc: 0.3779
2022-01-15 20:58:29,908 Epoch[090/300], Step[0500/1252], Avg Loss: 3.6195, Avg Acc: 0.3787
2022-01-15 20:59:57,080 Epoch[090/300], Step[0550/1252], Avg Loss: 3.6184, Avg Acc: 0.3797
2022-01-15 21:01:25,333 Epoch[090/300], Step[0600/1252], Avg Loss: 3.6116, Avg Acc: 0.3786
2022-01-15 21:02:53,258 Epoch[090/300], Step[0650/1252], Avg Loss: 3.6171, Avg Acc: 0.3778
2022-01-15 21:04:22,033 Epoch[090/300], Step[0700/1252], Avg Loss: 3.6184, Avg Acc: 0.3764
2022-01-15 21:05:50,256 Epoch[090/300], Step[0750/1252], Avg Loss: 3.6154, Avg Acc: 0.3788
2022-01-15 21:07:19,319 Epoch[090/300], Step[0800/1252], Avg Loss: 3.6202, Avg Acc: 0.3785
2022-01-15 21:08:47,780 Epoch[090/300], Step[0850/1252], Avg Loss: 3.6190, Avg Acc: 0.3784
2022-01-15 21:10:15,507 Epoch[090/300], Step[0900/1252], Avg Loss: 3.6247, Avg Acc: 0.3776
2022-01-15 21:11:43,720 Epoch[090/300], Step[0950/1252], Avg Loss: 3.6256, Avg Acc: 0.3774
2022-01-15 21:13:12,575 Epoch[090/300], Step[1000/1252], Avg Loss: 3.6245, Avg Acc: 0.3766
2022-01-15 21:14:41,077 Epoch[090/300], Step[1050/1252], Avg Loss: 3.6255, Avg Acc: 0.3756
2022-01-15 21:16:09,908 Epoch[090/300], Step[1100/1252], Avg Loss: 3.6297, Avg Acc: 0.3755
2022-01-15 21:17:38,230 Epoch[090/300], Step[1150/1252], Avg Loss: 3.6279, Avg Acc: 0.3755
2022-01-15 21:19:07,201 Epoch[090/300], Step[1200/1252], Avg Loss: 3.6280, Avg Acc: 0.3748
2022-01-15 21:20:34,618 Epoch[090/300], Step[1250/1252], Avg Loss: 3.6286, Avg Acc: 0.3755
2022-01-15 21:20:41,572 ----- Epoch[090/300], Train Loss: 3.6286, Train Acc: 0.3756, time: 2321.77, Best Val(epoch88) Acc@1: 0.6906
2022-01-15 21:20:41,572 ----- Validation after Epoch: 90
2022-01-15 21:22:11,238 Val Step[0000/1563], Avg Loss: 1.2218, Avg Acc@1: 0.6562, Avg Acc@5: 0.9375
2022-01-15 21:22:13,215 Val Step[0050/1563], Avg Loss: 1.3477, Avg Acc@1: 0.6881, Avg Acc@5: 0.9007
2022-01-15 21:22:15,161 Val Step[0100/1563], Avg Loss: 1.3826, Avg Acc@1: 0.6844, Avg Acc@5: 0.8942
2022-01-15 21:22:17,133 Val Step[0150/1563], Avg Loss: 1.3813, Avg Acc@1: 0.6875, Avg Acc@5: 0.8932
2022-01-15 21:22:19,178 Val Step[0200/1563], Avg Loss: 1.3846, Avg Acc@1: 0.6870, Avg Acc@5: 0.8923
2022-01-15 21:22:21,318 Val Step[0250/1563], Avg Loss: 1.3696, Avg Acc@1: 0.6915, Avg Acc@5: 0.8933
2022-01-15 21:22:23,449 Val Step[0300/1563], Avg Loss: 1.3672, Avg Acc@1: 0.6933, Avg Acc@5: 0.8941
2022-01-15 21:22:25,546 Val Step[0350/1563], Avg Loss: 1.3719, Avg Acc@1: 0.6928, Avg Acc@5: 0.8948
2022-01-15 21:22:27,603 Val Step[0400/1563], Avg Loss: 1.3754, Avg Acc@1: 0.6916, Avg Acc@5: 0.8944
2022-01-15 21:22:29,744 Val Step[0450/1563], Avg Loss: 1.3783, Avg Acc@1: 0.6900, Avg Acc@5: 0.8946
2022-01-15 21:22:31,901 Val Step[0500/1563], Avg Loss: 1.3814, Avg Acc@1: 0.6895, Avg Acc@5: 0.8950
2022-01-15 21:22:34,006 Val Step[0550/1563], Avg Loss: 1.3826, Avg Acc@1: 0.6901, Avg Acc@5: 0.8956
2022-01-15 21:22:36,135 Val Step[0600/1563], Avg Loss: 1.3806, Avg Acc@1: 0.6912, Avg Acc@5: 0.8956
2022-01-15 21:22:38,235 Val Step[0650/1563], Avg Loss: 1.3834, Avg Acc@1: 0.6909, Avg Acc@5: 0.8950
2022-01-15 21:22:40,339 Val Step[0700/1563], Avg Loss: 1.3812, Avg Acc@1: 0.6909, Avg Acc@5: 0.8957
2022-01-15 21:22:42,393 Val Step[0750/1563], Avg Loss: 1.3865, Avg Acc@1: 0.6894, Avg Acc@5: 0.8952
2022-01-15 21:22:44,438 Val Step[0800/1563], Avg Loss: 1.3845, Avg Acc@1: 0.6900, Avg Acc@5: 0.8958
2022-01-15 21:22:46,470 Val Step[0850/1563], Avg Loss: 1.3868, Avg Acc@1: 0.6897, Avg Acc@5: 0.8949
2022-01-15 21:22:48,569 Val Step[0900/1563], Avg Loss: 1.3834, Avg Acc@1: 0.6905, Avg Acc@5: 0.8954
2022-01-15 21:22:50,716 Val Step[0950/1563], Avg Loss: 1.3828, Avg Acc@1: 0.6905, Avg Acc@5: 0.8957
2022-01-15 21:22:52,886 Val Step[1000/1563], Avg Loss: 1.3818, Avg Acc@1: 0.6905, Avg Acc@5: 0.8954
2022-01-15 21:22:55,087 Val Step[1050/1563], Avg Loss: 1.3844, Avg Acc@1: 0.6896, Avg Acc@5: 0.8945
2022-01-15 21:22:57,233 Val Step[1100/1563], Avg Loss: 1.3847, Avg Acc@1: 0.6893, Avg Acc@5: 0.8945
2022-01-15 21:22:59,274 Val Step[1150/1563], Avg Loss: 1.3829, Avg Acc@1: 0.6894, Avg Acc@5: 0.8946
2022-01-15 21:23:01,421 Val Step[1200/1563], Avg Loss: 1.3833, Avg Acc@1: 0.6900, Avg Acc@5: 0.8940
2022-01-15 21:23:03,471 Val Step[1250/1563], Avg Loss: 1.3816, Avg Acc@1: 0.6895, Avg Acc@5: 0.8943
2022-01-15 21:23:05,503 Val Step[1300/1563], Avg Loss: 1.3846, Avg Acc@1: 0.6892, Avg Acc@5: 0.8938
2022-01-15 21:23:07,546 Val Step[1350/1563], Avg Loss: 1.3852, Avg Acc@1: 0.6892, Avg Acc@5: 0.8937
2022-01-15 21:23:09,556 Val Step[1400/1563], Avg Loss: 1.3846, Avg Acc@1: 0.6890, Avg Acc@5: 0.8938
2022-01-15 21:23:11,675 Val Step[1450/1563], Avg Loss: 1.3852, Avg Acc@1: 0.6888, Avg Acc@5: 0.8937
2022-01-15 21:23:13,815 Val Step[1500/1563], Avg Loss: 1.3833, Avg Acc@1: 0.6893, Avg Acc@5: 0.8940
2022-01-15 21:23:15,883 Val Step[1550/1563], Avg Loss: 1.3844, Avg Acc@1: 0.6894, Avg Acc@5: 0.8939
2022-01-15 21:23:17,878 ----- Epoch[090/300], Validation Loss: 1.3843, Validation Acc@1: 0.6894, Validation Acc@5: 0.8940, time: 156.30
2022-01-15 21:23:18,343 ----- Save model: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-90-Loss-3.628424893718017.pdparams
2022-01-15 21:23:18,343 ----- Save optim: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-90-Loss-3.628424893718017.pdopt
2022-01-15 21:23:18,344 Now training epoch 91. LR=0.000851
2022-01-15 21:25:11,833 Epoch[091/300], Step[0000/1252], Avg Loss: 3.8303, Avg Acc: 0.3584
2022-01-15 21:26:39,906 Epoch[091/300], Step[0050/1252], Avg Loss: 3.6644, Avg Acc: 0.3710
2022-01-15 21:28:07,170 Epoch[091/300], Step[0100/1252], Avg Loss: 3.6257, Avg Acc: 0.3672
2022-01-15 21:29:34,662 Epoch[091/300], Step[0150/1252], Avg Loss: 3.6418, Avg Acc: 0.3701
2022-01-15 21:31:01,275 Epoch[091/300], Step[0200/1252], Avg Loss: 3.6458, Avg Acc: 0.3739
2022-01-15 21:32:28,353 Epoch[091/300], Step[0250/1252], Avg Loss: 3.6419, Avg Acc: 0.3736
2022-01-15 21:33:55,534 Epoch[091/300], Step[0300/1252], Avg Loss: 3.6360, Avg Acc: 0.3757
2022-01-15 21:35:22,193 Epoch[091/300], Step[0350/1252], Avg Loss: 3.6325, Avg Acc: 0.3813
2022-01-15 21:36:49,986 Epoch[091/300], Step[0400/1252], Avg Loss: 3.6274, Avg Acc: 0.3816
2022-01-15 21:38:17,470 Epoch[091/300], Step[0450/1252], Avg Loss: 3.6244, Avg Acc: 0.3810
2022-01-15 21:39:46,206 Epoch[091/300], Step[0500/1252], Avg Loss: 3.6257, Avg Acc: 0.3819
2022-01-15 21:41:13,491 Epoch[091/300], Step[0550/1252], Avg Loss: 3.6220, Avg Acc: 0.3815
2022-01-15 21:42:39,958 Epoch[091/300], Step[0600/1252], Avg Loss: 3.6263, Avg Acc: 0.3809
2022-01-15 21:44:08,199 Epoch[091/300], Step[0650/1252], Avg Loss: 3.6271, Avg Acc: 0.3808
2022-01-15 21:45:35,250 Epoch[091/300], Step[0700/1252], Avg Loss: 3.6255, Avg Acc: 0.3808
2022-01-15 21:47:03,149 Epoch[091/300], Step[0750/1252], Avg Loss: 3.6261, Avg Acc: 0.3784
2022-01-15 21:48:31,601 Epoch[091/300], Step[0800/1252], Avg Loss: 3.6270, Avg Acc: 0.3772
2022-01-15 21:49:59,730 Epoch[091/300], Step[0850/1252], Avg Loss: 3.6252, Avg Acc: 0.3772
2022-01-15 21:51:28,042 Epoch[091/300], Step[0900/1252], Avg Loss: 3.6298, Avg Acc: 0.3765
2022-01-15 21:52:55,884 Epoch[091/300], Step[0950/1252], Avg Loss: 3.6313, Avg Acc: 0.3756
2022-01-15 21:54:23,780 Epoch[091/300], Step[1000/1252], Avg Loss: 3.6300, Avg Acc: 0.3760
2022-01-15 21:55:51,779 Epoch[091/300], Step[1050/1252], Avg Loss: 3.6325, Avg Acc: 0.3755
2022-01-15 21:57:19,790 Epoch[091/300], Step[1100/1252], Avg Loss: 3.6304, Avg Acc: 0.3756
2022-01-15 21:58:48,508 Epoch[091/300], Step[1150/1252], Avg Loss: 3.6268, Avg Acc: 0.3765
2022-01-15 22:00:15,592 Epoch[091/300], Step[1200/1252], Avg Loss: 3.6284, Avg Acc: 0.3767
2022-01-15 22:01:41,895 Epoch[091/300], Step[1250/1252], Avg Loss: 3.6270, Avg Acc: 0.3779
2022-01-15 22:01:48,968 ----- Epoch[091/300], Train Loss: 3.6270, Train Acc: 0.3779, time: 2310.62, Best Val(epoch88) Acc@1: 0.6906
2022-01-15 22:01:48,968 Now training epoch 92. LR=0.000847
2022-01-15 22:03:40,158 Epoch[092/300], Step[0000/1252], Avg Loss: 3.5858, Avg Acc: 0.3789
2022-01-15 22:05:07,751 Epoch[092/300], Step[0050/1252], Avg Loss: 3.6207, Avg Acc: 0.3907
2022-01-15 22:06:34,978 Epoch[092/300], Step[0100/1252], Avg Loss: 3.6556, Avg Acc: 0.3736
2022-01-15 22:08:03,410 Epoch[092/300], Step[0150/1252], Avg Loss: 3.6439, Avg Acc: 0.3764
2022-01-15 22:09:31,807 Epoch[092/300], Step[0200/1252], Avg Loss: 3.6272, Avg Acc: 0.3809
2022-01-15 22:11:00,362 Epoch[092/300], Step[0250/1252], Avg Loss: 3.6221, Avg Acc: 0.3776
2022-01-15 22:12:27,084 Epoch[092/300], Step[0300/1252], Avg Loss: 3.6243, Avg Acc: 0.3754
2022-01-15 22:13:54,749 Epoch[092/300], Step[0350/1252], Avg Loss: 3.6223, Avg Acc: 0.3777
2022-01-15 22:15:22,887 Epoch[092/300], Step[0400/1252], Avg Loss: 3.6255, Avg Acc: 0.3781
2022-01-15 22:16:51,394 Epoch[092/300], Step[0450/1252], Avg Loss: 3.6221, Avg Acc: 0.3755
2022-01-15 22:18:18,725 Epoch[092/300], Step[0500/1252], Avg Loss: 3.6196, Avg Acc: 0.3774
2022-01-15 22:19:46,874 Epoch[092/300], Step[0550/1252], Avg Loss: 3.6260, Avg Acc: 0.3773
2022-01-15 22:21:14,700 Epoch[092/300], Step[0600/1252], Avg Loss: 3.6263, Avg Acc: 0.3776
2022-01-15 22:22:42,482 Epoch[092/300], Step[0650/1252], Avg Loss: 3.6210, Avg Acc: 0.3786
2022-01-15 22:24:09,919 Epoch[092/300], Step[0700/1252], Avg Loss: 3.6131, Avg Acc: 0.3787
2022-01-15 22:25:37,012 Epoch[092/300], Step[0750/1252], Avg Loss: 3.6118, Avg Acc: 0.3786
2022-01-15 22:27:03,985 Epoch[092/300], Step[0800/1252], Avg Loss: 3.6114, Avg Acc: 0.3767
2022-01-15 22:28:32,146 Epoch[092/300], Step[0850/1252], Avg Loss: 3.6124, Avg Acc: 0.3767
2022-01-15 22:30:00,899 Epoch[092/300], Step[0900/1252], Avg Loss: 3.6098, Avg Acc: 0.3760
2022-01-15 22:31:29,442 Epoch[092/300], Step[0950/1252], Avg Loss: 3.6097, Avg Acc: 0.3732
2022-01-15 22:32:57,484 Epoch[092/300], Step[1000/1252], Avg Loss: 3.6121, Avg Acc: 0.3721
2022-01-15 22:34:24,833 Epoch[092/300], Step[1050/1252], Avg Loss: 3.6163, Avg Acc: 0.3714
2022-01-15 22:35:51,750 Epoch[092/300], Step[1100/1252], Avg Loss: 3.6189, Avg Acc: 0.3711
2022-01-15 22:37:18,020 Epoch[092/300], Step[1150/1252], Avg Loss: 3.6204, Avg Acc: 0.3714
2022-01-15 22:38:44,453 Epoch[092/300], Step[1200/1252], Avg Loss: 3.6188, Avg Acc: 0.3715
2022-01-15 22:40:08,819 Epoch[092/300], Step[1250/1252], Avg Loss: 3.6193, Avg Acc: 0.3719
2022-01-15 22:40:15,995 ----- Epoch[092/300], Train Loss: 3.6192, Train Acc: 0.3719, time: 2307.02, Best Val(epoch88) Acc@1: 0.6906
2022-01-15 22:40:15,995 ----- Validation after Epoch: 92
2022-01-15 22:41:39,814 Val Step[0000/1563], Avg Loss: 1.1468, Avg Acc@1: 0.6875, Avg Acc@5: 0.9688
2022-01-15 22:41:41,977 Val Step[0050/1563], Avg Loss: 1.3782, Avg Acc@1: 0.6973, Avg Acc@5: 0.8946
2022-01-15 22:41:44,221 Val Step[0100/1563], Avg Loss: 1.3917, Avg Acc@1: 0.6937, Avg Acc@5: 0.8963
2022-01-15 22:41:46,098 Val Step[0150/1563], Avg Loss: 1.4056, Avg Acc@1: 0.6916, Avg Acc@5: 0.8934
2022-01-15 22:41:47,982 Val Step[0200/1563], Avg Loss: 1.4101, Avg Acc@1: 0.6915, Avg Acc@5: 0.8919
2022-01-15 22:41:49,791 Val Step[0250/1563], Avg Loss: 1.3906, Avg Acc@1: 0.6970, Avg Acc@5: 0.8943
2022-01-15 22:41:51,583 Val Step[0300/1563], Avg Loss: 1.3897, Avg Acc@1: 0.6975, Avg Acc@5: 0.8944
2022-01-15 22:41:53,384 Val Step[0350/1563], Avg Loss: 1.3962, Avg Acc@1: 0.6954, Avg Acc@5: 0.8940
2022-01-15 22:41:55,179 Val Step[0400/1563], Avg Loss: 1.3951, Avg Acc@1: 0.6963, Avg Acc@5: 0.8951
2022-01-15 22:41:56,963 Val Step[0450/1563], Avg Loss: 1.3994, Avg Acc@1: 0.6940, Avg Acc@5: 0.8944
2022-01-15 22:41:58,763 Val Step[0500/1563], Avg Loss: 1.4034, Avg Acc@1: 0.6922, Avg Acc@5: 0.8936
2022-01-15 22:42:00,632 Val Step[0550/1563], Avg Loss: 1.4039, Avg Acc@1: 0.6918, Avg Acc@5: 0.8941
2022-01-15 22:42:02,557 Val Step[0600/1563], Avg Loss: 1.4043, Avg Acc@1: 0.6915, Avg Acc@5: 0.8948
2022-01-15 22:42:04,571 Val Step[0650/1563], Avg Loss: 1.4050, Avg Acc@1: 0.6915, Avg Acc@5: 0.8953
2022-01-15 22:42:06,727 Val Step[0700/1563], Avg Loss: 1.4032, Avg Acc@1: 0.6922, Avg Acc@5: 0.8958
2022-01-15 22:42:08,814 Val Step[0750/1563], Avg Loss: 1.4070, Avg Acc@1: 0.6922, Avg Acc@5: 0.8954
2022-01-15 22:42:10,703 Val Step[0800/1563], Avg Loss: 1.4062, Avg Acc@1: 0.6925, Avg Acc@5: 0.8958
2022-01-15 22:42:12,590 Val Step[0850/1563], Avg Loss: 1.4070, Avg Acc@1: 0.6922, Avg Acc@5: 0.8953
2022-01-15 22:42:14,479 Val Step[0900/1563], Avg Loss: 1.4043, Avg Acc@1: 0.6925, Avg Acc@5: 0.8958
2022-01-15 22:42:16,439 Val Step[0950/1563], Avg Loss: 1.4040, Avg Acc@1: 0.6926, Avg Acc@5: 0.8963
2022-01-15 22:42:18,419 Val Step[1000/1563], Avg Loss: 1.4026, Avg Acc@1: 0.6931, Avg Acc@5: 0.8964
2022-01-15 22:42:20,260 Val Step[1050/1563], Avg Loss: 1.4035, Avg Acc@1: 0.6926, Avg Acc@5: 0.8963
2022-01-15 22:42:22,244 Val Step[1100/1563], Avg Loss: 1.4029, Avg Acc@1: 0.6931, Avg Acc@5: 0.8961
2022-01-15 22:42:24,120 Val Step[1150/1563], Avg Loss: 1.4009, Avg Acc@1: 0.6933, Avg Acc@5: 0.8961
2022-01-15 22:42:25,946 Val Step[1200/1563], Avg Loss: 1.4012, Avg Acc@1: 0.6934, Avg Acc@5: 0.8959
2022-01-15 22:42:27,752 Val Step[1250/1563], Avg Loss: 1.4007, Avg Acc@1: 0.6934, Avg Acc@5: 0.8961
2022-01-15 22:42:29,516 Val Step[1300/1563], Avg Loss: 1.4034, Avg Acc@1: 0.6932, Avg Acc@5: 0.8958
2022-01-15 22:42:31,346 Val Step[1350/1563], Avg Loss: 1.4041, Avg Acc@1: 0.6929, Avg Acc@5: 0.8957
2022-01-15 22:42:33,160 Val Step[1400/1563], Avg Loss: 1.4039, Avg Acc@1: 0.6923, Avg Acc@5: 0.8956
2022-01-15 22:42:34,990 Val Step[1450/1563], Avg Loss: 1.4047, Avg Acc@1: 0.6920, Avg Acc@5: 0.8953
2022-01-15 22:42:36,797 Val Step[1500/1563], Avg Loss: 1.4040, Avg Acc@1: 0.6922, Avg Acc@5: 0.8958
2022-01-15 22:42:38,522 Val Step[1550/1563], Avg Loss: 1.4049, Avg Acc@1: 0.6919, Avg Acc@5: 0.8953
2022-01-15 22:42:40,402 ----- Epoch[092/300], Validation Loss: 1.4051, Validation Acc@1: 0.6919, Validation Acc@5: 0.8955, time: 144.40
2022-01-15 22:42:41,537 the pre best model acc:0.6906, at epoch 88
2022-01-15 22:42:41,538 current best model acc:0.6919, at epoch 92
2022-01-15 22:42:41,538 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-15 22:42:41,538 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-15 22:42:41,538 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-15 22:42:41,538 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-15 22:42:41,538 Now training epoch 93. LR=0.000843
2022-01-15 22:44:33,324 Epoch[093/300], Step[0000/1252], Avg Loss: 3.7845, Avg Acc: 0.3799
2022-01-15 22:46:00,953 Epoch[093/300], Step[0050/1252], Avg Loss: 3.5201, Avg Acc: 0.3924
2022-01-15 22:47:28,076 Epoch[093/300], Step[0100/1252], Avg Loss: 3.5618, Avg Acc: 0.3808
2022-01-15 22:48:55,194 Epoch[093/300], Step[0150/1252], Avg Loss: 3.5707, Avg Acc: 0.3859
2022-01-15 22:50:22,017 Epoch[093/300], Step[0200/1252], Avg Loss: 3.5728, Avg Acc: 0.3782
2022-01-15 22:51:49,771 Epoch[093/300], Step[0250/1252], Avg Loss: 3.5905, Avg Acc: 0.3777
2022-01-15 22:53:18,162 Epoch[093/300], Step[0300/1252], Avg Loss: 3.5970, Avg Acc: 0.3784
2022-01-15 22:54:44,761 Epoch[093/300], Step[0350/1252], Avg Loss: 3.5959, Avg Acc: 0.3809
2022-01-15 22:56:11,596 Epoch[093/300], Step[0400/1252], Avg Loss: 3.6097, Avg Acc: 0.3791
2022-01-15 22:57:38,552 Epoch[093/300], Step[0450/1252], Avg Loss: 3.6004, Avg Acc: 0.3810
2022-01-15 22:59:06,441 Epoch[093/300], Step[0500/1252], Avg Loss: 3.6091, Avg Acc: 0.3769
2022-01-15 23:00:33,314 Epoch[093/300], Step[0550/1252], Avg Loss: 3.6032, Avg Acc: 0.3753
2022-01-15 23:01:59,522 Epoch[093/300], Step[0600/1252], Avg Loss: 3.6033, Avg Acc: 0.3749
2022-01-15 23:03:27,330 Epoch[093/300], Step[0650/1252], Avg Loss: 3.6060, Avg Acc: 0.3743
2022-01-15 23:04:53,230 Epoch[093/300], Step[0700/1252], Avg Loss: 3.6044, Avg Acc: 0.3751
2022-01-15 23:06:19,003 Epoch[093/300], Step[0750/1252], Avg Loss: 3.6075, Avg Acc: 0.3755
2022-01-15 23:07:44,664 Epoch[093/300], Step[0800/1252], Avg Loss: 3.6090, Avg Acc: 0.3760
2022-01-15 23:09:11,557 Epoch[093/300], Step[0850/1252], Avg Loss: 3.6102, Avg Acc: 0.3758
2022-01-15 23:10:38,388 Epoch[093/300], Step[0900/1252], Avg Loss: 3.6100, Avg Acc: 0.3764
2022-01-15 23:12:05,164 Epoch[093/300], Step[0950/1252], Avg Loss: 3.6089, Avg Acc: 0.3765
2022-01-15 23:13:31,230 Epoch[093/300], Step[1000/1252], Avg Loss: 3.6097, Avg Acc: 0.3762
2022-01-15 23:14:57,583 Epoch[093/300], Step[1050/1252], Avg Loss: 3.6088, Avg Acc: 0.3760
2022-01-15 23:16:22,998 Epoch[093/300], Step[1100/1252], Avg Loss: 3.6120, Avg Acc: 0.3762
2022-01-15 23:17:49,146 Epoch[093/300], Step[1150/1252], Avg Loss: 3.6116, Avg Acc: 0.3765
2022-01-15 23:19:16,777 Epoch[093/300], Step[1200/1252], Avg Loss: 3.6093, Avg Acc: 0.3760
2022-01-15 23:20:43,682 Epoch[093/300], Step[1250/1252], Avg Loss: 3.6104, Avg Acc: 0.3755
2022-01-15 23:20:50,749 ----- Epoch[093/300], Train Loss: 3.6104, Train Acc: 0.3755, time: 2289.21, Best Val(epoch92) Acc@1: 0.6919
2022-01-15 23:20:50,749 Now training epoch 94. LR=0.000839
2022-01-15 23:22:36,826 Epoch[094/300], Step[0000/1252], Avg Loss: 3.8524, Avg Acc: 0.3418
2022-01-15 23:24:03,905 Epoch[094/300], Step[0050/1252], Avg Loss: 3.6839, Avg Acc: 0.3790
2022-01-15 23:25:32,108 Epoch[094/300], Step[0100/1252], Avg Loss: 3.6675, Avg Acc: 0.3718
2022-01-15 23:27:00,267 Epoch[094/300], Step[0150/1252], Avg Loss: 3.6386, Avg Acc: 0.3714
2022-01-15 23:28:28,787 Epoch[094/300], Step[0200/1252], Avg Loss: 3.6430, Avg Acc: 0.3765
2022-01-15 23:29:56,894 Epoch[094/300], Step[0250/1252], Avg Loss: 3.6344, Avg Acc: 0.3819
2022-01-15 23:31:25,266 Epoch[094/300], Step[0300/1252], Avg Loss: 3.6346, Avg Acc: 0.3819
2022-01-15 23:32:53,625 Epoch[094/300], Step[0350/1252], Avg Loss: 3.6335, Avg Acc: 0.3828
2022-01-15 23:34:22,205 Epoch[094/300], Step[0400/1252], Avg Loss: 3.6290, Avg Acc: 0.3797
2022-01-15 23:35:51,036 Epoch[094/300], Step[0450/1252], Avg Loss: 3.6255, Avg Acc: 0.3776
2022-01-15 23:37:19,435 Epoch[094/300], Step[0500/1252], Avg Loss: 3.6248, Avg Acc: 0.3763
2022-01-15 23:38:47,456 Epoch[094/300], Step[0550/1252], Avg Loss: 3.6227, Avg Acc: 0.3766
2022-01-15 23:40:15,160 Epoch[094/300], Step[0600/1252], Avg Loss: 3.6296, Avg Acc: 0.3756
2022-01-15 23:41:42,205 Epoch[094/300], Step[0650/1252], Avg Loss: 3.6277, Avg Acc: 0.3765
2022-01-15 23:43:09,227 Epoch[094/300], Step[0700/1252], Avg Loss: 3.6213, Avg Acc: 0.3788
2022-01-15 23:44:36,971 Epoch[094/300], Step[0750/1252], Avg Loss: 3.6170, Avg Acc: 0.3799
2022-01-15 23:46:04,907 Epoch[094/300], Step[0800/1252], Avg Loss: 3.6192, Avg Acc: 0.3797
2022-01-15 23:47:32,561 Epoch[094/300], Step[0850/1252], Avg Loss: 3.6235, Avg Acc: 0.3788
2022-01-15 23:48:59,322 Epoch[094/300], Step[0900/1252], Avg Loss: 3.6238, Avg Acc: 0.3783
2022-01-15 23:50:27,814 Epoch[094/300], Step[0950/1252], Avg Loss: 3.6255, Avg Acc: 0.3779
2022-01-15 23:51:56,264 Epoch[094/300], Step[1000/1252], Avg Loss: 3.6237, Avg Acc: 0.3778
2022-01-15 23:53:23,997 Epoch[094/300], Step[1050/1252], Avg Loss: 3.6258, Avg Acc: 0.3774
2022-01-15 23:54:50,887 Epoch[094/300], Step[1100/1252], Avg Loss: 3.6244, Avg Acc: 0.3774
2022-01-15 23:56:18,075 Epoch[094/300], Step[1150/1252], Avg Loss: 3.6249, Avg Acc: 0.3771
2022-01-15 23:57:44,870 Epoch[094/300], Step[1200/1252], Avg Loss: 3.6210, Avg Acc: 0.3776
2022-01-15 23:59:12,629 Epoch[094/300], Step[1250/1252], Avg Loss: 3.6233, Avg Acc: 0.3766
2022-01-15 23:59:19,868 ----- Epoch[094/300], Train Loss: 3.6232, Train Acc: 0.3765, time: 2309.11, Best Val(epoch92) Acc@1: 0.6919
2022-01-15 23:59:19,868 ----- Validation after Epoch: 94
2022-01-16 00:00:41,318 Val Step[0000/1563], Avg Loss: 1.0296, Avg Acc@1: 0.7500, Avg Acc@5: 0.9688
2022-01-16 00:00:43,156 Val Step[0050/1563], Avg Loss: 1.3348, Avg Acc@1: 0.7034, Avg Acc@5: 0.9013
2022-01-16 00:00:45,009 Val Step[0100/1563], Avg Loss: 1.3648, Avg Acc@1: 0.6890, Avg Acc@5: 0.8973
2022-01-16 00:00:46,897 Val Step[0150/1563], Avg Loss: 1.3591, Avg Acc@1: 0.6918, Avg Acc@5: 0.8957
2022-01-16 00:00:48,787 Val Step[0200/1563], Avg Loss: 1.3570, Avg Acc@1: 0.6965, Avg Acc@5: 0.8937
2022-01-16 00:00:50,616 Val Step[0250/1563], Avg Loss: 1.3438, Avg Acc@1: 0.6990, Avg Acc@5: 0.8944
2022-01-16 00:00:52,434 Val Step[0300/1563], Avg Loss: 1.3412, Avg Acc@1: 0.6994, Avg Acc@5: 0.8946
2022-01-16 00:00:54,268 Val Step[0350/1563], Avg Loss: 1.3431, Avg Acc@1: 0.6998, Avg Acc@5: 0.8952
2022-01-16 00:00:56,123 Val Step[0400/1563], Avg Loss: 1.3417, Avg Acc@1: 0.6993, Avg Acc@5: 0.8950
2022-01-16 00:00:58,033 Val Step[0450/1563], Avg Loss: 1.3477, Avg Acc@1: 0.6969, Avg Acc@5: 0.8944
2022-01-16 00:00:59,860 Val Step[0500/1563], Avg Loss: 1.3513, Avg Acc@1: 0.6958, Avg Acc@5: 0.8941
2022-01-16 00:01:01,746 Val Step[0550/1563], Avg Loss: 1.3515, Avg Acc@1: 0.6944, Avg Acc@5: 0.8947
2022-01-16 00:01:03,602 Val Step[0600/1563], Avg Loss: 1.3534, Avg Acc@1: 0.6932, Avg Acc@5: 0.8948
2022-01-16 00:01:05,456 Val Step[0650/1563], Avg Loss: 1.3548, Avg Acc@1: 0.6941, Avg Acc@5: 0.8946
2022-01-16 00:01:07,321 Val Step[0700/1563], Avg Loss: 1.3504, Avg Acc@1: 0.6958, Avg Acc@5: 0.8955
2022-01-16 00:01:09,230 Val Step[0750/1563], Avg Loss: 1.3561, Avg Acc@1: 0.6954, Avg Acc@5: 0.8946
2022-01-16 00:01:11,035 Val Step[0800/1563], Avg Loss: 1.3538, Avg Acc@1: 0.6965, Avg Acc@5: 0.8949
2022-01-16 00:01:12,878 Val Step[0850/1563], Avg Loss: 1.3549, Avg Acc@1: 0.6957, Avg Acc@5: 0.8948
2022-01-16 00:01:14,711 Val Step[0900/1563], Avg Loss: 1.3525, Avg Acc@1: 0.6962, Avg Acc@5: 0.8952
2022-01-16 00:01:16,514 Val Step[0950/1563], Avg Loss: 1.3529, Avg Acc@1: 0.6958, Avg Acc@5: 0.8956
2022-01-16 00:01:18,373 Val Step[1000/1563], Avg Loss: 1.3537, Avg Acc@1: 0.6956, Avg Acc@5: 0.8952
2022-01-16 00:01:20,250 Val Step[1050/1563], Avg Loss: 1.3563, Avg Acc@1: 0.6950, Avg Acc@5: 0.8945
2022-01-16 00:01:22,197 Val Step[1100/1563], Avg Loss: 1.3563, Avg Acc@1: 0.6950, Avg Acc@5: 0.8945
2022-01-16 00:01:24,065 Val Step[1150/1563], Avg Loss: 1.3550, Avg Acc@1: 0.6951, Avg Acc@5: 0.8946
2022-01-16 00:01:25,896 Val Step[1200/1563], Avg Loss: 1.3556, Avg Acc@1: 0.6947, Avg Acc@5: 0.8946
2022-01-16 00:01:27,704 Val Step[1250/1563], Avg Loss: 1.3538, Avg Acc@1: 0.6947, Avg Acc@5: 0.8951
2022-01-16 00:01:29,493 Val Step[1300/1563], Avg Loss: 1.3572, Avg Acc@1: 0.6941, Avg Acc@5: 0.8949
2022-01-16 00:01:31,359 Val Step[1350/1563], Avg Loss: 1.3568, Avg Acc@1: 0.6938, Avg Acc@5: 0.8948
2022-01-16 00:01:33,286 Val Step[1400/1563], Avg Loss: 1.3567, Avg Acc@1: 0.6933, Avg Acc@5: 0.8947
2022-01-16 00:01:35,159 Val Step[1450/1563], Avg Loss: 1.3575, Avg Acc@1: 0.6933, Avg Acc@5: 0.8945
2022-01-16 00:01:37,016 Val Step[1500/1563], Avg Loss: 1.3569, Avg Acc@1: 0.6936, Avg Acc@5: 0.8948
2022-01-16 00:01:38,859 Val Step[1550/1563], Avg Loss: 1.3573, Avg Acc@1: 0.6936, Avg Acc@5: 0.8945
2022-01-16 00:01:40,800 ----- Epoch[094/300], Validation Loss: 1.3572, Validation Acc@1: 0.6937, Validation Acc@5: 0.8947, time: 140.93
2022-01-16 00:01:41,936 the pre best model acc:0.6919, at epoch 92
2022-01-16 00:01:41,937 current best model acc:0.6937, at epoch 94
2022-01-16 00:01:41,937 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-16 00:01:41,937 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-16 00:01:41,937 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-16 00:01:41,937 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-16 00:01:41,937 Now training epoch 95. LR=0.000835
2022-01-16 00:03:38,360 Epoch[095/300], Step[0000/1252], Avg Loss: 3.7811, Avg Acc: 0.2959
2022-01-16 00:05:07,311 Epoch[095/300], Step[0050/1252], Avg Loss: 3.6184, Avg Acc: 0.3790
2022-01-16 00:06:35,082 Epoch[095/300], Step[0100/1252], Avg Loss: 3.5809, Avg Acc: 0.3719
2022-01-16 00:08:02,661 Epoch[095/300], Step[0150/1252], Avg Loss: 3.5703, Avg Acc: 0.3746
2022-01-16 00:09:32,118 Epoch[095/300], Step[0200/1252], Avg Loss: 3.5641, Avg Acc: 0.3768
2022-01-16 00:11:00,153 Epoch[095/300], Step[0250/1252], Avg Loss: 3.5734, Avg Acc: 0.3796
2022-01-16 00:12:27,926 Epoch[095/300], Step[0300/1252], Avg Loss: 3.5847, Avg Acc: 0.3824
2022-01-16 00:13:55,327 Epoch[095/300], Step[0350/1252], Avg Loss: 3.5783, Avg Acc: 0.3856
2022-01-16 00:15:22,593 Epoch[095/300], Step[0400/1252], Avg Loss: 3.5762, Avg Acc: 0.3849
2022-01-16 00:16:49,953 Epoch[095/300], Step[0450/1252], Avg Loss: 3.5793, Avg Acc: 0.3818
2022-01-16 00:18:18,294 Epoch[095/300], Step[0500/1252], Avg Loss: 3.5847, Avg Acc: 0.3840
2022-01-16 00:19:45,477 Epoch[095/300], Step[0550/1252], Avg Loss: 3.5811, Avg Acc: 0.3857
2022-01-16 00:21:14,176 Epoch[095/300], Step[0600/1252], Avg Loss: 3.5811, Avg Acc: 0.3854
2022-01-16 00:22:43,252 Epoch[095/300], Step[0650/1252], Avg Loss: 3.5866, Avg Acc: 0.3840
2022-01-16 00:24:11,474 Epoch[095/300], Step[0700/1252], Avg Loss: 3.5872, Avg Acc: 0.3826
2022-01-16 00:25:39,084 Epoch[095/300], Step[0750/1252], Avg Loss: 3.5906, Avg Acc: 0.3835
2022-01-16 00:27:06,625 Epoch[095/300], Step[0800/1252], Avg Loss: 3.5973, Avg Acc: 0.3827
2022-01-16 00:28:35,893 Epoch[095/300], Step[0850/1252], Avg Loss: 3.6018, Avg Acc: 0.3826
2022-01-16 00:30:04,196 Epoch[095/300], Step[0900/1252], Avg Loss: 3.5982, Avg Acc: 0.3845
2022-01-16 00:31:32,716 Epoch[095/300], Step[0950/1252], Avg Loss: 3.6039, Avg Acc: 0.3843
2022-01-16 00:33:02,103 Epoch[095/300], Step[1000/1252], Avg Loss: 3.6035, Avg Acc: 0.3842
2022-01-16 00:34:31,366 Epoch[095/300], Step[1050/1252], Avg Loss: 3.6062, Avg Acc: 0.3849
2022-01-16 00:35:59,700 Epoch[095/300], Step[1100/1252], Avg Loss: 3.6052, Avg Acc: 0.3845
2022-01-16 00:37:28,199 Epoch[095/300], Step[1150/1252], Avg Loss: 3.6067, Avg Acc: 0.3846
2022-01-16 00:38:57,488 Epoch[095/300], Step[1200/1252], Avg Loss: 3.6066, Avg Acc: 0.3842
2022-01-16 00:40:26,386 Epoch[095/300], Step[1250/1252], Avg Loss: 3.6064, Avg Acc: 0.3837
2022-01-16 00:40:33,434 ----- Epoch[095/300], Train Loss: 3.6063, Train Acc: 0.3837, time: 2331.49, Best Val(epoch94) Acc@1: 0.6937
2022-01-16 00:40:33,434 Now training epoch 96. LR=0.000831
2022-01-16 00:42:21,893 Epoch[096/300], Step[0000/1252], Avg Loss: 3.8732, Avg Acc: 0.0781
2022-01-16 00:43:48,288 Epoch[096/300], Step[0050/1252], Avg Loss: 3.6656, Avg Acc: 0.3599
2022-01-16 00:45:14,775 Epoch[096/300], Step[0100/1252], Avg Loss: 3.6147, Avg Acc: 0.3712
2022-01-16 00:46:42,156 Epoch[096/300], Step[0150/1252], Avg Loss: 3.6083, Avg Acc: 0.3713
2022-01-16 00:48:08,138 Epoch[096/300], Step[0200/1252], Avg Loss: 3.5966, Avg Acc: 0.3774
2022-01-16 00:49:35,531 Epoch[096/300], Step[0250/1252], Avg Loss: 3.6164, Avg Acc: 0.3760
2022-01-16 00:51:03,095 Epoch[096/300], Step[0300/1252], Avg Loss: 3.6065, Avg Acc: 0.3744
2022-01-16 00:52:29,692 Epoch[096/300], Step[0350/1252], Avg Loss: 3.6029, Avg Acc: 0.3793
2022-01-16 00:53:55,721 Epoch[096/300], Step[0400/1252], Avg Loss: 3.6044, Avg Acc: 0.3789
2022-01-16 00:55:22,235 Epoch[096/300], Step[0450/1252], Avg Loss: 3.6043, Avg Acc: 0.3777
2022-01-16 00:56:49,820 Epoch[096/300], Step[0500/1252], Avg Loss: 3.6036, Avg Acc: 0.3799
2022-01-16 00:58:17,538 Epoch[096/300], Step[0550/1252], Avg Loss: 3.6010, Avg Acc: 0.3812
2022-01-16 00:59:45,791 Epoch[096/300], Step[0600/1252], Avg Loss: 3.5999, Avg Acc: 0.3835
2022-01-16 01:01:14,050 Epoch[096/300], Step[0650/1252], Avg Loss: 3.6015, Avg Acc: 0.3824
2022-01-16 01:02:42,250 Epoch[096/300], Step[0700/1252], Avg Loss: 3.6005, Avg Acc: 0.3816
2022-01-16 01:04:11,376 Epoch[096/300], Step[0750/1252], Avg Loss: 3.6007, Avg Acc: 0.3812
2022-01-16 01:05:40,667 Epoch[096/300], Step[0800/1252], Avg Loss: 3.6020, Avg Acc: 0.3794
2022-01-16 01:07:08,689 Epoch[096/300], Step[0850/1252], Avg Loss: 3.6017, Avg Acc: 0.3796
2022-01-16 01:08:37,105 Epoch[096/300], Step[0900/1252], Avg Loss: 3.5993, Avg Acc: 0.3801
2022-01-16 01:10:06,159 Epoch[096/300], Step[0950/1252], Avg Loss: 3.5982, Avg Acc: 0.3802
2022-01-16 01:11:34,484 Epoch[096/300], Step[1000/1252], Avg Loss: 3.6016, Avg Acc: 0.3794
2022-01-16 01:13:03,443 Epoch[096/300], Step[1050/1252], Avg Loss: 3.6062, Avg Acc: 0.3786
2022-01-16 01:14:32,314 Epoch[096/300], Step[1100/1252], Avg Loss: 3.6058, Avg Acc: 0.3802
2022-01-16 01:16:00,185 Epoch[096/300], Step[1150/1252], Avg Loss: 3.6042, Avg Acc: 0.3799
2022-01-16 01:17:28,442 Epoch[096/300], Step[1200/1252], Avg Loss: 3.6063, Avg Acc: 0.3795
2022-01-16 01:18:56,508 Epoch[096/300], Step[1250/1252], Avg Loss: 3.6056, Avg Acc: 0.3800
2022-01-16 01:19:03,656 ----- Epoch[096/300], Train Loss: 3.6056, Train Acc: 0.3800, time: 2310.22, Best Val(epoch94) Acc@1: 0.6937
2022-01-16 01:19:03,656 ----- Validation after Epoch: 96
2022-01-16 01:20:17,744 Val Step[0000/1563], Avg Loss: 1.1552, Avg Acc@1: 0.7188, Avg Acc@5: 0.9375
2022-01-16 01:20:19,605 Val Step[0050/1563], Avg Loss: 1.3370, Avg Acc@1: 0.7004, Avg Acc@5: 0.9013
2022-01-16 01:20:21,506 Val Step[0100/1563], Avg Loss: 1.3559, Avg Acc@1: 0.6943, Avg Acc@5: 0.9019
2022-01-16 01:20:23,349 Val Step[0150/1563], Avg Loss: 1.3523, Avg Acc@1: 0.6972, Avg Acc@5: 0.9009
2022-01-16 01:20:25,141 Val Step[0200/1563], Avg Loss: 1.3495, Avg Acc@1: 0.6988, Avg Acc@5: 0.9003
2022-01-16 01:20:26,935 Val Step[0250/1563], Avg Loss: 1.3392, Avg Acc@1: 0.6996, Avg Acc@5: 0.9008
2022-01-16 01:20:28,746 Val Step[0300/1563], Avg Loss: 1.3398, Avg Acc@1: 0.6999, Avg Acc@5: 0.8996
2022-01-16 01:20:30,545 Val Step[0350/1563], Avg Loss: 1.3485, Avg Acc@1: 0.6988, Avg Acc@5: 0.8984
2022-01-16 01:20:32,373 Val Step[0400/1563], Avg Loss: 1.3479, Avg Acc@1: 0.6981, Avg Acc@5: 0.8985
2022-01-16 01:20:34,312 Val Step[0450/1563], Avg Loss: 1.3532, Avg Acc@1: 0.6971, Avg Acc@5: 0.8981
2022-01-16 01:20:36,118 Val Step[0500/1563], Avg Loss: 1.3529, Avg Acc@1: 0.6974, Avg Acc@5: 0.8988
2022-01-16 01:20:37,920 Val Step[0550/1563], Avg Loss: 1.3525, Avg Acc@1: 0.6974, Avg Acc@5: 0.8988
2022-01-16 01:20:39,808 Val Step[0600/1563], Avg Loss: 1.3517, Avg Acc@1: 0.6965, Avg Acc@5: 0.8995
2022-01-16 01:20:41,729 Val Step[0650/1563], Avg Loss: 1.3514, Avg Acc@1: 0.6965, Avg Acc@5: 0.8998
2022-01-16 01:20:43,615 Val Step[0700/1563], Avg Loss: 1.3495, Avg Acc@1: 0.6969, Avg Acc@5: 0.9001
2022-01-16 01:20:45,574 Val Step[0750/1563], Avg Loss: 1.3543, Avg Acc@1: 0.6959, Avg Acc@5: 0.8991
2022-01-16 01:20:47,531 Val Step[0800/1563], Avg Loss: 1.3540, Avg Acc@1: 0.6962, Avg Acc@5: 0.8988
2022-01-16 01:20:49,411 Val Step[0850/1563], Avg Loss: 1.3553, Avg Acc@1: 0.6960, Avg Acc@5: 0.8991
2022-01-16 01:20:51,216 Val Step[0900/1563], Avg Loss: 1.3518, Avg Acc@1: 0.6973, Avg Acc@5: 0.8997
2022-01-16 01:20:53,285 Val Step[0950/1563], Avg Loss: 1.3522, Avg Acc@1: 0.6976, Avg Acc@5: 0.8997
2022-01-16 01:20:55,344 Val Step[1000/1563], Avg Loss: 1.3518, Avg Acc@1: 0.6977, Avg Acc@5: 0.8998
2022-01-16 01:20:57,390 Val Step[1050/1563], Avg Loss: 1.3552, Avg Acc@1: 0.6973, Avg Acc@5: 0.8991
2022-01-16 01:20:59,154 Val Step[1100/1563], Avg Loss: 1.3544, Avg Acc@1: 0.6975, Avg Acc@5: 0.8992
2022-01-16 01:21:00,965 Val Step[1150/1563], Avg Loss: 1.3531, Avg Acc@1: 0.6975, Avg Acc@5: 0.8996
2022-01-16 01:21:02,790 Val Step[1200/1563], Avg Loss: 1.3533, Avg Acc@1: 0.6974, Avg Acc@5: 0.8996
2022-01-16 01:21:04,727 Val Step[1250/1563], Avg Loss: 1.3524, Avg Acc@1: 0.6973, Avg Acc@5: 0.8997
2022-01-16 01:21:06,654 Val Step[1300/1563], Avg Loss: 1.3553, Avg Acc@1: 0.6968, Avg Acc@5: 0.8994
2022-01-16 01:21:08,436 Val Step[1350/1563], Avg Loss: 1.3555, Avg Acc@1: 0.6968, Avg Acc@5: 0.8992
2022-01-16 01:21:10,309 Val Step[1400/1563], Avg Loss: 1.3541, Avg Acc@1: 0.6965, Avg Acc@5: 0.8992
2022-01-16 01:21:12,274 Val Step[1450/1563], Avg Loss: 1.3549, Avg Acc@1: 0.6964, Avg Acc@5: 0.8989
2022-01-16 01:21:14,233 Val Step[1500/1563], Avg Loss: 1.3546, Avg Acc@1: 0.6966, Avg Acc@5: 0.8990
2022-01-16 01:21:16,004 Val Step[1550/1563], Avg Loss: 1.3560, Avg Acc@1: 0.6962, Avg Acc@5: 0.8986
2022-01-16 01:21:17,901 ----- Epoch[096/300], Validation Loss: 1.3559, Validation Acc@1: 0.6962, Validation Acc@5: 0.8987, time: 134.24
2022-01-16 01:21:19,036 the pre best model acc:0.6937, at epoch 94
2022-01-16 01:21:19,037 current best model acc:0.6962, at epoch 96
2022-01-16 01:21:19,037 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-16 01:21:19,037 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-16 01:21:19,037 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-16 01:21:19,037 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-16 01:21:19,037 Now training epoch 97. LR=0.000826
2022-01-16 01:23:07,145 Epoch[097/300], Step[0000/1252], Avg Loss: 3.6275, Avg Acc: 0.4707
2022-01-16 01:24:34,916 Epoch[097/300], Step[0050/1252], Avg Loss: 3.5983, Avg Acc: 0.3747
2022-01-16 01:26:01,838 Epoch[097/300], Step[0100/1252], Avg Loss: 3.5659, Avg Acc: 0.3911
2022-01-16 01:27:30,556 Epoch[097/300], Step[0150/1252], Avg Loss: 3.5670, Avg Acc: 0.3921
2022-01-16 01:28:58,241 Epoch[097/300], Step[0200/1252], Avg Loss: 3.5547, Avg Acc: 0.3947
2022-01-16 01:30:25,678 Epoch[097/300], Step[0250/1252], Avg Loss: 3.5550, Avg Acc: 0.3921
2022-01-16 01:31:54,152 Epoch[097/300], Step[0300/1252], Avg Loss: 3.5744, Avg Acc: 0.3890
2022-01-16 01:33:22,316 Epoch[097/300], Step[0350/1252], Avg Loss: 3.5722, Avg Acc: 0.3882
2022-01-16 01:34:50,653 Epoch[097/300], Step[0400/1252], Avg Loss: 3.5818, Avg Acc: 0.3843
2022-01-16 01:36:20,156 Epoch[097/300], Step[0450/1252], Avg Loss: 3.5813, Avg Acc: 0.3822
2022-01-16 01:37:47,799 Epoch[097/300], Step[0500/1252], Avg Loss: 3.5848, Avg Acc: 0.3833
2022-01-16 01:39:16,029 Epoch[097/300], Step[0550/1252], Avg Loss: 3.5860, Avg Acc: 0.3794
2022-01-16 01:40:45,028 Epoch[097/300], Step[0600/1252], Avg Loss: 3.5867, Avg Acc: 0.3783
2022-01-16 01:42:13,557 Epoch[097/300], Step[0650/1252], Avg Loss: 3.5874, Avg Acc: 0.3780
2022-01-16 01:43:40,880 Epoch[097/300], Step[0700/1252], Avg Loss: 3.5867, Avg Acc: 0.3777
2022-01-16 01:45:08,627 Epoch[097/300], Step[0750/1252], Avg Loss: 3.5861, Avg Acc: 0.3786
2022-01-16 01:46:37,112 Epoch[097/300], Step[0800/1252], Avg Loss: 3.5868, Avg Acc: 0.3788
2022-01-16 01:48:04,028 Epoch[097/300], Step[0850/1252], Avg Loss: 3.5885, Avg Acc: 0.3787
2022-01-16 01:49:32,696 Epoch[097/300], Step[0900/1252], Avg Loss: 3.5902, Avg Acc: 0.3787
2022-01-16 01:51:00,868 Epoch[097/300], Step[0950/1252], Avg Loss: 3.5911, Avg Acc: 0.3787
2022-01-16 01:52:28,712 Epoch[097/300], Step[1000/1252], Avg Loss: 3.5945, Avg Acc: 0.3793
2022-01-16 01:53:56,902 Epoch[097/300], Step[1050/1252], Avg Loss: 3.5953, Avg Acc: 0.3782
2022-01-16 01:55:24,944 Epoch[097/300], Step[1100/1252], Avg Loss: 3.5922, Avg Acc: 0.3779
2022-01-16 01:56:53,242 Epoch[097/300], Step[1150/1252], Avg Loss: 3.5915, Avg Acc: 0.3783
2022-01-16 01:58:21,411 Epoch[097/300], Step[1200/1252], Avg Loss: 3.5899, Avg Acc: 0.3796
2022-01-16 01:59:47,703 Epoch[097/300], Step[1250/1252], Avg Loss: 3.5925, Avg Acc: 0.3796
2022-01-16 01:59:55,022 ----- Epoch[097/300], Train Loss: 3.5925, Train Acc: 0.3796, time: 2315.98, Best Val(epoch96) Acc@1: 0.6962
2022-01-16 01:59:55,022 Now training epoch 98. LR=0.000822
2022-01-16 02:01:41,130 Epoch[098/300], Step[0000/1252], Avg Loss: 3.7409, Avg Acc: 0.2744
2022-01-16 02:03:08,510 Epoch[098/300], Step[0050/1252], Avg Loss: 3.5959, Avg Acc: 0.3811
2022-01-16 02:04:33,373 Epoch[098/300], Step[0100/1252], Avg Loss: 3.5843, Avg Acc: 0.3933
2022-01-16 02:06:00,798 Epoch[098/300], Step[0150/1252], Avg Loss: 3.5835, Avg Acc: 0.3805
2022-01-16 02:07:28,570 Epoch[098/300], Step[0200/1252], Avg Loss: 3.5822, Avg Acc: 0.3867
2022-01-16 02:08:55,299 Epoch[098/300], Step[0250/1252], Avg Loss: 3.5772, Avg Acc: 0.3839
2022-01-16 02:10:22,643 Epoch[098/300], Step[0300/1252], Avg Loss: 3.5735, Avg Acc: 0.3835
2022-01-16 02:11:50,093 Epoch[098/300], Step[0350/1252], Avg Loss: 3.5724, Avg Acc: 0.3837
2022-01-16 02:13:17,267 Epoch[098/300], Step[0400/1252], Avg Loss: 3.5832, Avg Acc: 0.3820
2022-01-16 02:14:43,177 Epoch[098/300], Step[0450/1252], Avg Loss: 3.5861, Avg Acc: 0.3817
2022-01-16 02:16:10,986 Epoch[098/300], Step[0500/1252], Avg Loss: 3.5873, Avg Acc: 0.3811
2022-01-16 02:17:37,507 Epoch[098/300], Step[0550/1252], Avg Loss: 3.5854, Avg Acc: 0.3830
2022-01-16 02:19:05,202 Epoch[098/300], Step[0600/1252], Avg Loss: 3.5876, Avg Acc: 0.3828
2022-01-16 02:20:32,701 Epoch[098/300], Step[0650/1252], Avg Loss: 3.5917, Avg Acc: 0.3824
2022-01-16 02:22:01,173 Epoch[098/300], Step[0700/1252], Avg Loss: 3.5923, Avg Acc: 0.3804
2022-01-16 02:23:28,597 Epoch[098/300], Step[0750/1252], Avg Loss: 3.5924, Avg Acc: 0.3817
2022-01-16 02:24:56,332 Epoch[098/300], Step[0800/1252], Avg Loss: 3.5959, Avg Acc: 0.3806
2022-01-16 02:26:24,285 Epoch[098/300], Step[0850/1252], Avg Loss: 3.5966, Avg Acc: 0.3807
2022-01-16 02:27:52,118 Epoch[098/300], Step[0900/1252], Avg Loss: 3.5958, Avg Acc: 0.3823
2022-01-16 02:29:20,421 Epoch[098/300], Step[0950/1252], Avg Loss: 3.5917, Avg Acc: 0.3821
2022-01-16 02:30:48,835 Epoch[098/300], Step[1000/1252], Avg Loss: 3.5921, Avg Acc: 0.3819
2022-01-16 02:32:17,779 Epoch[098/300], Step[1050/1252], Avg Loss: 3.5937, Avg Acc: 0.3806
2022-01-16 02:33:45,890 Epoch[098/300], Step[1100/1252], Avg Loss: 3.5928, Avg Acc: 0.3800
2022-01-16 02:35:14,968 Epoch[098/300], Step[1150/1252], Avg Loss: 3.5891, Avg Acc: 0.3795
2022-01-16 02:36:42,590 Epoch[098/300], Step[1200/1252], Avg Loss: 3.5886, Avg Acc: 0.3801
2022-01-16 02:38:11,953 Epoch[098/300], Step[1250/1252], Avg Loss: 3.5905, Avg Acc: 0.3793
2022-01-16 02:38:19,178 ----- Epoch[098/300], Train Loss: 3.5905, Train Acc: 0.3793, time: 2304.15, Best Val(epoch96) Acc@1: 0.6962
2022-01-16 02:38:19,178 ----- Validation after Epoch: 98
2022-01-16 02:39:30,465 Val Step[0000/1563], Avg Loss: 1.1978, Avg Acc@1: 0.7188, Avg Acc@5: 0.9688
2022-01-16 02:39:32,669 Val Step[0050/1563], Avg Loss: 1.3767, Avg Acc@1: 0.6924, Avg Acc@5: 0.8964
2022-01-16 02:39:34,604 Val Step[0100/1563], Avg Loss: 1.3800, Avg Acc@1: 0.6872, Avg Acc@5: 0.8970
2022-01-16 02:39:36,386 Val Step[0150/1563], Avg Loss: 1.3721, Avg Acc@1: 0.6908, Avg Acc@5: 0.8942
2022-01-16 02:39:38,189 Val Step[0200/1563], Avg Loss: 1.3717, Avg Acc@1: 0.6948, Avg Acc@5: 0.8938
2022-01-16 02:39:40,065 Val Step[0250/1563], Avg Loss: 1.3563, Avg Acc@1: 0.7003, Avg Acc@5: 0.8947
2022-01-16 02:39:41,895 Val Step[0300/1563], Avg Loss: 1.3534, Avg Acc@1: 0.7025, Avg Acc@5: 0.8940
2022-01-16 02:39:43,684 Val Step[0350/1563], Avg Loss: 1.3588, Avg Acc@1: 0.7022, Avg Acc@5: 0.8940
2022-01-16 02:39:45,573 Val Step[0400/1563], Avg Loss: 1.3585, Avg Acc@1: 0.7018, Avg Acc@5: 0.8941
2022-01-16 02:39:47,383 Val Step[0450/1563], Avg Loss: 1.3639, Avg Acc@1: 0.6996, Avg Acc@5: 0.8941
2022-01-16 02:39:49,442 Val Step[0500/1563], Avg Loss: 1.3701, Avg Acc@1: 0.6979, Avg Acc@5: 0.8935
2022-01-16 02:39:51,505 Val Step[0550/1563], Avg Loss: 1.3705, Avg Acc@1: 0.6974, Avg Acc@5: 0.8946
2022-01-16 02:39:53,573 Val Step[0600/1563], Avg Loss: 1.3709, Avg Acc@1: 0.6976, Avg Acc@5: 0.8955
2022-01-16 02:39:55,621 Val Step[0650/1563], Avg Loss: 1.3705, Avg Acc@1: 0.6977, Avg Acc@5: 0.8958
2022-01-16 02:39:57,650 Val Step[0700/1563], Avg Loss: 1.3674, Avg Acc@1: 0.6979, Avg Acc@5: 0.8960
2022-01-16 02:39:59,677 Val Step[0750/1563], Avg Loss: 1.3730, Avg Acc@1: 0.6965, Avg Acc@5: 0.8946
2022-01-16 02:40:01,737 Val Step[0800/1563], Avg Loss: 1.3725, Avg Acc@1: 0.6971, Avg Acc@5: 0.8941
2022-01-16 02:40:03,802 Val Step[0850/1563], Avg Loss: 1.3730, Avg Acc@1: 0.6968, Avg Acc@5: 0.8939
2022-01-16 02:40:05,857 Val Step[0900/1563], Avg Loss: 1.3695, Avg Acc@1: 0.6973, Avg Acc@5: 0.8942
2022-01-16 02:40:07,983 Val Step[0950/1563], Avg Loss: 1.3698, Avg Acc@1: 0.6971, Avg Acc@5: 0.8945
2022-01-16 02:40:09,867 Val Step[1000/1563], Avg Loss: 1.3698, Avg Acc@1: 0.6971, Avg Acc@5: 0.8946
2022-01-16 02:40:11,820 Val Step[1050/1563], Avg Loss: 1.3731, Avg Acc@1: 0.6964, Avg Acc@5: 0.8940
2022-01-16 02:40:13,714 Val Step[1100/1563], Avg Loss: 1.3715, Avg Acc@1: 0.6968, Avg Acc@5: 0.8944
2022-01-16 02:40:15,488 Val Step[1150/1563], Avg Loss: 1.3701, Avg Acc@1: 0.6968, Avg Acc@5: 0.8948
2022-01-16 02:40:17,281 Val Step[1200/1563], Avg Loss: 1.3690, Avg Acc@1: 0.6968, Avg Acc@5: 0.8950
2022-01-16 02:40:19,067 Val Step[1250/1563], Avg Loss: 1.3683, Avg Acc@1: 0.6969, Avg Acc@5: 0.8953
2022-01-16 02:40:20,894 Val Step[1300/1563], Avg Loss: 1.3712, Avg Acc@1: 0.6967, Avg Acc@5: 0.8949
2022-01-16 02:40:22,774 Val Step[1350/1563], Avg Loss: 1.3716, Avg Acc@1: 0.6964, Avg Acc@5: 0.8948
2022-01-16 02:40:24,666 Val Step[1400/1563], Avg Loss: 1.3713, Avg Acc@1: 0.6960, Avg Acc@5: 0.8948
2022-01-16 02:40:26,489 Val Step[1450/1563], Avg Loss: 1.3714, Avg Acc@1: 0.6961, Avg Acc@5: 0.8948
2022-01-16 02:40:28,312 Val Step[1500/1563], Avg Loss: 1.3703, Avg Acc@1: 0.6964, Avg Acc@5: 0.8949
2022-01-16 02:40:30,080 Val Step[1550/1563], Avg Loss: 1.3713, Avg Acc@1: 0.6961, Avg Acc@5: 0.8948
2022-01-16 02:40:32,013 ----- Epoch[098/300], Validation Loss: 1.3714, Validation Acc@1: 0.6962, Validation Acc@5: 0.8949, time: 132.83
2022-01-16 02:40:32,013 Now training epoch 99. LR=0.000818
2022-01-16 02:42:17,226 Epoch[099/300], Step[0000/1252], Avg Loss: 2.9865, Avg Acc: 0.4932
2022-01-16 02:43:44,894 Epoch[099/300], Step[0050/1252], Avg Loss: 3.5916, Avg Acc: 0.3580
2022-01-16 02:45:13,338 Epoch[099/300], Step[0100/1252], Avg Loss: 3.5823, Avg Acc: 0.3715
2022-01-16 02:46:39,603 Epoch[099/300], Step[0150/1252], Avg Loss: 3.5865, Avg Acc: 0.3800
2022-01-16 02:48:07,985 Epoch[099/300], Step[0200/1252], Avg Loss: 3.5765, Avg Acc: 0.3776
2022-01-16 02:49:36,037 Epoch[099/300], Step[0250/1252], Avg Loss: 3.5566, Avg Acc: 0.3832
2022-01-16 02:51:03,631 Epoch[099/300], Step[0300/1252], Avg Loss: 3.5524, Avg Acc: 0.3836
2022-01-16 02:52:31,389 Epoch[099/300], Step[0350/1252], Avg Loss: 3.5602, Avg Acc: 0.3794
2022-01-16 02:53:59,169 Epoch[099/300], Step[0400/1252], Avg Loss: 3.5658, Avg Acc: 0.3792
2022-01-16 02:55:24,788 Epoch[099/300], Step[0450/1252], Avg Loss: 3.5754, Avg Acc: 0.3779
2022-01-16 02:56:52,143 Epoch[099/300], Step[0500/1252], Avg Loss: 3.5798, Avg Acc: 0.3800
2022-01-16 02:58:18,921 Epoch[099/300], Step[0550/1252], Avg Loss: 3.5789, Avg Acc: 0.3789
2022-01-16 02:59:45,679 Epoch[099/300], Step[0600/1252], Avg Loss: 3.5783, Avg Acc: 0.3796
2022-01-16 03:01:13,854 Epoch[099/300], Step[0650/1252], Avg Loss: 3.5864, Avg Acc: 0.3786
2022-01-16 03:02:42,629 Epoch[099/300], Step[0700/1252], Avg Loss: 3.5856, Avg Acc: 0.3787
2022-01-16 03:04:10,862 Epoch[099/300], Step[0750/1252], Avg Loss: 3.5811, Avg Acc: 0.3794
2022-01-16 03:05:38,553 Epoch[099/300], Step[0800/1252], Avg Loss: 3.5786, Avg Acc: 0.3807
2022-01-16 03:07:07,053 Epoch[099/300], Step[0850/1252], Avg Loss: 3.5831, Avg Acc: 0.3804
2022-01-16 03:08:35,446 Epoch[099/300], Step[0900/1252], Avg Loss: 3.5860, Avg Acc: 0.3790
2022-01-16 03:10:02,234 Epoch[099/300], Step[0950/1252], Avg Loss: 3.5864, Avg Acc: 0.3794
2022-01-16 03:11:28,949 Epoch[099/300], Step[1000/1252], Avg Loss: 3.5857, Avg Acc: 0.3813
2022-01-16 03:12:57,098 Epoch[099/300], Step[1050/1252], Avg Loss: 3.5853, Avg Acc: 0.3809
2022-01-16 03:14:25,409 Epoch[099/300], Step[1100/1252], Avg Loss: 3.5893, Avg Acc: 0.3813
2022-01-16 03:15:52,677 Epoch[099/300], Step[1150/1252], Avg Loss: 3.5912, Avg Acc: 0.3796
2022-01-16 03:17:21,058 Epoch[099/300], Step[1200/1252], Avg Loss: 3.5949, Avg Acc: 0.3792
2022-01-16 03:18:50,142 Epoch[099/300], Step[1250/1252], Avg Loss: 3.5944, Avg Acc: 0.3791
2022-01-16 03:18:57,223 ----- Epoch[099/300], Train Loss: 3.5944, Train Acc: 0.3791, time: 2305.21, Best Val(epoch96) Acc@1: 0.6962
2022-01-16 03:18:57,224 Now training epoch 100. LR=0.000814
2022-01-16 03:20:43,316 Epoch[100/300], Step[0000/1252], Avg Loss: 3.0329, Avg Acc: 0.4736
2022-01-16 03:22:11,454 Epoch[100/300], Step[0050/1252], Avg Loss: 3.6031, Avg Acc: 0.3886
2022-01-16 03:23:39,570 Epoch[100/300], Step[0100/1252], Avg Loss: 3.5707, Avg Acc: 0.3902
2022-01-16 03:25:07,641 Epoch[100/300], Step[0150/1252], Avg Loss: 3.5900, Avg Acc: 0.3870
2022-01-16 03:26:35,100 Epoch[100/300], Step[0200/1252], Avg Loss: 3.5815, Avg Acc: 0.3908
2022-01-16 03:28:03,235 Epoch[100/300], Step[0250/1252], Avg Loss: 3.5861, Avg Acc: 0.3863
2022-01-16 03:29:31,366 Epoch[100/300], Step[0300/1252], Avg Loss: 3.5787, Avg Acc: 0.3874
2022-01-16 03:31:00,182 Epoch[100/300], Step[0350/1252], Avg Loss: 3.5791, Avg Acc: 0.3849
2022-01-16 03:32:29,259 Epoch[100/300], Step[0400/1252], Avg Loss: 3.5844, Avg Acc: 0.3843
2022-01-16 03:33:57,983 Epoch[100/300], Step[0450/1252], Avg Loss: 3.5889, Avg Acc: 0.3831
2022-01-16 03:35:25,603 Epoch[100/300], Step[0500/1252], Avg Loss: 3.5891, Avg Acc: 0.3820
2022-01-16 03:36:52,036 Epoch[100/300], Step[0550/1252], Avg Loss: 3.5878, Avg Acc: 0.3833
2022-01-16 03:38:19,672 Epoch[100/300], Step[0600/1252], Avg Loss: 3.5937, Avg Acc: 0.3813
2022-01-16 03:39:45,878 Epoch[100/300], Step[0650/1252], Avg Loss: 3.5885, Avg Acc: 0.3816
2022-01-16 03:41:14,413 Epoch[100/300], Step[0700/1252], Avg Loss: 3.5879, Avg Acc: 0.3833
2022-01-16 03:42:42,169 Epoch[100/300], Step[0750/1252], Avg Loss: 3.5880, Avg Acc: 0.3819
2022-01-16 03:44:12,068 Epoch[100/300], Step[0800/1252], Avg Loss: 3.5871, Avg Acc: 0.3812
2022-01-16 03:45:41,624 Epoch[100/300], Step[0850/1252], Avg Loss: 3.5845, Avg Acc: 0.3798
2022-01-16 03:47:10,081 Epoch[100/300], Step[0900/1252], Avg Loss: 3.5835, Avg Acc: 0.3797
2022-01-16 03:48:39,352 Epoch[100/300], Step[0950/1252], Avg Loss: 3.5796, Avg Acc: 0.3788
2022-01-16 03:50:08,554 Epoch[100/300], Step[1000/1252], Avg Loss: 3.5811, Avg Acc: 0.3787
2022-01-16 03:51:38,385 Epoch[100/300], Step[1050/1252], Avg Loss: 3.5836, Avg Acc: 0.3774
2022-01-16 03:53:07,948 Epoch[100/300], Step[1100/1252], Avg Loss: 3.5816, Avg Acc: 0.3782
2022-01-16 03:54:36,558 Epoch[100/300], Step[1150/1252], Avg Loss: 3.5832, Avg Acc: 0.3779
2022-01-16 03:56:04,857 Epoch[100/300], Step[1200/1252], Avg Loss: 3.5856, Avg Acc: 0.3786
2022-01-16 03:57:34,502 Epoch[100/300], Step[1250/1252], Avg Loss: 3.5859, Avg Acc: 0.3786
2022-01-16 03:57:41,542 ----- Epoch[100/300], Train Loss: 3.5859, Train Acc: 0.3786, time: 2324.31, Best Val(epoch96) Acc@1: 0.6962
2022-01-16 03:57:41,543 ----- Validation after Epoch: 100
2022-01-16 03:58:56,085 Val Step[0000/1563], Avg Loss: 1.0030, Avg Acc@1: 0.7188, Avg Acc@5: 0.9688
2022-01-16 03:58:57,951 Val Step[0050/1563], Avg Loss: 1.2929, Avg Acc@1: 0.7028, Avg Acc@5: 0.9099
2022-01-16 03:58:59,782 Val Step[0100/1563], Avg Loss: 1.3332, Avg Acc@1: 0.6962, Avg Acc@5: 0.9019
2022-01-16 03:59:01,800 Val Step[0150/1563], Avg Loss: 1.3295, Avg Acc@1: 0.6978, Avg Acc@5: 0.9023
2022-01-16 03:59:03,849 Val Step[0200/1563], Avg Loss: 1.3341, Avg Acc@1: 0.7004, Avg Acc@5: 0.9002
2022-01-16 03:59:05,928 Val Step[0250/1563], Avg Loss: 1.3188, Avg Acc@1: 0.7044, Avg Acc@5: 0.9019
2022-01-16 03:59:08,031 Val Step[0300/1563], Avg Loss: 1.3183, Avg Acc@1: 0.7041, Avg Acc@5: 0.9021
2022-01-16 03:59:10,062 Val Step[0350/1563], Avg Loss: 1.3267, Avg Acc@1: 0.7015, Avg Acc@5: 0.9007
2022-01-16 03:59:12,099 Val Step[0400/1563], Avg Loss: 1.3244, Avg Acc@1: 0.7029, Avg Acc@5: 0.9010
2022-01-16 03:59:14,147 Val Step[0450/1563], Avg Loss: 1.3307, Avg Acc@1: 0.7000, Avg Acc@5: 0.8994
2022-01-16 03:59:16,196 Val Step[0500/1563], Avg Loss: 1.3343, Avg Acc@1: 0.6998, Avg Acc@5: 0.8988
2022-01-16 03:59:18,261 Val Step[0550/1563], Avg Loss: 1.3342, Avg Acc@1: 0.6988, Avg Acc@5: 0.8988
2022-01-16 03:59:20,078 Val Step[0600/1563], Avg Loss: 1.3341, Avg Acc@1: 0.6985, Avg Acc@5: 0.8991
2022-01-16 03:59:21,898 Val Step[0650/1563], Avg Loss: 1.3346, Avg Acc@1: 0.6992, Avg Acc@5: 0.8992
2022-01-16 03:59:23,687 Val Step[0700/1563], Avg Loss: 1.3327, Avg Acc@1: 0.6994, Avg Acc@5: 0.9001
2022-01-16 03:59:25,470 Val Step[0750/1563], Avg Loss: 1.3398, Avg Acc@1: 0.6983, Avg Acc@5: 0.8989
2022-01-16 03:59:27,355 Val Step[0800/1563], Avg Loss: 1.3404, Avg Acc@1: 0.6990, Avg Acc@5: 0.8991
2022-01-16 03:59:29,233 Val Step[0850/1563], Avg Loss: 1.3425, Avg Acc@1: 0.6988, Avg Acc@5: 0.8985
2022-01-16 03:59:31,023 Val Step[0900/1563], Avg Loss: 1.3389, Avg Acc@1: 0.6992, Avg Acc@5: 0.8987
2022-01-16 03:59:32,818 Val Step[0950/1563], Avg Loss: 1.3402, Avg Acc@1: 0.6987, Avg Acc@5: 0.8984
2022-01-16 03:59:34,808 Val Step[1000/1563], Avg Loss: 1.3404, Avg Acc@1: 0.6983, Avg Acc@5: 0.8985
2022-01-16 03:59:36,728 Val Step[1050/1563], Avg Loss: 1.3426, Avg Acc@1: 0.6971, Avg Acc@5: 0.8977
2022-01-16 03:59:38,589 Val Step[1100/1563], Avg Loss: 1.3428, Avg Acc@1: 0.6971, Avg Acc@5: 0.8975
2022-01-16 03:59:40,385 Val Step[1150/1563], Avg Loss: 1.3414, Avg Acc@1: 0.6981, Avg Acc@5: 0.8976
2022-01-16 03:59:42,290 Val Step[1200/1563], Avg Loss: 1.3403, Avg Acc@1: 0.6979, Avg Acc@5: 0.8979
2022-01-16 03:59:44,104 Val Step[1250/1563], Avg Loss: 1.3396, Avg Acc@1: 0.6983, Avg Acc@5: 0.8981
2022-01-16 03:59:45,888 Val Step[1300/1563], Avg Loss: 1.3430, Avg Acc@1: 0.6980, Avg Acc@5: 0.8979
2022-01-16 03:59:47,707 Val Step[1350/1563], Avg Loss: 1.3419, Avg Acc@1: 0.6980, Avg Acc@5: 0.8977
2022-01-16 03:59:49,493 Val Step[1400/1563], Avg Loss: 1.3417, Avg Acc@1: 0.6978, Avg Acc@5: 0.8978
2022-01-16 03:59:51,271 Val Step[1450/1563], Avg Loss: 1.3419, Avg Acc@1: 0.6978, Avg Acc@5: 0.8978
2022-01-16 03:59:53,054 Val Step[1500/1563], Avg Loss: 1.3406, Avg Acc@1: 0.6983, Avg Acc@5: 0.8983
2022-01-16 03:59:54,790 Val Step[1550/1563], Avg Loss: 1.3409, Avg Acc@1: 0.6982, Avg Acc@5: 0.8982
2022-01-16 03:59:56,754 ----- Epoch[100/300], Validation Loss: 1.3409, Validation Acc@1: 0.6983, Validation Acc@5: 0.8982, time: 135.21
2022-01-16 03:59:57,888 the pre best model acc:0.6962, at epoch 96
2022-01-16 03:59:57,889 current best model acc:0.6983, at epoch 100
2022-01-16 03:59:57,889 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-16 03:59:57,889 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-16 03:59:57,889 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-16 03:59:57,889 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-16 03:59:58,477 ----- Save model: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-100-Loss-3.5889533246313694.pdparams
2022-01-16 03:59:58,477 ----- Save optim: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-100-Loss-3.5889533246313694.pdopt
2022-01-16 03:59:58,477 Now training epoch 101. LR=0.000809
2022-01-16 04:01:49,511 Epoch[101/300], Step[0000/1252], Avg Loss: 3.3465, Avg Acc: 0.4375
2022-01-16 04:03:17,184 Epoch[101/300], Step[0050/1252], Avg Loss: 3.6069, Avg Acc: 0.3911
2022-01-16 04:04:43,959 Epoch[101/300], Step[0100/1252], Avg Loss: 3.5858, Avg Acc: 0.3902
2022-01-16 04:06:12,266 Epoch[101/300], Step[0150/1252], Avg Loss: 3.5795, Avg Acc: 0.3910
2022-01-16 04:07:39,257 Epoch[101/300], Step[0200/1252], Avg Loss: 3.5915, Avg Acc: 0.3865
2022-01-16 04:09:06,581 Epoch[101/300], Step[0250/1252], Avg Loss: 3.5760, Avg Acc: 0.3900
2022-01-16 04:10:34,686 Epoch[101/300], Step[0300/1252], Avg Loss: 3.5803, Avg Acc: 0.3903
2022-01-16 04:12:03,190 Epoch[101/300], Step[0350/1252], Avg Loss: 3.5796, Avg Acc: 0.3835
2022-01-16 04:13:31,610 Epoch[101/300], Step[0400/1252], Avg Loss: 3.5734, Avg Acc: 0.3849
2022-01-16 04:14:59,779 Epoch[101/300], Step[0450/1252], Avg Loss: 3.5755, Avg Acc: 0.3855
2022-01-16 04:16:27,218 Epoch[101/300], Step[0500/1252], Avg Loss: 3.5775, Avg Acc: 0.3839
2022-01-16 04:17:54,857 Epoch[101/300], Step[0550/1252], Avg Loss: 3.5826, Avg Acc: 0.3819
2022-01-16 04:19:21,892 Epoch[101/300], Step[0600/1252], Avg Loss: 3.5865, Avg Acc: 0.3819
2022-01-16 04:20:49,979 Epoch[101/300], Step[0650/1252], Avg Loss: 3.5855, Avg Acc: 0.3805
2022-01-16 04:22:18,356 Epoch[101/300], Step[0700/1252], Avg Loss: 3.5873, Avg Acc: 0.3807
2022-01-16 04:23:46,271 Epoch[101/300], Step[0750/1252], Avg Loss: 3.5914, Avg Acc: 0.3800
2022-01-16 04:25:14,433 Epoch[101/300], Step[0800/1252], Avg Loss: 3.5894, Avg Acc: 0.3809
2022-01-16 04:26:42,751 Epoch[101/300], Step[0850/1252], Avg Loss: 3.5877, Avg Acc: 0.3802
2022-01-16 04:28:10,749 Epoch[101/300], Step[0900/1252], Avg Loss: 3.5911, Avg Acc: 0.3804
2022-01-16 04:29:38,105 Epoch[101/300], Step[0950/1252], Avg Loss: 3.5938, Avg Acc: 0.3805
2022-01-16 04:31:06,557 Epoch[101/300], Step[1000/1252], Avg Loss: 3.5932, Avg Acc: 0.3789
2022-01-16 04:32:33,479 Epoch[101/300], Step[1050/1252], Avg Loss: 3.5937, Avg Acc: 0.3799
2022-01-16 04:34:01,768 Epoch[101/300], Step[1100/1252], Avg Loss: 3.5909, Avg Acc: 0.3800
2022-01-16 04:35:29,986 Epoch[101/300], Step[1150/1252], Avg Loss: 3.5920, Avg Acc: 0.3791
2022-01-16 04:36:58,130 Epoch[101/300], Step[1200/1252], Avg Loss: 3.5932, Avg Acc: 0.3789
2022-01-16 04:38:26,926 Epoch[101/300], Step[1250/1252], Avg Loss: 3.5954, Avg Acc: 0.3773
2022-01-16 04:38:34,043 ----- Epoch[101/300], Train Loss: 3.5954, Train Acc: 0.3773, time: 2315.56, Best Val(epoch100) Acc@1: 0.6983
2022-01-16 04:38:34,043 Now training epoch 102. LR=0.000805
2022-01-16 04:40:18,858 Epoch[102/300], Step[0000/1252], Avg Loss: 3.0259, Avg Acc: 0.6084
2022-01-16 04:41:46,318 Epoch[102/300], Step[0050/1252], Avg Loss: 3.4593, Avg Acc: 0.4011
2022-01-16 04:43:13,307 Epoch[102/300], Step[0100/1252], Avg Loss: 3.4741, Avg Acc: 0.4070
2022-01-16 04:44:41,627 Epoch[102/300], Step[0150/1252], Avg Loss: 3.4796, Avg Acc: 0.3956
2022-01-16 04:46:08,714 Epoch[102/300], Step[0200/1252], Avg Loss: 3.4904, Avg Acc: 0.3963
2022-01-16 04:47:37,297 Epoch[102/300], Step[0250/1252], Avg Loss: 3.5113, Avg Acc: 0.3916
2022-01-16 04:49:05,505 Epoch[102/300], Step[0300/1252], Avg Loss: 3.5134, Avg Acc: 0.3898
2022-01-16 04:50:33,022 Epoch[102/300], Step[0350/1252], Avg Loss: 3.5315, Avg Acc: 0.3915
2022-01-16 04:52:00,232 Epoch[102/300], Step[0400/1252], Avg Loss: 3.5366, Avg Acc: 0.3931
2022-01-16 04:53:28,626 Epoch[102/300], Step[0450/1252], Avg Loss: 3.5405, Avg Acc: 0.3900
2022-01-16 04:54:56,138 Epoch[102/300], Step[0500/1252], Avg Loss: 3.5447, Avg Acc: 0.3870
2022-01-16 04:56:24,080 Epoch[102/300], Step[0550/1252], Avg Loss: 3.5495, Avg Acc: 0.3874
2022-01-16 04:57:52,339 Epoch[102/300], Step[0600/1252], Avg Loss: 3.5570, Avg Acc: 0.3857
2022-01-16 04:59:18,667 Epoch[102/300], Step[0650/1252], Avg Loss: 3.5558, Avg Acc: 0.3866
2022-01-16 05:00:46,409 Epoch[102/300], Step[0700/1252], Avg Loss: 3.5580, Avg Acc: 0.3850
2022-01-16 05:02:14,698 Epoch[102/300], Step[0750/1252], Avg Loss: 3.5552, Avg Acc: 0.3849
2022-01-16 05:03:41,052 Epoch[102/300], Step[0800/1252], Avg Loss: 3.5591, Avg Acc: 0.3847
2022-01-16 05:05:07,402 Epoch[102/300], Step[0850/1252], Avg Loss: 3.5624, Avg Acc: 0.3831
2022-01-16 05:06:35,284 Epoch[102/300], Step[0900/1252], Avg Loss: 3.5619, Avg Acc: 0.3823
2022-01-16 05:08:02,792 Epoch[102/300], Step[0950/1252], Avg Loss: 3.5625, Avg Acc: 0.3833
2022-01-16 05:09:31,176 Epoch[102/300], Step[1000/1252], Avg Loss: 3.5652, Avg Acc: 0.3836
2022-01-16 05:11:00,258 Epoch[102/300], Step[1050/1252], Avg Loss: 3.5667, Avg Acc: 0.3825
2022-01-16 05:12:27,500 Epoch[102/300], Step[1100/1252], Avg Loss: 3.5668, Avg Acc: 0.3834
2022-01-16 05:13:54,631 Epoch[102/300], Step[1150/1252], Avg Loss: 3.5679, Avg Acc: 0.3827
2022-01-16 05:15:23,428 Epoch[102/300], Step[1200/1252], Avg Loss: 3.5702, Avg Acc: 0.3828
2022-01-16 05:16:52,169 Epoch[102/300], Step[1250/1252], Avg Loss: 3.5713, Avg Acc: 0.3814
2022-01-16 05:16:59,301 ----- Epoch[102/300], Train Loss: 3.5713, Train Acc: 0.3814, time: 2305.25, Best Val(epoch100) Acc@1: 0.6983
2022-01-16 05:16:59,301 ----- Validation after Epoch: 102
2022-01-16 05:18:11,374 Val Step[0000/1563], Avg Loss: 1.1740, Avg Acc@1: 0.6562, Avg Acc@5: 0.9062
2022-01-16 05:18:13,308 Val Step[0050/1563], Avg Loss: 1.3361, Avg Acc@1: 0.7016, Avg Acc@5: 0.8964
2022-01-16 05:18:15,143 Val Step[0100/1563], Avg Loss: 1.3382, Avg Acc@1: 0.6977, Avg Acc@5: 0.9007
2022-01-16 05:18:16,985 Val Step[0150/1563], Avg Loss: 1.3446, Avg Acc@1: 0.6956, Avg Acc@5: 0.8986
2022-01-16 05:18:18,867 Val Step[0200/1563], Avg Loss: 1.3491, Avg Acc@1: 0.6962, Avg Acc@5: 0.8966
2022-01-16 05:18:20,733 Val Step[0250/1563], Avg Loss: 1.3322, Avg Acc@1: 0.6997, Avg Acc@5: 0.8992
2022-01-16 05:18:22,646 Val Step[0300/1563], Avg Loss: 1.3317, Avg Acc@1: 0.7008, Avg Acc@5: 0.8989
2022-01-16 05:18:24,497 Val Step[0350/1563], Avg Loss: 1.3378, Avg Acc@1: 0.6997, Avg Acc@5: 0.8985
2022-01-16 05:18:26,338 Val Step[0400/1563], Avg Loss: 1.3331, Avg Acc@1: 0.7005, Avg Acc@5: 0.8984
2022-01-16 05:18:28,204 Val Step[0450/1563], Avg Loss: 1.3393, Avg Acc@1: 0.6989, Avg Acc@5: 0.8979
2022-01-16 05:18:30,028 Val Step[0500/1563], Avg Loss: 1.3419, Avg Acc@1: 0.6981, Avg Acc@5: 0.8978
2022-01-16 05:18:31,830 Val Step[0550/1563], Avg Loss: 1.3415, Avg Acc@1: 0.6978, Avg Acc@5: 0.8980
2022-01-16 05:18:33,642 Val Step[0600/1563], Avg Loss: 1.3416, Avg Acc@1: 0.6977, Avg Acc@5: 0.8984
2022-01-16 05:18:35,522 Val Step[0650/1563], Avg Loss: 1.3423, Avg Acc@1: 0.6973, Avg Acc@5: 0.8983
2022-01-16 05:18:37,613 Val Step[0700/1563], Avg Loss: 1.3380, Avg Acc@1: 0.6977, Avg Acc@5: 0.8991
2022-01-16 05:18:39,649 Val Step[0750/1563], Avg Loss: 1.3440, Avg Acc@1: 0.6966, Avg Acc@5: 0.8980
2022-01-16 05:18:41,691 Val Step[0800/1563], Avg Loss: 1.3428, Avg Acc@1: 0.6974, Avg Acc@5: 0.8982
2022-01-16 05:18:43,745 Val Step[0850/1563], Avg Loss: 1.3438, Avg Acc@1: 0.6969, Avg Acc@5: 0.8978
2022-01-16 05:18:45,659 Val Step[0900/1563], Avg Loss: 1.3405, Avg Acc@1: 0.6977, Avg Acc@5: 0.8985
2022-01-16 05:18:47,516 Val Step[0950/1563], Avg Loss: 1.3420, Avg Acc@1: 0.6977, Avg Acc@5: 0.8979
2022-01-16 05:18:49,391 Val Step[1000/1563], Avg Loss: 1.3423, Avg Acc@1: 0.6976, Avg Acc@5: 0.8980
2022-01-16 05:18:51,197 Val Step[1050/1563], Avg Loss: 1.3450, Avg Acc@1: 0.6966, Avg Acc@5: 0.8973
2022-01-16 05:18:52,992 Val Step[1100/1563], Avg Loss: 1.3438, Avg Acc@1: 0.6967, Avg Acc@5: 0.8973
2022-01-16 05:18:54,783 Val Step[1150/1563], Avg Loss: 1.3428, Avg Acc@1: 0.6966, Avg Acc@5: 0.8975
2022-01-16 05:18:56,609 Val Step[1200/1563], Avg Loss: 1.3429, Avg Acc@1: 0.6968, Avg Acc@5: 0.8977
2022-01-16 05:18:58,515 Val Step[1250/1563], Avg Loss: 1.3421, Avg Acc@1: 0.6971, Avg Acc@5: 0.8978
2022-01-16 05:19:00,572 Val Step[1300/1563], Avg Loss: 1.3447, Avg Acc@1: 0.6967, Avg Acc@5: 0.8973
2022-01-16 05:19:02,636 Val Step[1350/1563], Avg Loss: 1.3447, Avg Acc@1: 0.6966, Avg Acc@5: 0.8971
2022-01-16 05:19:04,711 Val Step[1400/1563], Avg Loss: 1.3432, Avg Acc@1: 0.6968, Avg Acc@5: 0.8971
2022-01-16 05:19:06,805 Val Step[1450/1563], Avg Loss: 1.3433, Avg Acc@1: 0.6966, Avg Acc@5: 0.8969
2022-01-16 05:19:08,910 Val Step[1500/1563], Avg Loss: 1.3427, Avg Acc@1: 0.6970, Avg Acc@5: 0.8972
2022-01-16 05:19:10,968 Val Step[1550/1563], Avg Loss: 1.3437, Avg Acc@1: 0.6969, Avg Acc@5: 0.8970
2022-01-16 05:19:12,885 ----- Epoch[102/300], Validation Loss: 1.3436, Validation Acc@1: 0.6969, Validation Acc@5: 0.8971, time: 133.58
2022-01-16 05:19:12,886 Now training epoch 103. LR=0.000800
2022-01-16 05:20:57,883 Epoch[103/300], Step[0000/1252], Avg Loss: 3.5079, Avg Acc: 0.3203
2022-01-16 05:22:26,230 Epoch[103/300], Step[0050/1252], Avg Loss: 3.6286, Avg Acc: 0.3684
2022-01-16 05:23:53,568 Epoch[103/300], Step[0100/1252], Avg Loss: 3.6119, Avg Acc: 0.3814
2022-01-16 05:25:19,998 Epoch[103/300], Step[0150/1252], Avg Loss: 3.5842, Avg Acc: 0.3806
2022-01-16 05:26:48,274 Epoch[103/300], Step[0200/1252], Avg Loss: 3.5750, Avg Acc: 0.3841
2022-01-16 05:28:15,248 Epoch[103/300], Step[0250/1252], Avg Loss: 3.5694, Avg Acc: 0.3934
2022-01-16 05:29:43,748 Epoch[103/300], Step[0300/1252], Avg Loss: 3.5817, Avg Acc: 0.3843
2022-01-16 05:31:10,726 Epoch[103/300], Step[0350/1252], Avg Loss: 3.5730, Avg Acc: 0.3862
2022-01-16 05:32:38,307 Epoch[103/300], Step[0400/1252], Avg Loss: 3.5819, Avg Acc: 0.3846
2022-01-16 05:34:05,944 Epoch[103/300], Step[0450/1252], Avg Loss: 3.5879, Avg Acc: 0.3819
2022-01-16 05:35:33,942 Epoch[103/300], Step[0500/1252], Avg Loss: 3.5875, Avg Acc: 0.3823
2022-01-16 05:37:02,524 Epoch[103/300], Step[0550/1252], Avg Loss: 3.5860, Avg Acc: 0.3803
2022-01-16 05:38:30,988 Epoch[103/300], Step[0600/1252], Avg Loss: 3.5834, Avg Acc: 0.3803
2022-01-16 05:40:00,300 Epoch[103/300], Step[0650/1252], Avg Loss: 3.5828, Avg Acc: 0.3797
2022-01-16 05:41:29,218 Epoch[103/300], Step[0700/1252], Avg Loss: 3.5821, Avg Acc: 0.3782
2022-01-16 05:42:58,456 Epoch[103/300], Step[0750/1252], Avg Loss: 3.5864, Avg Acc: 0.3769
2022-01-16 05:44:26,376 Epoch[103/300], Step[0800/1252], Avg Loss: 3.5870, Avg Acc: 0.3795
2022-01-16 05:45:54,914 Epoch[103/300], Step[0850/1252], Avg Loss: 3.5877, Avg Acc: 0.3801
2022-01-16 05:47:22,950 Epoch[103/300], Step[0900/1252], Avg Loss: 3.5877, Avg Acc: 0.3791
2022-01-16 05:48:51,086 Epoch[103/300], Step[0950/1252], Avg Loss: 3.5887, Avg Acc: 0.3787
2022-01-16 05:50:19,400 Epoch[103/300], Step[1000/1252], Avg Loss: 3.5857, Avg Acc: 0.3788
2022-01-16 05:51:45,202 Epoch[103/300], Step[1050/1252], Avg Loss: 3.5870, Avg Acc: 0.3791
2022-01-16 05:53:12,054 Epoch[103/300], Step[1100/1252], Avg Loss: 3.5885, Avg Acc: 0.3779
2022-01-16 05:54:38,797 Epoch[103/300], Step[1150/1252], Avg Loss: 3.5878, Avg Acc: 0.3788
2022-01-16 05:56:06,915 Epoch[103/300], Step[1200/1252], Avg Loss: 3.5910, Avg Acc: 0.3792
2022-01-16 05:57:35,582 Epoch[103/300], Step[1250/1252], Avg Loss: 3.5873, Avg Acc: 0.3801
2022-01-16 05:57:42,829 ----- Epoch[103/300], Train Loss: 3.5873, Train Acc: 0.3801, time: 2309.94, Best Val(epoch100) Acc@1: 0.6983
2022-01-16 05:57:42,829 Now training epoch 104. LR=0.000796
2022-01-16 05:59:29,812 Epoch[104/300], Step[0000/1252], Avg Loss: 3.8905, Avg Acc: 0.2061
2022-01-16 06:00:57,592 Epoch[104/300], Step[0050/1252], Avg Loss: 3.5421, Avg Acc: 0.3754
2022-01-16 06:02:24,074 Epoch[104/300], Step[0100/1252], Avg Loss: 3.5524, Avg Acc: 0.3904
2022-01-16 06:03:53,130 Epoch[104/300], Step[0150/1252], Avg Loss: 3.5591, Avg Acc: 0.3779
2022-01-16 06:05:22,060 Epoch[104/300], Step[0200/1252], Avg Loss: 3.5487, Avg Acc: 0.3731
2022-01-16 06:06:50,491 Epoch[104/300], Step[0250/1252], Avg Loss: 3.5545, Avg Acc: 0.3783
2022-01-16 06:08:17,611 Epoch[104/300], Step[0300/1252], Avg Loss: 3.5641, Avg Acc: 0.3819
2022-01-16 06:09:45,642 Epoch[104/300], Step[0350/1252], Avg Loss: 3.5531, Avg Acc: 0.3883
2022-01-16 06:11:14,014 Epoch[104/300], Step[0400/1252], Avg Loss: 3.5566, Avg Acc: 0.3910
2022-01-16 06:12:42,051 Epoch[104/300], Step[0450/1252], Avg Loss: 3.5529, Avg Acc: 0.3921
2022-01-16 06:14:10,667 Epoch[104/300], Step[0500/1252], Avg Loss: 3.5555, Avg Acc: 0.3915
2022-01-16 06:15:39,734 Epoch[104/300], Step[0550/1252], Avg Loss: 3.5581, Avg Acc: 0.3880
2022-01-16 06:17:09,416 Epoch[104/300], Step[0600/1252], Avg Loss: 3.5590, Avg Acc: 0.3872
2022-01-16 06:18:38,850 Epoch[104/300], Step[0650/1252], Avg Loss: 3.5580, Avg Acc: 0.3878
2022-01-16 06:20:07,133 Epoch[104/300], Step[0700/1252], Avg Loss: 3.5520, Avg Acc: 0.3893
2022-01-16 06:21:35,462 Epoch[104/300], Step[0750/1252], Avg Loss: 3.5525, Avg Acc: 0.3885
2022-01-16 06:23:04,454 Epoch[104/300], Step[0800/1252], Avg Loss: 3.5534, Avg Acc: 0.3876
2022-01-16 06:24:32,583 Epoch[104/300], Step[0850/1252], Avg Loss: 3.5573, Avg Acc: 0.3859
2022-01-16 06:26:01,388 Epoch[104/300], Step[0900/1252], Avg Loss: 3.5569, Avg Acc: 0.3844
2022-01-16 06:27:29,287 Epoch[104/300], Step[0950/1252], Avg Loss: 3.5568, Avg Acc: 0.3844
2022-01-16 06:28:57,284 Epoch[104/300], Step[1000/1252], Avg Loss: 3.5553, Avg Acc: 0.3858
2022-01-16 06:30:24,373 Epoch[104/300], Step[1050/1252], Avg Loss: 3.5602, Avg Acc: 0.3855
2022-01-16 06:31:52,096 Epoch[104/300], Step[1100/1252], Avg Loss: 3.5627, Avg Acc: 0.3844
2022-01-16 06:33:20,014 Epoch[104/300], Step[1150/1252], Avg Loss: 3.5625, Avg Acc: 0.3856
2022-01-16 06:34:47,526 Epoch[104/300], Step[1200/1252], Avg Loss: 3.5636, Avg Acc: 0.3841
2022-01-16 06:36:15,235 Epoch[104/300], Step[1250/1252], Avg Loss: 3.5668, Avg Acc: 0.3836
2022-01-16 06:36:22,547 ----- Epoch[104/300], Train Loss: 3.5669, Train Acc: 0.3836, time: 2319.71, Best Val(epoch100) Acc@1: 0.6983
2022-01-16 06:36:22,547 ----- Validation after Epoch: 104
2022-01-16 06:37:32,197 Val Step[0000/1563], Avg Loss: 1.0837, Avg Acc@1: 0.7188, Avg Acc@5: 0.9375
2022-01-16 06:37:34,512 Val Step[0050/1563], Avg Loss: 1.3596, Avg Acc@1: 0.7010, Avg Acc@5: 0.9001
2022-01-16 06:37:36,282 Val Step[0100/1563], Avg Loss: 1.3717, Avg Acc@1: 0.6974, Avg Acc@5: 0.9016
2022-01-16 06:37:38,058 Val Step[0150/1563], Avg Loss: 1.3702, Avg Acc@1: 0.6999, Avg Acc@5: 0.9025
2022-01-16 06:37:39,852 Val Step[0200/1563], Avg Loss: 1.3716, Avg Acc@1: 0.7010, Avg Acc@5: 0.9019
2022-01-16 06:37:41,638 Val Step[0250/1563], Avg Loss: 1.3608, Avg Acc@1: 0.7056, Avg Acc@5: 0.9030
2022-01-16 06:37:43,422 Val Step[0300/1563], Avg Loss: 1.3584, Avg Acc@1: 0.7053, Avg Acc@5: 0.9024
2022-01-16 06:37:45,324 Val Step[0350/1563], Avg Loss: 1.3615, Avg Acc@1: 0.7063, Avg Acc@5: 0.9019
2022-01-16 06:37:47,110 Val Step[0400/1563], Avg Loss: 1.3590, Avg Acc@1: 0.7067, Avg Acc@5: 0.9025
2022-01-16 06:37:48,887 Val Step[0450/1563], Avg Loss: 1.3666, Avg Acc@1: 0.7041, Avg Acc@5: 0.9020
2022-01-16 06:37:50,661 Val Step[0500/1563], Avg Loss: 1.3673, Avg Acc@1: 0.7038, Avg Acc@5: 0.9024
2022-01-16 06:37:52,565 Val Step[0550/1563], Avg Loss: 1.3708, Avg Acc@1: 0.7008, Avg Acc@5: 0.9025
2022-01-16 06:37:54,498 Val Step[0600/1563], Avg Loss: 1.3747, Avg Acc@1: 0.7003, Avg Acc@5: 0.9020
2022-01-16 06:37:56,383 Val Step[0650/1563], Avg Loss: 1.3745, Avg Acc@1: 0.7001, Avg Acc@5: 0.9024
2022-01-16 06:37:58,195 Val Step[0700/1563], Avg Loss: 1.3714, Avg Acc@1: 0.7011, Avg Acc@5: 0.9028
2022-01-16 06:38:00,098 Val Step[0750/1563], Avg Loss: 1.3760, Avg Acc@1: 0.7007, Avg Acc@5: 0.9018
2022-01-16 06:38:02,021 Val Step[0800/1563], Avg Loss: 1.3748, Avg Acc@1: 0.7014, Avg Acc@5: 0.9018
2022-01-16 06:38:03,913 Val Step[0850/1563], Avg Loss: 1.3764, Avg Acc@1: 0.7012, Avg Acc@5: 0.9008
2022-01-16 06:38:05,754 Val Step[0900/1563], Avg Loss: 1.3736, Avg Acc@1: 0.7014, Avg Acc@5: 0.9012
2022-01-16 06:38:07,607 Val Step[0950/1563], Avg Loss: 1.3745, Avg Acc@1: 0.7006, Avg Acc@5: 0.9014
2022-01-16 06:38:09,541 Val Step[1000/1563], Avg Loss: 1.3744, Avg Acc@1: 0.7007, Avg Acc@5: 0.9012
2022-01-16 06:38:11,473 Val Step[1050/1563], Avg Loss: 1.3763, Avg Acc@1: 0.6996, Avg Acc@5: 0.9007
2022-01-16 06:38:13,283 Val Step[1100/1563], Avg Loss: 1.3752, Avg Acc@1: 0.6995, Avg Acc@5: 0.9008
2022-01-16 06:38:15,115 Val Step[1150/1563], Avg Loss: 1.3749, Avg Acc@1: 0.6990, Avg Acc@5: 0.9007
2022-01-16 06:38:16,928 Val Step[1200/1563], Avg Loss: 1.3745, Avg Acc@1: 0.6992, Avg Acc@5: 0.9006
2022-01-16 06:38:18,723 Val Step[1250/1563], Avg Loss: 1.3728, Avg Acc@1: 0.6994, Avg Acc@5: 0.9007
2022-01-16 06:38:20,511 Val Step[1300/1563], Avg Loss: 1.3761, Avg Acc@1: 0.6989, Avg Acc@5: 0.9001
2022-01-16 06:38:22,360 Val Step[1350/1563], Avg Loss: 1.3763, Avg Acc@1: 0.6988, Avg Acc@5: 0.9002
2022-01-16 06:38:24,130 Val Step[1400/1563], Avg Loss: 1.3768, Avg Acc@1: 0.6986, Avg Acc@5: 0.8999
2022-01-16 06:38:25,905 Val Step[1450/1563], Avg Loss: 1.3777, Avg Acc@1: 0.6985, Avg Acc@5: 0.8996
2022-01-16 06:38:27,791 Val Step[1500/1563], Avg Loss: 1.3765, Avg Acc@1: 0.6990, Avg Acc@5: 0.8999
2022-01-16 06:38:29,584 Val Step[1550/1563], Avg Loss: 1.3776, Avg Acc@1: 0.6986, Avg Acc@5: 0.8996
2022-01-16 06:38:31,564 ----- Epoch[104/300], Validation Loss: 1.3776, Validation Acc@1: 0.6987, Validation Acc@5: 0.8997, time: 129.01
2022-01-16 06:38:32,722 the pre best model acc:0.6983, at epoch 100
2022-01-16 06:38:32,722 current best model acc:0.6987, at epoch 104
2022-01-16 06:38:32,722 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-16 06:38:32,722 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-16 06:38:32,722 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-16 06:38:32,723 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-16 06:38:32,723 Now training epoch 105. LR=0.000791
2022-01-16 06:40:16,039 Epoch[105/300], Step[0000/1252], Avg Loss: 3.6565, Avg Acc: 0.4863
2022-01-16 06:41:43,160 Epoch[105/300], Step[0050/1252], Avg Loss: 3.5106, Avg Acc: 0.4212
2022-01-16 06:43:09,810 Epoch[105/300], Step[0100/1252], Avg Loss: 3.5433, Avg Acc: 0.3892
2022-01-16 06:44:37,425 Epoch[105/300], Step[0150/1252], Avg Loss: 3.5589, Avg Acc: 0.3866
2022-01-16 06:46:04,685 Epoch[105/300], Step[0200/1252], Avg Loss: 3.5583, Avg Acc: 0.3920
2022-01-16 06:47:31,482 Epoch[105/300], Step[0250/1252], Avg Loss: 3.5633, Avg Acc: 0.3910
2022-01-16 06:48:59,508 Epoch[105/300], Step[0300/1252], Avg Loss: 3.5683, Avg Acc: 0.3870
2022-01-16 06:50:25,581 Epoch[105/300], Step[0350/1252], Avg Loss: 3.5741, Avg Acc: 0.3884
2022-01-16 06:51:51,735 Epoch[105/300], Step[0400/1252], Avg Loss: 3.5784, Avg Acc: 0.3895
2022-01-16 06:53:18,570 Epoch[105/300], Step[0450/1252], Avg Loss: 3.5811, Avg Acc: 0.3867
2022-01-16 06:54:44,596 Epoch[105/300], Step[0500/1252], Avg Loss: 3.5737, Avg Acc: 0.3889
2022-01-16 06:56:11,409 Epoch[105/300], Step[0550/1252], Avg Loss: 3.5685, Avg Acc: 0.3905
2022-01-16 06:57:36,377 Epoch[105/300], Step[0600/1252], Avg Loss: 3.5687, Avg Acc: 0.3904
2022-01-16 06:59:03,575 Epoch[105/300], Step[0650/1252], Avg Loss: 3.5647, Avg Acc: 0.3911
2022-01-16 07:00:31,318 Epoch[105/300], Step[0700/1252], Avg Loss: 3.5621, Avg Acc: 0.3899
2022-01-16 07:01:58,703 Epoch[105/300], Step[0750/1252], Avg Loss: 3.5583, Avg Acc: 0.3901
2022-01-16 07:03:27,388 Epoch[105/300], Step[0800/1252], Avg Loss: 3.5607, Avg Acc: 0.3886
2022-01-16 07:04:55,515 Epoch[105/300], Step[0850/1252], Avg Loss: 3.5633, Avg Acc: 0.3878
2022-01-16 07:06:23,583 Epoch[105/300], Step[0900/1252], Avg Loss: 3.5629, Avg Acc: 0.3870
2022-01-16 07:07:50,816 Epoch[105/300], Step[0950/1252], Avg Loss: 3.5675, Avg Acc: 0.3859
2022-01-16 07:09:18,526 Epoch[105/300], Step[1000/1252], Avg Loss: 3.5671, Avg Acc: 0.3848
2022-01-16 07:10:46,819 Epoch[105/300], Step[1050/1252], Avg Loss: 3.5712, Avg Acc: 0.3840
2022-01-16 07:12:15,596 Epoch[105/300], Step[1100/1252], Avg Loss: 3.5694, Avg Acc: 0.3831
2022-01-16 07:13:44,446 Epoch[105/300], Step[1150/1252], Avg Loss: 3.5688, Avg Acc: 0.3819
2022-01-16 07:15:13,187 Epoch[105/300], Step[1200/1252], Avg Loss: 3.5686, Avg Acc: 0.3816
2022-01-16 07:16:41,335 Epoch[105/300], Step[1250/1252], Avg Loss: 3.5666, Avg Acc: 0.3811
2022-01-16 07:16:48,573 ----- Epoch[105/300], Train Loss: 3.5666, Train Acc: 0.3811, time: 2295.85, Best Val(epoch104) Acc@1: 0.6987
2022-01-16 07:16:48,573 Now training epoch 106. LR=0.000787
