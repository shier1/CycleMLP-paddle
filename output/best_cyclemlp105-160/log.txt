2022-01-16 12:13:37,876 
AMP: False
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RE_COUNT: 1
  RE_MODE: pixel
  RE_PROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 256
  BATCH_SIZE_EVAL: 8
  CROP_PCT: 0.9
  DATASET: imagenet2012
  DATA_PATH: /root/paddlejob/workspace/train_data/datasets/Light_ILSVRC2012
  IMAGE_SIZE: 224
  NUM_WORKERS: 16
EVAL: False
LOCAL_RANK: 0
MODEL:
  MIXER:
    EMBED_DIMS: [64, 128, 320, 512]
    LAYERS: [2, 2, 4, 2]
    MLP_RATIOS: [4, 4, 4, 4]
    TRANSITIONS: [True, True, True, True]
  NAME: cyclemlp_b1
  NUM_CLASSES: 1000
  PRETRAINED: None
  RESUME: Best_CycleMLP
  TYPE: CycleMLP
NGPUS: 4
REPORT_FREQ: 50
SAVE: /root/paddlejob/workspace/output//train
SAVE_FREQ: 10
SEED: 0
TAG: default
TRAIN:
  ACCUM_ITER: 1
  AUTO_AUGMENT: True
  BASE_LR: 0.0005
  COLOR_JITTER: 0.4
  CUTMIX_ALPHA: 1.0
  CUTMIX_MINMAX: None
  END_LR: 5e-06
  GRAD_CLIP: 5.0
  LAST_EPOCH: 104
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    MILESTONES: 30, 60, 90
    NAME: warmupcosine
  MIXUP_ALPHA: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  NUM_EPOCHS: 300
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: AdamW
  RANDOM_ERASE_COUNT: 1
  RANDOM_ERASE_MODE: pixel
  RANDOM_ERASE_PROB: 0.25
  RANDOM_ERASE_SPLIT: False
  SMOOTHING: 0.1
  WARMUP_EPOCHS: 20
  WARMUP_START_LR: 5e-07
  WEIGHT_DECAY: 0.05
VALIDATE_FREQ: 2
VALIDATION:
  REQUIREMENTS: 0.789
2022-01-16 12:13:37,877 ----- world_size = 4, local_rank = 0
2022-01-16 12:13:38,634 ----- Total # of train batch (single gpu): 1252
2022-01-16 12:13:38,634 ----- Total # of val batch (single gpu): 1563
2022-01-16 12:13:40,099 ----- Resume Training: Load model and optmizer from Best_CycleMLP
2022-01-16 12:13:40,099 Start training from epoch 105.
2022-01-16 12:13:40,099 Now training epoch 105. LR=0.000791
2022-01-16 12:15:13,270 Epoch[105/300], Step[0000/1252], Avg Loss: 3.5917, Avg Acc: 0.2402
2022-01-16 12:16:33,981 Epoch[105/300], Step[0050/1252], Avg Loss: 3.6187, Avg Acc: 0.3619
2022-01-16 12:17:54,410 Epoch[105/300], Step[0100/1252], Avg Loss: 3.6126, Avg Acc: 0.3738
2022-01-16 12:19:14,548 Epoch[105/300], Step[0150/1252], Avg Loss: 3.5966, Avg Acc: 0.3718
2022-01-16 12:20:35,368 Epoch[105/300], Step[0200/1252], Avg Loss: 3.5764, Avg Acc: 0.3727
2022-01-16 12:21:57,046 Epoch[105/300], Step[0250/1252], Avg Loss: 3.5601, Avg Acc: 0.3755
2022-01-16 12:23:17,065 Epoch[105/300], Step[0300/1252], Avg Loss: 3.5461, Avg Acc: 0.3814
2022-01-16 12:24:39,113 Epoch[105/300], Step[0350/1252], Avg Loss: 3.5629, Avg Acc: 0.3828
2022-01-16 12:26:00,437 Epoch[105/300], Step[0400/1252], Avg Loss: 3.5609, Avg Acc: 0.3814
2022-01-16 12:27:22,100 Epoch[105/300], Step[0450/1252], Avg Loss: 3.5682, Avg Acc: 0.3790
2022-01-16 12:28:43,587 Epoch[105/300], Step[0500/1252], Avg Loss: 3.5668, Avg Acc: 0.3800
2022-01-16 12:30:04,211 Epoch[105/300], Step[0550/1252], Avg Loss: 3.5597, Avg Acc: 0.3812
2022-01-16 12:31:25,727 Epoch[105/300], Step[0600/1252], Avg Loss: 3.5632, Avg Acc: 0.3820
2022-01-16 12:32:46,939 Epoch[105/300], Step[0650/1252], Avg Loss: 3.5655, Avg Acc: 0.3803
2022-01-16 12:34:07,949 Epoch[105/300], Step[0700/1252], Avg Loss: 3.5662, Avg Acc: 0.3815
2022-01-16 12:35:29,285 Epoch[105/300], Step[0750/1252], Avg Loss: 3.5666, Avg Acc: 0.3827
2022-01-16 12:36:50,047 Epoch[105/300], Step[0800/1252], Avg Loss: 3.5682, Avg Acc: 0.3821
2022-01-16 12:38:11,230 Epoch[105/300], Step[0850/1252], Avg Loss: 3.5641, Avg Acc: 0.3812
2022-01-16 12:39:32,335 Epoch[105/300], Step[0900/1252], Avg Loss: 3.5647, Avg Acc: 0.3818
2022-01-16 12:40:53,416 Epoch[105/300], Step[0950/1252], Avg Loss: 3.5629, Avg Acc: 0.3824
2022-01-16 12:42:14,392 Epoch[105/300], Step[1000/1252], Avg Loss: 3.5634, Avg Acc: 0.3815
2022-01-16 12:43:34,679 Epoch[105/300], Step[1050/1252], Avg Loss: 3.5638, Avg Acc: 0.3795
2022-01-16 12:44:54,933 Epoch[105/300], Step[1100/1252], Avg Loss: 3.5643, Avg Acc: 0.3787
2022-01-16 12:46:15,159 Epoch[105/300], Step[1150/1252], Avg Loss: 3.5660, Avg Acc: 0.3781
2022-01-16 12:47:36,372 Epoch[105/300], Step[1200/1252], Avg Loss: 3.5670, Avg Acc: 0.3777
2022-01-16 12:48:55,930 Epoch[105/300], Step[1250/1252], Avg Loss: 3.5676, Avg Acc: 0.3771
2022-01-16 12:49:02,018 ----- Epoch[105/300], Train Loss: 3.5676, Train Acc: 0.3771, time: 2121.92
2022-01-16 12:49:02,018 Now training epoch 106. LR=0.000787
2022-01-16 12:50:43,996 Epoch[106/300], Step[0000/1252], Avg Loss: 3.6642, Avg Acc: 0.2275
2022-01-16 12:52:04,430 Epoch[106/300], Step[0050/1252], Avg Loss: 3.6503, Avg Acc: 0.3728
2022-01-16 12:53:24,974 Epoch[106/300], Step[0100/1252], Avg Loss: 3.6008, Avg Acc: 0.3694
2022-01-16 12:54:44,299 Epoch[106/300], Step[0150/1252], Avg Loss: 3.6048, Avg Acc: 0.3725
2022-01-16 12:56:05,509 Epoch[106/300], Step[0200/1252], Avg Loss: 3.5933, Avg Acc: 0.3762
2022-01-16 12:57:27,130 Epoch[106/300], Step[0250/1252], Avg Loss: 3.5814, Avg Acc: 0.3799
2022-01-16 12:58:46,757 Epoch[106/300], Step[0300/1252], Avg Loss: 3.5809, Avg Acc: 0.3826
2022-01-16 13:00:09,009 Epoch[106/300], Step[0350/1252], Avg Loss: 3.5833, Avg Acc: 0.3806
2022-01-16 13:01:30,815 Epoch[106/300], Step[0400/1252], Avg Loss: 3.5788, Avg Acc: 0.3813
2022-01-16 13:02:52,441 Epoch[106/300], Step[0450/1252], Avg Loss: 3.5792, Avg Acc: 0.3800
2022-01-16 13:04:13,827 Epoch[106/300], Step[0500/1252], Avg Loss: 3.5759, Avg Acc: 0.3784
2022-01-16 13:05:34,983 Epoch[106/300], Step[0550/1252], Avg Loss: 3.5747, Avg Acc: 0.3786
2022-01-16 13:06:56,464 Epoch[106/300], Step[0600/1252], Avg Loss: 3.5717, Avg Acc: 0.3811
2022-01-16 13:08:18,853 Epoch[106/300], Step[0650/1252], Avg Loss: 3.5665, Avg Acc: 0.3797
2022-01-16 13:09:38,802 Epoch[106/300], Step[0700/1252], Avg Loss: 3.5576, Avg Acc: 0.3811
2022-01-16 13:10:59,949 Epoch[106/300], Step[0750/1252], Avg Loss: 3.5552, Avg Acc: 0.3827
2022-01-16 13:12:20,518 Epoch[106/300], Step[0800/1252], Avg Loss: 3.5589, Avg Acc: 0.3808
2022-01-16 13:13:42,944 Epoch[106/300], Step[0850/1252], Avg Loss: 3.5588, Avg Acc: 0.3795
2022-01-16 13:15:03,247 Epoch[106/300], Step[0900/1252], Avg Loss: 3.5623, Avg Acc: 0.3793
2022-01-16 13:16:23,865 Epoch[106/300], Step[0950/1252], Avg Loss: 3.5653, Avg Acc: 0.3791
2022-01-16 13:17:45,344 Epoch[106/300], Step[1000/1252], Avg Loss: 3.5659, Avg Acc: 0.3783
2022-01-16 13:19:07,378 Epoch[106/300], Step[1050/1252], Avg Loss: 3.5651, Avg Acc: 0.3768
2022-01-16 13:20:28,800 Epoch[106/300], Step[1100/1252], Avg Loss: 3.5674, Avg Acc: 0.3767
2022-01-16 13:21:49,974 Epoch[106/300], Step[1150/1252], Avg Loss: 3.5675, Avg Acc: 0.3763
2022-01-16 13:23:12,063 Epoch[106/300], Step[1200/1252], Avg Loss: 3.5688, Avg Acc: 0.3776
2022-01-16 13:24:30,623 Epoch[106/300], Step[1250/1252], Avg Loss: 3.5699, Avg Acc: 0.3783
2022-01-16 13:24:37,838 ----- Epoch[106/300], Train Loss: 3.5699, Train Acc: 0.3783, time: 2135.82
2022-01-16 13:24:37,839 ----- Validation after Epoch: 106
2022-01-16 13:25:56,056 Val Step[0000/1563], Avg Loss: 1.0071, Avg Acc@1: 0.7188, Avg Acc@5: 0.9688
2022-01-16 13:25:57,809 Val Step[0050/1563], Avg Loss: 1.2834, Avg Acc@1: 0.7053, Avg Acc@5: 0.9032
2022-01-16 13:25:59,422 Val Step[0100/1563], Avg Loss: 1.2972, Avg Acc@1: 0.6977, Avg Acc@5: 0.9041
2022-01-16 13:26:01,054 Val Step[0150/1563], Avg Loss: 1.2978, Avg Acc@1: 0.7026, Avg Acc@5: 0.9019
2022-01-16 13:26:02,642 Val Step[0200/1563], Avg Loss: 1.3013, Avg Acc@1: 0.7034, Avg Acc@5: 0.9008
2022-01-16 13:26:04,264 Val Step[0250/1563], Avg Loss: 1.2897, Avg Acc@1: 0.7070, Avg Acc@5: 0.9015
2022-01-16 13:26:05,843 Val Step[0300/1563], Avg Loss: 1.2900, Avg Acc@1: 0.7083, Avg Acc@5: 0.9002
2022-01-16 13:26:07,416 Val Step[0350/1563], Avg Loss: 1.2963, Avg Acc@1: 0.7077, Avg Acc@5: 0.9003
2022-01-16 13:26:08,990 Val Step[0400/1563], Avg Loss: 1.2914, Avg Acc@1: 0.7081, Avg Acc@5: 0.9008
2022-01-16 13:26:10,599 Val Step[0450/1563], Avg Loss: 1.2976, Avg Acc@1: 0.7051, Avg Acc@5: 0.8995
2022-01-16 13:26:12,236 Val Step[0500/1563], Avg Loss: 1.2984, Avg Acc@1: 0.7055, Avg Acc@5: 0.8991
2022-01-16 13:26:13,793 Val Step[0550/1563], Avg Loss: 1.3007, Avg Acc@1: 0.7053, Avg Acc@5: 0.8985
2022-01-16 13:26:15,382 Val Step[0600/1563], Avg Loss: 1.3041, Avg Acc@1: 0.7042, Avg Acc@5: 0.8979
2022-01-16 13:26:17,038 Val Step[0650/1563], Avg Loss: 1.3046, Avg Acc@1: 0.7042, Avg Acc@5: 0.8982
2022-01-16 13:26:18,581 Val Step[0700/1563], Avg Loss: 1.3006, Avg Acc@1: 0.7048, Avg Acc@5: 0.8992
2022-01-16 13:26:20,203 Val Step[0750/1563], Avg Loss: 1.3065, Avg Acc@1: 0.7034, Avg Acc@5: 0.8983
2022-01-16 13:26:21,813 Val Step[0800/1563], Avg Loss: 1.3054, Avg Acc@1: 0.7037, Avg Acc@5: 0.8988
2022-01-16 13:26:23,378 Val Step[0850/1563], Avg Loss: 1.3077, Avg Acc@1: 0.7029, Avg Acc@5: 0.8982
2022-01-16 13:26:25,037 Val Step[0900/1563], Avg Loss: 1.3060, Avg Acc@1: 0.7024, Avg Acc@5: 0.8984
2022-01-16 13:26:26,621 Val Step[0950/1563], Avg Loss: 1.3067, Avg Acc@1: 0.7018, Avg Acc@5: 0.8983
2022-01-16 13:26:28,202 Val Step[1000/1563], Avg Loss: 1.3072, Avg Acc@1: 0.7015, Avg Acc@5: 0.8985
2022-01-16 13:26:29,772 Val Step[1050/1563], Avg Loss: 1.3099, Avg Acc@1: 0.7010, Avg Acc@5: 0.8982
2022-01-16 13:26:31,369 Val Step[1100/1563], Avg Loss: 1.3098, Avg Acc@1: 0.7006, Avg Acc@5: 0.8982
2022-01-16 13:26:33,131 Val Step[1150/1563], Avg Loss: 1.3082, Avg Acc@1: 0.7008, Avg Acc@5: 0.8984
2022-01-16 13:26:34,967 Val Step[1200/1563], Avg Loss: 1.3070, Avg Acc@1: 0.7014, Avg Acc@5: 0.8986
2022-01-16 13:26:36,834 Val Step[1250/1563], Avg Loss: 1.3070, Avg Acc@1: 0.7015, Avg Acc@5: 0.8986
2022-01-16 13:26:38,536 Val Step[1300/1563], Avg Loss: 1.3111, Avg Acc@1: 0.7012, Avg Acc@5: 0.8982
2022-01-16 13:26:40,285 Val Step[1350/1563], Avg Loss: 1.3107, Avg Acc@1: 0.7013, Avg Acc@5: 0.8980
2022-01-16 13:26:42,038 Val Step[1400/1563], Avg Loss: 1.3103, Avg Acc@1: 0.7009, Avg Acc@5: 0.8980
2022-01-16 13:26:43,601 Val Step[1450/1563], Avg Loss: 1.3105, Avg Acc@1: 0.7010, Avg Acc@5: 0.8980
2022-01-16 13:26:45,148 Val Step[1500/1563], Avg Loss: 1.3108, Avg Acc@1: 0.7013, Avg Acc@5: 0.8980
2022-01-16 13:26:46,696 Val Step[1550/1563], Avg Loss: 1.3123, Avg Acc@1: 0.7009, Avg Acc@5: 0.8977
2022-01-16 13:26:48,850 ----- Epoch[106/300], Validation Loss: 1.3123, Validation Acc@1: 0.7009, Validation Acc@5: 0.8978, time: 131.01
2022-01-16 13:26:49,233 the pre best model acc:0.0000, at epoch 0
2022-01-16 13:26:49,233 current best model acc:0.7009, at epoch 106
2022-01-16 13:26:49,233 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-16 13:26:49,233 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-16 13:26:49,233 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-16 13:26:49,233 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-16 13:26:49,233 Now training epoch 107. LR=0.000782
2022-01-16 13:28:30,048 Epoch[107/300], Step[0000/1252], Avg Loss: 3.3188, Avg Acc: 0.5078
2022-01-16 13:29:50,752 Epoch[107/300], Step[0050/1252], Avg Loss: 3.5262, Avg Acc: 0.4064
2022-01-16 13:31:11,412 Epoch[107/300], Step[0100/1252], Avg Loss: 3.5345, Avg Acc: 0.4080
2022-01-16 13:32:31,928 Epoch[107/300], Step[0150/1252], Avg Loss: 3.5378, Avg Acc: 0.3938
2022-01-16 13:33:52,503 Epoch[107/300], Step[0200/1252], Avg Loss: 3.5505, Avg Acc: 0.3927
2022-01-16 13:35:14,369 Epoch[107/300], Step[0250/1252], Avg Loss: 3.5514, Avg Acc: 0.3939
2022-01-16 13:36:35,899 Epoch[107/300], Step[0300/1252], Avg Loss: 3.5438, Avg Acc: 0.3943
2022-01-16 13:37:57,552 Epoch[107/300], Step[0350/1252], Avg Loss: 3.5487, Avg Acc: 0.3933
2022-01-16 13:39:17,960 Epoch[107/300], Step[0400/1252], Avg Loss: 3.5500, Avg Acc: 0.3929
2022-01-16 13:40:39,624 Epoch[107/300], Step[0450/1252], Avg Loss: 3.5475, Avg Acc: 0.3905
2022-01-16 13:42:01,215 Epoch[107/300], Step[0500/1252], Avg Loss: 3.5454, Avg Acc: 0.3881
2022-01-16 13:43:22,204 Epoch[107/300], Step[0550/1252], Avg Loss: 3.5524, Avg Acc: 0.3873
2022-01-16 13:44:43,458 Epoch[107/300], Step[0600/1252], Avg Loss: 3.5522, Avg Acc: 0.3882
2022-01-16 13:46:05,477 Epoch[107/300], Step[0650/1252], Avg Loss: 3.5520, Avg Acc: 0.3881
2022-01-16 13:47:26,692 Epoch[107/300], Step[0700/1252], Avg Loss: 3.5546, Avg Acc: 0.3856
2022-01-16 13:48:48,776 Epoch[107/300], Step[0750/1252], Avg Loss: 3.5596, Avg Acc: 0.3838
2022-01-16 13:50:10,882 Epoch[107/300], Step[0800/1252], Avg Loss: 3.5616, Avg Acc: 0.3848
2022-01-16 13:51:31,830 Epoch[107/300], Step[0850/1252], Avg Loss: 3.5619, Avg Acc: 0.3840
2022-01-16 13:52:53,878 Epoch[107/300], Step[0900/1252], Avg Loss: 3.5640, Avg Acc: 0.3813
2022-01-16 13:54:14,661 Epoch[107/300], Step[0950/1252], Avg Loss: 3.5618, Avg Acc: 0.3814
2022-01-16 13:55:36,161 Epoch[107/300], Step[1000/1252], Avg Loss: 3.5616, Avg Acc: 0.3819
2022-01-16 13:56:57,909 Epoch[107/300], Step[1050/1252], Avg Loss: 3.5617, Avg Acc: 0.3810
2022-01-16 13:58:20,027 Epoch[107/300], Step[1100/1252], Avg Loss: 3.5624, Avg Acc: 0.3806
2022-01-16 13:59:40,841 Epoch[107/300], Step[1150/1252], Avg Loss: 3.5656, Avg Acc: 0.3804
2022-01-16 14:01:01,763 Epoch[107/300], Step[1200/1252], Avg Loss: 3.5634, Avg Acc: 0.3805
2022-01-16 14:02:22,374 Epoch[107/300], Step[1250/1252], Avg Loss: 3.5671, Avg Acc: 0.3790
2022-01-16 14:02:30,210 ----- Epoch[107/300], Train Loss: 3.5671, Train Acc: 0.3790, time: 2140.97, Best Val(epoch106) Acc@1: 0.7009
2022-01-16 14:02:30,210 Now training epoch 108. LR=0.000778
2022-01-16 14:04:14,014 Epoch[108/300], Step[0000/1252], Avg Loss: 2.8241, Avg Acc: 0.4971
2022-01-16 14:05:34,048 Epoch[108/300], Step[0050/1252], Avg Loss: 3.5764, Avg Acc: 0.3916
2022-01-16 14:06:54,438 Epoch[108/300], Step[0100/1252], Avg Loss: 3.5692, Avg Acc: 0.3899
2022-01-16 14:08:14,847 Epoch[108/300], Step[0150/1252], Avg Loss: 3.5496, Avg Acc: 0.4007
2022-01-16 14:09:35,746 Epoch[108/300], Step[0200/1252], Avg Loss: 3.5500, Avg Acc: 0.3916
2022-01-16 14:10:56,394 Epoch[108/300], Step[0250/1252], Avg Loss: 3.5435, Avg Acc: 0.3846
2022-01-16 14:12:16,861 Epoch[108/300], Step[0300/1252], Avg Loss: 3.5519, Avg Acc: 0.3878
2022-01-16 14:13:37,413 Epoch[108/300], Step[0350/1252], Avg Loss: 3.5421, Avg Acc: 0.3895
2022-01-16 14:14:58,568 Epoch[108/300], Step[0400/1252], Avg Loss: 3.5400, Avg Acc: 0.3900
2022-01-16 14:16:19,207 Epoch[108/300], Step[0450/1252], Avg Loss: 3.5443, Avg Acc: 0.3884
2022-01-16 14:17:40,428 Epoch[108/300], Step[0500/1252], Avg Loss: 3.5520, Avg Acc: 0.3878
2022-01-16 14:19:01,647 Epoch[108/300], Step[0550/1252], Avg Loss: 3.5485, Avg Acc: 0.3891
2022-01-16 14:20:23,719 Epoch[108/300], Step[0600/1252], Avg Loss: 3.5510, Avg Acc: 0.3883
2022-01-16 14:21:44,089 Epoch[108/300], Step[0650/1252], Avg Loss: 3.5523, Avg Acc: 0.3870
2022-01-16 14:23:06,597 Epoch[108/300], Step[0700/1252], Avg Loss: 3.5574, Avg Acc: 0.3855
2022-01-16 14:24:28,443 Epoch[108/300], Step[0750/1252], Avg Loss: 3.5607, Avg Acc: 0.3842
2022-01-16 14:25:50,049 Epoch[108/300], Step[0800/1252], Avg Loss: 3.5659, Avg Acc: 0.3817
2022-01-16 14:27:11,413 Epoch[108/300], Step[0850/1252], Avg Loss: 3.5700, Avg Acc: 0.3802
2022-01-16 14:28:33,804 Epoch[108/300], Step[0900/1252], Avg Loss: 3.5715, Avg Acc: 0.3803
2022-01-16 14:29:56,182 Epoch[108/300], Step[0950/1252], Avg Loss: 3.5733, Avg Acc: 0.3800
2022-01-16 14:31:16,272 Epoch[108/300], Step[1000/1252], Avg Loss: 3.5714, Avg Acc: 0.3800
2022-01-16 14:32:36,922 Epoch[108/300], Step[1050/1252], Avg Loss: 3.5718, Avg Acc: 0.3807
2022-01-16 14:33:58,536 Epoch[108/300], Step[1100/1252], Avg Loss: 3.5709, Avg Acc: 0.3810
2022-01-16 14:35:19,369 Epoch[108/300], Step[1150/1252], Avg Loss: 3.5686, Avg Acc: 0.3817
2022-01-16 14:36:41,482 Epoch[108/300], Step[1200/1252], Avg Loss: 3.5690, Avg Acc: 0.3824
2022-01-16 14:38:01,953 Epoch[108/300], Step[1250/1252], Avg Loss: 3.5711, Avg Acc: 0.3815
2022-01-16 14:38:09,196 ----- Epoch[108/300], Train Loss: 3.5711, Train Acc: 0.3815, time: 2138.98, Best Val(epoch106) Acc@1: 0.7009
2022-01-16 14:38:09,196 ----- Validation after Epoch: 108
2022-01-16 14:39:25,764 Val Step[0000/1563], Avg Loss: 1.0668, Avg Acc@1: 0.7500, Avg Acc@5: 0.9375
2022-01-16 14:39:27,592 Val Step[0050/1563], Avg Loss: 1.3322, Avg Acc@1: 0.7089, Avg Acc@5: 0.9001
2022-01-16 14:39:29,322 Val Step[0100/1563], Avg Loss: 1.3367, Avg Acc@1: 0.7061, Avg Acc@5: 0.9022
2022-01-16 14:39:31,021 Val Step[0150/1563], Avg Loss: 1.3422, Avg Acc@1: 0.7096, Avg Acc@5: 0.8994
2022-01-16 14:39:32,884 Val Step[0200/1563], Avg Loss: 1.3471, Avg Acc@1: 0.7099, Avg Acc@5: 0.8975
2022-01-16 14:39:34,610 Val Step[0250/1563], Avg Loss: 1.3328, Avg Acc@1: 0.7118, Avg Acc@5: 0.8978
2022-01-16 14:39:36,303 Val Step[0300/1563], Avg Loss: 1.3325, Avg Acc@1: 0.7114, Avg Acc@5: 0.8975
2022-01-16 14:39:38,073 Val Step[0350/1563], Avg Loss: 1.3357, Avg Acc@1: 0.7106, Avg Acc@5: 0.8975
2022-01-16 14:39:39,752 Val Step[0400/1563], Avg Loss: 1.3369, Avg Acc@1: 0.7103, Avg Acc@5: 0.8974
2022-01-16 14:39:41,373 Val Step[0450/1563], Avg Loss: 1.3456, Avg Acc@1: 0.7053, Avg Acc@5: 0.8977
2022-01-16 14:39:42,931 Val Step[0500/1563], Avg Loss: 1.3464, Avg Acc@1: 0.7043, Avg Acc@5: 0.8983
2022-01-16 14:39:44,471 Val Step[0550/1563], Avg Loss: 1.3482, Avg Acc@1: 0.7034, Avg Acc@5: 0.8980
2022-01-16 14:39:46,220 Val Step[0600/1563], Avg Loss: 1.3490, Avg Acc@1: 0.7023, Avg Acc@5: 0.8980
2022-01-16 14:39:48,004 Val Step[0650/1563], Avg Loss: 1.3490, Avg Acc@1: 0.7025, Avg Acc@5: 0.8980
2022-01-16 14:39:49,958 Val Step[0700/1563], Avg Loss: 1.3464, Avg Acc@1: 0.7023, Avg Acc@5: 0.8982
2022-01-16 14:39:51,734 Val Step[0750/1563], Avg Loss: 1.3529, Avg Acc@1: 0.7018, Avg Acc@5: 0.8976
2022-01-16 14:39:53,371 Val Step[0800/1563], Avg Loss: 1.3517, Avg Acc@1: 0.7022, Avg Acc@5: 0.8979
2022-01-16 14:39:55,024 Val Step[0850/1563], Avg Loss: 1.3533, Avg Acc@1: 0.7013, Avg Acc@5: 0.8974
2022-01-16 14:39:56,664 Val Step[0900/1563], Avg Loss: 1.3496, Avg Acc@1: 0.7014, Avg Acc@5: 0.8980
2022-01-16 14:39:58,415 Val Step[0950/1563], Avg Loss: 1.3503, Avg Acc@1: 0.7013, Avg Acc@5: 0.8981
2022-01-16 14:40:00,261 Val Step[1000/1563], Avg Loss: 1.3514, Avg Acc@1: 0.7009, Avg Acc@5: 0.8982
2022-01-16 14:40:02,072 Val Step[1050/1563], Avg Loss: 1.3538, Avg Acc@1: 0.7002, Avg Acc@5: 0.8980
2022-01-16 14:40:03,764 Val Step[1100/1563], Avg Loss: 1.3539, Avg Acc@1: 0.6993, Avg Acc@5: 0.8978
2022-01-16 14:40:05,621 Val Step[1150/1563], Avg Loss: 1.3517, Avg Acc@1: 0.6996, Avg Acc@5: 0.8981
2022-01-16 14:40:07,176 Val Step[1200/1563], Avg Loss: 1.3504, Avg Acc@1: 0.6999, Avg Acc@5: 0.8982
2022-01-16 14:40:08,829 Val Step[1250/1563], Avg Loss: 1.3484, Avg Acc@1: 0.7000, Avg Acc@5: 0.8986
2022-01-16 14:40:10,699 Val Step[1300/1563], Avg Loss: 1.3518, Avg Acc@1: 0.6996, Avg Acc@5: 0.8981
2022-01-16 14:40:12,342 Val Step[1350/1563], Avg Loss: 1.3509, Avg Acc@1: 0.6996, Avg Acc@5: 0.8980
2022-01-16 14:40:13,911 Val Step[1400/1563], Avg Loss: 1.3507, Avg Acc@1: 0.6990, Avg Acc@5: 0.8982
2022-01-16 14:40:15,579 Val Step[1450/1563], Avg Loss: 1.3502, Avg Acc@1: 0.6988, Avg Acc@5: 0.8982
2022-01-16 14:40:17,374 Val Step[1500/1563], Avg Loss: 1.3493, Avg Acc@1: 0.6993, Avg Acc@5: 0.8984
2022-01-16 14:40:19,178 Val Step[1550/1563], Avg Loss: 1.3510, Avg Acc@1: 0.6990, Avg Acc@5: 0.8981
2022-01-16 14:40:21,383 ----- Epoch[108/300], Validation Loss: 1.3512, Validation Acc@1: 0.6989, Validation Acc@5: 0.8983, time: 132.18
2022-01-16 14:40:21,384 Now training epoch 109. LR=0.000773
2022-01-16 14:41:57,382 Epoch[109/300], Step[0000/1252], Avg Loss: 2.9752, Avg Acc: 0.6152
2022-01-16 14:43:17,907 Epoch[109/300], Step[0050/1252], Avg Loss: 3.5933, Avg Acc: 0.3612
2022-01-16 14:44:36,712 Epoch[109/300], Step[0100/1252], Avg Loss: 3.5666, Avg Acc: 0.3987
2022-01-16 14:45:57,536 Epoch[109/300], Step[0150/1252], Avg Loss: 3.5353, Avg Acc: 0.4054
2022-01-16 14:47:18,384 Epoch[109/300], Step[0200/1252], Avg Loss: 3.5372, Avg Acc: 0.3954
2022-01-16 14:48:39,553 Epoch[109/300], Step[0250/1252], Avg Loss: 3.5458, Avg Acc: 0.3941
2022-01-16 14:50:01,180 Epoch[109/300], Step[0300/1252], Avg Loss: 3.5488, Avg Acc: 0.3915
2022-01-16 14:51:22,742 Epoch[109/300], Step[0350/1252], Avg Loss: 3.5475, Avg Acc: 0.3891
2022-01-16 14:52:44,600 Epoch[109/300], Step[0400/1252], Avg Loss: 3.5542, Avg Acc: 0.3912
2022-01-16 14:54:06,828 Epoch[109/300], Step[0450/1252], Avg Loss: 3.5575, Avg Acc: 0.3895
2022-01-16 14:55:27,314 Epoch[109/300], Step[0500/1252], Avg Loss: 3.5537, Avg Acc: 0.3895
2022-01-16 14:56:47,827 Epoch[109/300], Step[0550/1252], Avg Loss: 3.5507, Avg Acc: 0.3882
2022-01-16 14:58:10,256 Epoch[109/300], Step[0600/1252], Avg Loss: 3.5545, Avg Acc: 0.3865
2022-01-16 14:59:30,922 Epoch[109/300], Step[0650/1252], Avg Loss: 3.5544, Avg Acc: 0.3832
2022-01-16 15:00:52,637 Epoch[109/300], Step[0700/1252], Avg Loss: 3.5557, Avg Acc: 0.3826
2022-01-16 15:02:14,189 Epoch[109/300], Step[0750/1252], Avg Loss: 3.5539, Avg Acc: 0.3830
2022-01-16 15:03:35,760 Epoch[109/300], Step[0800/1252], Avg Loss: 3.5512, Avg Acc: 0.3833
2022-01-16 15:04:57,200 Epoch[109/300], Step[0850/1252], Avg Loss: 3.5526, Avg Acc: 0.3833
2022-01-16 15:06:18,421 Epoch[109/300], Step[0900/1252], Avg Loss: 3.5558, Avg Acc: 0.3830
2022-01-16 15:07:39,127 Epoch[109/300], Step[0950/1252], Avg Loss: 3.5568, Avg Acc: 0.3835
2022-01-16 15:09:00,375 Epoch[109/300], Step[1000/1252], Avg Loss: 3.5604, Avg Acc: 0.3827
2022-01-16 15:10:22,113 Epoch[109/300], Step[1050/1252], Avg Loss: 3.5566, Avg Acc: 0.3834
2022-01-16 15:11:43,765 Epoch[109/300], Step[1100/1252], Avg Loss: 3.5575, Avg Acc: 0.3838
2022-01-16 15:13:05,322 Epoch[109/300], Step[1150/1252], Avg Loss: 3.5576, Avg Acc: 0.3837
2022-01-16 15:14:26,672 Epoch[109/300], Step[1200/1252], Avg Loss: 3.5578, Avg Acc: 0.3833
2022-01-16 15:15:47,163 Epoch[109/300], Step[1250/1252], Avg Loss: 3.5603, Avg Acc: 0.3824
2022-01-16 15:15:53,809 ----- Epoch[109/300], Train Loss: 3.5603, Train Acc: 0.3824, time: 2132.42, Best Val(epoch106) Acc@1: 0.7009
2022-01-16 15:15:53,809 Now training epoch 110. LR=0.000768
2022-01-16 15:17:33,184 Epoch[110/300], Step[0000/1252], Avg Loss: 3.8755, Avg Acc: 0.1240
2022-01-16 15:18:53,094 Epoch[110/300], Step[0050/1252], Avg Loss: 3.6530, Avg Acc: 0.3718
2022-01-16 15:20:13,498 Epoch[110/300], Step[0100/1252], Avg Loss: 3.6104, Avg Acc: 0.3844
2022-01-16 15:21:34,725 Epoch[110/300], Step[0150/1252], Avg Loss: 3.6114, Avg Acc: 0.3843
2022-01-16 15:22:55,295 Epoch[110/300], Step[0200/1252], Avg Loss: 3.6024, Avg Acc: 0.3750
2022-01-16 15:24:15,656 Epoch[110/300], Step[0250/1252], Avg Loss: 3.5887, Avg Acc: 0.3772
2022-01-16 15:25:37,056 Epoch[110/300], Step[0300/1252], Avg Loss: 3.5964, Avg Acc: 0.3776
2022-01-16 15:26:57,285 Epoch[110/300], Step[0350/1252], Avg Loss: 3.5890, Avg Acc: 0.3775
2022-01-16 15:28:17,782 Epoch[110/300], Step[0400/1252], Avg Loss: 3.5851, Avg Acc: 0.3790
2022-01-16 15:29:38,662 Epoch[110/300], Step[0450/1252], Avg Loss: 3.5747, Avg Acc: 0.3838
2022-01-16 15:31:00,054 Epoch[110/300], Step[0500/1252], Avg Loss: 3.5736, Avg Acc: 0.3823
2022-01-16 15:32:20,965 Epoch[110/300], Step[0550/1252], Avg Loss: 3.5740, Avg Acc: 0.3816
2022-01-16 15:33:41,792 Epoch[110/300], Step[0600/1252], Avg Loss: 3.5672, Avg Acc: 0.3804
2022-01-16 15:35:02,480 Epoch[110/300], Step[0650/1252], Avg Loss: 3.5666, Avg Acc: 0.3797
2022-01-16 15:36:24,308 Epoch[110/300], Step[0700/1252], Avg Loss: 3.5668, Avg Acc: 0.3803
2022-01-16 15:37:44,661 Epoch[110/300], Step[0750/1252], Avg Loss: 3.5641, Avg Acc: 0.3808
2022-01-16 15:39:03,081 Epoch[110/300], Step[0800/1252], Avg Loss: 3.5629, Avg Acc: 0.3826
2022-01-16 15:40:22,819 Epoch[110/300], Step[0850/1252], Avg Loss: 3.5640, Avg Acc: 0.3828
2022-01-16 15:41:43,626 Epoch[110/300], Step[0900/1252], Avg Loss: 3.5669, Avg Acc: 0.3824
2022-01-16 15:43:04,825 Epoch[110/300], Step[0950/1252], Avg Loss: 3.5635, Avg Acc: 0.3818
2022-01-16 15:44:26,080 Epoch[110/300], Step[1000/1252], Avg Loss: 3.5642, Avg Acc: 0.3808
2022-01-16 15:45:47,757 Epoch[110/300], Step[1050/1252], Avg Loss: 3.5647, Avg Acc: 0.3789
2022-01-16 15:47:08,935 Epoch[110/300], Step[1100/1252], Avg Loss: 3.5629, Avg Acc: 0.3791
2022-01-16 15:48:29,440 Epoch[110/300], Step[1150/1252], Avg Loss: 3.5618, Avg Acc: 0.3791
2022-01-16 15:49:50,414 Epoch[110/300], Step[1200/1252], Avg Loss: 3.5621, Avg Acc: 0.3796
2022-01-16 15:51:09,763 Epoch[110/300], Step[1250/1252], Avg Loss: 3.5649, Avg Acc: 0.3809
2022-01-16 15:51:16,432 ----- Epoch[110/300], Train Loss: 3.5649, Train Acc: 0.3809, time: 2122.62, Best Val(epoch106) Acc@1: 0.7009
2022-01-16 15:51:16,432 ----- Validation after Epoch: 110
2022-01-16 15:52:34,603 Val Step[0000/1563], Avg Loss: 1.2827, Avg Acc@1: 0.7500, Avg Acc@5: 0.9688
2022-01-16 15:52:36,202 Val Step[0050/1563], Avg Loss: 1.3309, Avg Acc@1: 0.7034, Avg Acc@5: 0.9044
2022-01-16 15:52:37,770 Val Step[0100/1563], Avg Loss: 1.3450, Avg Acc@1: 0.7042, Avg Acc@5: 0.9004
2022-01-16 15:52:39,454 Val Step[0150/1563], Avg Loss: 1.3483, Avg Acc@1: 0.7059, Avg Acc@5: 0.9009
2022-01-16 15:52:41,132 Val Step[0200/1563], Avg Loss: 1.3468, Avg Acc@1: 0.7058, Avg Acc@5: 0.9000
2022-01-16 15:52:42,720 Val Step[0250/1563], Avg Loss: 1.3323, Avg Acc@1: 0.7088, Avg Acc@5: 0.9019
2022-01-16 15:52:44,351 Val Step[0300/1563], Avg Loss: 1.3327, Avg Acc@1: 0.7103, Avg Acc@5: 0.9003
2022-01-16 15:52:45,897 Val Step[0350/1563], Avg Loss: 1.3402, Avg Acc@1: 0.7092, Avg Acc@5: 0.8992
2022-01-16 15:52:47,488 Val Step[0400/1563], Avg Loss: 1.3380, Avg Acc@1: 0.7092, Avg Acc@5: 0.8996
2022-01-16 15:52:49,054 Val Step[0450/1563], Avg Loss: 1.3436, Avg Acc@1: 0.7064, Avg Acc@5: 0.8990
2022-01-16 15:52:50,680 Val Step[0500/1563], Avg Loss: 1.3427, Avg Acc@1: 0.7065, Avg Acc@5: 0.8998
2022-01-16 15:52:52,329 Val Step[0550/1563], Avg Loss: 1.3415, Avg Acc@1: 0.7068, Avg Acc@5: 0.9005
2022-01-16 15:52:53,883 Val Step[0600/1563], Avg Loss: 1.3405, Avg Acc@1: 0.7059, Avg Acc@5: 0.9008
2022-01-16 15:52:55,511 Val Step[0650/1563], Avg Loss: 1.3409, Avg Acc@1: 0.7054, Avg Acc@5: 0.9007
2022-01-16 15:52:57,110 Val Step[0700/1563], Avg Loss: 1.3370, Avg Acc@1: 0.7058, Avg Acc@5: 0.9017
2022-01-16 15:52:58,667 Val Step[0750/1563], Avg Loss: 1.3430, Avg Acc@1: 0.7044, Avg Acc@5: 0.9010
2022-01-16 15:53:00,488 Val Step[0800/1563], Avg Loss: 1.3409, Avg Acc@1: 0.7049, Avg Acc@5: 0.9013
2022-01-16 15:53:02,403 Val Step[0850/1563], Avg Loss: 1.3431, Avg Acc@1: 0.7044, Avg Acc@5: 0.9009
2022-01-16 15:53:04,032 Val Step[0900/1563], Avg Loss: 1.3405, Avg Acc@1: 0.7040, Avg Acc@5: 0.9015
2022-01-16 15:53:05,607 Val Step[0950/1563], Avg Loss: 1.3402, Avg Acc@1: 0.7041, Avg Acc@5: 0.9016
2022-01-16 15:53:07,381 Val Step[1000/1563], Avg Loss: 1.3404, Avg Acc@1: 0.7040, Avg Acc@5: 0.9014
2022-01-16 15:53:09,235 Val Step[1050/1563], Avg Loss: 1.3428, Avg Acc@1: 0.7031, Avg Acc@5: 0.9011
2022-01-16 15:53:10,921 Val Step[1100/1563], Avg Loss: 1.3420, Avg Acc@1: 0.7029, Avg Acc@5: 0.9010
2022-01-16 15:53:12,539 Val Step[1150/1563], Avg Loss: 1.3405, Avg Acc@1: 0.7028, Avg Acc@5: 0.9012
2022-01-16 15:53:14,185 Val Step[1200/1563], Avg Loss: 1.3390, Avg Acc@1: 0.7032, Avg Acc@5: 0.9016
2022-01-16 15:53:15,741 Val Step[1250/1563], Avg Loss: 1.3383, Avg Acc@1: 0.7028, Avg Acc@5: 0.9016
2022-01-16 15:53:17,343 Val Step[1300/1563], Avg Loss: 1.3417, Avg Acc@1: 0.7023, Avg Acc@5: 0.9013
2022-01-16 15:53:18,963 Val Step[1350/1563], Avg Loss: 1.3412, Avg Acc@1: 0.7022, Avg Acc@5: 0.9010
2022-01-16 15:53:20,504 Val Step[1400/1563], Avg Loss: 1.3401, Avg Acc@1: 0.7018, Avg Acc@5: 0.9009
2022-01-16 15:53:22,044 Val Step[1450/1563], Avg Loss: 1.3408, Avg Acc@1: 0.7015, Avg Acc@5: 0.9005
2022-01-16 15:53:23,628 Val Step[1500/1563], Avg Loss: 1.3399, Avg Acc@1: 0.7017, Avg Acc@5: 0.9006
2022-01-16 15:53:25,168 Val Step[1550/1563], Avg Loss: 1.3400, Avg Acc@1: 0.7017, Avg Acc@5: 0.9006
2022-01-16 15:53:27,057 ----- Epoch[110/300], Validation Loss: 1.3398, Validation Acc@1: 0.7016, Validation Acc@5: 0.9008, time: 130.62
2022-01-16 15:53:28,148 the pre best model acc:0.7009, at epoch 106
2022-01-16 15:53:28,148 current best model acc:0.7016, at epoch 110
2022-01-16 15:53:28,148 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-16 15:53:28,148 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-16 15:53:28,148 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-16 15:53:28,148 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-16 15:53:28,705 ----- Save model: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-110-Loss-3.5545796865456563.pdparams
2022-01-16 15:53:28,705 ----- Save optim: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-110-Loss-3.5545796865456563.pdopt
2022-01-16 15:53:28,706 Now training epoch 111. LR=0.000764
2022-01-16 15:55:18,572 Epoch[111/300], Step[0000/1252], Avg Loss: 3.0476, Avg Acc: 0.3203
2022-01-16 15:56:38,097 Epoch[111/300], Step[0050/1252], Avg Loss: 3.5746, Avg Acc: 0.3806
2022-01-16 15:57:57,620 Epoch[111/300], Step[0100/1252], Avg Loss: 3.5529, Avg Acc: 0.3802
2022-01-16 15:59:17,799 Epoch[111/300], Step[0150/1252], Avg Loss: 3.5475, Avg Acc: 0.3771
2022-01-16 16:00:38,153 Epoch[111/300], Step[0200/1252], Avg Loss: 3.5430, Avg Acc: 0.3836
2022-01-16 16:01:58,468 Epoch[111/300], Step[0250/1252], Avg Loss: 3.5438, Avg Acc: 0.3843
2022-01-16 16:03:18,998 Epoch[111/300], Step[0300/1252], Avg Loss: 3.5436, Avg Acc: 0.3824
2022-01-16 16:04:39,670 Epoch[111/300], Step[0350/1252], Avg Loss: 3.5408, Avg Acc: 0.3824
2022-01-16 16:05:59,813 Epoch[111/300], Step[0400/1252], Avg Loss: 3.5543, Avg Acc: 0.3815
2022-01-16 16:07:20,267 Epoch[111/300], Step[0450/1252], Avg Loss: 3.5537, Avg Acc: 0.3825
2022-01-16 16:08:40,241 Epoch[111/300], Step[0500/1252], Avg Loss: 3.5490, Avg Acc: 0.3841
2022-01-16 16:09:59,619 Epoch[111/300], Step[0550/1252], Avg Loss: 3.5484, Avg Acc: 0.3835
2022-01-16 16:11:20,409 Epoch[111/300], Step[0600/1252], Avg Loss: 3.5496, Avg Acc: 0.3846
2022-01-16 16:12:41,676 Epoch[111/300], Step[0650/1252], Avg Loss: 3.5452, Avg Acc: 0.3866
2022-01-16 16:14:02,473 Epoch[111/300], Step[0700/1252], Avg Loss: 3.5472, Avg Acc: 0.3856
2022-01-16 16:15:22,823 Epoch[111/300], Step[0750/1252], Avg Loss: 3.5488, Avg Acc: 0.3857
2022-01-16 16:16:43,590 Epoch[111/300], Step[0800/1252], Avg Loss: 3.5511, Avg Acc: 0.3856
2022-01-16 16:18:04,597 Epoch[111/300], Step[0850/1252], Avg Loss: 3.5536, Avg Acc: 0.3857
2022-01-16 16:19:25,039 Epoch[111/300], Step[0900/1252], Avg Loss: 3.5520, Avg Acc: 0.3851
2022-01-16 16:20:46,201 Epoch[111/300], Step[0950/1252], Avg Loss: 3.5518, Avg Acc: 0.3851
2022-01-16 16:22:05,099 Epoch[111/300], Step[1000/1252], Avg Loss: 3.5517, Avg Acc: 0.3860
2022-01-16 16:23:25,811 Epoch[111/300], Step[1050/1252], Avg Loss: 3.5525, Avg Acc: 0.3862
2022-01-16 16:24:45,625 Epoch[111/300], Step[1100/1252], Avg Loss: 3.5553, Avg Acc: 0.3858
2022-01-16 16:26:07,215 Epoch[111/300], Step[1150/1252], Avg Loss: 3.5585, Avg Acc: 0.3852
2022-01-16 16:27:27,131 Epoch[111/300], Step[1200/1252], Avg Loss: 3.5574, Avg Acc: 0.3857
2022-01-16 16:28:46,868 Epoch[111/300], Step[1250/1252], Avg Loss: 3.5580, Avg Acc: 0.3858
2022-01-16 16:28:53,604 ----- Epoch[111/300], Train Loss: 3.5579, Train Acc: 0.3858, time: 2124.89, Best Val(epoch110) Acc@1: 0.7016
2022-01-16 16:28:53,604 Now training epoch 112. LR=0.000759
2022-01-16 16:30:31,447 Epoch[112/300], Step[0000/1252], Avg Loss: 3.2909, Avg Acc: 0.3281
2022-01-16 16:31:50,573 Epoch[112/300], Step[0050/1252], Avg Loss: 3.5145, Avg Acc: 0.4286
2022-01-16 16:33:12,279 Epoch[112/300], Step[0100/1252], Avg Loss: 3.5535, Avg Acc: 0.3950
2022-01-16 16:34:33,388 Epoch[112/300], Step[0150/1252], Avg Loss: 3.5427, Avg Acc: 0.3875
2022-01-16 16:35:54,586 Epoch[112/300], Step[0200/1252], Avg Loss: 3.5372, Avg Acc: 0.3859
2022-01-16 16:37:15,864 Epoch[112/300], Step[0250/1252], Avg Loss: 3.5285, Avg Acc: 0.3868
2022-01-16 16:38:37,764 Epoch[112/300], Step[0300/1252], Avg Loss: 3.5260, Avg Acc: 0.3896
2022-01-16 16:39:59,365 Epoch[112/300], Step[0350/1252], Avg Loss: 3.5247, Avg Acc: 0.3894
2022-01-16 16:41:20,373 Epoch[112/300], Step[0400/1252], Avg Loss: 3.5298, Avg Acc: 0.3914
2022-01-16 16:42:41,755 Epoch[112/300], Step[0450/1252], Avg Loss: 3.5287, Avg Acc: 0.3942
2022-01-16 16:44:02,114 Epoch[112/300], Step[0500/1252], Avg Loss: 3.5287, Avg Acc: 0.3943
2022-01-16 16:45:23,738 Epoch[112/300], Step[0550/1252], Avg Loss: 3.5303, Avg Acc: 0.3897
2022-01-16 16:46:44,251 Epoch[112/300], Step[0600/1252], Avg Loss: 3.5332, Avg Acc: 0.3924
2022-01-16 16:48:05,871 Epoch[112/300], Step[0650/1252], Avg Loss: 3.5392, Avg Acc: 0.3927
2022-01-16 16:49:27,415 Epoch[112/300], Step[0700/1252], Avg Loss: 3.5424, Avg Acc: 0.3922
2022-01-16 16:50:48,404 Epoch[112/300], Step[0750/1252], Avg Loss: 3.5439, Avg Acc: 0.3922
2022-01-16 16:52:09,013 Epoch[112/300], Step[0800/1252], Avg Loss: 3.5402, Avg Acc: 0.3926
2022-01-16 16:53:30,349 Epoch[112/300], Step[0850/1252], Avg Loss: 3.5410, Avg Acc: 0.3926
2022-01-16 16:54:51,190 Epoch[112/300], Step[0900/1252], Avg Loss: 3.5419, Avg Acc: 0.3917
2022-01-16 16:56:13,199 Epoch[112/300], Step[0950/1252], Avg Loss: 3.5447, Avg Acc: 0.3901
2022-01-16 16:57:33,793 Epoch[112/300], Step[1000/1252], Avg Loss: 3.5472, Avg Acc: 0.3891
2022-01-16 16:58:54,918 Epoch[112/300], Step[1050/1252], Avg Loss: 3.5479, Avg Acc: 0.3876
2022-01-16 17:00:15,931 Epoch[112/300], Step[1100/1252], Avg Loss: 3.5468, Avg Acc: 0.3892
2022-01-16 17:01:36,764 Epoch[112/300], Step[1150/1252], Avg Loss: 3.5467, Avg Acc: 0.3888
2022-01-16 17:02:57,817 Epoch[112/300], Step[1200/1252], Avg Loss: 3.5452, Avg Acc: 0.3879
2022-01-16 17:04:17,892 Epoch[112/300], Step[1250/1252], Avg Loss: 3.5430, Avg Acc: 0.3890
2022-01-16 17:04:23,829 ----- Epoch[112/300], Train Loss: 3.5430, Train Acc: 0.3890, time: 2130.22, Best Val(epoch110) Acc@1: 0.7016
2022-01-16 17:04:23,829 ----- Validation after Epoch: 112
2022-01-16 17:05:41,410 Val Step[0000/1563], Avg Loss: 1.1730, Avg Acc@1: 0.6875, Avg Acc@5: 0.9688
2022-01-16 17:05:42,975 Val Step[0050/1563], Avg Loss: 1.3421, Avg Acc@1: 0.6955, Avg Acc@5: 0.8946
2022-01-16 17:05:44,663 Val Step[0100/1563], Avg Loss: 1.3334, Avg Acc@1: 0.7027, Avg Acc@5: 0.8994
2022-01-16 17:05:46,320 Val Step[0150/1563], Avg Loss: 1.3291, Avg Acc@1: 0.7038, Avg Acc@5: 0.8965
2022-01-16 17:05:47,966 Val Step[0200/1563], Avg Loss: 1.3282, Avg Acc@1: 0.7060, Avg Acc@5: 0.8975
2022-01-16 17:05:49,587 Val Step[0250/1563], Avg Loss: 1.3131, Avg Acc@1: 0.7089, Avg Acc@5: 0.8980
2022-01-16 17:05:51,301 Val Step[0300/1563], Avg Loss: 1.3157, Avg Acc@1: 0.7089, Avg Acc@5: 0.8987
2022-01-16 17:05:52,935 Val Step[0350/1563], Avg Loss: 1.3239, Avg Acc@1: 0.7074, Avg Acc@5: 0.8981
2022-01-16 17:05:54,520 Val Step[0400/1563], Avg Loss: 1.3232, Avg Acc@1: 0.7079, Avg Acc@5: 0.8975
2022-01-16 17:05:56,223 Val Step[0450/1563], Avg Loss: 1.3311, Avg Acc@1: 0.7046, Avg Acc@5: 0.8966
2022-01-16 17:05:57,878 Val Step[0500/1563], Avg Loss: 1.3320, Avg Acc@1: 0.7045, Avg Acc@5: 0.8970
2022-01-16 17:05:59,590 Val Step[0550/1563], Avg Loss: 1.3327, Avg Acc@1: 0.7037, Avg Acc@5: 0.8976
2022-01-16 17:06:01,273 Val Step[0600/1563], Avg Loss: 1.3340, Avg Acc@1: 0.7032, Avg Acc@5: 0.8977
2022-01-16 17:06:02,791 Val Step[0650/1563], Avg Loss: 1.3366, Avg Acc@1: 0.7030, Avg Acc@5: 0.8973
2022-01-16 17:06:04,480 Val Step[0700/1563], Avg Loss: 1.3318, Avg Acc@1: 0.7045, Avg Acc@5: 0.8987
2022-01-16 17:06:06,352 Val Step[0750/1563], Avg Loss: 1.3366, Avg Acc@1: 0.7038, Avg Acc@5: 0.8980
2022-01-16 17:06:08,015 Val Step[0800/1563], Avg Loss: 1.3350, Avg Acc@1: 0.7042, Avg Acc@5: 0.8984
2022-01-16 17:06:09,547 Val Step[0850/1563], Avg Loss: 1.3370, Avg Acc@1: 0.7040, Avg Acc@5: 0.8975
2022-01-16 17:06:11,367 Val Step[0900/1563], Avg Loss: 1.3340, Avg Acc@1: 0.7041, Avg Acc@5: 0.8981
2022-01-16 17:06:13,246 Val Step[0950/1563], Avg Loss: 1.3340, Avg Acc@1: 0.7039, Avg Acc@5: 0.8983
2022-01-16 17:06:15,057 Val Step[1000/1563], Avg Loss: 1.3363, Avg Acc@1: 0.7037, Avg Acc@5: 0.8980
2022-01-16 17:06:16,883 Val Step[1050/1563], Avg Loss: 1.3383, Avg Acc@1: 0.7027, Avg Acc@5: 0.8976
2022-01-16 17:06:18,507 Val Step[1100/1563], Avg Loss: 1.3376, Avg Acc@1: 0.7024, Avg Acc@5: 0.8979
2022-01-16 17:06:20,153 Val Step[1150/1563], Avg Loss: 1.3359, Avg Acc@1: 0.7025, Avg Acc@5: 0.8983
2022-01-16 17:06:21,716 Val Step[1200/1563], Avg Loss: 1.3350, Avg Acc@1: 0.7030, Avg Acc@5: 0.8986
2022-01-16 17:06:23,321 Val Step[1250/1563], Avg Loss: 1.3346, Avg Acc@1: 0.7030, Avg Acc@5: 0.8985
2022-01-16 17:06:24,862 Val Step[1300/1563], Avg Loss: 1.3377, Avg Acc@1: 0.7026, Avg Acc@5: 0.8982
2022-01-16 17:06:26,544 Val Step[1350/1563], Avg Loss: 1.3379, Avg Acc@1: 0.7022, Avg Acc@5: 0.8981
2022-01-16 17:06:28,281 Val Step[1400/1563], Avg Loss: 1.3371, Avg Acc@1: 0.7017, Avg Acc@5: 0.8979
2022-01-16 17:06:29,906 Val Step[1450/1563], Avg Loss: 1.3363, Avg Acc@1: 0.7020, Avg Acc@5: 0.8977
2022-01-16 17:06:31,686 Val Step[1500/1563], Avg Loss: 1.3356, Avg Acc@1: 0.7023, Avg Acc@5: 0.8982
2022-01-16 17:06:33,234 Val Step[1550/1563], Avg Loss: 1.3363, Avg Acc@1: 0.7021, Avg Acc@5: 0.8981
2022-01-16 17:06:36,046 ----- Epoch[112/300], Validation Loss: 1.3365, Validation Acc@1: 0.7021, Validation Acc@5: 0.8981, time: 132.21
2022-01-16 17:06:37,107 the pre best model acc:0.7016, at epoch 110
2022-01-16 17:06:37,406 current best model acc:0.7021, at epoch 112
2022-01-16 17:06:37,406 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-16 17:06:37,406 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-16 17:06:37,406 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-16 17:06:37,406 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-16 17:06:37,407 Now training epoch 113. LR=0.000754
2022-01-16 17:08:20,197 Epoch[113/300], Step[0000/1252], Avg Loss: 3.7609, Avg Acc: 0.2539
2022-01-16 17:09:39,994 Epoch[113/300], Step[0050/1252], Avg Loss: 3.5998, Avg Acc: 0.3844
2022-01-16 17:11:00,297 Epoch[113/300], Step[0100/1252], Avg Loss: 3.5729, Avg Acc: 0.3899
2022-01-16 17:12:20,725 Epoch[113/300], Step[0150/1252], Avg Loss: 3.5520, Avg Acc: 0.3924
2022-01-16 17:13:39,160 Epoch[113/300], Step[0200/1252], Avg Loss: 3.5518, Avg Acc: 0.3882
2022-01-16 17:14:59,850 Epoch[113/300], Step[0250/1252], Avg Loss: 3.5458, Avg Acc: 0.3867
2022-01-16 17:16:19,613 Epoch[113/300], Step[0300/1252], Avg Loss: 3.5448, Avg Acc: 0.3896
2022-01-16 17:17:40,934 Epoch[113/300], Step[0350/1252], Avg Loss: 3.5510, Avg Acc: 0.3881
2022-01-16 17:19:02,379 Epoch[113/300], Step[0400/1252], Avg Loss: 3.5543, Avg Acc: 0.3860
2022-01-16 17:20:23,264 Epoch[113/300], Step[0450/1252], Avg Loss: 3.5489, Avg Acc: 0.3864
2022-01-16 17:21:43,212 Epoch[113/300], Step[0500/1252], Avg Loss: 3.5474, Avg Acc: 0.3852
2022-01-16 17:23:03,959 Epoch[113/300], Step[0550/1252], Avg Loss: 3.5442, Avg Acc: 0.3866
2022-01-16 17:24:24,060 Epoch[113/300], Step[0600/1252], Avg Loss: 3.5456, Avg Acc: 0.3889
2022-01-16 17:25:45,678 Epoch[113/300], Step[0650/1252], Avg Loss: 3.5478, Avg Acc: 0.3859
2022-01-16 17:27:07,080 Epoch[113/300], Step[0700/1252], Avg Loss: 3.5509, Avg Acc: 0.3846
2022-01-16 17:28:28,432 Epoch[113/300], Step[0750/1252], Avg Loss: 3.5512, Avg Acc: 0.3863
2022-01-16 17:29:48,906 Epoch[113/300], Step[0800/1252], Avg Loss: 3.5516, Avg Acc: 0.3885
2022-01-16 17:31:10,034 Epoch[113/300], Step[0850/1252], Avg Loss: 3.5540, Avg Acc: 0.3887
2022-01-16 17:32:31,048 Epoch[113/300], Step[0900/1252], Avg Loss: 3.5529, Avg Acc: 0.3901
2022-01-16 17:33:52,081 Epoch[113/300], Step[0950/1252], Avg Loss: 3.5506, Avg Acc: 0.3908
2022-01-16 17:35:12,966 Epoch[113/300], Step[1000/1252], Avg Loss: 3.5527, Avg Acc: 0.3897
2022-01-16 17:36:33,942 Epoch[113/300], Step[1050/1252], Avg Loss: 3.5509, Avg Acc: 0.3908
2022-01-16 17:37:55,348 Epoch[113/300], Step[1100/1252], Avg Loss: 3.5498, Avg Acc: 0.3906
2022-01-16 17:39:16,242 Epoch[113/300], Step[1150/1252], Avg Loss: 3.5496, Avg Acc: 0.3901
2022-01-16 17:40:36,518 Epoch[113/300], Step[1200/1252], Avg Loss: 3.5482, Avg Acc: 0.3896
2022-01-16 17:41:57,085 Epoch[113/300], Step[1250/1252], Avg Loss: 3.5460, Avg Acc: 0.3907
2022-01-16 17:42:03,348 ----- Epoch[113/300], Train Loss: 3.5460, Train Acc: 0.3907, time: 2125.94, Best Val(epoch112) Acc@1: 0.7021
2022-01-16 17:42:03,348 Now training epoch 114. LR=0.000749
2022-01-16 17:43:49,012 Epoch[114/300], Step[0000/1252], Avg Loss: 3.7921, Avg Acc: 0.5322
2022-01-16 17:45:08,427 Epoch[114/300], Step[0050/1252], Avg Loss: 3.6019, Avg Acc: 0.3666
2022-01-16 17:46:28,868 Epoch[114/300], Step[0100/1252], Avg Loss: 3.5880, Avg Acc: 0.3807
2022-01-16 17:47:50,017 Epoch[114/300], Step[0150/1252], Avg Loss: 3.5728, Avg Acc: 0.3819
2022-01-16 17:49:11,467 Epoch[114/300], Step[0200/1252], Avg Loss: 3.5863, Avg Acc: 0.3741
2022-01-16 17:50:31,464 Epoch[114/300], Step[0250/1252], Avg Loss: 3.5803, Avg Acc: 0.3783
2022-01-16 17:51:52,906 Epoch[114/300], Step[0300/1252], Avg Loss: 3.5749, Avg Acc: 0.3767
2022-01-16 17:53:13,541 Epoch[114/300], Step[0350/1252], Avg Loss: 3.5703, Avg Acc: 0.3801
2022-01-16 17:54:34,064 Epoch[114/300], Step[0400/1252], Avg Loss: 3.5672, Avg Acc: 0.3801
2022-01-16 17:55:54,581 Epoch[114/300], Step[0450/1252], Avg Loss: 3.5651, Avg Acc: 0.3826
2022-01-16 17:57:14,916 Epoch[114/300], Step[0500/1252], Avg Loss: 3.5614, Avg Acc: 0.3852
2022-01-16 17:58:34,463 Epoch[114/300], Step[0550/1252], Avg Loss: 3.5609, Avg Acc: 0.3872
2022-01-16 17:59:54,976 Epoch[114/300], Step[0600/1252], Avg Loss: 3.5605, Avg Acc: 0.3871
2022-01-16 18:01:15,867 Epoch[114/300], Step[0650/1252], Avg Loss: 3.5649, Avg Acc: 0.3852
2022-01-16 18:02:36,416 Epoch[114/300], Step[0700/1252], Avg Loss: 3.5664, Avg Acc: 0.3852
2022-01-16 18:03:56,844 Epoch[114/300], Step[0750/1252], Avg Loss: 3.5610, Avg Acc: 0.3830
2022-01-16 18:05:16,741 Epoch[114/300], Step[0800/1252], Avg Loss: 3.5604, Avg Acc: 0.3849
2022-01-16 18:06:35,747 Epoch[114/300], Step[0850/1252], Avg Loss: 3.5570, Avg Acc: 0.3856
2022-01-16 18:07:56,380 Epoch[114/300], Step[0900/1252], Avg Loss: 3.5563, Avg Acc: 0.3869
2022-01-16 18:09:17,537 Epoch[114/300], Step[0950/1252], Avg Loss: 3.5555, Avg Acc: 0.3863
2022-01-16 18:10:38,675 Epoch[114/300], Step[1000/1252], Avg Loss: 3.5565, Avg Acc: 0.3839
2022-01-16 18:11:59,408 Epoch[114/300], Step[1050/1252], Avg Loss: 3.5570, Avg Acc: 0.3830
2022-01-16 18:13:20,009 Epoch[114/300], Step[1100/1252], Avg Loss: 3.5565, Avg Acc: 0.3830
2022-01-16 18:14:41,208 Epoch[114/300], Step[1150/1252], Avg Loss: 3.5555, Avg Acc: 0.3830
2022-01-16 18:16:01,811 Epoch[114/300], Step[1200/1252], Avg Loss: 3.5569, Avg Acc: 0.3836
2022-01-16 18:17:21,594 Epoch[114/300], Step[1250/1252], Avg Loss: 3.5580, Avg Acc: 0.3836
2022-01-16 18:17:29,708 ----- Epoch[114/300], Train Loss: 3.5580, Train Acc: 0.3837, time: 2126.36, Best Val(epoch112) Acc@1: 0.7021
2022-01-16 18:17:29,708 ----- Validation after Epoch: 114
2022-01-16 18:18:49,652 Val Step[0000/1563], Avg Loss: 1.1288, Avg Acc@1: 0.6562, Avg Acc@5: 0.9375
2022-01-16 18:18:51,336 Val Step[0050/1563], Avg Loss: 1.2876, Avg Acc@1: 0.7126, Avg Acc@5: 0.9026
2022-01-16 18:18:52,963 Val Step[0100/1563], Avg Loss: 1.2946, Avg Acc@1: 0.7116, Avg Acc@5: 0.9069
2022-01-16 18:18:54,695 Val Step[0150/1563], Avg Loss: 1.3046, Avg Acc@1: 0.7099, Avg Acc@5: 0.9048
2022-01-16 18:18:56,257 Val Step[0200/1563], Avg Loss: 1.3048, Avg Acc@1: 0.7107, Avg Acc@5: 0.9038
2022-01-16 18:18:58,025 Val Step[0250/1563], Avg Loss: 1.2913, Avg Acc@1: 0.7143, Avg Acc@5: 0.9038
2022-01-16 18:18:59,810 Val Step[0300/1563], Avg Loss: 1.2946, Avg Acc@1: 0.7156, Avg Acc@5: 0.9028
2022-01-16 18:19:01,494 Val Step[0350/1563], Avg Loss: 1.3029, Avg Acc@1: 0.7143, Avg Acc@5: 0.9030
2022-01-16 18:19:03,051 Val Step[0400/1563], Avg Loss: 1.2996, Avg Acc@1: 0.7153, Avg Acc@5: 0.9034
2022-01-16 18:19:04,645 Val Step[0450/1563], Avg Loss: 1.3010, Avg Acc@1: 0.7131, Avg Acc@5: 0.9034
2022-01-16 18:19:06,394 Val Step[0500/1563], Avg Loss: 1.3026, Avg Acc@1: 0.7116, Avg Acc@5: 0.9033
2022-01-16 18:19:08,111 Val Step[0550/1563], Avg Loss: 1.3026, Avg Acc@1: 0.7120, Avg Acc@5: 0.9036
2022-01-16 18:19:09,733 Val Step[0600/1563], Avg Loss: 1.3032, Avg Acc@1: 0.7111, Avg Acc@5: 0.9038
2022-01-16 18:19:11,412 Val Step[0650/1563], Avg Loss: 1.3053, Avg Acc@1: 0.7104, Avg Acc@5: 0.9036
2022-01-16 18:19:12,957 Val Step[0700/1563], Avg Loss: 1.3030, Avg Acc@1: 0.7113, Avg Acc@5: 0.9037
2022-01-16 18:19:14,654 Val Step[0750/1563], Avg Loss: 1.3087, Avg Acc@1: 0.7098, Avg Acc@5: 0.9029
2022-01-16 18:19:16,486 Val Step[0800/1563], Avg Loss: 1.3079, Avg Acc@1: 0.7107, Avg Acc@5: 0.9032
2022-01-16 18:19:18,229 Val Step[0850/1563], Avg Loss: 1.3104, Avg Acc@1: 0.7100, Avg Acc@5: 0.9027
2022-01-16 18:19:20,069 Val Step[0900/1563], Avg Loss: 1.3091, Avg Acc@1: 0.7099, Avg Acc@5: 0.9029
2022-01-16 18:19:21,752 Val Step[0950/1563], Avg Loss: 1.3090, Avg Acc@1: 0.7096, Avg Acc@5: 0.9031
2022-01-16 18:19:23,394 Val Step[1000/1563], Avg Loss: 1.3097, Avg Acc@1: 0.7092, Avg Acc@5: 0.9031
2022-01-16 18:19:25,129 Val Step[1050/1563], Avg Loss: 1.3122, Avg Acc@1: 0.7084, Avg Acc@5: 0.9025
2022-01-16 18:19:26,754 Val Step[1100/1563], Avg Loss: 1.3122, Avg Acc@1: 0.7080, Avg Acc@5: 0.9024
2022-01-16 18:19:28,313 Val Step[1150/1563], Avg Loss: 1.3110, Avg Acc@1: 0.7082, Avg Acc@5: 0.9027
2022-01-16 18:19:29,979 Val Step[1200/1563], Avg Loss: 1.3100, Avg Acc@1: 0.7087, Avg Acc@5: 0.9027
2022-01-16 18:19:31,581 Val Step[1250/1563], Avg Loss: 1.3092, Avg Acc@1: 0.7087, Avg Acc@5: 0.9030
2022-01-16 18:19:33,319 Val Step[1300/1563], Avg Loss: 1.3121, Avg Acc@1: 0.7088, Avg Acc@5: 0.9028
2022-01-16 18:19:34,980 Val Step[1350/1563], Avg Loss: 1.3121, Avg Acc@1: 0.7083, Avg Acc@5: 0.9027
2022-01-16 18:19:36,753 Val Step[1400/1563], Avg Loss: 1.3116, Avg Acc@1: 0.7081, Avg Acc@5: 0.9026
2022-01-16 18:19:38,506 Val Step[1450/1563], Avg Loss: 1.3125, Avg Acc@1: 0.7079, Avg Acc@5: 0.9024
2022-01-16 18:19:40,186 Val Step[1500/1563], Avg Loss: 1.3114, Avg Acc@1: 0.7083, Avg Acc@5: 0.9027
2022-01-16 18:19:41,722 Val Step[1550/1563], Avg Loss: 1.3127, Avg Acc@1: 0.7080, Avg Acc@5: 0.9024
2022-01-16 18:19:43,978 ----- Epoch[114/300], Validation Loss: 1.3129, Validation Acc@1: 0.7080, Validation Acc@5: 0.9025, time: 134.27
2022-01-16 18:19:45,044 the pre best model acc:0.7021, at epoch 112
2022-01-16 18:19:45,336 current best model acc:0.7080, at epoch 114
2022-01-16 18:19:45,336 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-16 18:19:45,336 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-16 18:19:45,336 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-16 18:19:45,336 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-16 18:19:45,336 Now training epoch 115. LR=0.000744
2022-01-16 18:21:25,666 Epoch[115/300], Step[0000/1252], Avg Loss: 3.7453, Avg Acc: 0.4932
2022-01-16 18:22:45,462 Epoch[115/300], Step[0050/1252], Avg Loss: 3.5389, Avg Acc: 0.3905
2022-01-16 18:24:05,114 Epoch[115/300], Step[0100/1252], Avg Loss: 3.5526, Avg Acc: 0.3762
2022-01-16 18:25:24,835 Epoch[115/300], Step[0150/1252], Avg Loss: 3.5572, Avg Acc: 0.3715
2022-01-16 18:26:44,147 Epoch[115/300], Step[0200/1252], Avg Loss: 3.5687, Avg Acc: 0.3766
2022-01-16 18:28:03,978 Epoch[115/300], Step[0250/1252], Avg Loss: 3.5568, Avg Acc: 0.3760
2022-01-16 18:29:23,580 Epoch[115/300], Step[0300/1252], Avg Loss: 3.5517, Avg Acc: 0.3764
2022-01-16 18:30:42,723 Epoch[115/300], Step[0350/1252], Avg Loss: 3.5437, Avg Acc: 0.3788
2022-01-16 18:32:02,361 Epoch[115/300], Step[0400/1252], Avg Loss: 3.5518, Avg Acc: 0.3797
2022-01-16 18:33:21,931 Epoch[115/300], Step[0450/1252], Avg Loss: 3.5461, Avg Acc: 0.3837
2022-01-16 18:34:42,502 Epoch[115/300], Step[0500/1252], Avg Loss: 3.5449, Avg Acc: 0.3843
2022-01-16 18:36:01,846 Epoch[115/300], Step[0550/1252], Avg Loss: 3.5445, Avg Acc: 0.3857
2022-01-16 18:37:22,730 Epoch[115/300], Step[0600/1252], Avg Loss: 3.5456, Avg Acc: 0.3852
2022-01-16 18:38:43,378 Epoch[115/300], Step[0650/1252], Avg Loss: 3.5440, Avg Acc: 0.3843
2022-01-16 18:40:03,088 Epoch[115/300], Step[0700/1252], Avg Loss: 3.5452, Avg Acc: 0.3856
2022-01-16 18:41:23,439 Epoch[115/300], Step[0750/1252], Avg Loss: 3.5512, Avg Acc: 0.3842
2022-01-16 18:42:43,512 Epoch[115/300], Step[0800/1252], Avg Loss: 3.5473, Avg Acc: 0.3857
2022-01-16 18:44:04,236 Epoch[115/300], Step[0850/1252], Avg Loss: 3.5486, Avg Acc: 0.3854
2022-01-16 18:45:24,676 Epoch[115/300], Step[0900/1252], Avg Loss: 3.5473, Avg Acc: 0.3857
2022-01-16 18:46:44,839 Epoch[115/300], Step[0950/1252], Avg Loss: 3.5503, Avg Acc: 0.3839
2022-01-16 18:48:04,320 Epoch[115/300], Step[1000/1252], Avg Loss: 3.5477, Avg Acc: 0.3854
2022-01-16 18:49:24,290 Epoch[115/300], Step[1050/1252], Avg Loss: 3.5437, Avg Acc: 0.3864
2022-01-16 18:50:44,692 Epoch[115/300], Step[1100/1252], Avg Loss: 3.5404, Avg Acc: 0.3876
2022-01-16 18:52:03,573 Epoch[115/300], Step[1150/1252], Avg Loss: 3.5382, Avg Acc: 0.3884
2022-01-16 18:53:24,882 Epoch[115/300], Step[1200/1252], Avg Loss: 3.5392, Avg Acc: 0.3883
2022-01-16 18:54:44,698 Epoch[115/300], Step[1250/1252], Avg Loss: 3.5403, Avg Acc: 0.3896
2022-01-16 18:54:52,249 ----- Epoch[115/300], Train Loss: 3.5403, Train Acc: 0.3896, time: 2106.91, Best Val(epoch114) Acc@1: 0.7080
2022-01-16 18:54:52,250 Now training epoch 116. LR=0.000740
2022-01-16 18:56:32,016 Epoch[116/300], Step[0000/1252], Avg Loss: 3.3044, Avg Acc: 0.2314
2022-01-16 18:57:51,840 Epoch[116/300], Step[0050/1252], Avg Loss: 3.4924, Avg Acc: 0.3898
2022-01-16 18:59:11,109 Epoch[116/300], Step[0100/1252], Avg Loss: 3.5006, Avg Acc: 0.4010
2022-01-16 19:00:31,596 Epoch[116/300], Step[0150/1252], Avg Loss: 3.5019, Avg Acc: 0.3950
2022-01-16 19:01:52,428 Epoch[116/300], Step[0200/1252], Avg Loss: 3.4991, Avg Acc: 0.3939
2022-01-16 19:03:12,293 Epoch[116/300], Step[0250/1252], Avg Loss: 3.5001, Avg Acc: 0.3926
2022-01-16 19:04:32,274 Epoch[116/300], Step[0300/1252], Avg Loss: 3.4982, Avg Acc: 0.3951
2022-01-16 19:05:52,428 Epoch[116/300], Step[0350/1252], Avg Loss: 3.5067, Avg Acc: 0.3935
2022-01-16 19:07:12,255 Epoch[116/300], Step[0400/1252], Avg Loss: 3.5124, Avg Acc: 0.3921
2022-01-16 19:08:32,174 Epoch[116/300], Step[0450/1252], Avg Loss: 3.5178, Avg Acc: 0.3903
2022-01-16 19:09:51,682 Epoch[116/300], Step[0500/1252], Avg Loss: 3.5154, Avg Acc: 0.3899
2022-01-16 19:11:11,777 Epoch[116/300], Step[0550/1252], Avg Loss: 3.5112, Avg Acc: 0.3905
2022-01-16 19:12:31,131 Epoch[116/300], Step[0600/1252], Avg Loss: 3.5142, Avg Acc: 0.3920
2022-01-16 19:13:50,905 Epoch[116/300], Step[0650/1252], Avg Loss: 3.5169, Avg Acc: 0.3928
2022-01-16 19:15:11,240 Epoch[116/300], Step[0700/1252], Avg Loss: 3.5213, Avg Acc: 0.3916
2022-01-16 19:16:31,190 Epoch[116/300], Step[0750/1252], Avg Loss: 3.5258, Avg Acc: 0.3896
2022-01-16 19:17:51,242 Epoch[116/300], Step[0800/1252], Avg Loss: 3.5260, Avg Acc: 0.3888
2022-01-16 19:19:10,585 Epoch[116/300], Step[0850/1252], Avg Loss: 3.5290, Avg Acc: 0.3881
2022-01-16 19:20:29,693 Epoch[116/300], Step[0900/1252], Avg Loss: 3.5293, Avg Acc: 0.3883
2022-01-16 19:21:49,364 Epoch[116/300], Step[0950/1252], Avg Loss: 3.5248, Avg Acc: 0.3884
2022-01-16 19:23:09,524 Epoch[116/300], Step[1000/1252], Avg Loss: 3.5279, Avg Acc: 0.3886
2022-01-16 19:24:29,094 Epoch[116/300], Step[1050/1252], Avg Loss: 3.5276, Avg Acc: 0.3891
2022-01-16 19:25:50,048 Epoch[116/300], Step[1100/1252], Avg Loss: 3.5307, Avg Acc: 0.3882
2022-01-16 19:27:10,247 Epoch[116/300], Step[1150/1252], Avg Loss: 3.5313, Avg Acc: 0.3874
2022-01-16 19:28:29,800 Epoch[116/300], Step[1200/1252], Avg Loss: 3.5288, Avg Acc: 0.3879
2022-01-16 19:29:49,607 Epoch[116/300], Step[1250/1252], Avg Loss: 3.5312, Avg Acc: 0.3880
2022-01-16 19:29:56,414 ----- Epoch[116/300], Train Loss: 3.5311, Train Acc: 0.3880, time: 2104.16, Best Val(epoch114) Acc@1: 0.7080
2022-01-16 19:29:56,414 ----- Validation after Epoch: 116
2022-01-16 19:31:22,506 Val Step[0000/1563], Avg Loss: 1.3048, Avg Acc@1: 0.7500, Avg Acc@5: 0.9375
2022-01-16 19:31:24,198 Val Step[0050/1563], Avg Loss: 1.3858, Avg Acc@1: 0.6991, Avg Acc@5: 0.8989
2022-01-16 19:31:25,803 Val Step[0100/1563], Avg Loss: 1.3955, Avg Acc@1: 0.7017, Avg Acc@5: 0.9007
2022-01-16 19:31:27,562 Val Step[0150/1563], Avg Loss: 1.3906, Avg Acc@1: 0.7043, Avg Acc@5: 0.8986
2022-01-16 19:31:29,172 Val Step[0200/1563], Avg Loss: 1.3946, Avg Acc@1: 0.7020, Avg Acc@5: 0.8988
2022-01-16 19:31:30,781 Val Step[0250/1563], Avg Loss: 1.3760, Avg Acc@1: 0.7062, Avg Acc@5: 0.9004
2022-01-16 19:31:32,429 Val Step[0300/1563], Avg Loss: 1.3766, Avg Acc@1: 0.7064, Avg Acc@5: 0.9001
2022-01-16 19:31:34,115 Val Step[0350/1563], Avg Loss: 1.3801, Avg Acc@1: 0.7058, Avg Acc@5: 0.8998
2022-01-16 19:31:35,748 Val Step[0400/1563], Avg Loss: 1.3795, Avg Acc@1: 0.7057, Avg Acc@5: 0.9004
2022-01-16 19:31:37,332 Val Step[0450/1563], Avg Loss: 1.3835, Avg Acc@1: 0.7041, Avg Acc@5: 0.9001
2022-01-16 19:31:39,109 Val Step[0500/1563], Avg Loss: 1.3850, Avg Acc@1: 0.7030, Avg Acc@5: 0.9002
2022-01-16 19:31:40,664 Val Step[0550/1563], Avg Loss: 1.3870, Avg Acc@1: 0.7025, Avg Acc@5: 0.9001
2022-01-16 19:31:42,182 Val Step[0600/1563], Avg Loss: 1.3880, Avg Acc@1: 0.7025, Avg Acc@5: 0.9001
2022-01-16 19:31:43,897 Val Step[0650/1563], Avg Loss: 1.3865, Avg Acc@1: 0.7029, Avg Acc@5: 0.9001
2022-01-16 19:31:45,516 Val Step[0700/1563], Avg Loss: 1.3823, Avg Acc@1: 0.7041, Avg Acc@5: 0.9012
2022-01-16 19:31:47,127 Val Step[0750/1563], Avg Loss: 1.3867, Avg Acc@1: 0.7040, Avg Acc@5: 0.9008
2022-01-16 19:31:48,672 Val Step[0800/1563], Avg Loss: 1.3860, Avg Acc@1: 0.7044, Avg Acc@5: 0.9014
2022-01-16 19:31:50,275 Val Step[0850/1563], Avg Loss: 1.3876, Avg Acc@1: 0.7039, Avg Acc@5: 0.9008
2022-01-16 19:31:51,891 Val Step[0900/1563], Avg Loss: 1.3842, Avg Acc@1: 0.7048, Avg Acc@5: 0.9009
2022-01-16 19:31:53,606 Val Step[0950/1563], Avg Loss: 1.3830, Avg Acc@1: 0.7051, Avg Acc@5: 0.9011
2022-01-16 19:31:55,491 Val Step[1000/1563], Avg Loss: 1.3843, Avg Acc@1: 0.7050, Avg Acc@5: 0.9009
2022-01-16 19:31:57,146 Val Step[1050/1563], Avg Loss: 1.3871, Avg Acc@1: 0.7039, Avg Acc@5: 0.9001
2022-01-16 19:31:58,695 Val Step[1100/1563], Avg Loss: 1.3872, Avg Acc@1: 0.7034, Avg Acc@5: 0.9003
2022-01-16 19:32:00,367 Val Step[1150/1563], Avg Loss: 1.3867, Avg Acc@1: 0.7034, Avg Acc@5: 0.9001
2022-01-16 19:32:02,079 Val Step[1200/1563], Avg Loss: 1.3848, Avg Acc@1: 0.7043, Avg Acc@5: 0.9004
2022-01-16 19:32:03,779 Val Step[1250/1563], Avg Loss: 1.3843, Avg Acc@1: 0.7044, Avg Acc@5: 0.9007
2022-01-16 19:32:05,346 Val Step[1300/1563], Avg Loss: 1.3875, Avg Acc@1: 0.7042, Avg Acc@5: 0.9004
2022-01-16 19:32:06,897 Val Step[1350/1563], Avg Loss: 1.3873, Avg Acc@1: 0.7039, Avg Acc@5: 0.9004
2022-01-16 19:32:08,483 Val Step[1400/1563], Avg Loss: 1.3868, Avg Acc@1: 0.7041, Avg Acc@5: 0.9004
2022-01-16 19:32:10,227 Val Step[1450/1563], Avg Loss: 1.3876, Avg Acc@1: 0.7038, Avg Acc@5: 0.9003
2022-01-16 19:32:11,797 Val Step[1500/1563], Avg Loss: 1.3875, Avg Acc@1: 0.7040, Avg Acc@5: 0.9003
2022-01-16 19:32:13,373 Val Step[1550/1563], Avg Loss: 1.3884, Avg Acc@1: 0.7037, Avg Acc@5: 0.9000
2022-01-16 19:32:15,345 ----- Epoch[116/300], Validation Loss: 1.3886, Validation Acc@1: 0.7038, Validation Acc@5: 0.9001, time: 138.93
2022-01-16 19:32:15,345 Now training epoch 117. LR=0.000735
2022-01-16 19:33:58,184 Epoch[117/300], Step[0000/1252], Avg Loss: 3.4959, Avg Acc: 0.3770
2022-01-16 19:35:18,685 Epoch[117/300], Step[0050/1252], Avg Loss: 3.5016, Avg Acc: 0.4215
2022-01-16 19:36:38,997 Epoch[117/300], Step[0100/1252], Avg Loss: 3.5426, Avg Acc: 0.3773
2022-01-16 19:37:57,969 Epoch[117/300], Step[0150/1252], Avg Loss: 3.5455, Avg Acc: 0.3827
2022-01-16 19:39:16,878 Epoch[117/300], Step[0200/1252], Avg Loss: 3.5403, Avg Acc: 0.3856
2022-01-16 19:40:35,738 Epoch[117/300], Step[0250/1252], Avg Loss: 3.5523, Avg Acc: 0.3808
2022-01-16 19:41:55,961 Epoch[117/300], Step[0300/1252], Avg Loss: 3.5547, Avg Acc: 0.3785
2022-01-16 19:43:15,723 Epoch[117/300], Step[0350/1252], Avg Loss: 3.5597, Avg Acc: 0.3793
2022-01-16 19:44:35,941 Epoch[117/300], Step[0400/1252], Avg Loss: 3.5529, Avg Acc: 0.3819
2022-01-16 19:45:55,740 Epoch[117/300], Step[0450/1252], Avg Loss: 3.5426, Avg Acc: 0.3841
2022-01-16 19:47:15,693 Epoch[117/300], Step[0500/1252], Avg Loss: 3.5398, Avg Acc: 0.3869
2022-01-16 19:48:35,942 Epoch[117/300], Step[0550/1252], Avg Loss: 3.5489, Avg Acc: 0.3837
2022-01-16 19:49:57,217 Epoch[117/300], Step[0600/1252], Avg Loss: 3.5484, Avg Acc: 0.3815
2022-01-16 19:51:17,596 Epoch[117/300], Step[0650/1252], Avg Loss: 3.5498, Avg Acc: 0.3805
2022-01-16 19:52:37,691 Epoch[117/300], Step[0700/1252], Avg Loss: 3.5485, Avg Acc: 0.3809
2022-01-16 19:53:57,242 Epoch[117/300], Step[0750/1252], Avg Loss: 3.5518, Avg Acc: 0.3820
2022-01-16 19:55:17,736 Epoch[117/300], Step[0800/1252], Avg Loss: 3.5491, Avg Acc: 0.3836
2022-01-16 19:56:38,084 Epoch[117/300], Step[0850/1252], Avg Loss: 3.5482, Avg Acc: 0.3839
2022-01-16 19:57:58,524 Epoch[117/300], Step[0900/1252], Avg Loss: 3.5486, Avg Acc: 0.3827
2022-01-16 19:59:19,682 Epoch[117/300], Step[0950/1252], Avg Loss: 3.5476, Avg Acc: 0.3820
2022-01-16 20:00:39,943 Epoch[117/300], Step[1000/1252], Avg Loss: 3.5476, Avg Acc: 0.3833
2022-01-16 20:02:00,582 Epoch[117/300], Step[1050/1252], Avg Loss: 3.5490, Avg Acc: 0.3824
2022-01-16 20:03:20,506 Epoch[117/300], Step[1100/1252], Avg Loss: 3.5497, Avg Acc: 0.3818
2022-01-16 20:04:41,135 Epoch[117/300], Step[1150/1252], Avg Loss: 3.5506, Avg Acc: 0.3824
2022-01-16 20:06:02,193 Epoch[117/300], Step[1200/1252], Avg Loss: 3.5516, Avg Acc: 0.3820
2022-01-16 20:07:21,610 Epoch[117/300], Step[1250/1252], Avg Loss: 3.5494, Avg Acc: 0.3822
2022-01-16 20:07:27,948 ----- Epoch[117/300], Train Loss: 3.5493, Train Acc: 0.3822, time: 2112.60, Best Val(epoch114) Acc@1: 0.7080
2022-01-16 20:07:27,948 Now training epoch 118. LR=0.000730
2022-01-16 20:09:12,744 Epoch[118/300], Step[0000/1252], Avg Loss: 3.7427, Avg Acc: 0.3896
2022-01-16 20:10:32,834 Epoch[118/300], Step[0050/1252], Avg Loss: 3.4981, Avg Acc: 0.3930
2022-01-16 20:11:51,726 Epoch[118/300], Step[0100/1252], Avg Loss: 3.5018, Avg Acc: 0.3871
2022-01-16 20:13:10,300 Epoch[118/300], Step[0150/1252], Avg Loss: 3.4898, Avg Acc: 0.3787
2022-01-16 20:14:28,784 Epoch[118/300], Step[0200/1252], Avg Loss: 3.5007, Avg Acc: 0.3862
2022-01-16 20:15:47,516 Epoch[118/300], Step[0250/1252], Avg Loss: 3.5069, Avg Acc: 0.3852
2022-01-16 20:17:05,792 Epoch[118/300], Step[0300/1252], Avg Loss: 3.5092, Avg Acc: 0.3831
2022-01-16 20:18:25,148 Epoch[118/300], Step[0350/1252], Avg Loss: 3.5143, Avg Acc: 0.3821
2022-01-16 20:19:44,175 Epoch[118/300], Step[0400/1252], Avg Loss: 3.5143, Avg Acc: 0.3869
2022-01-16 20:21:03,325 Epoch[118/300], Step[0450/1252], Avg Loss: 3.5209, Avg Acc: 0.3867
2022-01-16 20:22:22,481 Epoch[118/300], Step[0500/1252], Avg Loss: 3.5234, Avg Acc: 0.3875
2022-01-16 20:23:41,698 Epoch[118/300], Step[0550/1252], Avg Loss: 3.5241, Avg Acc: 0.3890
2022-01-16 20:25:01,170 Epoch[118/300], Step[0600/1252], Avg Loss: 3.5320, Avg Acc: 0.3888
2022-01-16 20:26:20,483 Epoch[118/300], Step[0650/1252], Avg Loss: 3.5262, Avg Acc: 0.3888
2022-01-16 20:27:40,051 Epoch[118/300], Step[0700/1252], Avg Loss: 3.5295, Avg Acc: 0.3881
2022-01-16 20:28:58,493 Epoch[118/300], Step[0750/1252], Avg Loss: 3.5298, Avg Acc: 0.3889
2022-01-16 20:30:17,233 Epoch[118/300], Step[0800/1252], Avg Loss: 3.5331, Avg Acc: 0.3870
2022-01-16 20:31:36,144 Epoch[118/300], Step[0850/1252], Avg Loss: 3.5287, Avg Acc: 0.3858
2022-01-16 20:32:55,244 Epoch[118/300], Step[0900/1252], Avg Loss: 3.5316, Avg Acc: 0.3849
2022-01-16 20:34:14,862 Epoch[118/300], Step[0950/1252], Avg Loss: 3.5298, Avg Acc: 0.3843
2022-01-16 20:35:34,384 Epoch[118/300], Step[1000/1252], Avg Loss: 3.5289, Avg Acc: 0.3844
2022-01-16 20:36:53,529 Epoch[118/300], Step[1050/1252], Avg Loss: 3.5308, Avg Acc: 0.3839
2022-01-16 20:38:11,840 Epoch[118/300], Step[1100/1252], Avg Loss: 3.5299, Avg Acc: 0.3836
2022-01-16 20:39:30,884 Epoch[118/300], Step[1150/1252], Avg Loss: 3.5280, Avg Acc: 0.3846
2022-01-16 20:40:50,177 Epoch[118/300], Step[1200/1252], Avg Loss: 3.5272, Avg Acc: 0.3849
2022-01-16 20:42:09,645 Epoch[118/300], Step[1250/1252], Avg Loss: 3.5262, Avg Acc: 0.3855
2022-01-16 20:42:16,558 ----- Epoch[118/300], Train Loss: 3.5261, Train Acc: 0.3855, time: 2088.61, Best Val(epoch114) Acc@1: 0.7080
2022-01-16 20:42:16,558 ----- Validation after Epoch: 118
2022-01-16 20:43:48,398 Val Step[0000/1563], Avg Loss: 1.1071, Avg Acc@1: 0.7812, Avg Acc@5: 0.9688
2022-01-16 20:43:50,051 Val Step[0050/1563], Avg Loss: 1.2969, Avg Acc@1: 0.7145, Avg Acc@5: 0.8971
2022-01-16 20:43:51,640 Val Step[0100/1563], Avg Loss: 1.3012, Avg Acc@1: 0.7119, Avg Acc@5: 0.9004
2022-01-16 20:43:53,457 Val Step[0150/1563], Avg Loss: 1.2981, Avg Acc@1: 0.7090, Avg Acc@5: 0.9005
2022-01-16 20:43:55,152 Val Step[0200/1563], Avg Loss: 1.2985, Avg Acc@1: 0.7083, Avg Acc@5: 0.9008
2022-01-16 20:43:56,840 Val Step[0250/1563], Avg Loss: 1.2797, Avg Acc@1: 0.7118, Avg Acc@5: 0.9029
2022-01-16 20:43:58,574 Val Step[0300/1563], Avg Loss: 1.2805, Avg Acc@1: 0.7096, Avg Acc@5: 0.9024
2022-01-16 20:44:00,427 Val Step[0350/1563], Avg Loss: 1.2884, Avg Acc@1: 0.7098, Avg Acc@5: 0.9025
2022-01-16 20:44:01,978 Val Step[0400/1563], Avg Loss: 1.2863, Avg Acc@1: 0.7103, Avg Acc@5: 0.9032
2022-01-16 20:44:03,650 Val Step[0450/1563], Avg Loss: 1.2930, Avg Acc@1: 0.7070, Avg Acc@5: 0.9029
2022-01-16 20:44:05,255 Val Step[0500/1563], Avg Loss: 1.2959, Avg Acc@1: 0.7059, Avg Acc@5: 0.9029
2022-01-16 20:44:06,832 Val Step[0550/1563], Avg Loss: 1.2951, Avg Acc@1: 0.7064, Avg Acc@5: 0.9034
2022-01-16 20:44:08,441 Val Step[0600/1563], Avg Loss: 1.2955, Avg Acc@1: 0.7061, Avg Acc@5: 0.9038
2022-01-16 20:44:10,121 Val Step[0650/1563], Avg Loss: 1.2969, Avg Acc@1: 0.7058, Avg Acc@5: 0.9036
2022-01-16 20:44:11,792 Val Step[0700/1563], Avg Loss: 1.2928, Avg Acc@1: 0.7070, Avg Acc@5: 0.9044
2022-01-16 20:44:13,424 Val Step[0750/1563], Avg Loss: 1.2989, Avg Acc@1: 0.7059, Avg Acc@5: 0.9035
2022-01-16 20:44:15,035 Val Step[0800/1563], Avg Loss: 1.2991, Avg Acc@1: 0.7069, Avg Acc@5: 0.9034
2022-01-16 20:44:16,706 Val Step[0850/1563], Avg Loss: 1.3016, Avg Acc@1: 0.7062, Avg Acc@5: 0.9029
2022-01-16 20:44:18,333 Val Step[0900/1563], Avg Loss: 1.2975, Avg Acc@1: 0.7063, Avg Acc@5: 0.9033
2022-01-16 20:44:20,182 Val Step[0950/1563], Avg Loss: 1.2969, Avg Acc@1: 0.7065, Avg Acc@5: 0.9038
2022-01-16 20:44:21,772 Val Step[1000/1563], Avg Loss: 1.2972, Avg Acc@1: 0.7061, Avg Acc@5: 0.9038
2022-01-16 20:44:23,302 Val Step[1050/1563], Avg Loss: 1.3000, Avg Acc@1: 0.7052, Avg Acc@5: 0.9034
2022-01-16 20:44:25,039 Val Step[1100/1563], Avg Loss: 1.2994, Avg Acc@1: 0.7051, Avg Acc@5: 0.9035
2022-01-16 20:44:26,769 Val Step[1150/1563], Avg Loss: 1.2983, Avg Acc@1: 0.7052, Avg Acc@5: 0.9034
2022-01-16 20:44:28,402 Val Step[1200/1563], Avg Loss: 1.2978, Avg Acc@1: 0.7054, Avg Acc@5: 0.9033
2022-01-16 20:44:29,983 Val Step[1250/1563], Avg Loss: 1.2972, Avg Acc@1: 0.7051, Avg Acc@5: 0.9037
2022-01-16 20:44:31,651 Val Step[1300/1563], Avg Loss: 1.2994, Avg Acc@1: 0.7049, Avg Acc@5: 0.9035
2022-01-16 20:44:33,198 Val Step[1350/1563], Avg Loss: 1.2997, Avg Acc@1: 0.7047, Avg Acc@5: 0.9034
2022-01-16 20:44:34,742 Val Step[1400/1563], Avg Loss: 1.2988, Avg Acc@1: 0.7046, Avg Acc@5: 0.9030
2022-01-16 20:44:36,451 Val Step[1450/1563], Avg Loss: 1.2999, Avg Acc@1: 0.7043, Avg Acc@5: 0.9030
2022-01-16 20:44:38,096 Val Step[1500/1563], Avg Loss: 1.3001, Avg Acc@1: 0.7042, Avg Acc@5: 0.9030
2022-01-16 20:44:39,623 Val Step[1550/1563], Avg Loss: 1.3001, Avg Acc@1: 0.7041, Avg Acc@5: 0.9031
2022-01-16 20:44:41,734 ----- Epoch[118/300], Validation Loss: 1.3002, Validation Acc@1: 0.7041, Validation Acc@5: 0.9032, time: 145.17
2022-01-16 20:44:41,734 Now training epoch 119. LR=0.000725
2022-01-16 20:46:32,215 Epoch[119/300], Step[0000/1252], Avg Loss: 2.9762, Avg Acc: 0.6064
2022-01-16 20:47:51,759 Epoch[119/300], Step[0050/1252], Avg Loss: 3.4860, Avg Acc: 0.3829
2022-01-16 20:49:10,716 Epoch[119/300], Step[0100/1252], Avg Loss: 3.4623, Avg Acc: 0.3816
2022-01-16 20:50:29,087 Epoch[119/300], Step[0150/1252], Avg Loss: 3.4775, Avg Acc: 0.3857
2022-01-16 20:51:49,404 Epoch[119/300], Step[0200/1252], Avg Loss: 3.4840, Avg Acc: 0.3867
2022-01-16 20:53:09,151 Epoch[119/300], Step[0250/1252], Avg Loss: 3.4932, Avg Acc: 0.3855
2022-01-16 20:54:28,690 Epoch[119/300], Step[0300/1252], Avg Loss: 3.4885, Avg Acc: 0.3884
2022-01-16 20:55:48,251 Epoch[119/300], Step[0350/1252], Avg Loss: 3.4963, Avg Acc: 0.3913
2022-01-16 20:57:08,332 Epoch[119/300], Step[0400/1252], Avg Loss: 3.5018, Avg Acc: 0.3908
2022-01-16 20:58:27,518 Epoch[119/300], Step[0450/1252], Avg Loss: 3.5031, Avg Acc: 0.3909
2022-01-16 20:59:46,470 Epoch[119/300], Step[0500/1252], Avg Loss: 3.4971, Avg Acc: 0.3914
2022-01-16 21:01:06,764 Epoch[119/300], Step[0550/1252], Avg Loss: 3.5068, Avg Acc: 0.3885
2022-01-16 21:02:26,905 Epoch[119/300], Step[0600/1252], Avg Loss: 3.5113, Avg Acc: 0.3888
2022-01-16 21:03:47,129 Epoch[119/300], Step[0650/1252], Avg Loss: 3.5158, Avg Acc: 0.3894
2022-01-16 21:05:06,657 Epoch[119/300], Step[0700/1252], Avg Loss: 3.5209, Avg Acc: 0.3882
2022-01-16 21:06:26,389 Epoch[119/300], Step[0750/1252], Avg Loss: 3.5220, Avg Acc: 0.3872
2022-01-16 21:07:46,823 Epoch[119/300], Step[0800/1252], Avg Loss: 3.5226, Avg Acc: 0.3872
2022-01-16 21:09:07,119 Epoch[119/300], Step[0850/1252], Avg Loss: 3.5235, Avg Acc: 0.3872
2022-01-16 21:10:26,578 Epoch[119/300], Step[0900/1252], Avg Loss: 3.5266, Avg Acc: 0.3884
2022-01-16 21:11:47,354 Epoch[119/300], Step[0950/1252], Avg Loss: 3.5278, Avg Acc: 0.3886
2022-01-16 21:13:08,088 Epoch[119/300], Step[1000/1252], Avg Loss: 3.5266, Avg Acc: 0.3891
2022-01-16 21:14:28,488 Epoch[119/300], Step[1050/1252], Avg Loss: 3.5277, Avg Acc: 0.3891
2022-01-16 21:15:48,117 Epoch[119/300], Step[1100/1252], Avg Loss: 3.5267, Avg Acc: 0.3893
2022-01-16 21:17:08,995 Epoch[119/300], Step[1150/1252], Avg Loss: 3.5256, Avg Acc: 0.3893
2022-01-16 21:18:28,508 Epoch[119/300], Step[1200/1252], Avg Loss: 3.5262, Avg Acc: 0.3904
2022-01-16 21:19:45,598 Epoch[119/300], Step[1250/1252], Avg Loss: 3.5289, Avg Acc: 0.3893
2022-01-16 21:19:52,073 ----- Epoch[119/300], Train Loss: 3.5289, Train Acc: 0.3892, time: 2110.34, Best Val(epoch114) Acc@1: 0.7080
2022-01-16 21:19:52,073 Now training epoch 120. LR=0.000720
2022-01-16 21:21:38,662 Epoch[120/300], Step[0000/1252], Avg Loss: 3.7647, Avg Acc: 0.2275
2022-01-16 21:22:58,663 Epoch[120/300], Step[0050/1252], Avg Loss: 3.4676, Avg Acc: 0.4132
2022-01-16 21:24:18,455 Epoch[120/300], Step[0100/1252], Avg Loss: 3.4612, Avg Acc: 0.4122
2022-01-16 21:25:39,061 Epoch[120/300], Step[0150/1252], Avg Loss: 3.4890, Avg Acc: 0.4062
2022-01-16 21:26:58,548 Epoch[120/300], Step[0200/1252], Avg Loss: 3.4923, Avg Acc: 0.4024
2022-01-16 21:28:17,900 Epoch[120/300], Step[0250/1252], Avg Loss: 3.5032, Avg Acc: 0.4022
2022-01-16 21:29:38,383 Epoch[120/300], Step[0300/1252], Avg Loss: 3.5017, Avg Acc: 0.3953
2022-01-16 21:30:58,564 Epoch[120/300], Step[0350/1252], Avg Loss: 3.5058, Avg Acc: 0.3965
2022-01-16 21:32:20,193 Epoch[120/300], Step[0400/1252], Avg Loss: 3.5002, Avg Acc: 0.3906
2022-01-16 21:33:39,619 Epoch[120/300], Step[0450/1252], Avg Loss: 3.5082, Avg Acc: 0.3905
2022-01-16 21:35:00,415 Epoch[120/300], Step[0500/1252], Avg Loss: 3.5104, Avg Acc: 0.3915
2022-01-16 21:36:20,954 Epoch[120/300], Step[0550/1252], Avg Loss: 3.5098, Avg Acc: 0.3898
2022-01-16 21:37:40,902 Epoch[120/300], Step[0600/1252], Avg Loss: 3.5123, Avg Acc: 0.3915
2022-01-16 21:39:00,434 Epoch[120/300], Step[0650/1252], Avg Loss: 3.5151, Avg Acc: 0.3919
2022-01-16 21:40:20,439 Epoch[120/300], Step[0700/1252], Avg Loss: 3.5168, Avg Acc: 0.3904
2022-01-16 21:41:40,770 Epoch[120/300], Step[0750/1252], Avg Loss: 3.5176, Avg Acc: 0.3935
2022-01-16 21:43:01,323 Epoch[120/300], Step[0800/1252], Avg Loss: 3.5189, Avg Acc: 0.3932
2022-01-16 21:44:22,469 Epoch[120/300], Step[0850/1252], Avg Loss: 3.5195, Avg Acc: 0.3928
2022-01-16 21:45:42,749 Epoch[120/300], Step[0900/1252], Avg Loss: 3.5156, Avg Acc: 0.3930
2022-01-16 21:47:02,887 Epoch[120/300], Step[0950/1252], Avg Loss: 3.5186, Avg Acc: 0.3927
2022-01-16 21:48:23,434 Epoch[120/300], Step[1000/1252], Avg Loss: 3.5166, Avg Acc: 0.3941
2022-01-16 21:49:43,473 Epoch[120/300], Step[1050/1252], Avg Loss: 3.5162, Avg Acc: 0.3927
2022-01-16 21:51:03,447 Epoch[120/300], Step[1100/1252], Avg Loss: 3.5161, Avg Acc: 0.3925
2022-01-16 21:52:24,189 Epoch[120/300], Step[1150/1252], Avg Loss: 3.5169, Avg Acc: 0.3932
2022-01-16 21:53:44,338 Epoch[120/300], Step[1200/1252], Avg Loss: 3.5171, Avg Acc: 0.3936
2022-01-16 21:55:03,697 Epoch[120/300], Step[1250/1252], Avg Loss: 3.5186, Avg Acc: 0.3930
2022-01-16 21:55:10,040 ----- Epoch[120/300], Train Loss: 3.5187, Train Acc: 0.3929, time: 2117.96, Best Val(epoch114) Acc@1: 0.7080
2022-01-16 21:55:10,040 ----- Validation after Epoch: 120
2022-01-16 21:56:34,993 Val Step[0000/1563], Avg Loss: 1.2468, Avg Acc@1: 0.7188, Avg Acc@5: 0.8750
2022-01-16 21:56:36,790 Val Step[0050/1563], Avg Loss: 1.3742, Avg Acc@1: 0.6961, Avg Acc@5: 0.9007
2022-01-16 21:56:38,399 Val Step[0100/1563], Avg Loss: 1.3648, Avg Acc@1: 0.7014, Avg Acc@5: 0.9016
2022-01-16 21:56:40,020 Val Step[0150/1563], Avg Loss: 1.3585, Avg Acc@1: 0.7014, Avg Acc@5: 0.9002
2022-01-16 21:56:41,948 Val Step[0200/1563], Avg Loss: 1.3585, Avg Acc@1: 0.7026, Avg Acc@5: 0.9022
2022-01-16 21:56:43,728 Val Step[0250/1563], Avg Loss: 1.3471, Avg Acc@1: 0.7051, Avg Acc@5: 0.9020
2022-01-16 21:56:45,394 Val Step[0300/1563], Avg Loss: 1.3499, Avg Acc@1: 0.7070, Avg Acc@5: 0.9001
2022-01-16 21:56:47,028 Val Step[0350/1563], Avg Loss: 1.3594, Avg Acc@1: 0.7057, Avg Acc@5: 0.8991
2022-01-16 21:56:48,566 Val Step[0400/1563], Avg Loss: 1.3546, Avg Acc@1: 0.7071, Avg Acc@5: 0.8988
2022-01-16 21:56:50,159 Val Step[0450/1563], Avg Loss: 1.3615, Avg Acc@1: 0.7051, Avg Acc@5: 0.8984
2022-01-16 21:56:51,905 Val Step[0500/1563], Avg Loss: 1.3603, Avg Acc@1: 0.7048, Avg Acc@5: 0.8996
2022-01-16 21:56:53,669 Val Step[0550/1563], Avg Loss: 1.3632, Avg Acc@1: 0.7043, Avg Acc@5: 0.8993
2022-01-16 21:56:55,419 Val Step[0600/1563], Avg Loss: 1.3642, Avg Acc@1: 0.7041, Avg Acc@5: 0.8996
2022-01-16 21:56:57,192 Val Step[0650/1563], Avg Loss: 1.3652, Avg Acc@1: 0.7032, Avg Acc@5: 0.8999
2022-01-16 21:56:58,827 Val Step[0700/1563], Avg Loss: 1.3618, Avg Acc@1: 0.7037, Avg Acc@5: 0.9007
2022-01-16 21:57:00,590 Val Step[0750/1563], Avg Loss: 1.3661, Avg Acc@1: 0.7026, Avg Acc@5: 0.9000
2022-01-16 21:57:02,355 Val Step[0800/1563], Avg Loss: 1.3642, Avg Acc@1: 0.7038, Avg Acc@5: 0.9003
2022-01-16 21:57:04,218 Val Step[0850/1563], Avg Loss: 1.3661, Avg Acc@1: 0.7036, Avg Acc@5: 0.8997
2022-01-16 21:57:05,938 Val Step[0900/1563], Avg Loss: 1.3631, Avg Acc@1: 0.7038, Avg Acc@5: 0.9002
2022-01-16 21:57:07,761 Val Step[0950/1563], Avg Loss: 1.3624, Avg Acc@1: 0.7040, Avg Acc@5: 0.9005
2022-01-16 21:57:09,462 Val Step[1000/1563], Avg Loss: 1.3636, Avg Acc@1: 0.7041, Avg Acc@5: 0.9005
2022-01-16 21:57:11,038 Val Step[1050/1563], Avg Loss: 1.3660, Avg Acc@1: 0.7033, Avg Acc@5: 0.9003
2022-01-16 21:57:12,897 Val Step[1100/1563], Avg Loss: 1.3657, Avg Acc@1: 0.7034, Avg Acc@5: 0.9004
2022-01-16 21:57:14,532 Val Step[1150/1563], Avg Loss: 1.3646, Avg Acc@1: 0.7037, Avg Acc@5: 0.9006
2022-01-16 21:57:16,084 Val Step[1200/1563], Avg Loss: 1.3642, Avg Acc@1: 0.7044, Avg Acc@5: 0.9006
2022-01-16 21:57:17,938 Val Step[1250/1563], Avg Loss: 1.3635, Avg Acc@1: 0.7044, Avg Acc@5: 0.9007
2022-01-16 21:57:19,568 Val Step[1300/1563], Avg Loss: 1.3672, Avg Acc@1: 0.7041, Avg Acc@5: 0.9002
2022-01-16 21:57:21,381 Val Step[1350/1563], Avg Loss: 1.3664, Avg Acc@1: 0.7042, Avg Acc@5: 0.9002
2022-01-16 21:57:23,276 Val Step[1400/1563], Avg Loss: 1.3662, Avg Acc@1: 0.7040, Avg Acc@5: 0.9004
2022-01-16 21:57:24,835 Val Step[1450/1563], Avg Loss: 1.3664, Avg Acc@1: 0.7039, Avg Acc@5: 0.9001
2022-01-16 21:57:26,386 Val Step[1500/1563], Avg Loss: 1.3666, Avg Acc@1: 0.7039, Avg Acc@5: 0.9000
2022-01-16 21:57:28,002 Val Step[1550/1563], Avg Loss: 1.3673, Avg Acc@1: 0.7039, Avg Acc@5: 0.8998
2022-01-16 21:57:30,277 ----- Epoch[120/300], Validation Loss: 1.3672, Validation Acc@1: 0.7039, Validation Acc@5: 0.8998, time: 140.24
2022-01-16 21:57:30,644 ----- Save model: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-120-Loss-3.5246432570226798.pdparams
2022-01-16 21:57:30,644 ----- Save optim: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-120-Loss-3.5246432570226798.pdopt
2022-01-16 21:57:30,644 Now training epoch 121. LR=0.000715
2022-01-16 21:59:19,550 Epoch[121/300], Step[0000/1252], Avg Loss: 3.7644, Avg Acc: 0.3506
2022-01-16 22:00:39,547 Epoch[121/300], Step[0050/1252], Avg Loss: 3.5635, Avg Acc: 0.4071
2022-01-16 22:01:58,380 Epoch[121/300], Step[0100/1252], Avg Loss: 3.5272, Avg Acc: 0.4126
2022-01-16 22:03:17,642 Epoch[121/300], Step[0150/1252], Avg Loss: 3.4760, Avg Acc: 0.4154
2022-01-16 22:04:38,575 Epoch[121/300], Step[0200/1252], Avg Loss: 3.4770, Avg Acc: 0.4097
2022-01-16 22:05:58,989 Epoch[121/300], Step[0250/1252], Avg Loss: 3.4857, Avg Acc: 0.4087
2022-01-16 22:07:18,020 Epoch[121/300], Step[0300/1252], Avg Loss: 3.4833, Avg Acc: 0.4120
2022-01-16 22:08:37,717 Epoch[121/300], Step[0350/1252], Avg Loss: 3.4888, Avg Acc: 0.4100
2022-01-16 22:09:58,208 Epoch[121/300], Step[0400/1252], Avg Loss: 3.4892, Avg Acc: 0.4085
2022-01-16 22:11:18,010 Epoch[121/300], Step[0450/1252], Avg Loss: 3.5028, Avg Acc: 0.4050
2022-01-16 22:12:38,466 Epoch[121/300], Step[0500/1252], Avg Loss: 3.5072, Avg Acc: 0.4049
2022-01-16 22:13:59,346 Epoch[121/300], Step[0550/1252], Avg Loss: 3.5031, Avg Acc: 0.4062
2022-01-16 22:15:18,896 Epoch[121/300], Step[0600/1252], Avg Loss: 3.5026, Avg Acc: 0.4041
2022-01-16 22:16:39,685 Epoch[121/300], Step[0650/1252], Avg Loss: 3.5094, Avg Acc: 0.4020
2022-01-16 22:17:59,924 Epoch[121/300], Step[0700/1252], Avg Loss: 3.5100, Avg Acc: 0.3993
2022-01-16 22:19:19,932 Epoch[121/300], Step[0750/1252], Avg Loss: 3.5126, Avg Acc: 0.3979
2022-01-16 22:20:40,484 Epoch[121/300], Step[0800/1252], Avg Loss: 3.5123, Avg Acc: 0.3977
2022-01-16 22:22:01,378 Epoch[121/300], Step[0850/1252], Avg Loss: 3.5109, Avg Acc: 0.3966
2022-01-16 22:23:22,340 Epoch[121/300], Step[0900/1252], Avg Loss: 3.5158, Avg Acc: 0.3962
2022-01-16 22:24:42,784 Epoch[121/300], Step[0950/1252], Avg Loss: 3.5124, Avg Acc: 0.3961
2022-01-16 22:26:03,580 Epoch[121/300], Step[1000/1252], Avg Loss: 3.5142, Avg Acc: 0.3956
2022-01-16 22:27:24,517 Epoch[121/300], Step[1050/1252], Avg Loss: 3.5123, Avg Acc: 0.3949
2022-01-16 22:28:45,211 Epoch[121/300], Step[1100/1252], Avg Loss: 3.5134, Avg Acc: 0.3943
2022-01-16 22:30:04,441 Epoch[121/300], Step[1150/1252], Avg Loss: 3.5136, Avg Acc: 0.3953
2022-01-16 22:31:24,737 Epoch[121/300], Step[1200/1252], Avg Loss: 3.5157, Avg Acc: 0.3954
2022-01-16 22:32:43,494 Epoch[121/300], Step[1250/1252], Avg Loss: 3.5189, Avg Acc: 0.3949
2022-01-16 22:32:49,509 ----- Epoch[121/300], Train Loss: 3.5190, Train Acc: 0.3949, time: 2118.86, Best Val(epoch114) Acc@1: 0.7080
2022-01-16 22:32:49,509 Now training epoch 122. LR=0.000710
2022-01-16 22:34:36,135 Epoch[122/300], Step[0000/1252], Avg Loss: 3.0819, Avg Acc: 0.5107
2022-01-16 22:35:56,052 Epoch[122/300], Step[0050/1252], Avg Loss: 3.4903, Avg Acc: 0.3482
2022-01-16 22:37:15,918 Epoch[122/300], Step[0100/1252], Avg Loss: 3.5076, Avg Acc: 0.3712
2022-01-16 22:38:36,002 Epoch[122/300], Step[0150/1252], Avg Loss: 3.5020, Avg Acc: 0.3806
2022-01-16 22:39:57,028 Epoch[122/300], Step[0200/1252], Avg Loss: 3.5007, Avg Acc: 0.3831
2022-01-16 22:41:17,324 Epoch[122/300], Step[0250/1252], Avg Loss: 3.4994, Avg Acc: 0.3907
2022-01-16 22:42:37,859 Epoch[122/300], Step[0300/1252], Avg Loss: 3.4969, Avg Acc: 0.3921
2022-01-16 22:43:57,408 Epoch[122/300], Step[0350/1252], Avg Loss: 3.5034, Avg Acc: 0.3906
2022-01-16 22:45:17,873 Epoch[122/300], Step[0400/1252], Avg Loss: 3.5033, Avg Acc: 0.3896
2022-01-16 22:46:37,672 Epoch[122/300], Step[0450/1252], Avg Loss: 3.5053, Avg Acc: 0.3895
2022-01-16 22:47:58,557 Epoch[122/300], Step[0500/1252], Avg Loss: 3.5058, Avg Acc: 0.3887
2022-01-16 22:49:19,130 Epoch[122/300], Step[0550/1252], Avg Loss: 3.5083, Avg Acc: 0.3896
2022-01-16 22:50:40,370 Epoch[122/300], Step[0600/1252], Avg Loss: 3.5071, Avg Acc: 0.3897
2022-01-16 22:52:00,748 Epoch[122/300], Step[0650/1252], Avg Loss: 3.5153, Avg Acc: 0.3891
2022-01-16 22:53:20,767 Epoch[122/300], Step[0700/1252], Avg Loss: 3.5124, Avg Acc: 0.3909
2022-01-16 22:54:41,475 Epoch[122/300], Step[0750/1252], Avg Loss: 3.5118, Avg Acc: 0.3904
2022-01-16 22:56:02,001 Epoch[122/300], Step[0800/1252], Avg Loss: 3.5132, Avg Acc: 0.3899
2022-01-16 22:57:22,501 Epoch[122/300], Step[0850/1252], Avg Loss: 3.5135, Avg Acc: 0.3893
2022-01-16 22:58:43,402 Epoch[122/300], Step[0900/1252], Avg Loss: 3.5160, Avg Acc: 0.3879
2022-01-16 23:00:03,598 Epoch[122/300], Step[0950/1252], Avg Loss: 3.5209, Avg Acc: 0.3866
2022-01-16 23:01:24,579 Epoch[122/300], Step[1000/1252], Avg Loss: 3.5246, Avg Acc: 0.3863
2022-01-16 23:02:44,081 Epoch[122/300], Step[1050/1252], Avg Loss: 3.5260, Avg Acc: 0.3856
2022-01-16 23:04:04,368 Epoch[122/300], Step[1100/1252], Avg Loss: 3.5257, Avg Acc: 0.3858
2022-01-16 23:05:25,326 Epoch[122/300], Step[1150/1252], Avg Loss: 3.5252, Avg Acc: 0.3853
2022-01-16 23:06:45,526 Epoch[122/300], Step[1200/1252], Avg Loss: 3.5271, Avg Acc: 0.3858
2022-01-16 23:08:05,152 Epoch[122/300], Step[1250/1252], Avg Loss: 3.5276, Avg Acc: 0.3862
2022-01-16 23:08:11,325 ----- Epoch[122/300], Train Loss: 3.5277, Train Acc: 0.3862, time: 2121.81, Best Val(epoch114) Acc@1: 0.7080
2022-01-16 23:08:11,326 ----- Validation after Epoch: 122
2022-01-16 23:09:44,550 Val Step[0000/1563], Avg Loss: 1.1440, Avg Acc@1: 0.7188, Avg Acc@5: 1.0000
2022-01-16 23:09:46,241 Val Step[0050/1563], Avg Loss: 1.2960, Avg Acc@1: 0.7065, Avg Acc@5: 0.9130
2022-01-16 23:09:47,974 Val Step[0100/1563], Avg Loss: 1.3071, Avg Acc@1: 0.7045, Avg Acc@5: 0.9069
2022-01-16 23:09:49,752 Val Step[0150/1563], Avg Loss: 1.3029, Avg Acc@1: 0.7086, Avg Acc@5: 0.9046
2022-01-16 23:09:51,439 Val Step[0200/1563], Avg Loss: 1.3039, Avg Acc@1: 0.7093, Avg Acc@5: 0.9042
2022-01-16 23:09:53,205 Val Step[0250/1563], Avg Loss: 1.2907, Avg Acc@1: 0.7118, Avg Acc@5: 0.9039
2022-01-16 23:09:55,042 Val Step[0300/1563], Avg Loss: 1.2905, Avg Acc@1: 0.7110, Avg Acc@5: 0.9026
2022-01-16 23:09:56,645 Val Step[0350/1563], Avg Loss: 1.2969, Avg Acc@1: 0.7106, Avg Acc@5: 0.9019
2022-01-16 23:09:58,389 Val Step[0400/1563], Avg Loss: 1.2978, Avg Acc@1: 0.7099, Avg Acc@5: 0.9013
2022-01-16 23:10:00,129 Val Step[0450/1563], Avg Loss: 1.3048, Avg Acc@1: 0.7084, Avg Acc@5: 0.9008
2022-01-16 23:10:01,872 Val Step[0500/1563], Avg Loss: 1.3062, Avg Acc@1: 0.7078, Avg Acc@5: 0.9012
2022-01-16 23:10:03,462 Val Step[0550/1563], Avg Loss: 1.3089, Avg Acc@1: 0.7067, Avg Acc@5: 0.9011
2022-01-16 23:10:05,162 Val Step[0600/1563], Avg Loss: 1.3099, Avg Acc@1: 0.7060, Avg Acc@5: 0.9016
2022-01-16 23:10:06,882 Val Step[0650/1563], Avg Loss: 1.3110, Avg Acc@1: 0.7053, Avg Acc@5: 0.9018
2022-01-16 23:10:08,538 Val Step[0700/1563], Avg Loss: 1.3062, Avg Acc@1: 0.7070, Avg Acc@5: 0.9029
2022-01-16 23:10:10,377 Val Step[0750/1563], Avg Loss: 1.3126, Avg Acc@1: 0.7056, Avg Acc@5: 0.9023
2022-01-16 23:10:12,164 Val Step[0800/1563], Avg Loss: 1.3137, Avg Acc@1: 0.7060, Avg Acc@5: 0.9016
2022-01-16 23:10:14,051 Val Step[0850/1563], Avg Loss: 1.3146, Avg Acc@1: 0.7055, Avg Acc@5: 0.9014
2022-01-16 23:10:15,931 Val Step[0900/1563], Avg Loss: 1.3108, Avg Acc@1: 0.7060, Avg Acc@5: 0.9021
2022-01-16 23:10:17,560 Val Step[0950/1563], Avg Loss: 1.3115, Avg Acc@1: 0.7061, Avg Acc@5: 0.9022
2022-01-16 23:10:19,235 Val Step[1000/1563], Avg Loss: 1.3128, Avg Acc@1: 0.7060, Avg Acc@5: 0.9020
2022-01-16 23:10:20,753 Val Step[1050/1563], Avg Loss: 1.3160, Avg Acc@1: 0.7055, Avg Acc@5: 0.9014
2022-01-16 23:10:22,367 Val Step[1100/1563], Avg Loss: 1.3159, Avg Acc@1: 0.7052, Avg Acc@5: 0.9015
2022-01-16 23:10:23,967 Val Step[1150/1563], Avg Loss: 1.3139, Avg Acc@1: 0.7060, Avg Acc@5: 0.9016
2022-01-16 23:10:25,780 Val Step[1200/1563], Avg Loss: 1.3127, Avg Acc@1: 0.7065, Avg Acc@5: 0.9019
2022-01-16 23:10:27,734 Val Step[1250/1563], Avg Loss: 1.3123, Avg Acc@1: 0.7068, Avg Acc@5: 0.9018
2022-01-16 23:10:29,461 Val Step[1300/1563], Avg Loss: 1.3157, Avg Acc@1: 0.7059, Avg Acc@5: 0.9013
2022-01-16 23:10:31,002 Val Step[1350/1563], Avg Loss: 1.3148, Avg Acc@1: 0.7061, Avg Acc@5: 0.9012
2022-01-16 23:10:32,563 Val Step[1400/1563], Avg Loss: 1.3147, Avg Acc@1: 0.7058, Avg Acc@5: 0.9010
2022-01-16 23:10:34,160 Val Step[1450/1563], Avg Loss: 1.3156, Avg Acc@1: 0.7059, Avg Acc@5: 0.9005
2022-01-16 23:10:35,748 Val Step[1500/1563], Avg Loss: 1.3158, Avg Acc@1: 0.7058, Avg Acc@5: 0.9007
2022-01-16 23:10:37,384 Val Step[1550/1563], Avg Loss: 1.3169, Avg Acc@1: 0.7054, Avg Acc@5: 0.9006
2022-01-16 23:10:39,185 ----- Epoch[122/300], Validation Loss: 1.3168, Validation Acc@1: 0.7055, Validation Acc@5: 0.9006, time: 147.86
2022-01-16 23:10:39,185 Now training epoch 123. LR=0.000705
2022-01-16 23:12:23,043 Epoch[123/300], Step[0000/1252], Avg Loss: 3.5633, Avg Acc: 0.3174
2022-01-16 23:13:42,597 Epoch[123/300], Step[0050/1252], Avg Loss: 3.5440, Avg Acc: 0.4042
2022-01-16 23:15:02,636 Epoch[123/300], Step[0100/1252], Avg Loss: 3.4890, Avg Acc: 0.3978
2022-01-16 23:16:22,713 Epoch[123/300], Step[0150/1252], Avg Loss: 3.4759, Avg Acc: 0.3991
2022-01-16 23:17:41,889 Epoch[123/300], Step[0200/1252], Avg Loss: 3.4880, Avg Acc: 0.4045
2022-01-16 23:19:02,319 Epoch[123/300], Step[0250/1252], Avg Loss: 3.4961, Avg Acc: 0.3984
2022-01-16 23:20:22,014 Epoch[123/300], Step[0300/1252], Avg Loss: 3.4943, Avg Acc: 0.3997
2022-01-16 23:21:41,975 Epoch[123/300], Step[0350/1252], Avg Loss: 3.4894, Avg Acc: 0.3989
2022-01-16 23:23:00,507 Epoch[123/300], Step[0400/1252], Avg Loss: 3.4950, Avg Acc: 0.3983
2022-01-16 23:24:20,483 Epoch[123/300], Step[0450/1252], Avg Loss: 3.4948, Avg Acc: 0.3965
2022-01-16 23:25:40,294 Epoch[123/300], Step[0500/1252], Avg Loss: 3.5008, Avg Acc: 0.3984
2022-01-16 23:27:01,331 Epoch[123/300], Step[0550/1252], Avg Loss: 3.5004, Avg Acc: 0.3985
2022-01-16 23:28:22,399 Epoch[123/300], Step[0600/1252], Avg Loss: 3.5007, Avg Acc: 0.3975
2022-01-16 23:29:42,978 Epoch[123/300], Step[0650/1252], Avg Loss: 3.5032, Avg Acc: 0.3975
2022-01-16 23:31:03,377 Epoch[123/300], Step[0700/1252], Avg Loss: 3.5056, Avg Acc: 0.3967
2022-01-16 23:32:23,081 Epoch[123/300], Step[0750/1252], Avg Loss: 3.5074, Avg Acc: 0.3966
2022-01-16 23:33:43,888 Epoch[123/300], Step[0800/1252], Avg Loss: 3.5097, Avg Acc: 0.3945
2022-01-16 23:35:04,227 Epoch[123/300], Step[0850/1252], Avg Loss: 3.5064, Avg Acc: 0.3932
2022-01-16 23:36:25,517 Epoch[123/300], Step[0900/1252], Avg Loss: 3.5049, Avg Acc: 0.3928
2022-01-16 23:37:46,251 Epoch[123/300], Step[0950/1252], Avg Loss: 3.5051, Avg Acc: 0.3927
2022-01-16 23:39:07,549 Epoch[123/300], Step[1000/1252], Avg Loss: 3.5069, Avg Acc: 0.3913
2022-01-16 23:40:28,132 Epoch[123/300], Step[1050/1252], Avg Loss: 3.5080, Avg Acc: 0.3918
2022-01-16 23:41:48,810 Epoch[123/300], Step[1100/1252], Avg Loss: 3.5133, Avg Acc: 0.3913
2022-01-16 23:43:09,966 Epoch[123/300], Step[1150/1252], Avg Loss: 3.5123, Avg Acc: 0.3914
2022-01-16 23:44:30,346 Epoch[123/300], Step[1200/1252], Avg Loss: 3.5144, Avg Acc: 0.3913
2022-01-16 23:45:49,754 Epoch[123/300], Step[1250/1252], Avg Loss: 3.5151, Avg Acc: 0.3909
2022-01-16 23:45:55,983 ----- Epoch[123/300], Train Loss: 3.5151, Train Acc: 0.3909, time: 2116.79, Best Val(epoch114) Acc@1: 0.7080
2022-01-16 23:45:55,983 Now training epoch 124. LR=0.000700
2022-01-16 23:47:35,640 Epoch[124/300], Step[0000/1252], Avg Loss: 3.4977, Avg Acc: 0.3301
2022-01-16 23:48:54,460 Epoch[124/300], Step[0050/1252], Avg Loss: 3.5181, Avg Acc: 0.3943
2022-01-16 23:50:14,370 Epoch[124/300], Step[0100/1252], Avg Loss: 3.5285, Avg Acc: 0.3866
2022-01-16 23:51:33,333 Epoch[124/300], Step[0150/1252], Avg Loss: 3.5212, Avg Acc: 0.3914
2022-01-16 23:52:53,224 Epoch[124/300], Step[0200/1252], Avg Loss: 3.5204, Avg Acc: 0.3974
2022-01-16 23:54:13,608 Epoch[124/300], Step[0250/1252], Avg Loss: 3.5060, Avg Acc: 0.3994
2022-01-16 23:55:32,714 Epoch[124/300], Step[0300/1252], Avg Loss: 3.5036, Avg Acc: 0.3988
2022-01-16 23:56:51,920 Epoch[124/300], Step[0350/1252], Avg Loss: 3.5086, Avg Acc: 0.3950
2022-01-16 23:58:12,202 Epoch[124/300], Step[0400/1252], Avg Loss: 3.5155, Avg Acc: 0.3939
2022-01-16 23:59:33,037 Epoch[124/300], Step[0450/1252], Avg Loss: 3.5109, Avg Acc: 0.3931
2022-01-17 00:00:53,740 Epoch[124/300], Step[0500/1252], Avg Loss: 3.5179, Avg Acc: 0.3917
2022-01-17 00:02:13,489 Epoch[124/300], Step[0550/1252], Avg Loss: 3.5201, Avg Acc: 0.3921
2022-01-17 00:03:34,057 Epoch[124/300], Step[0600/1252], Avg Loss: 3.5191, Avg Acc: 0.3924
2022-01-17 00:04:55,719 Epoch[124/300], Step[0650/1252], Avg Loss: 3.5161, Avg Acc: 0.3905
2022-01-17 00:06:15,377 Epoch[124/300], Step[0700/1252], Avg Loss: 3.5114, Avg Acc: 0.3902
2022-01-17 00:07:35,912 Epoch[124/300], Step[0750/1252], Avg Loss: 3.5129, Avg Acc: 0.3876
2022-01-17 00:08:55,808 Epoch[124/300], Step[0800/1252], Avg Loss: 3.5132, Avg Acc: 0.3878
2022-01-17 00:10:16,515 Epoch[124/300], Step[0850/1252], Avg Loss: 3.5150, Avg Acc: 0.3886
2022-01-17 00:11:36,789 Epoch[124/300], Step[0900/1252], Avg Loss: 3.5133, Avg Acc: 0.3889
2022-01-17 00:12:56,553 Epoch[124/300], Step[0950/1252], Avg Loss: 3.5174, Avg Acc: 0.3872
2022-01-17 00:14:16,612 Epoch[124/300], Step[1000/1252], Avg Loss: 3.5162, Avg Acc: 0.3878
2022-01-17 00:15:37,600 Epoch[124/300], Step[1050/1252], Avg Loss: 3.5171, Avg Acc: 0.3870
2022-01-17 00:16:57,490 Epoch[124/300], Step[1100/1252], Avg Loss: 3.5148, Avg Acc: 0.3877
2022-01-17 00:18:17,486 Epoch[124/300], Step[1150/1252], Avg Loss: 3.5138, Avg Acc: 0.3891
2022-01-17 00:19:38,238 Epoch[124/300], Step[1200/1252], Avg Loss: 3.5154, Avg Acc: 0.3885
2022-01-17 00:20:57,229 Epoch[124/300], Step[1250/1252], Avg Loss: 3.5152, Avg Acc: 0.3887
2022-01-17 00:21:03,514 ----- Epoch[124/300], Train Loss: 3.5153, Train Acc: 0.3887, time: 2107.53, Best Val(epoch114) Acc@1: 0.7080
2022-01-17 00:21:03,514 ----- Validation after Epoch: 124
2022-01-17 00:22:19,602 Val Step[0000/1563], Avg Loss: 1.0737, Avg Acc@1: 0.7188, Avg Acc@5: 0.9375
2022-01-17 00:22:21,284 Val Step[0050/1563], Avg Loss: 1.2743, Avg Acc@1: 0.7138, Avg Acc@5: 0.9075
2022-01-17 00:22:22,997 Val Step[0100/1563], Avg Loss: 1.2791, Avg Acc@1: 0.7116, Avg Acc@5: 0.9041
2022-01-17 00:22:24,737 Val Step[0150/1563], Avg Loss: 1.2796, Avg Acc@1: 0.7125, Avg Acc@5: 0.9027
2022-01-17 00:22:26,443 Val Step[0200/1563], Avg Loss: 1.2793, Avg Acc@1: 0.7161, Avg Acc@5: 0.9016
2022-01-17 00:22:28,107 Val Step[0250/1563], Avg Loss: 1.2654, Avg Acc@1: 0.7179, Avg Acc@5: 0.9028
2022-01-17 00:22:29,820 Val Step[0300/1563], Avg Loss: 1.2634, Avg Acc@1: 0.7192, Avg Acc@5: 0.9025
2022-01-17 00:22:31,398 Val Step[0350/1563], Avg Loss: 1.2703, Avg Acc@1: 0.7181, Avg Acc@5: 0.9017
2022-01-17 00:22:32,974 Val Step[0400/1563], Avg Loss: 1.2691, Avg Acc@1: 0.7174, Avg Acc@5: 0.9024
2022-01-17 00:22:34,703 Val Step[0450/1563], Avg Loss: 1.2733, Avg Acc@1: 0.7151, Avg Acc@5: 0.9024
2022-01-17 00:22:36,400 Val Step[0500/1563], Avg Loss: 1.2749, Avg Acc@1: 0.7148, Avg Acc@5: 0.9029
2022-01-17 00:22:38,167 Val Step[0550/1563], Avg Loss: 1.2781, Avg Acc@1: 0.7132, Avg Acc@5: 0.9029
2022-01-17 00:22:39,817 Val Step[0600/1563], Avg Loss: 1.2775, Avg Acc@1: 0.7137, Avg Acc@5: 0.9035
2022-01-17 00:22:41,460 Val Step[0650/1563], Avg Loss: 1.2778, Avg Acc@1: 0.7138, Avg Acc@5: 0.9037
2022-01-17 00:22:43,276 Val Step[0700/1563], Avg Loss: 1.2765, Avg Acc@1: 0.7138, Avg Acc@5: 0.9044
2022-01-17 00:22:45,053 Val Step[0750/1563], Avg Loss: 1.2808, Avg Acc@1: 0.7131, Avg Acc@5: 0.9039
2022-01-17 00:22:46,628 Val Step[0800/1563], Avg Loss: 1.2803, Avg Acc@1: 0.7140, Avg Acc@5: 0.9039
2022-01-17 00:22:48,270 Val Step[0850/1563], Avg Loss: 1.2822, Avg Acc@1: 0.7136, Avg Acc@5: 0.9035
2022-01-17 00:22:50,005 Val Step[0900/1563], Avg Loss: 1.2797, Avg Acc@1: 0.7142, Avg Acc@5: 0.9040
2022-01-17 00:22:51,686 Val Step[0950/1563], Avg Loss: 1.2801, Avg Acc@1: 0.7138, Avg Acc@5: 0.9045
2022-01-17 00:22:53,482 Val Step[1000/1563], Avg Loss: 1.2807, Avg Acc@1: 0.7141, Avg Acc@5: 0.9042
2022-01-17 00:22:55,123 Val Step[1050/1563], Avg Loss: 1.2821, Avg Acc@1: 0.7132, Avg Acc@5: 0.9039
2022-01-17 00:22:56,645 Val Step[1100/1563], Avg Loss: 1.2821, Avg Acc@1: 0.7130, Avg Acc@5: 0.9039
2022-01-17 00:22:58,228 Val Step[1150/1563], Avg Loss: 1.2803, Avg Acc@1: 0.7138, Avg Acc@5: 0.9041
2022-01-17 00:23:00,020 Val Step[1200/1563], Avg Loss: 1.2794, Avg Acc@1: 0.7139, Avg Acc@5: 0.9044
2022-01-17 00:23:01,568 Val Step[1250/1563], Avg Loss: 1.2779, Avg Acc@1: 0.7143, Avg Acc@5: 0.9047
2022-01-17 00:23:03,401 Val Step[1300/1563], Avg Loss: 1.2819, Avg Acc@1: 0.7136, Avg Acc@5: 0.9044
2022-01-17 00:23:05,135 Val Step[1350/1563], Avg Loss: 1.2820, Avg Acc@1: 0.7132, Avg Acc@5: 0.9041
2022-01-17 00:23:06,791 Val Step[1400/1563], Avg Loss: 1.2819, Avg Acc@1: 0.7126, Avg Acc@5: 0.9040
2022-01-17 00:23:08,420 Val Step[1450/1563], Avg Loss: 1.2825, Avg Acc@1: 0.7121, Avg Acc@5: 0.9038
2022-01-17 00:23:10,039 Val Step[1500/1563], Avg Loss: 1.2820, Avg Acc@1: 0.7123, Avg Acc@5: 0.9042
2022-01-17 00:23:11,606 Val Step[1550/1563], Avg Loss: 1.2832, Avg Acc@1: 0.7117, Avg Acc@5: 0.9041
2022-01-17 00:23:13,790 ----- Epoch[124/300], Validation Loss: 1.2833, Validation Acc@1: 0.7117, Validation Acc@5: 0.9042, time: 130.27
2022-01-17 00:23:14,854 the pre best model acc:0.7080, at epoch 114
2022-01-17 00:23:15,154 current best model acc:0.7117, at epoch 124
2022-01-17 00:23:15,154 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 00:23:15,154 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 00:23:15,154 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 00:23:15,154 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 00:23:15,154 Now training epoch 125. LR=0.000694
2022-01-17 00:24:50,585 Epoch[125/300], Step[0000/1252], Avg Loss: 3.4238, Avg Acc: 0.5557
2022-01-17 00:26:09,725 Epoch[125/300], Step[0050/1252], Avg Loss: 3.5004, Avg Acc: 0.3756
2022-01-17 00:27:28,816 Epoch[125/300], Step[0100/1252], Avg Loss: 3.4910, Avg Acc: 0.4039
2022-01-17 00:28:48,051 Epoch[125/300], Step[0150/1252], Avg Loss: 3.4874, Avg Acc: 0.4013
2022-01-17 00:30:07,144 Epoch[125/300], Step[0200/1252], Avg Loss: 3.5206, Avg Acc: 0.3976
2022-01-17 00:31:27,195 Epoch[125/300], Step[0250/1252], Avg Loss: 3.5172, Avg Acc: 0.4026
2022-01-17 00:32:46,593 Epoch[125/300], Step[0300/1252], Avg Loss: 3.5226, Avg Acc: 0.4046
2022-01-17 00:34:05,265 Epoch[125/300], Step[0350/1252], Avg Loss: 3.5163, Avg Acc: 0.4046
2022-01-17 00:35:25,820 Epoch[125/300], Step[0400/1252], Avg Loss: 3.5176, Avg Acc: 0.4063
2022-01-17 00:36:45,649 Epoch[125/300], Step[0450/1252], Avg Loss: 3.5147, Avg Acc: 0.4044
2022-01-17 00:38:04,799 Epoch[125/300], Step[0500/1252], Avg Loss: 3.5059, Avg Acc: 0.4032
2022-01-17 00:39:23,995 Epoch[125/300], Step[0550/1252], Avg Loss: 3.5050, Avg Acc: 0.4039
2022-01-17 00:40:44,439 Epoch[125/300], Step[0600/1252], Avg Loss: 3.5012, Avg Acc: 0.4030
2022-01-17 00:42:04,572 Epoch[125/300], Step[0650/1252], Avg Loss: 3.5048, Avg Acc: 0.4015
2022-01-17 00:43:25,484 Epoch[125/300], Step[0700/1252], Avg Loss: 3.5041, Avg Acc: 0.4003
2022-01-17 00:44:45,917 Epoch[125/300], Step[0750/1252], Avg Loss: 3.5068, Avg Acc: 0.4004
2022-01-17 00:46:06,428 Epoch[125/300], Step[0800/1252], Avg Loss: 3.5077, Avg Acc: 0.4007
2022-01-17 00:47:26,319 Epoch[125/300], Step[0850/1252], Avg Loss: 3.5050, Avg Acc: 0.4012
2022-01-17 00:48:45,784 Epoch[125/300], Step[0900/1252], Avg Loss: 3.5045, Avg Acc: 0.4000
2022-01-17 00:50:04,115 Epoch[125/300], Step[0950/1252], Avg Loss: 3.5045, Avg Acc: 0.4003
2022-01-17 00:51:24,455 Epoch[125/300], Step[1000/1252], Avg Loss: 3.5046, Avg Acc: 0.3990
2022-01-17 00:52:44,861 Epoch[125/300], Step[1050/1252], Avg Loss: 3.5062, Avg Acc: 0.3990
2022-01-17 00:54:06,685 Epoch[125/300], Step[1100/1252], Avg Loss: 3.5079, Avg Acc: 0.3985
2022-01-17 00:55:26,679 Epoch[125/300], Step[1150/1252], Avg Loss: 3.5104, Avg Acc: 0.3976
2022-01-17 00:56:46,988 Epoch[125/300], Step[1200/1252], Avg Loss: 3.5073, Avg Acc: 0.3975
2022-01-17 00:58:06,251 Epoch[125/300], Step[1250/1252], Avg Loss: 3.5061, Avg Acc: 0.3965
2022-01-17 00:58:12,700 ----- Epoch[125/300], Train Loss: 3.5061, Train Acc: 0.3965, time: 2097.54, Best Val(epoch124) Acc@1: 0.7117
2022-01-17 00:58:12,700 Now training epoch 126. LR=0.000689
2022-01-17 00:59:50,398 Epoch[126/300], Step[0000/1252], Avg Loss: 3.1448, Avg Acc: 0.5557
2022-01-17 01:01:09,798 Epoch[126/300], Step[0050/1252], Avg Loss: 3.4551, Avg Acc: 0.4027
2022-01-17 01:02:28,935 Epoch[126/300], Step[0100/1252], Avg Loss: 3.4837, Avg Acc: 0.4131
2022-01-17 01:03:49,100 Epoch[126/300], Step[0150/1252], Avg Loss: 3.4793, Avg Acc: 0.4027
2022-01-17 01:05:09,180 Epoch[126/300], Step[0200/1252], Avg Loss: 3.4798, Avg Acc: 0.4040
2022-01-17 01:06:28,163 Epoch[126/300], Step[0250/1252], Avg Loss: 3.4941, Avg Acc: 0.4026
2022-01-17 01:07:47,114 Epoch[126/300], Step[0300/1252], Avg Loss: 3.4981, Avg Acc: 0.4053
2022-01-17 01:09:06,595 Epoch[126/300], Step[0350/1252], Avg Loss: 3.4968, Avg Acc: 0.4035
2022-01-17 01:10:25,792 Epoch[126/300], Step[0400/1252], Avg Loss: 3.5092, Avg Acc: 0.4005
2022-01-17 01:11:44,995 Epoch[126/300], Step[0450/1252], Avg Loss: 3.5117, Avg Acc: 0.4005
2022-01-17 01:13:04,695 Epoch[126/300], Step[0500/1252], Avg Loss: 3.5087, Avg Acc: 0.3997
2022-01-17 01:14:24,224 Epoch[126/300], Step[0550/1252], Avg Loss: 3.5144, Avg Acc: 0.3994
2022-01-17 01:15:43,082 Epoch[126/300], Step[0600/1252], Avg Loss: 3.5133, Avg Acc: 0.3972
2022-01-17 01:17:02,855 Epoch[126/300], Step[0650/1252], Avg Loss: 3.5139, Avg Acc: 0.3973
2022-01-17 01:18:22,245 Epoch[126/300], Step[0700/1252], Avg Loss: 3.5075, Avg Acc: 0.3968
2022-01-17 01:19:40,108 Epoch[126/300], Step[0750/1252], Avg Loss: 3.5076, Avg Acc: 0.3959
2022-01-17 01:20:58,720 Epoch[126/300], Step[0800/1252], Avg Loss: 3.5044, Avg Acc: 0.3959
2022-01-17 01:22:18,364 Epoch[126/300], Step[0850/1252], Avg Loss: 3.5071, Avg Acc: 0.3951
2022-01-17 01:23:37,298 Epoch[126/300], Step[0900/1252], Avg Loss: 3.5090, Avg Acc: 0.3943
2022-01-17 01:24:56,481 Epoch[126/300], Step[0950/1252], Avg Loss: 3.5078, Avg Acc: 0.3946
2022-01-17 01:26:14,856 Epoch[126/300], Step[1000/1252], Avg Loss: 3.5092, Avg Acc: 0.3933
2022-01-17 01:27:33,959 Epoch[126/300], Step[1050/1252], Avg Loss: 3.5097, Avg Acc: 0.3936
2022-01-17 01:28:53,647 Epoch[126/300], Step[1100/1252], Avg Loss: 3.5094, Avg Acc: 0.3947
2022-01-17 01:30:12,526 Epoch[126/300], Step[1150/1252], Avg Loss: 3.5072, Avg Acc: 0.3950
2022-01-17 01:31:31,526 Epoch[126/300], Step[1200/1252], Avg Loss: 3.5066, Avg Acc: 0.3951
2022-01-17 01:32:50,921 Epoch[126/300], Step[1250/1252], Avg Loss: 3.5097, Avg Acc: 0.3945
2022-01-17 01:32:58,578 ----- Epoch[126/300], Train Loss: 3.5097, Train Acc: 0.3946, time: 2085.88, Best Val(epoch124) Acc@1: 0.7117
2022-01-17 01:32:58,579 ----- Validation after Epoch: 126
2022-01-17 01:34:13,194 Val Step[0000/1563], Avg Loss: 1.2023, Avg Acc@1: 0.7188, Avg Acc@5: 0.9688
2022-01-17 01:34:15,003 Val Step[0050/1563], Avg Loss: 1.2937, Avg Acc@1: 0.7114, Avg Acc@5: 0.9038
2022-01-17 01:34:16,707 Val Step[0100/1563], Avg Loss: 1.3089, Avg Acc@1: 0.7054, Avg Acc@5: 0.9069
2022-01-17 01:34:18,257 Val Step[0150/1563], Avg Loss: 1.3021, Avg Acc@1: 0.7121, Avg Acc@5: 0.9060
2022-01-17 01:34:19,829 Val Step[0200/1563], Avg Loss: 1.2981, Avg Acc@1: 0.7121, Avg Acc@5: 0.9052
2022-01-17 01:34:21,394 Val Step[0250/1563], Avg Loss: 1.2889, Avg Acc@1: 0.7134, Avg Acc@5: 0.9053
2022-01-17 01:34:22,998 Val Step[0300/1563], Avg Loss: 1.2906, Avg Acc@1: 0.7151, Avg Acc@5: 0.9044
2022-01-17 01:34:24,591 Val Step[0350/1563], Avg Loss: 1.2979, Avg Acc@1: 0.7145, Avg Acc@5: 0.9037
2022-01-17 01:34:26,153 Val Step[0400/1563], Avg Loss: 1.2971, Avg Acc@1: 0.7153, Avg Acc@5: 0.9036
2022-01-17 01:34:27,758 Val Step[0450/1563], Avg Loss: 1.3040, Avg Acc@1: 0.7127, Avg Acc@5: 0.9034
2022-01-17 01:34:29,509 Val Step[0500/1563], Avg Loss: 1.3070, Avg Acc@1: 0.7125, Avg Acc@5: 0.9031
2022-01-17 01:34:31,168 Val Step[0550/1563], Avg Loss: 1.3084, Avg Acc@1: 0.7123, Avg Acc@5: 0.9032
2022-01-17 01:34:32,867 Val Step[0600/1563], Avg Loss: 1.3089, Avg Acc@1: 0.7120, Avg Acc@5: 0.9040
2022-01-17 01:34:34,869 Val Step[0650/1563], Avg Loss: 1.3114, Avg Acc@1: 0.7115, Avg Acc@5: 0.9039
2022-01-17 01:34:36,780 Val Step[0700/1563], Avg Loss: 1.3089, Avg Acc@1: 0.7122, Avg Acc@5: 0.9047
2022-01-17 01:34:38,470 Val Step[0750/1563], Avg Loss: 1.3136, Avg Acc@1: 0.7113, Avg Acc@5: 0.9040
2022-01-17 01:34:40,039 Val Step[0800/1563], Avg Loss: 1.3127, Avg Acc@1: 0.7121, Avg Acc@5: 0.9039
2022-01-17 01:34:41,590 Val Step[0850/1563], Avg Loss: 1.3155, Avg Acc@1: 0.7106, Avg Acc@5: 0.9039
2022-01-17 01:34:43,539 Val Step[0900/1563], Avg Loss: 1.3131, Avg Acc@1: 0.7106, Avg Acc@5: 0.9047
2022-01-17 01:34:45,394 Val Step[0950/1563], Avg Loss: 1.3129, Avg Acc@1: 0.7104, Avg Acc@5: 0.9045
2022-01-17 01:34:47,206 Val Step[1000/1563], Avg Loss: 1.3138, Avg Acc@1: 0.7100, Avg Acc@5: 0.9044
2022-01-17 01:34:49,043 Val Step[1050/1563], Avg Loss: 1.3164, Avg Acc@1: 0.7093, Avg Acc@5: 0.9043
2022-01-17 01:34:50,758 Val Step[1100/1563], Avg Loss: 1.3159, Avg Acc@1: 0.7092, Avg Acc@5: 0.9047
2022-01-17 01:34:52,297 Val Step[1150/1563], Avg Loss: 1.3147, Avg Acc@1: 0.7092, Avg Acc@5: 0.9049
2022-01-17 01:34:53,891 Val Step[1200/1563], Avg Loss: 1.3137, Avg Acc@1: 0.7092, Avg Acc@5: 0.9051
2022-01-17 01:34:55,840 Val Step[1250/1563], Avg Loss: 1.3124, Avg Acc@1: 0.7097, Avg Acc@5: 0.9054
2022-01-17 01:34:57,531 Val Step[1300/1563], Avg Loss: 1.3153, Avg Acc@1: 0.7090, Avg Acc@5: 0.9050
2022-01-17 01:34:59,149 Val Step[1350/1563], Avg Loss: 1.3156, Avg Acc@1: 0.7089, Avg Acc@5: 0.9047
2022-01-17 01:35:00,752 Val Step[1400/1563], Avg Loss: 1.3151, Avg Acc@1: 0.7083, Avg Acc@5: 0.9048
2022-01-17 01:35:02,361 Val Step[1450/1563], Avg Loss: 1.3154, Avg Acc@1: 0.7081, Avg Acc@5: 0.9047
2022-01-17 01:35:03,900 Val Step[1500/1563], Avg Loss: 1.3150, Avg Acc@1: 0.7082, Avg Acc@5: 0.9051
2022-01-17 01:35:05,425 Val Step[1550/1563], Avg Loss: 1.3158, Avg Acc@1: 0.7080, Avg Acc@5: 0.9049
2022-01-17 01:35:08,700 ----- Epoch[126/300], Validation Loss: 1.3158, Validation Acc@1: 0.7080, Validation Acc@5: 0.9051, time: 130.12
2022-01-17 01:35:08,700 Now training epoch 127. LR=0.000684
2022-01-17 01:36:48,482 Epoch[127/300], Step[0000/1252], Avg Loss: 3.1819, Avg Acc: 0.5342
2022-01-17 01:38:07,799 Epoch[127/300], Step[0050/1252], Avg Loss: 3.4523, Avg Acc: 0.3881
2022-01-17 01:39:27,016 Epoch[127/300], Step[0100/1252], Avg Loss: 3.4882, Avg Acc: 0.3850
2022-01-17 01:40:47,406 Epoch[127/300], Step[0150/1252], Avg Loss: 3.4683, Avg Acc: 0.3891
2022-01-17 01:42:07,284 Epoch[127/300], Step[0200/1252], Avg Loss: 3.4813, Avg Acc: 0.3904
2022-01-17 01:43:27,795 Epoch[127/300], Step[0250/1252], Avg Loss: 3.4824, Avg Acc: 0.3883
2022-01-17 01:44:47,416 Epoch[127/300], Step[0300/1252], Avg Loss: 3.4931, Avg Acc: 0.3895
2022-01-17 01:46:08,088 Epoch[127/300], Step[0350/1252], Avg Loss: 3.4881, Avg Acc: 0.3891
2022-01-17 01:47:28,426 Epoch[127/300], Step[0400/1252], Avg Loss: 3.4835, Avg Acc: 0.3900
2022-01-17 01:48:49,534 Epoch[127/300], Step[0450/1252], Avg Loss: 3.4861, Avg Acc: 0.3895
2022-01-17 01:50:10,012 Epoch[127/300], Step[0500/1252], Avg Loss: 3.4922, Avg Acc: 0.3919
2022-01-17 01:51:30,055 Epoch[127/300], Step[0550/1252], Avg Loss: 3.4936, Avg Acc: 0.3945
2022-01-17 01:52:50,752 Epoch[127/300], Step[0600/1252], Avg Loss: 3.4905, Avg Acc: 0.3924
2022-01-17 01:54:11,569 Epoch[127/300], Step[0650/1252], Avg Loss: 3.4925, Avg Acc: 0.3914
2022-01-17 01:55:32,326 Epoch[127/300], Step[0700/1252], Avg Loss: 3.4914, Avg Acc: 0.3934
2022-01-17 01:56:52,039 Epoch[127/300], Step[0750/1252], Avg Loss: 3.4901, Avg Acc: 0.3945
2022-01-17 01:58:11,953 Epoch[127/300], Step[0800/1252], Avg Loss: 3.4922, Avg Acc: 0.3944
2022-01-17 01:59:31,372 Epoch[127/300], Step[0850/1252], Avg Loss: 3.4915, Avg Acc: 0.3956
2022-01-17 02:00:51,278 Epoch[127/300], Step[0900/1252], Avg Loss: 3.4877, Avg Acc: 0.3971
2022-01-17 02:02:11,574 Epoch[127/300], Step[0950/1252], Avg Loss: 3.4880, Avg Acc: 0.3965
2022-01-17 02:03:32,499 Epoch[127/300], Step[1000/1252], Avg Loss: 3.4885, Avg Acc: 0.3958
2022-01-17 02:04:53,304 Epoch[127/300], Step[1050/1252], Avg Loss: 3.4911, Avg Acc: 0.3941
2022-01-17 02:06:13,372 Epoch[127/300], Step[1100/1252], Avg Loss: 3.4939, Avg Acc: 0.3923
2022-01-17 02:07:33,442 Epoch[127/300], Step[1150/1252], Avg Loss: 3.4974, Avg Acc: 0.3920
2022-01-17 02:08:54,385 Epoch[127/300], Step[1200/1252], Avg Loss: 3.4986, Avg Acc: 0.3920
2022-01-17 02:10:14,535 Epoch[127/300], Step[1250/1252], Avg Loss: 3.5007, Avg Acc: 0.3915
2022-01-17 02:10:21,315 ----- Epoch[127/300], Train Loss: 3.5008, Train Acc: 0.3915, time: 2112.61, Best Val(epoch124) Acc@1: 0.7117
2022-01-17 02:10:21,315 Now training epoch 128. LR=0.000679
2022-01-17 02:12:05,052 Epoch[128/300], Step[0000/1252], Avg Loss: 2.9198, Avg Acc: 0.1191
2022-01-17 02:13:26,034 Epoch[128/300], Step[0050/1252], Avg Loss: 3.5099, Avg Acc: 0.4024
2022-01-17 02:14:45,245 Epoch[128/300], Step[0100/1252], Avg Loss: 3.4918, Avg Acc: 0.3875
2022-01-17 02:16:04,673 Epoch[128/300], Step[0150/1252], Avg Loss: 3.4764, Avg Acc: 0.4014
2022-01-17 02:17:23,304 Epoch[128/300], Step[0200/1252], Avg Loss: 3.4670, Avg Acc: 0.4020
2022-01-17 02:18:43,559 Epoch[128/300], Step[0250/1252], Avg Loss: 3.4669, Avg Acc: 0.3919
2022-01-17 02:20:03,681 Epoch[128/300], Step[0300/1252], Avg Loss: 3.4782, Avg Acc: 0.3908
2022-01-17 02:21:23,520 Epoch[128/300], Step[0350/1252], Avg Loss: 3.4732, Avg Acc: 0.3958
2022-01-17 02:22:44,141 Epoch[128/300], Step[0400/1252], Avg Loss: 3.4730, Avg Acc: 0.3995
2022-01-17 02:24:04,906 Epoch[128/300], Step[0450/1252], Avg Loss: 3.4769, Avg Acc: 0.3981
2022-01-17 02:25:24,721 Epoch[128/300], Step[0500/1252], Avg Loss: 3.4825, Avg Acc: 0.3968
2022-01-17 02:26:44,828 Epoch[128/300], Step[0550/1252], Avg Loss: 3.4845, Avg Acc: 0.3972
2022-01-17 02:28:04,712 Epoch[128/300], Step[0600/1252], Avg Loss: 3.4833, Avg Acc: 0.3978
2022-01-17 02:29:25,345 Epoch[128/300], Step[0650/1252], Avg Loss: 3.4805, Avg Acc: 0.3966
2022-01-17 02:30:45,723 Epoch[128/300], Step[0700/1252], Avg Loss: 3.4829, Avg Acc: 0.3975
2022-01-17 02:32:06,509 Epoch[128/300], Step[0750/1252], Avg Loss: 3.4810, Avg Acc: 0.3976
2022-01-17 02:33:26,216 Epoch[128/300], Step[0800/1252], Avg Loss: 3.4804, Avg Acc: 0.3955
2022-01-17 02:34:46,344 Epoch[128/300], Step[0850/1252], Avg Loss: 3.4802, Avg Acc: 0.3959
2022-01-17 02:36:06,100 Epoch[128/300], Step[0900/1252], Avg Loss: 3.4839, Avg Acc: 0.3932
2022-01-17 02:37:26,009 Epoch[128/300], Step[0950/1252], Avg Loss: 3.4861, Avg Acc: 0.3931
2022-01-17 02:38:46,115 Epoch[128/300], Step[1000/1252], Avg Loss: 3.4838, Avg Acc: 0.3932
2022-01-17 02:40:06,100 Epoch[128/300], Step[1050/1252], Avg Loss: 3.4867, Avg Acc: 0.3913
2022-01-17 02:41:26,938 Epoch[128/300], Step[1100/1252], Avg Loss: 3.4852, Avg Acc: 0.3939
2022-01-17 02:42:45,895 Epoch[128/300], Step[1150/1252], Avg Loss: 3.4865, Avg Acc: 0.3950
2022-01-17 02:44:06,401 Epoch[128/300], Step[1200/1252], Avg Loss: 3.4900, Avg Acc: 0.3936
2022-01-17 02:45:26,237 Epoch[128/300], Step[1250/1252], Avg Loss: 3.4932, Avg Acc: 0.3936
2022-01-17 02:45:32,594 ----- Epoch[128/300], Train Loss: 3.4932, Train Acc: 0.3936, time: 2111.27, Best Val(epoch124) Acc@1: 0.7117
2022-01-17 02:45:32,594 ----- Validation after Epoch: 128
2022-01-17 02:46:45,896 Val Step[0000/1563], Avg Loss: 1.2212, Avg Acc@1: 0.7500, Avg Acc@5: 0.9375
2022-01-17 02:46:47,690 Val Step[0050/1563], Avg Loss: 1.3221, Avg Acc@1: 0.7224, Avg Acc@5: 0.9044
2022-01-17 02:46:49,388 Val Step[0100/1563], Avg Loss: 1.3429, Avg Acc@1: 0.7153, Avg Acc@5: 0.9066
2022-01-17 02:46:51,046 Val Step[0150/1563], Avg Loss: 1.3418, Avg Acc@1: 0.7154, Avg Acc@5: 0.9054
2022-01-17 02:46:52,660 Val Step[0200/1563], Avg Loss: 1.3457, Avg Acc@1: 0.7155, Avg Acc@5: 0.9045
2022-01-17 02:46:54,259 Val Step[0250/1563], Avg Loss: 1.3371, Avg Acc@1: 0.7169, Avg Acc@5: 0.9051
2022-01-17 02:46:55,931 Val Step[0300/1563], Avg Loss: 1.3338, Avg Acc@1: 0.7171, Avg Acc@5: 0.9043
2022-01-17 02:46:57,707 Val Step[0350/1563], Avg Loss: 1.3392, Avg Acc@1: 0.7157, Avg Acc@5: 0.9039
2022-01-17 02:46:59,520 Val Step[0400/1563], Avg Loss: 1.3362, Avg Acc@1: 0.7153, Avg Acc@5: 0.9037
2022-01-17 02:47:01,321 Val Step[0450/1563], Avg Loss: 1.3401, Avg Acc@1: 0.7131, Avg Acc@5: 0.9027
2022-01-17 02:47:02,865 Val Step[0500/1563], Avg Loss: 1.3407, Avg Acc@1: 0.7138, Avg Acc@5: 0.9027
2022-01-17 02:47:04,426 Val Step[0550/1563], Avg Loss: 1.3435, Avg Acc@1: 0.7125, Avg Acc@5: 0.9026
2022-01-17 02:47:06,116 Val Step[0600/1563], Avg Loss: 1.3440, Avg Acc@1: 0.7121, Avg Acc@5: 0.9033
2022-01-17 02:47:07,810 Val Step[0650/1563], Avg Loss: 1.3441, Avg Acc@1: 0.7126, Avg Acc@5: 0.9035
2022-01-17 02:47:09,548 Val Step[0700/1563], Avg Loss: 1.3413, Avg Acc@1: 0.7130, Avg Acc@5: 0.9043
2022-01-17 02:47:11,078 Val Step[0750/1563], Avg Loss: 1.3463, Avg Acc@1: 0.7115, Avg Acc@5: 0.9038
2022-01-17 02:47:12,896 Val Step[0800/1563], Avg Loss: 1.3456, Avg Acc@1: 0.7120, Avg Acc@5: 0.9038
2022-01-17 02:47:14,724 Val Step[0850/1563], Avg Loss: 1.3484, Avg Acc@1: 0.7111, Avg Acc@5: 0.9038
2022-01-17 02:47:16,441 Val Step[0900/1563], Avg Loss: 1.3458, Avg Acc@1: 0.7115, Avg Acc@5: 0.9039
2022-01-17 02:47:18,327 Val Step[0950/1563], Avg Loss: 1.3456, Avg Acc@1: 0.7113, Avg Acc@5: 0.9042
2022-01-17 02:47:20,118 Val Step[1000/1563], Avg Loss: 1.3460, Avg Acc@1: 0.7109, Avg Acc@5: 0.9041
2022-01-17 02:47:21,753 Val Step[1050/1563], Avg Loss: 1.3469, Avg Acc@1: 0.7105, Avg Acc@5: 0.9039
2022-01-17 02:47:23,431 Val Step[1100/1563], Avg Loss: 1.3474, Avg Acc@1: 0.7102, Avg Acc@5: 0.9039
2022-01-17 02:47:25,088 Val Step[1150/1563], Avg Loss: 1.3459, Avg Acc@1: 0.7106, Avg Acc@5: 0.9039
2022-01-17 02:47:26,640 Val Step[1200/1563], Avg Loss: 1.3461, Avg Acc@1: 0.7105, Avg Acc@5: 0.9041
2022-01-17 02:47:28,341 Val Step[1250/1563], Avg Loss: 1.3454, Avg Acc@1: 0.7103, Avg Acc@5: 0.9043
2022-01-17 02:47:30,179 Val Step[1300/1563], Avg Loss: 1.3485, Avg Acc@1: 0.7097, Avg Acc@5: 0.9039
2022-01-17 02:47:31,772 Val Step[1350/1563], Avg Loss: 1.3486, Avg Acc@1: 0.7095, Avg Acc@5: 0.9039
2022-01-17 02:47:33,326 Val Step[1400/1563], Avg Loss: 1.3485, Avg Acc@1: 0.7092, Avg Acc@5: 0.9038
2022-01-17 02:47:34,921 Val Step[1450/1563], Avg Loss: 1.3481, Avg Acc@1: 0.7096, Avg Acc@5: 0.9038
2022-01-17 02:47:36,562 Val Step[1500/1563], Avg Loss: 1.3467, Avg Acc@1: 0.7098, Avg Acc@5: 0.9044
2022-01-17 02:47:38,105 Val Step[1550/1563], Avg Loss: 1.3475, Avg Acc@1: 0.7098, Avg Acc@5: 0.9042
2022-01-17 02:47:40,194 ----- Epoch[128/300], Validation Loss: 1.3475, Validation Acc@1: 0.7097, Validation Acc@5: 0.9044, time: 127.60
2022-01-17 02:47:40,194 Now training epoch 129. LR=0.000674
2022-01-17 02:49:15,557 Epoch[129/300], Step[0000/1252], Avg Loss: 3.6137, Avg Acc: 0.3691
2022-01-17 02:50:34,223 Epoch[129/300], Step[0050/1252], Avg Loss: 3.4822, Avg Acc: 0.3823
2022-01-17 02:51:52,869 Epoch[129/300], Step[0100/1252], Avg Loss: 3.5125, Avg Acc: 0.3821
2022-01-17 02:53:12,788 Epoch[129/300], Step[0150/1252], Avg Loss: 3.5185, Avg Acc: 0.3879
2022-01-17 02:54:31,782 Epoch[129/300], Step[0200/1252], Avg Loss: 3.5185, Avg Acc: 0.3901
2022-01-17 02:55:51,324 Epoch[129/300], Step[0250/1252], Avg Loss: 3.5188, Avg Acc: 0.3881
2022-01-17 02:57:11,643 Epoch[129/300], Step[0300/1252], Avg Loss: 3.5155, Avg Acc: 0.3875
2022-01-17 02:58:31,121 Epoch[129/300], Step[0350/1252], Avg Loss: 3.5152, Avg Acc: 0.3879
2022-01-17 02:59:50,985 Epoch[129/300], Step[0400/1252], Avg Loss: 3.5182, Avg Acc: 0.3906
2022-01-17 03:01:10,444 Epoch[129/300], Step[0450/1252], Avg Loss: 3.5214, Avg Acc: 0.3928
2022-01-17 03:02:31,392 Epoch[129/300], Step[0500/1252], Avg Loss: 3.5158, Avg Acc: 0.3909
2022-01-17 03:03:51,278 Epoch[129/300], Step[0550/1252], Avg Loss: 3.5083, Avg Acc: 0.3920
2022-01-17 03:05:10,748 Epoch[129/300], Step[0600/1252], Avg Loss: 3.5058, Avg Acc: 0.3916
2022-01-17 03:06:30,443 Epoch[129/300], Step[0650/1252], Avg Loss: 3.5034, Avg Acc: 0.3913
2022-01-17 03:07:49,444 Epoch[129/300], Step[0700/1252], Avg Loss: 3.5031, Avg Acc: 0.3895
2022-01-17 03:09:08,844 Epoch[129/300], Step[0750/1252], Avg Loss: 3.5027, Avg Acc: 0.3890
2022-01-17 03:10:28,215 Epoch[129/300], Step[0800/1252], Avg Loss: 3.5092, Avg Acc: 0.3888
2022-01-17 03:11:48,026 Epoch[129/300], Step[0850/1252], Avg Loss: 3.5120, Avg Acc: 0.3882
2022-01-17 03:13:07,705 Epoch[129/300], Step[0900/1252], Avg Loss: 3.5105, Avg Acc: 0.3887
2022-01-17 03:14:28,293 Epoch[129/300], Step[0950/1252], Avg Loss: 3.5087, Avg Acc: 0.3889
2022-01-17 03:15:48,577 Epoch[129/300], Step[1000/1252], Avg Loss: 3.5077, Avg Acc: 0.3894
2022-01-17 03:17:08,400 Epoch[129/300], Step[1050/1252], Avg Loss: 3.5063, Avg Acc: 0.3895
2022-01-17 03:18:28,152 Epoch[129/300], Step[1100/1252], Avg Loss: 3.5108, Avg Acc: 0.3887
2022-01-17 03:19:47,695 Epoch[129/300], Step[1150/1252], Avg Loss: 3.5123, Avg Acc: 0.3893
2022-01-17 03:21:08,197 Epoch[129/300], Step[1200/1252], Avg Loss: 3.5115, Avg Acc: 0.3891
2022-01-17 03:22:28,232 Epoch[129/300], Step[1250/1252], Avg Loss: 3.5139, Avg Acc: 0.3894
2022-01-17 03:22:35,245 ----- Epoch[129/300], Train Loss: 3.5138, Train Acc: 0.3894, time: 2095.05, Best Val(epoch124) Acc@1: 0.7117
2022-01-17 03:22:35,245 Now training epoch 130. LR=0.000668
2022-01-17 03:24:09,123 Epoch[130/300], Step[0000/1252], Avg Loss: 3.9193, Avg Acc: 0.4414
2022-01-17 03:25:27,574 Epoch[130/300], Step[0050/1252], Avg Loss: 3.5150, Avg Acc: 0.3997
2022-01-17 03:26:45,650 Epoch[130/300], Step[0100/1252], Avg Loss: 3.4862, Avg Acc: 0.4026
2022-01-17 03:28:03,978 Epoch[130/300], Step[0150/1252], Avg Loss: 3.4905, Avg Acc: 0.3905
2022-01-17 03:29:22,974 Epoch[130/300], Step[0200/1252], Avg Loss: 3.4854, Avg Acc: 0.3895
2022-01-17 03:30:41,400 Epoch[130/300], Step[0250/1252], Avg Loss: 3.4729, Avg Acc: 0.3923
2022-01-17 03:32:01,717 Epoch[130/300], Step[0300/1252], Avg Loss: 3.4743, Avg Acc: 0.3928
2022-01-17 03:33:22,245 Epoch[130/300], Step[0350/1252], Avg Loss: 3.4735, Avg Acc: 0.3947
2022-01-17 03:34:42,005 Epoch[130/300], Step[0400/1252], Avg Loss: 3.4847, Avg Acc: 0.3929
2022-01-17 03:36:01,817 Epoch[130/300], Step[0450/1252], Avg Loss: 3.4888, Avg Acc: 0.3910
2022-01-17 03:37:22,711 Epoch[130/300], Step[0500/1252], Avg Loss: 3.4927, Avg Acc: 0.3875
2022-01-17 03:38:42,197 Epoch[130/300], Step[0550/1252], Avg Loss: 3.4927, Avg Acc: 0.3893
2022-01-17 03:40:02,563 Epoch[130/300], Step[0600/1252], Avg Loss: 3.4945, Avg Acc: 0.3877
2022-01-17 03:41:22,989 Epoch[130/300], Step[0650/1252], Avg Loss: 3.4925, Avg Acc: 0.3869
2022-01-17 03:42:43,483 Epoch[130/300], Step[0700/1252], Avg Loss: 3.4924, Avg Acc: 0.3876
2022-01-17 03:44:04,033 Epoch[130/300], Step[0750/1252], Avg Loss: 3.4915, Avg Acc: 0.3888
2022-01-17 03:45:22,904 Epoch[130/300], Step[0800/1252], Avg Loss: 3.4942, Avg Acc: 0.3900
2022-01-17 03:46:43,614 Epoch[130/300], Step[0850/1252], Avg Loss: 3.4965, Avg Acc: 0.3893
2022-01-17 03:48:03,834 Epoch[130/300], Step[0900/1252], Avg Loss: 3.5018, Avg Acc: 0.3890
2022-01-17 03:49:24,099 Epoch[130/300], Step[0950/1252], Avg Loss: 3.5009, Avg Acc: 0.3874
2022-01-17 03:50:43,872 Epoch[130/300], Step[1000/1252], Avg Loss: 3.5018, Avg Acc: 0.3865
2022-01-17 03:52:04,929 Epoch[130/300], Step[1050/1252], Avg Loss: 3.5031, Avg Acc: 0.3864
2022-01-17 03:53:25,798 Epoch[130/300], Step[1100/1252], Avg Loss: 3.5043, Avg Acc: 0.3865
2022-01-17 03:54:46,617 Epoch[130/300], Step[1150/1252], Avg Loss: 3.5040, Avg Acc: 0.3874
2022-01-17 03:56:06,641 Epoch[130/300], Step[1200/1252], Avg Loss: 3.5006, Avg Acc: 0.3888
2022-01-17 03:57:25,625 Epoch[130/300], Step[1250/1252], Avg Loss: 3.5008, Avg Acc: 0.3897
2022-01-17 03:57:32,306 ----- Epoch[130/300], Train Loss: 3.5008, Train Acc: 0.3898, time: 2097.06, Best Val(epoch124) Acc@1: 0.7117
2022-01-17 03:57:32,306 ----- Validation after Epoch: 130
2022-01-17 03:58:47,161 Val Step[0000/1563], Avg Loss: 0.9940, Avg Acc@1: 0.7188, Avg Acc@5: 0.9688
2022-01-17 03:58:48,870 Val Step[0050/1563], Avg Loss: 1.2927, Avg Acc@1: 0.7132, Avg Acc@5: 0.9056
2022-01-17 03:58:50,559 Val Step[0100/1563], Avg Loss: 1.3054, Avg Acc@1: 0.7123, Avg Acc@5: 0.9025
2022-01-17 03:58:52,221 Val Step[0150/1563], Avg Loss: 1.3078, Avg Acc@1: 0.7132, Avg Acc@5: 0.9013
2022-01-17 03:58:53,800 Val Step[0200/1563], Avg Loss: 1.3041, Avg Acc@1: 0.7138, Avg Acc@5: 0.9017
2022-01-17 03:58:55,401 Val Step[0250/1563], Avg Loss: 1.2932, Avg Acc@1: 0.7150, Avg Acc@5: 0.9031
2022-01-17 03:58:57,154 Val Step[0300/1563], Avg Loss: 1.2960, Avg Acc@1: 0.7131, Avg Acc@5: 0.9028
2022-01-17 03:58:58,857 Val Step[0350/1563], Avg Loss: 1.3017, Avg Acc@1: 0.7130, Avg Acc@5: 0.9031
2022-01-17 03:59:00,568 Val Step[0400/1563], Avg Loss: 1.2999, Avg Acc@1: 0.7136, Avg Acc@5: 0.9039
2022-01-17 03:59:02,220 Val Step[0450/1563], Avg Loss: 1.3056, Avg Acc@1: 0.7119, Avg Acc@5: 0.9021
2022-01-17 03:59:03,918 Val Step[0500/1563], Avg Loss: 1.3047, Avg Acc@1: 0.7118, Avg Acc@5: 0.9024
2022-01-17 03:59:05,648 Val Step[0550/1563], Avg Loss: 1.3039, Avg Acc@1: 0.7122, Avg Acc@5: 0.9027
2022-01-17 03:59:07,263 Val Step[0600/1563], Avg Loss: 1.3023, Avg Acc@1: 0.7118, Avg Acc@5: 0.9034
2022-01-17 03:59:08,869 Val Step[0650/1563], Avg Loss: 1.3010, Avg Acc@1: 0.7120, Avg Acc@5: 0.9042
2022-01-17 03:59:10,657 Val Step[0700/1563], Avg Loss: 1.2983, Avg Acc@1: 0.7130, Avg Acc@5: 0.9048
2022-01-17 03:59:12,267 Val Step[0750/1563], Avg Loss: 1.3027, Avg Acc@1: 0.7120, Avg Acc@5: 0.9043
2022-01-17 03:59:13,862 Val Step[0800/1563], Avg Loss: 1.3012, Avg Acc@1: 0.7122, Avg Acc@5: 0.9047
2022-01-17 03:59:15,598 Val Step[0850/1563], Avg Loss: 1.3024, Avg Acc@1: 0.7115, Avg Acc@5: 0.9043
2022-01-17 03:59:17,253 Val Step[0900/1563], Avg Loss: 1.2991, Avg Acc@1: 0.7121, Avg Acc@5: 0.9048
2022-01-17 03:59:19,037 Val Step[0950/1563], Avg Loss: 1.2991, Avg Acc@1: 0.7124, Avg Acc@5: 0.9052
2022-01-17 03:59:20,816 Val Step[1000/1563], Avg Loss: 1.2994, Avg Acc@1: 0.7124, Avg Acc@5: 0.9053
2022-01-17 03:59:22,459 Val Step[1050/1563], Avg Loss: 1.3021, Avg Acc@1: 0.7117, Avg Acc@5: 0.9045
2022-01-17 03:59:24,000 Val Step[1100/1563], Avg Loss: 1.3032, Avg Acc@1: 0.7115, Avg Acc@5: 0.9044
2022-01-17 03:59:25,607 Val Step[1150/1563], Avg Loss: 1.3024, Avg Acc@1: 0.7113, Avg Acc@5: 0.9045
2022-01-17 03:59:27,190 Val Step[1200/1563], Avg Loss: 1.3015, Avg Acc@1: 0.7119, Avg Acc@5: 0.9048
2022-01-17 03:59:28,953 Val Step[1250/1563], Avg Loss: 1.3003, Avg Acc@1: 0.7117, Avg Acc@5: 0.9047
2022-01-17 03:59:30,708 Val Step[1300/1563], Avg Loss: 1.3030, Avg Acc@1: 0.7114, Avg Acc@5: 0.9044
2022-01-17 03:59:32,291 Val Step[1350/1563], Avg Loss: 1.3032, Avg Acc@1: 0.7110, Avg Acc@5: 0.9043
2022-01-17 03:59:33,823 Val Step[1400/1563], Avg Loss: 1.3028, Avg Acc@1: 0.7108, Avg Acc@5: 0.9043
2022-01-17 03:59:35,643 Val Step[1450/1563], Avg Loss: 1.3025, Avg Acc@1: 0.7109, Avg Acc@5: 0.9042
2022-01-17 03:59:37,241 Val Step[1500/1563], Avg Loss: 1.3011, Avg Acc@1: 0.7113, Avg Acc@5: 0.9045
2022-01-17 03:59:38,854 Val Step[1550/1563], Avg Loss: 1.3020, Avg Acc@1: 0.7108, Avg Acc@5: 0.9044
2022-01-17 03:59:41,054 ----- Epoch[130/300], Validation Loss: 1.3023, Validation Acc@1: 0.7107, Validation Acc@5: 0.9044, time: 128.75
2022-01-17 03:59:41,428 ----- Save model: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-130-Loss-3.520757599678977.pdparams
2022-01-17 03:59:41,428 ----- Save optim: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-130-Loss-3.520757599678977.pdopt
2022-01-17 03:59:41,429 Now training epoch 131. LR=0.000663
2022-01-17 04:01:14,480 Epoch[131/300], Step[0000/1252], Avg Loss: 3.9450, Avg Acc: 0.3906
2022-01-17 04:02:33,995 Epoch[131/300], Step[0050/1252], Avg Loss: 3.5429, Avg Acc: 0.4047
2022-01-17 04:03:53,492 Epoch[131/300], Step[0100/1252], Avg Loss: 3.5177, Avg Acc: 0.4123
2022-01-17 04:05:13,587 Epoch[131/300], Step[0150/1252], Avg Loss: 3.5063, Avg Acc: 0.4048
2022-01-17 04:06:32,749 Epoch[131/300], Step[0200/1252], Avg Loss: 3.4959, Avg Acc: 0.3966
2022-01-17 04:07:52,951 Epoch[131/300], Step[0250/1252], Avg Loss: 3.4927, Avg Acc: 0.3980
2022-01-17 04:09:13,200 Epoch[131/300], Step[0300/1252], Avg Loss: 3.5004, Avg Acc: 0.3980
2022-01-17 04:10:32,734 Epoch[131/300], Step[0350/1252], Avg Loss: 3.4921, Avg Acc: 0.3978
2022-01-17 04:11:52,747 Epoch[131/300], Step[0400/1252], Avg Loss: 3.4921, Avg Acc: 0.3956
2022-01-17 04:13:13,484 Epoch[131/300], Step[0450/1252], Avg Loss: 3.4862, Avg Acc: 0.3957
2022-01-17 04:14:34,017 Epoch[131/300], Step[0500/1252], Avg Loss: 3.4942, Avg Acc: 0.3942
2022-01-17 04:15:53,650 Epoch[131/300], Step[0550/1252], Avg Loss: 3.4953, Avg Acc: 0.3939
2022-01-17 04:17:13,855 Epoch[131/300], Step[0600/1252], Avg Loss: 3.4953, Avg Acc: 0.3923
2022-01-17 04:18:33,958 Epoch[131/300], Step[0650/1252], Avg Loss: 3.4977, Avg Acc: 0.3916
2022-01-17 04:19:53,533 Epoch[131/300], Step[0700/1252], Avg Loss: 3.4988, Avg Acc: 0.3932
2022-01-17 04:21:14,523 Epoch[131/300], Step[0750/1252], Avg Loss: 3.5011, Avg Acc: 0.3911
2022-01-17 04:22:35,631 Epoch[131/300], Step[0800/1252], Avg Loss: 3.5042, Avg Acc: 0.3887
2022-01-17 04:23:55,515 Epoch[131/300], Step[0850/1252], Avg Loss: 3.5027, Avg Acc: 0.3902
2022-01-17 04:25:15,915 Epoch[131/300], Step[0900/1252], Avg Loss: 3.5004, Avg Acc: 0.3913
2022-01-17 04:26:35,950 Epoch[131/300], Step[0950/1252], Avg Loss: 3.5017, Avg Acc: 0.3917
2022-01-17 04:27:55,259 Epoch[131/300], Step[1000/1252], Avg Loss: 3.5024, Avg Acc: 0.3921
2022-01-17 04:29:16,365 Epoch[131/300], Step[1050/1252], Avg Loss: 3.5019, Avg Acc: 0.3918
2022-01-17 04:30:37,167 Epoch[131/300], Step[1100/1252], Avg Loss: 3.5009, Avg Acc: 0.3928
2022-01-17 04:31:57,991 Epoch[131/300], Step[1150/1252], Avg Loss: 3.4991, Avg Acc: 0.3936
2022-01-17 04:33:17,552 Epoch[131/300], Step[1200/1252], Avg Loss: 3.5009, Avg Acc: 0.3937
2022-01-17 04:34:37,948 Epoch[131/300], Step[1250/1252], Avg Loss: 3.4984, Avg Acc: 0.3940
2022-01-17 04:34:44,691 ----- Epoch[131/300], Train Loss: 3.4984, Train Acc: 0.3940, time: 2103.26, Best Val(epoch124) Acc@1: 0.7117
2022-01-17 04:34:44,692 Now training epoch 132. LR=0.000658
2022-01-17 04:36:20,246 Epoch[132/300], Step[0000/1252], Avg Loss: 3.8027, Avg Acc: 0.3984
2022-01-17 04:37:39,366 Epoch[132/300], Step[0050/1252], Avg Loss: 3.5263, Avg Acc: 0.3919
2022-01-17 04:38:58,410 Epoch[132/300], Step[0100/1252], Avg Loss: 3.5023, Avg Acc: 0.3832
2022-01-17 04:40:17,981 Epoch[132/300], Step[0150/1252], Avg Loss: 3.4935, Avg Acc: 0.3826
2022-01-17 04:41:39,164 Epoch[132/300], Step[0200/1252], Avg Loss: 3.4880, Avg Acc: 0.3903
2022-01-17 04:42:59,275 Epoch[132/300], Step[0250/1252], Avg Loss: 3.4839, Avg Acc: 0.3918
2022-01-17 04:44:20,473 Epoch[132/300], Step[0300/1252], Avg Loss: 3.4847, Avg Acc: 0.3905
2022-01-17 04:45:40,368 Epoch[132/300], Step[0350/1252], Avg Loss: 3.4798, Avg Acc: 0.3932
2022-01-17 04:47:01,002 Epoch[132/300], Step[0400/1252], Avg Loss: 3.4720, Avg Acc: 0.3938
2022-01-17 04:48:22,091 Epoch[132/300], Step[0450/1252], Avg Loss: 3.4728, Avg Acc: 0.3928
2022-01-17 04:49:42,738 Epoch[132/300], Step[0500/1252], Avg Loss: 3.4709, Avg Acc: 0.3945
2022-01-17 04:51:01,882 Epoch[132/300], Step[0550/1252], Avg Loss: 3.4744, Avg Acc: 0.3967
2022-01-17 04:52:21,588 Epoch[132/300], Step[0600/1252], Avg Loss: 3.4812, Avg Acc: 0.3965
2022-01-17 04:53:41,778 Epoch[132/300], Step[0650/1252], Avg Loss: 3.4770, Avg Acc: 0.3966
2022-01-17 04:55:02,261 Epoch[132/300], Step[0700/1252], Avg Loss: 3.4772, Avg Acc: 0.3953
2022-01-17 04:56:23,123 Epoch[132/300], Step[0750/1252], Avg Loss: 3.4796, Avg Acc: 0.3959
2022-01-17 04:57:41,851 Epoch[132/300], Step[0800/1252], Avg Loss: 3.4797, Avg Acc: 0.3958
2022-01-17 04:59:02,087 Epoch[132/300], Step[0850/1252], Avg Loss: 3.4761, Avg Acc: 0.3964
2022-01-17 05:00:22,676 Epoch[132/300], Step[0900/1252], Avg Loss: 3.4800, Avg Acc: 0.3958
2022-01-17 05:01:42,019 Epoch[132/300], Step[0950/1252], Avg Loss: 3.4823, Avg Acc: 0.3958
2022-01-17 05:03:02,995 Epoch[132/300], Step[1000/1252], Avg Loss: 3.4843, Avg Acc: 0.3961
2022-01-17 05:04:23,085 Epoch[132/300], Step[1050/1252], Avg Loss: 3.4852, Avg Acc: 0.3959
2022-01-17 05:05:43,148 Epoch[132/300], Step[1100/1252], Avg Loss: 3.4847, Avg Acc: 0.3950
2022-01-17 05:07:02,277 Epoch[132/300], Step[1150/1252], Avg Loss: 3.4873, Avg Acc: 0.3942
2022-01-17 05:08:22,994 Epoch[132/300], Step[1200/1252], Avg Loss: 3.4858, Avg Acc: 0.3959
2022-01-17 05:09:42,666 Epoch[132/300], Step[1250/1252], Avg Loss: 3.4845, Avg Acc: 0.3958
2022-01-17 05:09:48,865 ----- Epoch[132/300], Train Loss: 3.4844, Train Acc: 0.3958, time: 2104.17, Best Val(epoch124) Acc@1: 0.7117
2022-01-17 05:09:48,865 ----- Validation after Epoch: 132
2022-01-17 05:11:06,272 Val Step[0000/1563], Avg Loss: 1.1478, Avg Acc@1: 0.7188, Avg Acc@5: 0.9375
2022-01-17 05:11:08,626 Val Step[0050/1563], Avg Loss: 1.2927, Avg Acc@1: 0.7181, Avg Acc@5: 0.9069
2022-01-17 05:11:10,474 Val Step[0100/1563], Avg Loss: 1.3133, Avg Acc@1: 0.7119, Avg Acc@5: 0.9075
2022-01-17 05:11:12,101 Val Step[0150/1563], Avg Loss: 1.3147, Avg Acc@1: 0.7159, Avg Acc@5: 0.9056
2022-01-17 05:11:13,628 Val Step[0200/1563], Avg Loss: 1.3093, Avg Acc@1: 0.7166, Avg Acc@5: 0.9072
2022-01-17 05:11:15,196 Val Step[0250/1563], Avg Loss: 1.2962, Avg Acc@1: 0.7186, Avg Acc@5: 0.9074
2022-01-17 05:11:16,952 Val Step[0300/1563], Avg Loss: 1.3003, Avg Acc@1: 0.7188, Avg Acc@5: 0.9065
2022-01-17 05:11:18,769 Val Step[0350/1563], Avg Loss: 1.3093, Avg Acc@1: 0.7178, Avg Acc@5: 0.9054
2022-01-17 05:11:20,488 Val Step[0400/1563], Avg Loss: 1.3051, Avg Acc@1: 0.7178, Avg Acc@5: 0.9055
2022-01-17 05:11:22,332 Val Step[0450/1563], Avg Loss: 1.3119, Avg Acc@1: 0.7152, Avg Acc@5: 0.9040
2022-01-17 05:11:24,161 Val Step[0500/1563], Avg Loss: 1.3150, Avg Acc@1: 0.7143, Avg Acc@5: 0.9045
2022-01-17 05:11:26,048 Val Step[0550/1563], Avg Loss: 1.3164, Avg Acc@1: 0.7139, Avg Acc@5: 0.9055
2022-01-17 05:11:27,756 Val Step[0600/1563], Avg Loss: 1.3194, Avg Acc@1: 0.7122, Avg Acc@5: 0.9055
2022-01-17 05:11:29,445 Val Step[0650/1563], Avg Loss: 1.3198, Avg Acc@1: 0.7123, Avg Acc@5: 0.9051
2022-01-17 05:11:31,265 Val Step[0700/1563], Avg Loss: 1.3159, Avg Acc@1: 0.7138, Avg Acc@5: 0.9062
2022-01-17 05:11:33,036 Val Step[0750/1563], Avg Loss: 1.3230, Avg Acc@1: 0.7123, Avg Acc@5: 0.9054
2022-01-17 05:11:34,570 Val Step[0800/1563], Avg Loss: 1.3223, Avg Acc@1: 0.7132, Avg Acc@5: 0.9056
2022-01-17 05:11:36,096 Val Step[0850/1563], Avg Loss: 1.3249, Avg Acc@1: 0.7125, Avg Acc@5: 0.9050
2022-01-17 05:11:37,735 Val Step[0900/1563], Avg Loss: 1.3216, Avg Acc@1: 0.7129, Avg Acc@5: 0.9055
2022-01-17 05:11:39,503 Val Step[0950/1563], Avg Loss: 1.3224, Avg Acc@1: 0.7127, Avg Acc@5: 0.9054
2022-01-17 05:11:41,134 Val Step[1000/1563], Avg Loss: 1.3218, Avg Acc@1: 0.7133, Avg Acc@5: 0.9054
2022-01-17 05:11:42,737 Val Step[1050/1563], Avg Loss: 1.3234, Avg Acc@1: 0.7127, Avg Acc@5: 0.9051
2022-01-17 05:11:44,313 Val Step[1100/1563], Avg Loss: 1.3246, Avg Acc@1: 0.7124, Avg Acc@5: 0.9047
2022-01-17 05:11:46,044 Val Step[1150/1563], Avg Loss: 1.3240, Avg Acc@1: 0.7121, Avg Acc@5: 0.9049
2022-01-17 05:11:47,575 Val Step[1200/1563], Avg Loss: 1.3232, Avg Acc@1: 0.7125, Avg Acc@5: 0.9050
2022-01-17 05:11:49,162 Val Step[1250/1563], Avg Loss: 1.3223, Avg Acc@1: 0.7124, Avg Acc@5: 0.9053
2022-01-17 05:11:50,784 Val Step[1300/1563], Avg Loss: 1.3251, Avg Acc@1: 0.7121, Avg Acc@5: 0.9048
2022-01-17 05:11:52,333 Val Step[1350/1563], Avg Loss: 1.3247, Avg Acc@1: 0.7116, Avg Acc@5: 0.9049
2022-01-17 05:11:53,863 Val Step[1400/1563], Avg Loss: 1.3247, Avg Acc@1: 0.7115, Avg Acc@5: 0.9046
2022-01-17 05:11:55,567 Val Step[1450/1563], Avg Loss: 1.3243, Avg Acc@1: 0.7118, Avg Acc@5: 0.9044
2022-01-17 05:11:57,134 Val Step[1500/1563], Avg Loss: 1.3238, Avg Acc@1: 0.7119, Avg Acc@5: 0.9050
2022-01-17 05:11:58,880 Val Step[1550/1563], Avg Loss: 1.3251, Avg Acc@1: 0.7115, Avg Acc@5: 0.9045
2022-01-17 05:12:00,988 ----- Epoch[132/300], Validation Loss: 1.3248, Validation Acc@1: 0.7116, Validation Acc@5: 0.9046, time: 132.12
2022-01-17 05:12:00,988 Now training epoch 133. LR=0.000653
2022-01-17 05:13:35,603 Epoch[133/300], Step[0000/1252], Avg Loss: 3.4524, Avg Acc: 0.3027
2022-01-17 05:14:54,309 Epoch[133/300], Step[0050/1252], Avg Loss: 3.4805, Avg Acc: 0.4164
2022-01-17 05:16:14,245 Epoch[133/300], Step[0100/1252], Avg Loss: 3.5397, Avg Acc: 0.3931
2022-01-17 05:17:34,762 Epoch[133/300], Step[0150/1252], Avg Loss: 3.5284, Avg Acc: 0.3902
2022-01-17 05:18:55,211 Epoch[133/300], Step[0200/1252], Avg Loss: 3.5262, Avg Acc: 0.3944
2022-01-17 05:20:16,380 Epoch[133/300], Step[0250/1252], Avg Loss: 3.5127, Avg Acc: 0.3874
2022-01-17 05:21:35,816 Epoch[133/300], Step[0300/1252], Avg Loss: 3.5038, Avg Acc: 0.3874
2022-01-17 05:22:56,286 Epoch[133/300], Step[0350/1252], Avg Loss: 3.5004, Avg Acc: 0.3876
2022-01-17 05:24:17,499 Epoch[133/300], Step[0400/1252], Avg Loss: 3.4971, Avg Acc: 0.3880
2022-01-17 05:25:37,744 Epoch[133/300], Step[0450/1252], Avg Loss: 3.4926, Avg Acc: 0.3870
2022-01-17 05:26:59,156 Epoch[133/300], Step[0500/1252], Avg Loss: 3.4883, Avg Acc: 0.3865
2022-01-17 05:28:19,749 Epoch[133/300], Step[0550/1252], Avg Loss: 3.4881, Avg Acc: 0.3871
2022-01-17 05:29:39,789 Epoch[133/300], Step[0600/1252], Avg Loss: 3.4823, Avg Acc: 0.3870
2022-01-17 05:30:59,812 Epoch[133/300], Step[0650/1252], Avg Loss: 3.4833, Avg Acc: 0.3873
2022-01-17 05:32:20,968 Epoch[133/300], Step[0700/1252], Avg Loss: 3.4786, Avg Acc: 0.3885
2022-01-17 05:33:42,313 Epoch[133/300], Step[0750/1252], Avg Loss: 3.4768, Avg Acc: 0.3872
2022-01-17 05:35:02,139 Epoch[133/300], Step[0800/1252], Avg Loss: 3.4795, Avg Acc: 0.3888
2022-01-17 05:36:21,418 Epoch[133/300], Step[0850/1252], Avg Loss: 3.4757, Avg Acc: 0.3900
2022-01-17 05:37:42,311 Epoch[133/300], Step[0900/1252], Avg Loss: 3.4757, Avg Acc: 0.3896
2022-01-17 05:39:03,314 Epoch[133/300], Step[0950/1252], Avg Loss: 3.4755, Avg Acc: 0.3907
2022-01-17 05:40:24,157 Epoch[133/300], Step[1000/1252], Avg Loss: 3.4765, Avg Acc: 0.3916
2022-01-17 05:41:45,093 Epoch[133/300], Step[1050/1252], Avg Loss: 3.4773, Avg Acc: 0.3914
2022-01-17 05:43:05,766 Epoch[133/300], Step[1100/1252], Avg Loss: 3.4770, Avg Acc: 0.3905
2022-01-17 05:44:25,805 Epoch[133/300], Step[1150/1252], Avg Loss: 3.4767, Avg Acc: 0.3903
2022-01-17 05:45:46,027 Epoch[133/300], Step[1200/1252], Avg Loss: 3.4768, Avg Acc: 0.3913
2022-01-17 05:47:06,032 Epoch[133/300], Step[1250/1252], Avg Loss: 3.4776, Avg Acc: 0.3906
2022-01-17 05:47:12,353 ----- Epoch[133/300], Train Loss: 3.4776, Train Acc: 0.3906, time: 2111.36, Best Val(epoch124) Acc@1: 0.7117
2022-01-17 05:47:12,353 Now training epoch 134. LR=0.000647
2022-01-17 05:48:49,151 Epoch[134/300], Step[0000/1252], Avg Loss: 3.6400, Avg Acc: 0.3057
2022-01-17 05:50:08,312 Epoch[134/300], Step[0050/1252], Avg Loss: 3.4386, Avg Acc: 0.4000
2022-01-17 05:51:27,955 Epoch[134/300], Step[0100/1252], Avg Loss: 3.4710, Avg Acc: 0.3956
2022-01-17 05:52:47,320 Epoch[134/300], Step[0150/1252], Avg Loss: 3.4645, Avg Acc: 0.3915
2022-01-17 05:54:06,757 Epoch[134/300], Step[0200/1252], Avg Loss: 3.4469, Avg Acc: 0.3954
2022-01-17 05:55:25,458 Epoch[134/300], Step[0250/1252], Avg Loss: 3.4584, Avg Acc: 0.3952
2022-01-17 05:56:43,882 Epoch[134/300], Step[0300/1252], Avg Loss: 3.4631, Avg Acc: 0.3975
2022-01-17 05:58:02,827 Epoch[134/300], Step[0350/1252], Avg Loss: 3.4623, Avg Acc: 0.3997
2022-01-17 05:59:22,027 Epoch[134/300], Step[0400/1252], Avg Loss: 3.4656, Avg Acc: 0.3976
2022-01-17 06:00:40,759 Epoch[134/300], Step[0450/1252], Avg Loss: 3.4672, Avg Acc: 0.3987
2022-01-17 06:02:00,429 Epoch[134/300], Step[0500/1252], Avg Loss: 3.4661, Avg Acc: 0.3976
2022-01-17 06:03:19,590 Epoch[134/300], Step[0550/1252], Avg Loss: 3.4632, Avg Acc: 0.3943
2022-01-17 06:04:39,235 Epoch[134/300], Step[0600/1252], Avg Loss: 3.4684, Avg Acc: 0.3931
2022-01-17 06:05:57,494 Epoch[134/300], Step[0650/1252], Avg Loss: 3.4689, Avg Acc: 0.3941
2022-01-17 06:07:17,893 Epoch[134/300], Step[0700/1252], Avg Loss: 3.4745, Avg Acc: 0.3921
2022-01-17 06:08:37,267 Epoch[134/300], Step[0750/1252], Avg Loss: 3.4740, Avg Acc: 0.3911
2022-01-17 06:09:57,627 Epoch[134/300], Step[0800/1252], Avg Loss: 3.4767, Avg Acc: 0.3911
2022-01-17 06:11:17,813 Epoch[134/300], Step[0850/1252], Avg Loss: 3.4775, Avg Acc: 0.3904
2022-01-17 06:12:37,306 Epoch[134/300], Step[0900/1252], Avg Loss: 3.4749, Avg Acc: 0.3916
2022-01-17 06:13:56,212 Epoch[134/300], Step[0950/1252], Avg Loss: 3.4751, Avg Acc: 0.3930
2022-01-17 06:15:16,375 Epoch[134/300], Step[1000/1252], Avg Loss: 3.4769, Avg Acc: 0.3927
2022-01-17 06:16:36,082 Epoch[134/300], Step[1050/1252], Avg Loss: 3.4746, Avg Acc: 0.3932
2022-01-17 06:17:56,186 Epoch[134/300], Step[1100/1252], Avg Loss: 3.4764, Avg Acc: 0.3926
2022-01-17 06:19:15,626 Epoch[134/300], Step[1150/1252], Avg Loss: 3.4768, Avg Acc: 0.3921
2022-01-17 06:20:34,205 Epoch[134/300], Step[1200/1252], Avg Loss: 3.4764, Avg Acc: 0.3931
2022-01-17 06:21:54,381 Epoch[134/300], Step[1250/1252], Avg Loss: 3.4766, Avg Acc: 0.3923
2022-01-17 06:22:00,657 ----- Epoch[134/300], Train Loss: 3.4766, Train Acc: 0.3923, time: 2088.30, Best Val(epoch124) Acc@1: 0.7117
2022-01-17 06:22:00,657 ----- Validation after Epoch: 134
2022-01-17 06:23:12,089 Val Step[0000/1563], Avg Loss: 1.2228, Avg Acc@1: 0.7500, Avg Acc@5: 0.9062
2022-01-17 06:23:13,939 Val Step[0050/1563], Avg Loss: 1.2990, Avg Acc@1: 0.7145, Avg Acc@5: 0.9007
2022-01-17 06:23:15,478 Val Step[0100/1563], Avg Loss: 1.3081, Avg Acc@1: 0.7132, Avg Acc@5: 0.9028
2022-01-17 06:23:17,013 Val Step[0150/1563], Avg Loss: 1.3058, Avg Acc@1: 0.7142, Avg Acc@5: 0.9017
2022-01-17 06:23:18,636 Val Step[0200/1563], Avg Loss: 1.2989, Avg Acc@1: 0.7149, Avg Acc@5: 0.9036
2022-01-17 06:23:20,274 Val Step[0250/1563], Avg Loss: 1.2878, Avg Acc@1: 0.7178, Avg Acc@5: 0.9049
2022-01-17 06:23:21,876 Val Step[0300/1563], Avg Loss: 1.2897, Avg Acc@1: 0.7179, Avg Acc@5: 0.9043
2022-01-17 06:23:23,537 Val Step[0350/1563], Avg Loss: 1.2959, Avg Acc@1: 0.7177, Avg Acc@5: 0.9048
2022-01-17 06:23:25,170 Val Step[0400/1563], Avg Loss: 1.2928, Avg Acc@1: 0.7188, Avg Acc@5: 0.9051
2022-01-17 06:23:26,977 Val Step[0450/1563], Avg Loss: 1.2984, Avg Acc@1: 0.7176, Avg Acc@5: 0.9045
2022-01-17 06:23:28,800 Val Step[0500/1563], Avg Loss: 1.3042, Avg Acc@1: 0.7166, Avg Acc@5: 0.9043
2022-01-17 06:23:30,438 Val Step[0550/1563], Avg Loss: 1.3051, Avg Acc@1: 0.7156, Avg Acc@5: 0.9046
2022-01-17 06:23:32,023 Val Step[0600/1563], Avg Loss: 1.3061, Avg Acc@1: 0.7153, Avg Acc@5: 0.9044
2022-01-17 06:23:33,713 Val Step[0650/1563], Avg Loss: 1.3057, Avg Acc@1: 0.7156, Avg Acc@5: 0.9043
2022-01-17 06:23:35,501 Val Step[0700/1563], Avg Loss: 1.3041, Avg Acc@1: 0.7153, Avg Acc@5: 0.9049
2022-01-17 06:23:37,266 Val Step[0750/1563], Avg Loss: 1.3101, Avg Acc@1: 0.7145, Avg Acc@5: 0.9043
2022-01-17 06:23:39,005 Val Step[0800/1563], Avg Loss: 1.3092, Avg Acc@1: 0.7145, Avg Acc@5: 0.9048
2022-01-17 06:23:40,812 Val Step[0850/1563], Avg Loss: 1.3095, Avg Acc@1: 0.7140, Avg Acc@5: 0.9048
2022-01-17 06:23:42,660 Val Step[0900/1563], Avg Loss: 1.3066, Avg Acc@1: 0.7142, Avg Acc@5: 0.9051
2022-01-17 06:23:44,569 Val Step[0950/1563], Avg Loss: 1.3071, Avg Acc@1: 0.7142, Avg Acc@5: 0.9054
2022-01-17 06:23:46,321 Val Step[1000/1563], Avg Loss: 1.3067, Avg Acc@1: 0.7143, Avg Acc@5: 0.9058
2022-01-17 06:23:48,170 Val Step[1050/1563], Avg Loss: 1.3094, Avg Acc@1: 0.7135, Avg Acc@5: 0.9052
2022-01-17 06:23:50,051 Val Step[1100/1563], Avg Loss: 1.3093, Avg Acc@1: 0.7134, Avg Acc@5: 0.9052
2022-01-17 06:23:51,634 Val Step[1150/1563], Avg Loss: 1.3073, Avg Acc@1: 0.7139, Avg Acc@5: 0.9053
2022-01-17 06:23:53,182 Val Step[1200/1563], Avg Loss: 1.3067, Avg Acc@1: 0.7142, Avg Acc@5: 0.9054
2022-01-17 06:23:54,818 Val Step[1250/1563], Avg Loss: 1.3058, Avg Acc@1: 0.7139, Avg Acc@5: 0.9057
2022-01-17 06:23:56,398 Val Step[1300/1563], Avg Loss: 1.3093, Avg Acc@1: 0.7135, Avg Acc@5: 0.9052
2022-01-17 06:23:58,177 Val Step[1350/1563], Avg Loss: 1.3097, Avg Acc@1: 0.7132, Avg Acc@5: 0.9050
2022-01-17 06:24:00,036 Val Step[1400/1563], Avg Loss: 1.3098, Avg Acc@1: 0.7131, Avg Acc@5: 0.9049
2022-01-17 06:24:01,679 Val Step[1450/1563], Avg Loss: 1.3105, Avg Acc@1: 0.7131, Avg Acc@5: 0.9046
2022-01-17 06:24:03,572 Val Step[1500/1563], Avg Loss: 1.3096, Avg Acc@1: 0.7131, Avg Acc@5: 0.9048
2022-01-17 06:24:05,096 Val Step[1550/1563], Avg Loss: 1.3098, Avg Acc@1: 0.7133, Avg Acc@5: 0.9048
2022-01-17 06:24:07,060 ----- Epoch[134/300], Validation Loss: 1.3098, Validation Acc@1: 0.7132, Validation Acc@5: 0.9050, time: 126.40
2022-01-17 06:24:08,138 the pre best model acc:0.7117, at epoch 124
2022-01-17 06:24:08,439 current best model acc:0.7132, at epoch 134
2022-01-17 06:24:08,439 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 06:24:08,439 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 06:24:08,439 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 06:24:08,439 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 06:24:08,439 Now training epoch 135. LR=0.000642
2022-01-17 06:25:42,388 Epoch[135/300], Step[0000/1252], Avg Loss: 3.8783, Avg Acc: 0.4590
2022-01-17 06:27:02,995 Epoch[135/300], Step[0050/1252], Avg Loss: 3.4620, Avg Acc: 0.4184
2022-01-17 06:28:23,467 Epoch[135/300], Step[0100/1252], Avg Loss: 3.4916, Avg Acc: 0.4048
2022-01-17 06:29:43,305 Epoch[135/300], Step[0150/1252], Avg Loss: 3.4815, Avg Acc: 0.4094
2022-01-17 06:31:03,757 Epoch[135/300], Step[0200/1252], Avg Loss: 3.4639, Avg Acc: 0.4114
2022-01-17 06:32:24,752 Epoch[135/300], Step[0250/1252], Avg Loss: 3.4637, Avg Acc: 0.4047
2022-01-17 06:33:44,893 Epoch[135/300], Step[0300/1252], Avg Loss: 3.4630, Avg Acc: 0.4069
2022-01-17 06:35:04,733 Epoch[135/300], Step[0350/1252], Avg Loss: 3.4603, Avg Acc: 0.4065
2022-01-17 06:36:24,639 Epoch[135/300], Step[0400/1252], Avg Loss: 3.4513, Avg Acc: 0.4065
2022-01-17 06:37:44,913 Epoch[135/300], Step[0450/1252], Avg Loss: 3.4436, Avg Acc: 0.4065
2022-01-17 06:39:05,125 Epoch[135/300], Step[0500/1252], Avg Loss: 3.4408, Avg Acc: 0.4062
2022-01-17 06:40:25,112 Epoch[135/300], Step[0550/1252], Avg Loss: 3.4435, Avg Acc: 0.4058
2022-01-17 06:41:46,301 Epoch[135/300], Step[0600/1252], Avg Loss: 3.4493, Avg Acc: 0.4025
2022-01-17 06:43:06,625 Epoch[135/300], Step[0650/1252], Avg Loss: 3.4556, Avg Acc: 0.4044
2022-01-17 06:44:26,711 Epoch[135/300], Step[0700/1252], Avg Loss: 3.4649, Avg Acc: 0.4038
2022-01-17 06:45:47,078 Epoch[135/300], Step[0750/1252], Avg Loss: 3.4676, Avg Acc: 0.4047
2022-01-17 06:47:08,286 Epoch[135/300], Step[0800/1252], Avg Loss: 3.4694, Avg Acc: 0.4041
2022-01-17 06:48:29,011 Epoch[135/300], Step[0850/1252], Avg Loss: 3.4694, Avg Acc: 0.4037
2022-01-17 06:49:48,516 Epoch[135/300], Step[0900/1252], Avg Loss: 3.4685, Avg Acc: 0.4054
2022-01-17 06:51:08,410 Epoch[135/300], Step[0950/1252], Avg Loss: 3.4713, Avg Acc: 0.4067
2022-01-17 06:52:29,167 Epoch[135/300], Step[1000/1252], Avg Loss: 3.4711, Avg Acc: 0.4053
2022-01-17 06:53:50,091 Epoch[135/300], Step[1050/1252], Avg Loss: 3.4705, Avg Acc: 0.4053
2022-01-17 06:55:11,577 Epoch[135/300], Step[1100/1252], Avg Loss: 3.4686, Avg Acc: 0.4049
2022-01-17 06:56:32,370 Epoch[135/300], Step[1150/1252], Avg Loss: 3.4688, Avg Acc: 0.4051
2022-01-17 06:57:53,289 Epoch[135/300], Step[1200/1252], Avg Loss: 3.4698, Avg Acc: 0.4054
2022-01-17 06:59:13,377 Epoch[135/300], Step[1250/1252], Avg Loss: 3.4684, Avg Acc: 0.4047
2022-01-17 06:59:20,212 ----- Epoch[135/300], Train Loss: 3.4684, Train Acc: 0.4046, time: 2111.77, Best Val(epoch134) Acc@1: 0.7132
2022-01-17 06:59:20,212 Now training epoch 136. LR=0.000637
2022-01-17 07:00:56,552 Epoch[136/300], Step[0000/1252], Avg Loss: 3.4193, Avg Acc: 0.4277
2022-01-17 07:02:16,008 Epoch[136/300], Step[0050/1252], Avg Loss: 3.4193, Avg Acc: 0.4175
2022-01-17 07:03:35,221 Epoch[136/300], Step[0100/1252], Avg Loss: 3.4413, Avg Acc: 0.4125
2022-01-17 07:04:54,218 Epoch[136/300], Step[0150/1252], Avg Loss: 3.4482, Avg Acc: 0.3994
2022-01-17 07:06:13,439 Epoch[136/300], Step[0200/1252], Avg Loss: 3.4508, Avg Acc: 0.3997
2022-01-17 07:07:33,529 Epoch[136/300], Step[0250/1252], Avg Loss: 3.4543, Avg Acc: 0.3998
2022-01-17 07:08:54,216 Epoch[136/300], Step[0300/1252], Avg Loss: 3.4602, Avg Acc: 0.4005
2022-01-17 07:10:13,752 Epoch[136/300], Step[0350/1252], Avg Loss: 3.4685, Avg Acc: 0.3985
2022-01-17 07:11:34,129 Epoch[136/300], Step[0400/1252], Avg Loss: 3.4703, Avg Acc: 0.3997
2022-01-17 07:12:53,739 Epoch[136/300], Step[0450/1252], Avg Loss: 3.4755, Avg Acc: 0.4007
2022-01-17 07:14:13,817 Epoch[136/300], Step[0500/1252], Avg Loss: 3.4749, Avg Acc: 0.4022
2022-01-17 07:15:34,870 Epoch[136/300], Step[0550/1252], Avg Loss: 3.4775, Avg Acc: 0.3986
2022-01-17 07:16:55,603 Epoch[136/300], Step[0600/1252], Avg Loss: 3.4795, Avg Acc: 0.3980
2022-01-17 07:18:16,454 Epoch[136/300], Step[0650/1252], Avg Loss: 3.4785, Avg Acc: 0.3994
2022-01-17 07:19:37,034 Epoch[136/300], Step[0700/1252], Avg Loss: 3.4813, Avg Acc: 0.3985
2022-01-17 07:20:57,000 Epoch[136/300], Step[0750/1252], Avg Loss: 3.4786, Avg Acc: 0.3987
2022-01-17 07:22:16,300 Epoch[136/300], Step[0800/1252], Avg Loss: 3.4848, Avg Acc: 0.3991
2022-01-17 07:23:36,381 Epoch[136/300], Step[0850/1252], Avg Loss: 3.4864, Avg Acc: 0.3994
2022-01-17 07:24:55,891 Epoch[136/300], Step[0900/1252], Avg Loss: 3.4846, Avg Acc: 0.4002
2022-01-17 07:26:17,123 Epoch[136/300], Step[0950/1252], Avg Loss: 3.4848, Avg Acc: 0.4000
2022-01-17 07:27:36,641 Epoch[136/300], Step[1000/1252], Avg Loss: 3.4819, Avg Acc: 0.4006
2022-01-17 07:28:56,806 Epoch[136/300], Step[1050/1252], Avg Loss: 3.4804, Avg Acc: 0.4018
2022-01-17 07:30:17,657 Epoch[136/300], Step[1100/1252], Avg Loss: 3.4790, Avg Acc: 0.4016
2022-01-17 07:31:38,841 Epoch[136/300], Step[1150/1252], Avg Loss: 3.4785, Avg Acc: 0.4019
2022-01-17 07:32:59,548 Epoch[136/300], Step[1200/1252], Avg Loss: 3.4794, Avg Acc: 0.4020
2022-01-17 07:34:18,453 Epoch[136/300], Step[1250/1252], Avg Loss: 3.4770, Avg Acc: 0.4031
2022-01-17 07:34:25,427 ----- Epoch[136/300], Train Loss: 3.4770, Train Acc: 0.4031, time: 2105.21, Best Val(epoch134) Acc@1: 0.7132
2022-01-17 07:34:25,428 ----- Validation after Epoch: 136
2022-01-17 07:35:39,706 Val Step[0000/1563], Avg Loss: 1.0845, Avg Acc@1: 0.7812, Avg Acc@5: 0.9688
2022-01-17 07:35:41,469 Val Step[0050/1563], Avg Loss: 1.2671, Avg Acc@1: 0.7138, Avg Acc@5: 0.9044
2022-01-17 07:35:43,150 Val Step[0100/1563], Avg Loss: 1.2820, Avg Acc@1: 0.7061, Avg Acc@5: 0.9078
2022-01-17 07:35:44,681 Val Step[0150/1563], Avg Loss: 1.2742, Avg Acc@1: 0.7088, Avg Acc@5: 0.9075
2022-01-17 07:35:46,258 Val Step[0200/1563], Avg Loss: 1.2687, Avg Acc@1: 0.7107, Avg Acc@5: 0.9090
2022-01-17 07:35:47,833 Val Step[0250/1563], Avg Loss: 1.2605, Avg Acc@1: 0.7134, Avg Acc@5: 0.9086
2022-01-17 07:35:49,376 Val Step[0300/1563], Avg Loss: 1.2608, Avg Acc@1: 0.7139, Avg Acc@5: 0.9076
2022-01-17 07:35:50,956 Val Step[0350/1563], Avg Loss: 1.2652, Avg Acc@1: 0.7138, Avg Acc@5: 0.9070
2022-01-17 07:35:52,608 Val Step[0400/1563], Avg Loss: 1.2636, Avg Acc@1: 0.7149, Avg Acc@5: 0.9073
2022-01-17 07:35:54,294 Val Step[0450/1563], Avg Loss: 1.2660, Avg Acc@1: 0.7124, Avg Acc@5: 0.9076
2022-01-17 07:35:55,832 Val Step[0500/1563], Avg Loss: 1.2676, Avg Acc@1: 0.7122, Avg Acc@5: 0.9076
2022-01-17 07:35:57,465 Val Step[0550/1563], Avg Loss: 1.2680, Avg Acc@1: 0.7115, Avg Acc@5: 0.9076
2022-01-17 07:35:59,145 Val Step[0600/1563], Avg Loss: 1.2668, Avg Acc@1: 0.7119, Avg Acc@5: 0.9074
2022-01-17 07:36:00,789 Val Step[0650/1563], Avg Loss: 1.2685, Avg Acc@1: 0.7116, Avg Acc@5: 0.9074
2022-01-17 07:36:02,354 Val Step[0700/1563], Avg Loss: 1.2644, Avg Acc@1: 0.7129, Avg Acc@5: 0.9084
2022-01-17 07:36:03,915 Val Step[0750/1563], Avg Loss: 1.2706, Avg Acc@1: 0.7123, Avg Acc@5: 0.9075
2022-01-17 07:36:05,671 Val Step[0800/1563], Avg Loss: 1.2691, Avg Acc@1: 0.7138, Avg Acc@5: 0.9077
2022-01-17 07:36:07,487 Val Step[0850/1563], Avg Loss: 1.2690, Avg Acc@1: 0.7134, Avg Acc@5: 0.9078
2022-01-17 07:36:09,037 Val Step[0900/1563], Avg Loss: 1.2670, Avg Acc@1: 0.7140, Avg Acc@5: 0.9081
2022-01-17 07:36:10,598 Val Step[0950/1563], Avg Loss: 1.2665, Avg Acc@1: 0.7144, Avg Acc@5: 0.9083
2022-01-17 07:36:12,141 Val Step[1000/1563], Avg Loss: 1.2677, Avg Acc@1: 0.7146, Avg Acc@5: 0.9082
2022-01-17 07:36:13,816 Val Step[1050/1563], Avg Loss: 1.2698, Avg Acc@1: 0.7138, Avg Acc@5: 0.9078
2022-01-17 07:36:15,562 Val Step[1100/1563], Avg Loss: 1.2704, Avg Acc@1: 0.7140, Avg Acc@5: 0.9077
2022-01-17 07:36:17,192 Val Step[1150/1563], Avg Loss: 1.2690, Avg Acc@1: 0.7144, Avg Acc@5: 0.9076
2022-01-17 07:36:18,910 Val Step[1200/1563], Avg Loss: 1.2679, Avg Acc@1: 0.7150, Avg Acc@5: 0.9076
2022-01-17 07:36:20,577 Val Step[1250/1563], Avg Loss: 1.2668, Avg Acc@1: 0.7149, Avg Acc@5: 0.9079
2022-01-17 07:36:22,190 Val Step[1300/1563], Avg Loss: 1.2696, Avg Acc@1: 0.7147, Avg Acc@5: 0.9072
2022-01-17 07:36:23,814 Val Step[1350/1563], Avg Loss: 1.2699, Avg Acc@1: 0.7144, Avg Acc@5: 0.9068
2022-01-17 07:36:25,352 Val Step[1400/1563], Avg Loss: 1.2698, Avg Acc@1: 0.7140, Avg Acc@5: 0.9068
2022-01-17 07:36:26,898 Val Step[1450/1563], Avg Loss: 1.2697, Avg Acc@1: 0.7143, Avg Acc@5: 0.9066
2022-01-17 07:36:28,516 Val Step[1500/1563], Avg Loss: 1.2687, Avg Acc@1: 0.7149, Avg Acc@5: 0.9071
2022-01-17 07:36:30,258 Val Step[1550/1563], Avg Loss: 1.2696, Avg Acc@1: 0.7148, Avg Acc@5: 0.9069
2022-01-17 07:36:32,383 ----- Epoch[136/300], Validation Loss: 1.2694, Validation Acc@1: 0.7147, Validation Acc@5: 0.9071, time: 126.95
2022-01-17 07:36:33,587 the pre best model acc:0.7132, at epoch 134
2022-01-17 07:36:33,865 current best model acc:0.7147, at epoch 136
2022-01-17 07:36:33,865 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 07:36:33,865 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 07:36:33,865 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 07:36:33,865 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 07:36:33,865 Now training epoch 137. LR=0.000631
2022-01-17 07:38:05,462 Epoch[137/300], Step[0000/1252], Avg Loss: 3.4281, Avg Acc: 0.6133
2022-01-17 07:39:25,616 Epoch[137/300], Step[0050/1252], Avg Loss: 3.4797, Avg Acc: 0.4019
2022-01-17 07:40:44,655 Epoch[137/300], Step[0100/1252], Avg Loss: 3.4837, Avg Acc: 0.4053
2022-01-17 07:42:04,594 Epoch[137/300], Step[0150/1252], Avg Loss: 3.4734, Avg Acc: 0.4051
2022-01-17 07:43:25,027 Epoch[137/300], Step[0200/1252], Avg Loss: 3.4662, Avg Acc: 0.3976
2022-01-17 07:44:43,666 Epoch[137/300], Step[0250/1252], Avg Loss: 3.4703, Avg Acc: 0.3947
2022-01-17 07:46:03,517 Epoch[137/300], Step[0300/1252], Avg Loss: 3.4656, Avg Acc: 0.3991
2022-01-17 07:47:22,839 Epoch[137/300], Step[0350/1252], Avg Loss: 3.4627, Avg Acc: 0.4008
2022-01-17 07:48:42,554 Epoch[137/300], Step[0400/1252], Avg Loss: 3.4685, Avg Acc: 0.3981
2022-01-17 07:50:02,220 Epoch[137/300], Step[0450/1252], Avg Loss: 3.4812, Avg Acc: 0.3971
2022-01-17 07:51:22,511 Epoch[137/300], Step[0500/1252], Avg Loss: 3.4784, Avg Acc: 0.3981
2022-01-17 07:52:42,047 Epoch[137/300], Step[0550/1252], Avg Loss: 3.4774, Avg Acc: 0.3994
2022-01-17 07:54:01,279 Epoch[137/300], Step[0600/1252], Avg Loss: 3.4771, Avg Acc: 0.4020
2022-01-17 07:55:21,643 Epoch[137/300], Step[0650/1252], Avg Loss: 3.4773, Avg Acc: 0.4009
2022-01-17 07:56:42,108 Epoch[137/300], Step[0700/1252], Avg Loss: 3.4751, Avg Acc: 0.4017
2022-01-17 07:58:01,947 Epoch[137/300], Step[0750/1252], Avg Loss: 3.4759, Avg Acc: 0.4009
2022-01-17 07:59:22,032 Epoch[137/300], Step[0800/1252], Avg Loss: 3.4726, Avg Acc: 0.3994
2022-01-17 08:00:42,376 Epoch[137/300], Step[0850/1252], Avg Loss: 3.4741, Avg Acc: 0.3993
2022-01-17 08:02:02,755 Epoch[137/300], Step[0900/1252], Avg Loss: 3.4749, Avg Acc: 0.3992
2022-01-17 08:03:22,645 Epoch[137/300], Step[0950/1252], Avg Loss: 3.4737, Avg Acc: 0.3988
2022-01-17 08:04:42,332 Epoch[137/300], Step[1000/1252], Avg Loss: 3.4730, Avg Acc: 0.3998
2022-01-17 08:06:02,403 Epoch[137/300], Step[1050/1252], Avg Loss: 3.4758, Avg Acc: 0.3992
2022-01-17 08:07:22,676 Epoch[137/300], Step[1100/1252], Avg Loss: 3.4760, Avg Acc: 0.3984
2022-01-17 08:08:41,760 Epoch[137/300], Step[1150/1252], Avg Loss: 3.4776, Avg Acc: 0.3986
2022-01-17 08:10:00,981 Epoch[137/300], Step[1200/1252], Avg Loss: 3.4773, Avg Acc: 0.3994
2022-01-17 08:11:20,282 Epoch[137/300], Step[1250/1252], Avg Loss: 3.4799, Avg Acc: 0.3991
2022-01-17 08:11:26,935 ----- Epoch[137/300], Train Loss: 3.4799, Train Acc: 0.3991, time: 2093.07, Best Val(epoch136) Acc@1: 0.7147
2022-01-17 08:11:26,935 Now training epoch 138. LR=0.000626
2022-01-17 08:13:00,377 Epoch[138/300], Step[0000/1252], Avg Loss: 3.1353, Avg Acc: 0.3262
2022-01-17 08:14:19,126 Epoch[138/300], Step[0050/1252], Avg Loss: 3.4085, Avg Acc: 0.3929
2022-01-17 08:15:38,764 Epoch[138/300], Step[0100/1252], Avg Loss: 3.4376, Avg Acc: 0.3921
2022-01-17 08:16:57,453 Epoch[138/300], Step[0150/1252], Avg Loss: 3.4526, Avg Acc: 0.3994
2022-01-17 08:18:17,161 Epoch[138/300], Step[0200/1252], Avg Loss: 3.4416, Avg Acc: 0.4045
2022-01-17 08:19:34,737 Epoch[138/300], Step[0250/1252], Avg Loss: 3.4336, Avg Acc: 0.4101
2022-01-17 08:20:55,012 Epoch[138/300], Step[0300/1252], Avg Loss: 3.4311, Avg Acc: 0.4101
2022-01-17 08:22:14,302 Epoch[138/300], Step[0350/1252], Avg Loss: 3.4246, Avg Acc: 0.4124
2022-01-17 08:23:33,357 Epoch[138/300], Step[0400/1252], Avg Loss: 3.4256, Avg Acc: 0.4095
2022-01-17 08:24:53,003 Epoch[138/300], Step[0450/1252], Avg Loss: 3.4262, Avg Acc: 0.4069
2022-01-17 08:26:13,302 Epoch[138/300], Step[0500/1252], Avg Loss: 3.4330, Avg Acc: 0.4066
2022-01-17 08:27:32,383 Epoch[138/300], Step[0550/1252], Avg Loss: 3.4353, Avg Acc: 0.4072
2022-01-17 08:28:52,241 Epoch[138/300], Step[0600/1252], Avg Loss: 3.4407, Avg Acc: 0.4055
2022-01-17 08:30:11,500 Epoch[138/300], Step[0650/1252], Avg Loss: 3.4428, Avg Acc: 0.4054
2022-01-17 08:31:31,460 Epoch[138/300], Step[0700/1252], Avg Loss: 3.4510, Avg Acc: 0.4041
2022-01-17 08:32:51,509 Epoch[138/300], Step[0750/1252], Avg Loss: 3.4522, Avg Acc: 0.4035
2022-01-17 08:34:12,038 Epoch[138/300], Step[0800/1252], Avg Loss: 3.4500, Avg Acc: 0.4038
2022-01-17 08:35:31,140 Epoch[138/300], Step[0850/1252], Avg Loss: 3.4506, Avg Acc: 0.4041
2022-01-17 08:36:51,458 Epoch[138/300], Step[0900/1252], Avg Loss: 3.4535, Avg Acc: 0.4043
2022-01-17 08:38:10,720 Epoch[138/300], Step[0950/1252], Avg Loss: 3.4538, Avg Acc: 0.4043
2022-01-17 08:39:30,256 Epoch[138/300], Step[1000/1252], Avg Loss: 3.4513, Avg Acc: 0.4044
2022-01-17 08:40:49,979 Epoch[138/300], Step[1050/1252], Avg Loss: 3.4528, Avg Acc: 0.4040
2022-01-17 08:42:08,921 Epoch[138/300], Step[1100/1252], Avg Loss: 3.4532, Avg Acc: 0.4035
2022-01-17 08:43:29,545 Epoch[138/300], Step[1150/1252], Avg Loss: 3.4558, Avg Acc: 0.4023
2022-01-17 08:44:49,724 Epoch[138/300], Step[1200/1252], Avg Loss: 3.4573, Avg Acc: 0.4021
2022-01-17 08:46:08,854 Epoch[138/300], Step[1250/1252], Avg Loss: 3.4554, Avg Acc: 0.4026
2022-01-17 08:46:15,035 ----- Epoch[138/300], Train Loss: 3.4554, Train Acc: 0.4026, time: 2088.10, Best Val(epoch136) Acc@1: 0.7147
2022-01-17 08:46:15,036 ----- Validation after Epoch: 138
2022-01-17 08:47:34,731 Val Step[0000/1563], Avg Loss: 1.0169, Avg Acc@1: 0.7812, Avg Acc@5: 1.0000
2022-01-17 08:47:36,840 Val Step[0050/1563], Avg Loss: 1.2524, Avg Acc@1: 0.7188, Avg Acc@5: 0.9075
2022-01-17 08:47:38,592 Val Step[0100/1563], Avg Loss: 1.2675, Avg Acc@1: 0.7132, Avg Acc@5: 0.9106
2022-01-17 08:47:40,206 Val Step[0150/1563], Avg Loss: 1.2799, Avg Acc@1: 0.7173, Avg Acc@5: 0.9073
2022-01-17 08:47:41,802 Val Step[0200/1563], Avg Loss: 1.2725, Avg Acc@1: 0.7228, Avg Acc@5: 0.9097
2022-01-17 08:47:43,349 Val Step[0250/1563], Avg Loss: 1.2623, Avg Acc@1: 0.7242, Avg Acc@5: 0.9084
2022-01-17 08:47:45,057 Val Step[0300/1563], Avg Loss: 1.2672, Avg Acc@1: 0.7217, Avg Acc@5: 0.9073
2022-01-17 08:47:46,820 Val Step[0350/1563], Avg Loss: 1.2768, Avg Acc@1: 0.7208, Avg Acc@5: 0.9069
2022-01-17 08:47:48,546 Val Step[0400/1563], Avg Loss: 1.2753, Avg Acc@1: 0.7209, Avg Acc@5: 0.9080
2022-01-17 08:47:50,331 Val Step[0450/1563], Avg Loss: 1.2802, Avg Acc@1: 0.7188, Avg Acc@5: 0.9079
2022-01-17 08:47:52,070 Val Step[0500/1563], Avg Loss: 1.2806, Avg Acc@1: 0.7174, Avg Acc@5: 0.9084
2022-01-17 08:47:53,722 Val Step[0550/1563], Avg Loss: 1.2815, Avg Acc@1: 0.7168, Avg Acc@5: 0.9083
2022-01-17 08:47:55,420 Val Step[0600/1563], Avg Loss: 1.2808, Avg Acc@1: 0.7171, Avg Acc@5: 0.9082
2022-01-17 08:47:57,157 Val Step[0650/1563], Avg Loss: 1.2805, Avg Acc@1: 0.7173, Avg Acc@5: 0.9084
2022-01-17 08:47:58,839 Val Step[0700/1563], Avg Loss: 1.2784, Avg Acc@1: 0.7179, Avg Acc@5: 0.9089
2022-01-17 08:48:00,610 Val Step[0750/1563], Avg Loss: 1.2844, Avg Acc@1: 0.7173, Avg Acc@5: 0.9080
2022-01-17 08:48:02,326 Val Step[0800/1563], Avg Loss: 1.2841, Avg Acc@1: 0.7173, Avg Acc@5: 0.9083
2022-01-17 08:48:03,903 Val Step[0850/1563], Avg Loss: 1.2854, Avg Acc@1: 0.7162, Avg Acc@5: 0.9085
2022-01-17 08:48:05,670 Val Step[0900/1563], Avg Loss: 1.2830, Avg Acc@1: 0.7170, Avg Acc@5: 0.9086
2022-01-17 08:48:07,253 Val Step[0950/1563], Avg Loss: 1.2821, Avg Acc@1: 0.7173, Avg Acc@5: 0.9090
2022-01-17 08:48:09,043 Val Step[1000/1563], Avg Loss: 1.2836, Avg Acc@1: 0.7174, Avg Acc@5: 0.9089
2022-01-17 08:48:10,881 Val Step[1050/1563], Avg Loss: 1.2853, Avg Acc@1: 0.7171, Avg Acc@5: 0.9085
2022-01-17 08:48:12,435 Val Step[1100/1563], Avg Loss: 1.2864, Avg Acc@1: 0.7167, Avg Acc@5: 0.9083
2022-01-17 08:48:14,309 Val Step[1150/1563], Avg Loss: 1.2859, Avg Acc@1: 0.7168, Avg Acc@5: 0.9084
2022-01-17 08:48:16,094 Val Step[1200/1563], Avg Loss: 1.2848, Avg Acc@1: 0.7170, Avg Acc@5: 0.9083
2022-01-17 08:48:17,810 Val Step[1250/1563], Avg Loss: 1.2837, Avg Acc@1: 0.7171, Avg Acc@5: 0.9086
2022-01-17 08:48:19,516 Val Step[1300/1563], Avg Loss: 1.2863, Avg Acc@1: 0.7167, Avg Acc@5: 0.9084
2022-01-17 08:48:21,379 Val Step[1350/1563], Avg Loss: 1.2865, Avg Acc@1: 0.7163, Avg Acc@5: 0.9082
2022-01-17 08:48:22,992 Val Step[1400/1563], Avg Loss: 1.2865, Avg Acc@1: 0.7161, Avg Acc@5: 0.9081
2022-01-17 08:48:24,585 Val Step[1450/1563], Avg Loss: 1.2861, Avg Acc@1: 0.7162, Avg Acc@5: 0.9081
2022-01-17 08:48:26,324 Val Step[1500/1563], Avg Loss: 1.2855, Avg Acc@1: 0.7165, Avg Acc@5: 0.9085
2022-01-17 08:48:27,972 Val Step[1550/1563], Avg Loss: 1.2861, Avg Acc@1: 0.7169, Avg Acc@5: 0.9084
2022-01-17 08:48:29,875 ----- Epoch[138/300], Validation Loss: 1.2864, Validation Acc@1: 0.7168, Validation Acc@5: 0.9085, time: 134.84
2022-01-17 08:48:30,965 the pre best model acc:0.7147, at epoch 136
2022-01-17 08:48:31,250 current best model acc:0.7168, at epoch 138
2022-01-17 08:48:31,250 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 08:48:31,250 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 08:48:31,250 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 08:48:31,250 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 08:48:31,250 Now training epoch 139. LR=0.000621
2022-01-17 08:50:05,643 Epoch[139/300], Step[0000/1252], Avg Loss: 4.1367, Avg Acc: 0.3564
2022-01-17 08:51:25,640 Epoch[139/300], Step[0050/1252], Avg Loss: 3.3838, Avg Acc: 0.4048
2022-01-17 08:52:45,300 Epoch[139/300], Step[0100/1252], Avg Loss: 3.4439, Avg Acc: 0.4075
2022-01-17 08:54:04,925 Epoch[139/300], Step[0150/1252], Avg Loss: 3.4352, Avg Acc: 0.4100
2022-01-17 08:55:23,779 Epoch[139/300], Step[0200/1252], Avg Loss: 3.4418, Avg Acc: 0.4065
2022-01-17 08:56:44,615 Epoch[139/300], Step[0250/1252], Avg Loss: 3.4439, Avg Acc: 0.4003
2022-01-17 08:58:05,237 Epoch[139/300], Step[0300/1252], Avg Loss: 3.4441, Avg Acc: 0.3979
2022-01-17 08:59:25,228 Epoch[139/300], Step[0350/1252], Avg Loss: 3.4424, Avg Acc: 0.4028
2022-01-17 09:00:44,984 Epoch[139/300], Step[0400/1252], Avg Loss: 3.4445, Avg Acc: 0.4014
2022-01-17 09:02:05,610 Epoch[139/300], Step[0450/1252], Avg Loss: 3.4483, Avg Acc: 0.3998
2022-01-17 09:03:26,913 Epoch[139/300], Step[0500/1252], Avg Loss: 3.4518, Avg Acc: 0.3984
2022-01-17 09:04:47,254 Epoch[139/300], Step[0550/1252], Avg Loss: 3.4583, Avg Acc: 0.3980
2022-01-17 09:06:07,227 Epoch[139/300], Step[0600/1252], Avg Loss: 3.4563, Avg Acc: 0.4003
2022-01-17 09:07:27,206 Epoch[139/300], Step[0650/1252], Avg Loss: 3.4595, Avg Acc: 0.3986
2022-01-17 09:08:48,246 Epoch[139/300], Step[0700/1252], Avg Loss: 3.4635, Avg Acc: 0.3989
2022-01-17 09:10:07,886 Epoch[139/300], Step[0750/1252], Avg Loss: 3.4666, Avg Acc: 0.3976
2022-01-17 09:11:28,385 Epoch[139/300], Step[0800/1252], Avg Loss: 3.4636, Avg Acc: 0.3993
2022-01-17 09:12:49,174 Epoch[139/300], Step[0850/1252], Avg Loss: 3.4636, Avg Acc: 0.3996
2022-01-17 09:14:10,618 Epoch[139/300], Step[0900/1252], Avg Loss: 3.4632, Avg Acc: 0.3996
2022-01-17 09:15:31,899 Epoch[139/300], Step[0950/1252], Avg Loss: 3.4641, Avg Acc: 0.3991
2022-01-17 09:16:51,223 Epoch[139/300], Step[1000/1252], Avg Loss: 3.4651, Avg Acc: 0.3978
2022-01-17 09:18:12,367 Epoch[139/300], Step[1050/1252], Avg Loss: 3.4668, Avg Acc: 0.3981
2022-01-17 09:19:33,530 Epoch[139/300], Step[1100/1252], Avg Loss: 3.4659, Avg Acc: 0.3977
2022-01-17 09:20:54,022 Epoch[139/300], Step[1150/1252], Avg Loss: 3.4688, Avg Acc: 0.3976
2022-01-17 09:22:13,663 Epoch[139/300], Step[1200/1252], Avg Loss: 3.4683, Avg Acc: 0.3979
2022-01-17 09:23:33,173 Epoch[139/300], Step[1250/1252], Avg Loss: 3.4670, Avg Acc: 0.3976
2022-01-17 09:23:40,055 ----- Epoch[139/300], Train Loss: 3.4671, Train Acc: 0.3976, time: 2108.80, Best Val(epoch138) Acc@1: 0.7168
2022-01-17 09:23:40,055 Now training epoch 140. LR=0.000615
2022-01-17 09:25:15,805 Epoch[140/300], Step[0000/1252], Avg Loss: 3.8707, Avg Acc: 0.4180
2022-01-17 09:26:35,295 Epoch[140/300], Step[0050/1252], Avg Loss: 3.5514, Avg Acc: 0.3945
2022-01-17 09:27:54,370 Epoch[140/300], Step[0100/1252], Avg Loss: 3.4912, Avg Acc: 0.4025
2022-01-17 09:29:13,226 Epoch[140/300], Step[0150/1252], Avg Loss: 3.4869, Avg Acc: 0.4022
2022-01-17 09:30:31,724 Epoch[140/300], Step[0200/1252], Avg Loss: 3.4858, Avg Acc: 0.3983
2022-01-17 09:31:50,377 Epoch[140/300], Step[0250/1252], Avg Loss: 3.4937, Avg Acc: 0.4021
2022-01-17 09:33:09,175 Epoch[140/300], Step[0300/1252], Avg Loss: 3.4961, Avg Acc: 0.3954
2022-01-17 09:34:28,220 Epoch[140/300], Step[0350/1252], Avg Loss: 3.4902, Avg Acc: 0.3952
2022-01-17 09:35:48,575 Epoch[140/300], Step[0400/1252], Avg Loss: 3.4861, Avg Acc: 0.3924
2022-01-17 09:37:08,307 Epoch[140/300], Step[0450/1252], Avg Loss: 3.4869, Avg Acc: 0.3957
2022-01-17 09:38:28,178 Epoch[140/300], Step[0500/1252], Avg Loss: 3.4819, Avg Acc: 0.3956
2022-01-17 09:39:48,400 Epoch[140/300], Step[0550/1252], Avg Loss: 3.4839, Avg Acc: 0.3959
2022-01-17 09:41:08,695 Epoch[140/300], Step[0600/1252], Avg Loss: 3.4835, Avg Acc: 0.3974
2022-01-17 09:42:27,621 Epoch[140/300], Step[0650/1252], Avg Loss: 3.4842, Avg Acc: 0.3976
2022-01-17 09:43:47,073 Epoch[140/300], Step[0700/1252], Avg Loss: 3.4821, Avg Acc: 0.3978
2022-01-17 09:45:07,586 Epoch[140/300], Step[0750/1252], Avg Loss: 3.4772, Avg Acc: 0.3951
2022-01-17 09:46:26,727 Epoch[140/300], Step[0800/1252], Avg Loss: 3.4791, Avg Acc: 0.3955
2022-01-17 09:47:47,408 Epoch[140/300], Step[0850/1252], Avg Loss: 3.4771, Avg Acc: 0.3944
2022-01-17 09:49:08,462 Epoch[140/300], Step[0900/1252], Avg Loss: 3.4758, Avg Acc: 0.3958
2022-01-17 09:50:28,303 Epoch[140/300], Step[0950/1252], Avg Loss: 3.4780, Avg Acc: 0.3955
2022-01-17 09:51:48,138 Epoch[140/300], Step[1000/1252], Avg Loss: 3.4791, Avg Acc: 0.3955
2022-01-17 09:53:08,415 Epoch[140/300], Step[1050/1252], Avg Loss: 3.4736, Avg Acc: 0.3949
2022-01-17 09:54:29,003 Epoch[140/300], Step[1100/1252], Avg Loss: 3.4719, Avg Acc: 0.3956
2022-01-17 09:55:48,267 Epoch[140/300], Step[1150/1252], Avg Loss: 3.4695, Avg Acc: 0.3966
2022-01-17 09:57:08,522 Epoch[140/300], Step[1200/1252], Avg Loss: 3.4699, Avg Acc: 0.3962
2022-01-17 09:58:27,074 Epoch[140/300], Step[1250/1252], Avg Loss: 3.4679, Avg Acc: 0.3959
2022-01-17 09:58:33,486 ----- Epoch[140/300], Train Loss: 3.4679, Train Acc: 0.3959, time: 2093.43, Best Val(epoch138) Acc@1: 0.7168
2022-01-17 09:58:33,487 ----- Validation after Epoch: 140
2022-01-17 09:59:56,754 Val Step[0000/1563], Avg Loss: 1.0788, Avg Acc@1: 0.7812, Avg Acc@5: 0.9688
2022-01-17 09:59:58,435 Val Step[0050/1563], Avg Loss: 1.2552, Avg Acc@1: 0.7200, Avg Acc@5: 0.9056
2022-01-17 10:00:00,136 Val Step[0100/1563], Avg Loss: 1.2659, Avg Acc@1: 0.7181, Avg Acc@5: 0.9059
2022-01-17 10:00:01,824 Val Step[0150/1563], Avg Loss: 1.2727, Avg Acc@1: 0.7183, Avg Acc@5: 0.9054
2022-01-17 10:00:03,429 Val Step[0200/1563], Avg Loss: 1.2742, Avg Acc@1: 0.7180, Avg Acc@5: 0.9061
2022-01-17 10:00:05,219 Val Step[0250/1563], Avg Loss: 1.2621, Avg Acc@1: 0.7199, Avg Acc@5: 0.9069
2022-01-17 10:00:06,819 Val Step[0300/1563], Avg Loss: 1.2620, Avg Acc@1: 0.7211, Avg Acc@5: 0.9062
2022-01-17 10:00:08,402 Val Step[0350/1563], Avg Loss: 1.2665, Avg Acc@1: 0.7212, Avg Acc@5: 0.9075
2022-01-17 10:00:10,028 Val Step[0400/1563], Avg Loss: 1.2627, Avg Acc@1: 0.7213, Avg Acc@5: 0.9078
2022-01-17 10:00:11,636 Val Step[0450/1563], Avg Loss: 1.2665, Avg Acc@1: 0.7188, Avg Acc@5: 0.9073
2022-01-17 10:00:13,433 Val Step[0500/1563], Avg Loss: 1.2702, Avg Acc@1: 0.7181, Avg Acc@5: 0.9072
2022-01-17 10:00:15,002 Val Step[0550/1563], Avg Loss: 1.2727, Avg Acc@1: 0.7173, Avg Acc@5: 0.9074
2022-01-17 10:00:16,712 Val Step[0600/1563], Avg Loss: 1.2711, Avg Acc@1: 0.7168, Avg Acc@5: 0.9081
2022-01-17 10:00:18,531 Val Step[0650/1563], Avg Loss: 1.2701, Avg Acc@1: 0.7174, Avg Acc@5: 0.9087
2022-01-17 10:00:20,185 Val Step[0700/1563], Avg Loss: 1.2678, Avg Acc@1: 0.7184, Avg Acc@5: 0.9094
2022-01-17 10:00:21,848 Val Step[0750/1563], Avg Loss: 1.2738, Avg Acc@1: 0.7173, Avg Acc@5: 0.9089
2022-01-17 10:00:23,454 Val Step[0800/1563], Avg Loss: 1.2735, Avg Acc@1: 0.7182, Avg Acc@5: 0.9091
2022-01-17 10:00:25,202 Val Step[0850/1563], Avg Loss: 1.2752, Avg Acc@1: 0.7177, Avg Acc@5: 0.9090
2022-01-17 10:00:27,056 Val Step[0900/1563], Avg Loss: 1.2726, Avg Acc@1: 0.7178, Avg Acc@5: 0.9092
2022-01-17 10:00:28,715 Val Step[0950/1563], Avg Loss: 1.2719, Avg Acc@1: 0.7183, Avg Acc@5: 0.9095
2022-01-17 10:00:30,380 Val Step[1000/1563], Avg Loss: 1.2729, Avg Acc@1: 0.7184, Avg Acc@5: 0.9092
2022-01-17 10:00:32,074 Val Step[1050/1563], Avg Loss: 1.2733, Avg Acc@1: 0.7181, Avg Acc@5: 0.9090
2022-01-17 10:00:33,886 Val Step[1100/1563], Avg Loss: 1.2740, Avg Acc@1: 0.7179, Avg Acc@5: 0.9089
2022-01-17 10:00:35,492 Val Step[1150/1563], Avg Loss: 1.2738, Avg Acc@1: 0.7179, Avg Acc@5: 0.9087
2022-01-17 10:00:37,142 Val Step[1200/1563], Avg Loss: 1.2732, Avg Acc@1: 0.7182, Avg Acc@5: 0.9089
2022-01-17 10:00:38,928 Val Step[1250/1563], Avg Loss: 1.2725, Avg Acc@1: 0.7185, Avg Acc@5: 0.9092
2022-01-17 10:00:40,624 Val Step[1300/1563], Avg Loss: 1.2748, Avg Acc@1: 0.7185, Avg Acc@5: 0.9087
2022-01-17 10:00:42,168 Val Step[1350/1563], Avg Loss: 1.2756, Avg Acc@1: 0.7181, Avg Acc@5: 0.9084
2022-01-17 10:00:43,697 Val Step[1400/1563], Avg Loss: 1.2757, Avg Acc@1: 0.7174, Avg Acc@5: 0.9084
2022-01-17 10:00:45,341 Val Step[1450/1563], Avg Loss: 1.2755, Avg Acc@1: 0.7172, Avg Acc@5: 0.9082
2022-01-17 10:00:46,916 Val Step[1500/1563], Avg Loss: 1.2752, Avg Acc@1: 0.7173, Avg Acc@5: 0.9084
2022-01-17 10:00:48,402 Val Step[1550/1563], Avg Loss: 1.2764, Avg Acc@1: 0.7168, Avg Acc@5: 0.9084
2022-01-17 10:00:50,559 ----- Epoch[140/300], Validation Loss: 1.2763, Validation Acc@1: 0.7168, Validation Acc@5: 0.9085, time: 137.07
2022-01-17 10:00:51,805 the pre best model acc:0.7168, at epoch 138
2022-01-17 10:00:51,805 current best model acc:0.7168, at epoch 140
2022-01-17 10:00:51,805 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 10:00:51,805 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 10:00:51,805 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 10:00:51,805 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 10:00:52,121 ----- Save model: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-140-Loss-3.475283673603615.pdparams
2022-01-17 10:00:52,121 ----- Save optim: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-140-Loss-3.475283673603615.pdopt
2022-01-17 10:00:52,121 Now training epoch 141. LR=0.000610
2022-01-17 10:02:37,373 Epoch[141/300], Step[0000/1252], Avg Loss: 3.1001, Avg Acc: 0.5781
2022-01-17 10:03:56,675 Epoch[141/300], Step[0050/1252], Avg Loss: 3.4562, Avg Acc: 0.3662
2022-01-17 10:05:14,194 Epoch[141/300], Step[0100/1252], Avg Loss: 3.4376, Avg Acc: 0.3948
2022-01-17 10:06:33,325 Epoch[141/300], Step[0150/1252], Avg Loss: 3.4355, Avg Acc: 0.3958
2022-01-17 10:07:53,138 Epoch[141/300], Step[0200/1252], Avg Loss: 3.4383, Avg Acc: 0.3997
2022-01-17 10:09:12,740 Epoch[141/300], Step[0250/1252], Avg Loss: 3.4508, Avg Acc: 0.3975
2022-01-17 10:10:31,927 Epoch[141/300], Step[0300/1252], Avg Loss: 3.4510, Avg Acc: 0.3984
2022-01-17 10:11:52,144 Epoch[141/300], Step[0350/1252], Avg Loss: 3.4612, Avg Acc: 0.3971
2022-01-17 10:13:11,672 Epoch[141/300], Step[0400/1252], Avg Loss: 3.4625, Avg Acc: 0.3999
2022-01-17 10:14:31,340 Epoch[141/300], Step[0450/1252], Avg Loss: 3.4560, Avg Acc: 0.3988
2022-01-17 10:15:51,113 Epoch[141/300], Step[0500/1252], Avg Loss: 3.4560, Avg Acc: 0.3986
2022-01-17 10:17:11,861 Epoch[141/300], Step[0550/1252], Avg Loss: 3.4571, Avg Acc: 0.3982
2022-01-17 10:18:31,913 Epoch[141/300], Step[0600/1252], Avg Loss: 3.4584, Avg Acc: 0.3994
2022-01-17 10:19:51,640 Epoch[141/300], Step[0650/1252], Avg Loss: 3.4620, Avg Acc: 0.4007
2022-01-17 10:21:11,342 Epoch[141/300], Step[0700/1252], Avg Loss: 3.4562, Avg Acc: 0.4015
2022-01-17 10:22:30,274 Epoch[141/300], Step[0750/1252], Avg Loss: 3.4551, Avg Acc: 0.4037
2022-01-17 10:23:49,214 Epoch[141/300], Step[0800/1252], Avg Loss: 3.4533, Avg Acc: 0.4043
2022-01-17 10:25:09,008 Epoch[141/300], Step[0850/1252], Avg Loss: 3.4561, Avg Acc: 0.4040
2022-01-17 10:26:29,008 Epoch[141/300], Step[0900/1252], Avg Loss: 3.4590, Avg Acc: 0.4035
2022-01-17 10:27:49,117 Epoch[141/300], Step[0950/1252], Avg Loss: 3.4606, Avg Acc: 0.4028
2022-01-17 10:29:09,144 Epoch[141/300], Step[1000/1252], Avg Loss: 3.4618, Avg Acc: 0.4031
2022-01-17 10:30:29,206 Epoch[141/300], Step[1050/1252], Avg Loss: 3.4619, Avg Acc: 0.4034
2022-01-17 10:31:50,015 Epoch[141/300], Step[1100/1252], Avg Loss: 3.4633, Avg Acc: 0.4042
2022-01-17 10:33:10,540 Epoch[141/300], Step[1150/1252], Avg Loss: 3.4641, Avg Acc: 0.4041
2022-01-17 10:34:30,004 Epoch[141/300], Step[1200/1252], Avg Loss: 3.4640, Avg Acc: 0.4058
2022-01-17 10:35:48,416 Epoch[141/300], Step[1250/1252], Avg Loss: 3.4637, Avg Acc: 0.4054
2022-01-17 10:35:54,975 ----- Epoch[141/300], Train Loss: 3.4638, Train Acc: 0.4054, time: 2102.85, Best Val(epoch140) Acc@1: 0.7168
2022-01-17 10:35:54,976 Now training epoch 142. LR=0.000604
2022-01-17 10:37:39,691 Epoch[142/300], Step[0000/1252], Avg Loss: 3.1483, Avg Acc: 0.5996
2022-01-17 10:38:59,480 Epoch[142/300], Step[0050/1252], Avg Loss: 3.4813, Avg Acc: 0.4115
2022-01-17 10:40:19,520 Epoch[142/300], Step[0100/1252], Avg Loss: 3.4902, Avg Acc: 0.3997
2022-01-17 10:41:38,716 Epoch[142/300], Step[0150/1252], Avg Loss: 3.4710, Avg Acc: 0.3948
2022-01-17 10:42:58,175 Epoch[142/300], Step[0200/1252], Avg Loss: 3.4607, Avg Acc: 0.3979
2022-01-17 10:44:18,181 Epoch[142/300], Step[0250/1252], Avg Loss: 3.4613, Avg Acc: 0.4006
2022-01-17 10:45:37,779 Epoch[142/300], Step[0300/1252], Avg Loss: 3.4551, Avg Acc: 0.4034
2022-01-17 10:46:57,817 Epoch[142/300], Step[0350/1252], Avg Loss: 3.4481, Avg Acc: 0.4045
2022-01-17 10:48:16,834 Epoch[142/300], Step[0400/1252], Avg Loss: 3.4496, Avg Acc: 0.4045
2022-01-17 10:49:37,097 Epoch[142/300], Step[0450/1252], Avg Loss: 3.4507, Avg Acc: 0.4010
2022-01-17 10:50:55,543 Epoch[142/300], Step[0500/1252], Avg Loss: 3.4564, Avg Acc: 0.4022
2022-01-17 10:52:15,380 Epoch[142/300], Step[0550/1252], Avg Loss: 3.4544, Avg Acc: 0.4036
2022-01-17 10:53:35,641 Epoch[142/300], Step[0600/1252], Avg Loss: 3.4470, Avg Acc: 0.4025
2022-01-17 10:54:54,894 Epoch[142/300], Step[0650/1252], Avg Loss: 3.4516, Avg Acc: 0.4018
2022-01-17 10:56:14,856 Epoch[142/300], Step[0700/1252], Avg Loss: 3.4537, Avg Acc: 0.4002
2022-01-17 10:57:35,086 Epoch[142/300], Step[0750/1252], Avg Loss: 3.4517, Avg Acc: 0.4027
2022-01-17 10:58:55,155 Epoch[142/300], Step[0800/1252], Avg Loss: 3.4568, Avg Acc: 0.4024
2022-01-17 11:00:15,297 Epoch[142/300], Step[0850/1252], Avg Loss: 3.4549, Avg Acc: 0.4023
2022-01-17 11:01:35,321 Epoch[142/300], Step[0900/1252], Avg Loss: 3.4602, Avg Acc: 0.4016
2022-01-17 11:02:55,862 Epoch[142/300], Step[0950/1252], Avg Loss: 3.4606, Avg Acc: 0.4013
2022-01-17 11:04:16,743 Epoch[142/300], Step[1000/1252], Avg Loss: 3.4596, Avg Acc: 0.4005
2022-01-17 11:05:37,823 Epoch[142/300], Step[1050/1252], Avg Loss: 3.4608, Avg Acc: 0.3994
2022-01-17 11:06:58,835 Epoch[142/300], Step[1100/1252], Avg Loss: 3.4652, Avg Acc: 0.3994
2022-01-17 11:08:19,638 Epoch[142/300], Step[1150/1252], Avg Loss: 3.4635, Avg Acc: 0.3994
2022-01-17 11:09:40,749 Epoch[142/300], Step[1200/1252], Avg Loss: 3.4637, Avg Acc: 0.3986
2022-01-17 11:10:59,606 Epoch[142/300], Step[1250/1252], Avg Loss: 3.4646, Avg Acc: 0.3994
2022-01-17 11:11:05,908 ----- Epoch[142/300], Train Loss: 3.4645, Train Acc: 0.3994, time: 2110.93, Best Val(epoch140) Acc@1: 0.7168
2022-01-17 11:11:05,908 ----- Validation after Epoch: 142
2022-01-17 11:12:23,492 Val Step[0000/1563], Avg Loss: 1.1137, Avg Acc@1: 0.6875, Avg Acc@5: 0.9062
2022-01-17 11:12:25,064 Val Step[0050/1563], Avg Loss: 1.2751, Avg Acc@1: 0.7175, Avg Acc@5: 0.9075
2022-01-17 11:12:26,700 Val Step[0100/1563], Avg Loss: 1.2826, Avg Acc@1: 0.7200, Avg Acc@5: 0.9032
2022-01-17 11:12:28,301 Val Step[0150/1563], Avg Loss: 1.2688, Avg Acc@1: 0.7204, Avg Acc@5: 0.9044
2022-01-17 11:12:30,004 Val Step[0200/1563], Avg Loss: 1.2647, Avg Acc@1: 0.7231, Avg Acc@5: 0.9047
2022-01-17 11:12:31,651 Val Step[0250/1563], Avg Loss: 1.2563, Avg Acc@1: 0.7242, Avg Acc@5: 0.9062
2022-01-17 11:12:33,286 Val Step[0300/1563], Avg Loss: 1.2561, Avg Acc@1: 0.7241, Avg Acc@5: 0.9061
2022-01-17 11:12:34,904 Val Step[0350/1563], Avg Loss: 1.2623, Avg Acc@1: 0.7218, Avg Acc@5: 0.9061
2022-01-17 11:12:36,588 Val Step[0400/1563], Avg Loss: 1.2619, Avg Acc@1: 0.7205, Avg Acc@5: 0.9061
2022-01-17 11:12:38,168 Val Step[0450/1563], Avg Loss: 1.2629, Avg Acc@1: 0.7187, Avg Acc@5: 0.9061
2022-01-17 11:12:39,741 Val Step[0500/1563], Avg Loss: 1.2644, Avg Acc@1: 0.7185, Avg Acc@5: 0.9061
2022-01-17 11:12:41,342 Val Step[0550/1563], Avg Loss: 1.2653, Avg Acc@1: 0.7176, Avg Acc@5: 0.9059
2022-01-17 11:12:42,932 Val Step[0600/1563], Avg Loss: 1.2670, Avg Acc@1: 0.7172, Avg Acc@5: 0.9058
2022-01-17 11:12:44,527 Val Step[0650/1563], Avg Loss: 1.2674, Avg Acc@1: 0.7172, Avg Acc@5: 0.9063
2022-01-17 11:12:46,122 Val Step[0700/1563], Avg Loss: 1.2631, Avg Acc@1: 0.7184, Avg Acc@5: 0.9078
2022-01-17 11:12:47,910 Val Step[0750/1563], Avg Loss: 1.2677, Avg Acc@1: 0.7173, Avg Acc@5: 0.9076
2022-01-17 11:12:49,575 Val Step[0800/1563], Avg Loss: 1.2668, Avg Acc@1: 0.7181, Avg Acc@5: 0.9077
2022-01-17 11:12:51,360 Val Step[0850/1563], Avg Loss: 1.2681, Avg Acc@1: 0.7175, Avg Acc@5: 0.9074
2022-01-17 11:12:53,116 Val Step[0900/1563], Avg Loss: 1.2651, Avg Acc@1: 0.7179, Avg Acc@5: 0.9075
2022-01-17 11:12:54,782 Val Step[0950/1563], Avg Loss: 1.2643, Avg Acc@1: 0.7186, Avg Acc@5: 0.9078
2022-01-17 11:12:56,423 Val Step[1000/1563], Avg Loss: 1.2637, Avg Acc@1: 0.7189, Avg Acc@5: 0.9077
2022-01-17 11:12:58,005 Val Step[1050/1563], Avg Loss: 1.2651, Avg Acc@1: 0.7186, Avg Acc@5: 0.9074
2022-01-17 11:12:59,681 Val Step[1100/1563], Avg Loss: 1.2645, Avg Acc@1: 0.7186, Avg Acc@5: 0.9078
2022-01-17 11:13:01,296 Val Step[1150/1563], Avg Loss: 1.2642, Avg Acc@1: 0.7184, Avg Acc@5: 0.9080
2022-01-17 11:13:02,924 Val Step[1200/1563], Avg Loss: 1.2639, Avg Acc@1: 0.7189, Avg Acc@5: 0.9082
2022-01-17 11:13:04,481 Val Step[1250/1563], Avg Loss: 1.2646, Avg Acc@1: 0.7180, Avg Acc@5: 0.9083
2022-01-17 11:13:06,175 Val Step[1300/1563], Avg Loss: 1.2683, Avg Acc@1: 0.7178, Avg Acc@5: 0.9079
2022-01-17 11:13:07,840 Val Step[1350/1563], Avg Loss: 1.2680, Avg Acc@1: 0.7178, Avg Acc@5: 0.9079
2022-01-17 11:13:09,457 Val Step[1400/1563], Avg Loss: 1.2679, Avg Acc@1: 0.7177, Avg Acc@5: 0.9079
2022-01-17 11:13:11,286 Val Step[1450/1563], Avg Loss: 1.2681, Avg Acc@1: 0.7177, Avg Acc@5: 0.9080
2022-01-17 11:13:12,874 Val Step[1500/1563], Avg Loss: 1.2677, Avg Acc@1: 0.7179, Avg Acc@5: 0.9083
2022-01-17 11:13:14,380 Val Step[1550/1563], Avg Loss: 1.2684, Avg Acc@1: 0.7176, Avg Acc@5: 0.9081
2022-01-17 11:13:16,556 ----- Epoch[142/300], Validation Loss: 1.2683, Validation Acc@1: 0.7177, Validation Acc@5: 0.9082, time: 130.65
2022-01-17 11:13:17,623 the pre best model acc:0.7168, at epoch 140
2022-01-17 11:13:17,918 current best model acc:0.7177, at epoch 142
2022-01-17 11:13:17,918 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 11:13:17,918 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 11:13:17,918 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 11:13:17,918 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 11:13:17,918 Now training epoch 143. LR=0.000599
2022-01-17 11:14:58,976 Epoch[143/300], Step[0000/1252], Avg Loss: 3.7649, Avg Acc: 0.3779
2022-01-17 11:16:19,563 Epoch[143/300], Step[0050/1252], Avg Loss: 3.4939, Avg Acc: 0.3951
2022-01-17 11:17:39,455 Epoch[143/300], Step[0100/1252], Avg Loss: 3.4625, Avg Acc: 0.3901
2022-01-17 11:18:59,062 Epoch[143/300], Step[0150/1252], Avg Loss: 3.4766, Avg Acc: 0.3926
2022-01-17 11:20:18,304 Epoch[143/300], Step[0200/1252], Avg Loss: 3.4805, Avg Acc: 0.3968
2022-01-17 11:21:38,330 Epoch[143/300], Step[0250/1252], Avg Loss: 3.4784, Avg Acc: 0.3964
2022-01-17 11:22:58,299 Epoch[143/300], Step[0300/1252], Avg Loss: 3.4725, Avg Acc: 0.3990
2022-01-17 11:24:18,232 Epoch[143/300], Step[0350/1252], Avg Loss: 3.4683, Avg Acc: 0.4052
2022-01-17 11:25:38,498 Epoch[143/300], Step[0400/1252], Avg Loss: 3.4633, Avg Acc: 0.4060
2022-01-17 11:26:58,702 Epoch[143/300], Step[0450/1252], Avg Loss: 3.4602, Avg Acc: 0.4053
2022-01-17 11:28:19,278 Epoch[143/300], Step[0500/1252], Avg Loss: 3.4609, Avg Acc: 0.4065
2022-01-17 11:29:38,960 Epoch[143/300], Step[0550/1252], Avg Loss: 3.4567, Avg Acc: 0.4061
2022-01-17 11:30:57,678 Epoch[143/300], Step[0600/1252], Avg Loss: 3.4613, Avg Acc: 0.4057
2022-01-17 11:32:18,000 Epoch[143/300], Step[0650/1252], Avg Loss: 3.4626, Avg Acc: 0.4054
2022-01-17 11:33:37,622 Epoch[143/300], Step[0700/1252], Avg Loss: 3.4610, Avg Acc: 0.4054
2022-01-17 11:34:57,734 Epoch[143/300], Step[0750/1252], Avg Loss: 3.4614, Avg Acc: 0.4029
2022-01-17 11:36:17,949 Epoch[143/300], Step[0800/1252], Avg Loss: 3.4617, Avg Acc: 0.4017
2022-01-17 11:37:37,919 Epoch[143/300], Step[0850/1252], Avg Loss: 3.4600, Avg Acc: 0.4016
2022-01-17 11:38:58,778 Epoch[143/300], Step[0900/1252], Avg Loss: 3.4640, Avg Acc: 0.4008
2022-01-17 11:40:18,910 Epoch[143/300], Step[0950/1252], Avg Loss: 3.4657, Avg Acc: 0.3998
2022-01-17 11:41:39,162 Epoch[143/300], Step[1000/1252], Avg Loss: 3.4651, Avg Acc: 0.4002
2022-01-17 11:42:59,489 Epoch[143/300], Step[1050/1252], Avg Loss: 3.4674, Avg Acc: 0.3997
2022-01-17 11:44:20,146 Epoch[143/300], Step[1100/1252], Avg Loss: 3.4649, Avg Acc: 0.3999
2022-01-17 11:45:41,183 Epoch[143/300], Step[1150/1252], Avg Loss: 3.4609, Avg Acc: 0.4008
2022-01-17 11:47:01,405 Epoch[143/300], Step[1200/1252], Avg Loss: 3.4627, Avg Acc: 0.4009
2022-01-17 11:48:20,037 Epoch[143/300], Step[1250/1252], Avg Loss: 3.4612, Avg Acc: 0.4020
2022-01-17 11:48:27,177 ----- Epoch[143/300], Train Loss: 3.4612, Train Acc: 0.4021, time: 2109.25, Best Val(epoch142) Acc@1: 0.7177
2022-01-17 11:48:27,177 Now training epoch 144. LR=0.000593
2022-01-17 11:50:05,635 Epoch[144/300], Step[0000/1252], Avg Loss: 3.3757, Avg Acc: 0.4111
2022-01-17 11:51:24,622 Epoch[144/300], Step[0050/1252], Avg Loss: 3.4507, Avg Acc: 0.4188
2022-01-17 11:52:44,411 Epoch[144/300], Step[0100/1252], Avg Loss: 3.4817, Avg Acc: 0.3997
2022-01-17 11:54:05,306 Epoch[144/300], Step[0150/1252], Avg Loss: 3.4725, Avg Acc: 0.4025
2022-01-17 11:55:25,944 Epoch[144/300], Step[0200/1252], Avg Loss: 3.4565, Avg Acc: 0.4065
2022-01-17 11:56:47,102 Epoch[144/300], Step[0250/1252], Avg Loss: 3.4522, Avg Acc: 0.4027
2022-01-17 11:58:06,202 Epoch[144/300], Step[0300/1252], Avg Loss: 3.4564, Avg Acc: 0.4000
2022-01-17 11:59:26,357 Epoch[144/300], Step[0350/1252], Avg Loss: 3.4556, Avg Acc: 0.4014
2022-01-17 12:00:46,615 Epoch[144/300], Step[0400/1252], Avg Loss: 3.4594, Avg Acc: 0.4018
2022-01-17 12:02:07,447 Epoch[144/300], Step[0450/1252], Avg Loss: 3.4557, Avg Acc: 0.3993
2022-01-17 12:03:27,613 Epoch[144/300], Step[0500/1252], Avg Loss: 3.4530, Avg Acc: 0.4017
2022-01-17 12:04:48,820 Epoch[144/300], Step[0550/1252], Avg Loss: 3.4593, Avg Acc: 0.4012
2022-01-17 12:06:09,525 Epoch[144/300], Step[0600/1252], Avg Loss: 3.4591, Avg Acc: 0.4017
2022-01-17 12:07:30,468 Epoch[144/300], Step[0650/1252], Avg Loss: 3.4552, Avg Acc: 0.4026
2022-01-17 12:08:50,966 Epoch[144/300], Step[0700/1252], Avg Loss: 3.4476, Avg Acc: 0.4027
2022-01-17 12:10:11,147 Epoch[144/300], Step[0750/1252], Avg Loss: 3.4468, Avg Acc: 0.4026
2022-01-17 12:11:31,058 Epoch[144/300], Step[0800/1252], Avg Loss: 3.4459, Avg Acc: 0.4009
2022-01-17 12:12:52,425 Epoch[144/300], Step[0850/1252], Avg Loss: 3.4471, Avg Acc: 0.4007
2022-01-17 12:14:14,031 Epoch[144/300], Step[0900/1252], Avg Loss: 3.4444, Avg Acc: 0.4000
2022-01-17 12:15:35,323 Epoch[144/300], Step[0950/1252], Avg Loss: 3.4447, Avg Acc: 0.3971
2022-01-17 12:16:56,126 Epoch[144/300], Step[1000/1252], Avg Loss: 3.4469, Avg Acc: 0.3961
2022-01-17 12:18:16,418 Epoch[144/300], Step[1050/1252], Avg Loss: 3.4503, Avg Acc: 0.3958
2022-01-17 12:19:37,319 Epoch[144/300], Step[1100/1252], Avg Loss: 3.4524, Avg Acc: 0.3955
2022-01-17 12:20:58,252 Epoch[144/300], Step[1150/1252], Avg Loss: 3.4538, Avg Acc: 0.3957
2022-01-17 12:22:19,018 Epoch[144/300], Step[1200/1252], Avg Loss: 3.4526, Avg Acc: 0.3959
2022-01-17 12:23:37,366 Epoch[144/300], Step[1250/1252], Avg Loss: 3.4533, Avg Acc: 0.3963
2022-01-17 12:23:43,554 ----- Epoch[144/300], Train Loss: 3.4532, Train Acc: 0.3963, time: 2116.37, Best Val(epoch142) Acc@1: 0.7177
2022-01-17 12:23:43,554 ----- Validation after Epoch: 144
2022-01-17 12:25:00,182 Val Step[0000/1563], Avg Loss: 1.0491, Avg Acc@1: 0.6875, Avg Acc@5: 0.9688
2022-01-17 12:25:02,337 Val Step[0050/1563], Avg Loss: 1.2622, Avg Acc@1: 0.7243, Avg Acc@5: 0.9093
2022-01-17 12:25:03,927 Val Step[0100/1563], Avg Loss: 1.2642, Avg Acc@1: 0.7206, Avg Acc@5: 0.9103
2022-01-17 12:25:05,556 Val Step[0150/1563], Avg Loss: 1.2691, Avg Acc@1: 0.7216, Avg Acc@5: 0.9094
2022-01-17 12:25:07,283 Val Step[0200/1563], Avg Loss: 1.2655, Avg Acc@1: 0.7243, Avg Acc@5: 0.9101
2022-01-17 12:25:08,951 Val Step[0250/1563], Avg Loss: 1.2592, Avg Acc@1: 0.7249, Avg Acc@5: 0.9097
2022-01-17 12:25:10,526 Val Step[0300/1563], Avg Loss: 1.2571, Avg Acc@1: 0.7262, Avg Acc@5: 0.9085
2022-01-17 12:25:12,104 Val Step[0350/1563], Avg Loss: 1.2640, Avg Acc@1: 0.7240, Avg Acc@5: 0.9076
2022-01-17 12:25:13,718 Val Step[0400/1563], Avg Loss: 1.2653, Avg Acc@1: 0.7232, Avg Acc@5: 0.9077
2022-01-17 12:25:15,446 Val Step[0450/1563], Avg Loss: 1.2683, Avg Acc@1: 0.7213, Avg Acc@5: 0.9077
2022-01-17 12:25:17,014 Val Step[0500/1563], Avg Loss: 1.2706, Avg Acc@1: 0.7212, Avg Acc@5: 0.9079
2022-01-17 12:25:18,769 Val Step[0550/1563], Avg Loss: 1.2723, Avg Acc@1: 0.7195, Avg Acc@5: 0.9081
2022-01-17 12:25:20,474 Val Step[0600/1563], Avg Loss: 1.2714, Avg Acc@1: 0.7192, Avg Acc@5: 0.9087
2022-01-17 12:25:22,254 Val Step[0650/1563], Avg Loss: 1.2728, Avg Acc@1: 0.7195, Avg Acc@5: 0.9087
2022-01-17 12:25:23,806 Val Step[0700/1563], Avg Loss: 1.2704, Avg Acc@1: 0.7199, Avg Acc@5: 0.9095
2022-01-17 12:25:25,458 Val Step[0750/1563], Avg Loss: 1.2753, Avg Acc@1: 0.7189, Avg Acc@5: 0.9089
2022-01-17 12:25:27,070 Val Step[0800/1563], Avg Loss: 1.2756, Avg Acc@1: 0.7192, Avg Acc@5: 0.9087
2022-01-17 12:25:28,687 Val Step[0850/1563], Avg Loss: 1.2777, Avg Acc@1: 0.7190, Avg Acc@5: 0.9081
2022-01-17 12:25:30,457 Val Step[0900/1563], Avg Loss: 1.2744, Avg Acc@1: 0.7195, Avg Acc@5: 0.9086
2022-01-17 12:25:32,070 Val Step[0950/1563], Avg Loss: 1.2733, Avg Acc@1: 0.7198, Avg Acc@5: 0.9090
2022-01-17 12:25:33,699 Val Step[1000/1563], Avg Loss: 1.2738, Avg Acc@1: 0.7199, Avg Acc@5: 0.9090
2022-01-17 12:25:35,407 Val Step[1050/1563], Avg Loss: 1.2749, Avg Acc@1: 0.7190, Avg Acc@5: 0.9088
2022-01-17 12:25:37,042 Val Step[1100/1563], Avg Loss: 1.2745, Avg Acc@1: 0.7190, Avg Acc@5: 0.9087
2022-01-17 12:25:38,577 Val Step[1150/1563], Avg Loss: 1.2730, Avg Acc@1: 0.7189, Avg Acc@5: 0.9091
2022-01-17 12:25:40,155 Val Step[1200/1563], Avg Loss: 1.2716, Avg Acc@1: 0.7196, Avg Acc@5: 0.9092
2022-01-17 12:25:41,848 Val Step[1250/1563], Avg Loss: 1.2720, Avg Acc@1: 0.7195, Avg Acc@5: 0.9092
2022-01-17 12:25:43,479 Val Step[1300/1563], Avg Loss: 1.2746, Avg Acc@1: 0.7194, Avg Acc@5: 0.9092
2022-01-17 12:25:45,168 Val Step[1350/1563], Avg Loss: 1.2754, Avg Acc@1: 0.7188, Avg Acc@5: 0.9090
2022-01-17 12:25:46,878 Val Step[1400/1563], Avg Loss: 1.2755, Avg Acc@1: 0.7185, Avg Acc@5: 0.9090
2022-01-17 12:25:48,577 Val Step[1450/1563], Avg Loss: 1.2759, Avg Acc@1: 0.7182, Avg Acc@5: 0.9087
2022-01-17 12:25:50,180 Val Step[1500/1563], Avg Loss: 1.2754, Avg Acc@1: 0.7184, Avg Acc@5: 0.9091
2022-01-17 12:25:51,742 Val Step[1550/1563], Avg Loss: 1.2759, Avg Acc@1: 0.7180, Avg Acc@5: 0.9090
2022-01-17 12:25:53,872 ----- Epoch[144/300], Validation Loss: 1.2756, Validation Acc@1: 0.7179, Validation Acc@5: 0.9092, time: 130.32
2022-01-17 12:25:54,964 the pre best model acc:0.7177, at epoch 142
2022-01-17 12:25:54,964 current best model acc:0.7179, at epoch 144
2022-01-17 12:25:54,964 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 12:25:54,964 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 12:25:54,964 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 12:25:54,964 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 12:25:54,964 Now training epoch 145. LR=0.000588
2022-01-17 12:27:38,410 Epoch[145/300], Step[0000/1252], Avg Loss: 3.5440, Avg Acc: 0.4170
2022-01-17 12:28:58,582 Epoch[145/300], Step[0050/1252], Avg Loss: 3.3539, Avg Acc: 0.4170
2022-01-17 12:30:18,891 Epoch[145/300], Step[0100/1252], Avg Loss: 3.3956, Avg Acc: 0.4055
2022-01-17 12:31:38,119 Epoch[145/300], Step[0150/1252], Avg Loss: 3.4018, Avg Acc: 0.4116
2022-01-17 12:32:57,410 Epoch[145/300], Step[0200/1252], Avg Loss: 3.4075, Avg Acc: 0.4031
2022-01-17 12:34:17,290 Epoch[145/300], Step[0250/1252], Avg Loss: 3.4222, Avg Acc: 0.4031
2022-01-17 12:35:38,020 Epoch[145/300], Step[0300/1252], Avg Loss: 3.4296, Avg Acc: 0.4036
2022-01-17 12:36:58,395 Epoch[145/300], Step[0350/1252], Avg Loss: 3.4300, Avg Acc: 0.4059
2022-01-17 12:38:19,212 Epoch[145/300], Step[0400/1252], Avg Loss: 3.4441, Avg Acc: 0.4038
2022-01-17 12:39:39,104 Epoch[145/300], Step[0450/1252], Avg Loss: 3.4359, Avg Acc: 0.4059
2022-01-17 12:40:59,061 Epoch[145/300], Step[0500/1252], Avg Loss: 3.4431, Avg Acc: 0.4019
2022-01-17 12:42:18,747 Epoch[145/300], Step[0550/1252], Avg Loss: 3.4373, Avg Acc: 0.4002
2022-01-17 12:43:37,915 Epoch[145/300], Step[0600/1252], Avg Loss: 3.4375, Avg Acc: 0.3997
2022-01-17 12:44:58,243 Epoch[145/300], Step[0650/1252], Avg Loss: 3.4401, Avg Acc: 0.3992
2022-01-17 12:46:17,580 Epoch[145/300], Step[0700/1252], Avg Loss: 3.4395, Avg Acc: 0.3999
2022-01-17 12:47:37,443 Epoch[145/300], Step[0750/1252], Avg Loss: 3.4416, Avg Acc: 0.4005
2022-01-17 12:48:57,330 Epoch[145/300], Step[0800/1252], Avg Loss: 3.4424, Avg Acc: 0.4012
2022-01-17 12:50:18,019 Epoch[145/300], Step[0850/1252], Avg Loss: 3.4433, Avg Acc: 0.4010
2022-01-17 12:51:38,853 Epoch[145/300], Step[0900/1252], Avg Loss: 3.4429, Avg Acc: 0.4017
2022-01-17 12:52:59,699 Epoch[145/300], Step[0950/1252], Avg Loss: 3.4415, Avg Acc: 0.4020
2022-01-17 12:54:20,066 Epoch[145/300], Step[1000/1252], Avg Loss: 3.4426, Avg Acc: 0.4016
2022-01-17 12:55:40,776 Epoch[145/300], Step[1050/1252], Avg Loss: 3.4417, Avg Acc: 0.4014
2022-01-17 12:57:00,660 Epoch[145/300], Step[1100/1252], Avg Loss: 3.4447, Avg Acc: 0.4017
2022-01-17 12:58:20,214 Epoch[145/300], Step[1150/1252], Avg Loss: 3.4444, Avg Acc: 0.4019
2022-01-17 12:59:41,311 Epoch[145/300], Step[1200/1252], Avg Loss: 3.4421, Avg Acc: 0.4014
2022-01-17 13:01:00,469 Epoch[145/300], Step[1250/1252], Avg Loss: 3.4431, Avg Acc: 0.4007
2022-01-17 13:01:07,016 ----- Epoch[145/300], Train Loss: 3.4431, Train Acc: 0.4007, time: 2112.05, Best Val(epoch144) Acc@1: 0.7179
2022-01-17 13:01:07,016 Now training epoch 146. LR=0.000582
2022-01-17 13:02:50,724 Epoch[146/300], Step[0000/1252], Avg Loss: 3.7488, Avg Acc: 0.3535
2022-01-17 13:04:10,101 Epoch[146/300], Step[0050/1252], Avg Loss: 3.5124, Avg Acc: 0.4074
2022-01-17 13:05:30,196 Epoch[146/300], Step[0100/1252], Avg Loss: 3.4987, Avg Acc: 0.3984
2022-01-17 13:06:50,523 Epoch[146/300], Step[0150/1252], Avg Loss: 3.4713, Avg Acc: 0.3965
2022-01-17 13:08:11,249 Epoch[146/300], Step[0200/1252], Avg Loss: 3.4748, Avg Acc: 0.4015
2022-01-17 13:09:31,867 Epoch[146/300], Step[0250/1252], Avg Loss: 3.4671, Avg Acc: 0.4065
2022-01-17 13:10:52,058 Epoch[146/300], Step[0300/1252], Avg Loss: 3.4665, Avg Acc: 0.4070
2022-01-17 13:12:12,047 Epoch[146/300], Step[0350/1252], Avg Loss: 3.4654, Avg Acc: 0.4076
2022-01-17 13:13:33,036 Epoch[146/300], Step[0400/1252], Avg Loss: 3.4623, Avg Acc: 0.4042
2022-01-17 13:14:54,242 Epoch[146/300], Step[0450/1252], Avg Loss: 3.4591, Avg Acc: 0.4017
2022-01-17 13:16:15,093 Epoch[146/300], Step[0500/1252], Avg Loss: 3.4577, Avg Acc: 0.4008
2022-01-17 13:17:35,371 Epoch[146/300], Step[0550/1252], Avg Loss: 3.4558, Avg Acc: 0.4012
2022-01-17 13:18:55,905 Epoch[146/300], Step[0600/1252], Avg Loss: 3.4628, Avg Acc: 0.4000
2022-01-17 13:20:15,855 Epoch[146/300], Step[0650/1252], Avg Loss: 3.4608, Avg Acc: 0.4008
2022-01-17 13:21:35,881 Epoch[146/300], Step[0700/1252], Avg Loss: 3.4540, Avg Acc: 0.4035
2022-01-17 13:22:56,100 Epoch[146/300], Step[0750/1252], Avg Loss: 3.4507, Avg Acc: 0.4045
2022-01-17 13:24:17,344 Epoch[146/300], Step[0800/1252], Avg Loss: 3.4533, Avg Acc: 0.4043
2022-01-17 13:25:37,850 Epoch[146/300], Step[0850/1252], Avg Loss: 3.4568, Avg Acc: 0.4035
2022-01-17 13:26:57,522 Epoch[146/300], Step[0900/1252], Avg Loss: 3.4571, Avg Acc: 0.4031
2022-01-17 13:28:18,940 Epoch[146/300], Step[0950/1252], Avg Loss: 3.4584, Avg Acc: 0.4027
2022-01-17 13:29:40,601 Epoch[146/300], Step[1000/1252], Avg Loss: 3.4571, Avg Acc: 0.4025
2022-01-17 13:31:01,209 Epoch[146/300], Step[1050/1252], Avg Loss: 3.4593, Avg Acc: 0.4020
2022-01-17 13:32:21,357 Epoch[146/300], Step[1100/1252], Avg Loss: 3.4585, Avg Acc: 0.4020
2022-01-17 13:33:41,597 Epoch[146/300], Step[1150/1252], Avg Loss: 3.4589, Avg Acc: 0.4017
2022-01-17 13:35:01,321 Epoch[146/300], Step[1200/1252], Avg Loss: 3.4552, Avg Acc: 0.4023
2022-01-17 13:36:20,829 Epoch[146/300], Step[1250/1252], Avg Loss: 3.4577, Avg Acc: 0.4011
2022-01-17 13:36:27,101 ----- Epoch[146/300], Train Loss: 3.4577, Train Acc: 0.4011, time: 2120.08, Best Val(epoch144) Acc@1: 0.7179
2022-01-17 13:36:27,101 ----- Validation after Epoch: 146
2022-01-17 13:37:52,577 Val Step[0000/1563], Avg Loss: 0.9378, Avg Acc@1: 0.7500, Avg Acc@5: 0.9688
2022-01-17 13:37:54,326 Val Step[0050/1563], Avg Loss: 1.2509, Avg Acc@1: 0.7206, Avg Acc@5: 0.9081
2022-01-17 13:37:56,046 Val Step[0100/1563], Avg Loss: 1.2602, Avg Acc@1: 0.7209, Avg Acc@5: 0.9131
2022-01-17 13:37:57,577 Val Step[0150/1563], Avg Loss: 1.2599, Avg Acc@1: 0.7208, Avg Acc@5: 0.9108
2022-01-17 13:37:59,226 Val Step[0200/1563], Avg Loss: 1.2527, Avg Acc@1: 0.7248, Avg Acc@5: 0.9117
2022-01-17 13:38:01,032 Val Step[0250/1563], Avg Loss: 1.2432, Avg Acc@1: 0.7273, Avg Acc@5: 0.9111
2022-01-17 13:38:02,650 Val Step[0300/1563], Avg Loss: 1.2413, Avg Acc@1: 0.7268, Avg Acc@5: 0.9105
2022-01-17 13:38:04,277 Val Step[0350/1563], Avg Loss: 1.2462, Avg Acc@1: 0.7269, Avg Acc@5: 0.9106
2022-01-17 13:38:05,921 Val Step[0400/1563], Avg Loss: 1.2477, Avg Acc@1: 0.7264, Avg Acc@5: 0.9095
2022-01-17 13:38:07,597 Val Step[0450/1563], Avg Loss: 1.2535, Avg Acc@1: 0.7244, Avg Acc@5: 0.9085
2022-01-17 13:38:09,217 Val Step[0500/1563], Avg Loss: 1.2556, Avg Acc@1: 0.7240, Avg Acc@5: 0.9082
2022-01-17 13:38:10,924 Val Step[0550/1563], Avg Loss: 1.2576, Avg Acc@1: 0.7229, Avg Acc@5: 0.9080
2022-01-17 13:38:12,509 Val Step[0600/1563], Avg Loss: 1.2600, Avg Acc@1: 0.7218, Avg Acc@5: 0.9084
2022-01-17 13:38:14,088 Val Step[0650/1563], Avg Loss: 1.2603, Avg Acc@1: 0.7217, Avg Acc@5: 0.9087
2022-01-17 13:38:15,938 Val Step[0700/1563], Avg Loss: 1.2577, Avg Acc@1: 0.7229, Avg Acc@5: 0.9088
2022-01-17 13:38:17,630 Val Step[0750/1563], Avg Loss: 1.2638, Avg Acc@1: 0.7222, Avg Acc@5: 0.9083
2022-01-17 13:38:19,193 Val Step[0800/1563], Avg Loss: 1.2636, Avg Acc@1: 0.7229, Avg Acc@5: 0.9077
2022-01-17 13:38:20,907 Val Step[0850/1563], Avg Loss: 1.2658, Avg Acc@1: 0.7219, Avg Acc@5: 0.9077
2022-01-17 13:38:22,513 Val Step[0900/1563], Avg Loss: 1.2631, Avg Acc@1: 0.7220, Avg Acc@5: 0.9081
2022-01-17 13:38:24,250 Val Step[0950/1563], Avg Loss: 1.2623, Avg Acc@1: 0.7220, Avg Acc@5: 0.9086
2022-01-17 13:38:25,810 Val Step[1000/1563], Avg Loss: 1.2627, Avg Acc@1: 0.7218, Avg Acc@5: 0.9083
2022-01-17 13:38:27,352 Val Step[1050/1563], Avg Loss: 1.2641, Avg Acc@1: 0.7211, Avg Acc@5: 0.9080
2022-01-17 13:38:29,004 Val Step[1100/1563], Avg Loss: 1.2649, Avg Acc@1: 0.7209, Avg Acc@5: 0.9077
2022-01-17 13:38:30,600 Val Step[1150/1563], Avg Loss: 1.2650, Avg Acc@1: 0.7209, Avg Acc@5: 0.9076
2022-01-17 13:38:32,260 Val Step[1200/1563], Avg Loss: 1.2640, Avg Acc@1: 0.7212, Avg Acc@5: 0.9077
2022-01-17 13:38:33,809 Val Step[1250/1563], Avg Loss: 1.2631, Avg Acc@1: 0.7215, Avg Acc@5: 0.9080
2022-01-17 13:38:35,357 Val Step[1300/1563], Avg Loss: 1.2661, Avg Acc@1: 0.7211, Avg Acc@5: 0.9080
2022-01-17 13:38:37,176 Val Step[1350/1563], Avg Loss: 1.2656, Avg Acc@1: 0.7212, Avg Acc@5: 0.9080
2022-01-17 13:38:38,798 Val Step[1400/1563], Avg Loss: 1.2651, Avg Acc@1: 0.7209, Avg Acc@5: 0.9081
2022-01-17 13:38:40,549 Val Step[1450/1563], Avg Loss: 1.2655, Avg Acc@1: 0.7208, Avg Acc@5: 0.9080
2022-01-17 13:38:42,166 Val Step[1500/1563], Avg Loss: 1.2650, Avg Acc@1: 0.7211, Avg Acc@5: 0.9083
2022-01-17 13:38:43,858 Val Step[1550/1563], Avg Loss: 1.2649, Avg Acc@1: 0.7209, Avg Acc@5: 0.9081
2022-01-17 13:38:46,005 ----- Epoch[146/300], Validation Loss: 1.2648, Validation Acc@1: 0.7210, Validation Acc@5: 0.9082, time: 138.90
2022-01-17 13:38:47,075 the pre best model acc:0.7179, at epoch 144
2022-01-17 13:38:47,374 current best model acc:0.7210, at epoch 146
2022-01-17 13:38:47,374 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 13:38:47,374 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 13:38:47,374 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 13:38:47,374 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 13:38:47,374 Now training epoch 147. LR=0.000577
2022-01-17 13:40:27,809 Epoch[147/300], Step[0000/1252], Avg Loss: 3.5925, Avg Acc: 0.3115
2022-01-17 13:41:48,101 Epoch[147/300], Step[0050/1252], Avg Loss: 3.4549, Avg Acc: 0.3994
2022-01-17 13:43:08,926 Epoch[147/300], Step[0100/1252], Avg Loss: 3.4114, Avg Acc: 0.3950
2022-01-17 13:44:28,806 Epoch[147/300], Step[0150/1252], Avg Loss: 3.4044, Avg Acc: 0.3971
2022-01-17 13:45:48,934 Epoch[147/300], Step[0200/1252], Avg Loss: 3.3999, Avg Acc: 0.3987
2022-01-17 13:47:09,009 Epoch[147/300], Step[0250/1252], Avg Loss: 3.4085, Avg Acc: 0.4024
2022-01-17 13:48:28,655 Epoch[147/300], Step[0300/1252], Avg Loss: 3.4194, Avg Acc: 0.4055
2022-01-17 13:49:47,364 Epoch[147/300], Step[0350/1252], Avg Loss: 3.4150, Avg Acc: 0.4088
2022-01-17 13:51:06,582 Epoch[147/300], Step[0400/1252], Avg Loss: 3.4112, Avg Acc: 0.4090
2022-01-17 13:52:26,310 Epoch[147/300], Step[0450/1252], Avg Loss: 3.4145, Avg Acc: 0.4057
2022-01-17 13:53:46,503 Epoch[147/300], Step[0500/1252], Avg Loss: 3.4202, Avg Acc: 0.4078
2022-01-17 13:55:06,518 Epoch[147/300], Step[0550/1252], Avg Loss: 3.4170, Avg Acc: 0.4094
2022-01-17 13:56:26,938 Epoch[147/300], Step[0600/1252], Avg Loss: 3.4178, Avg Acc: 0.4091
2022-01-17 13:57:48,247 Epoch[147/300], Step[0650/1252], Avg Loss: 3.4240, Avg Acc: 0.4076
2022-01-17 13:59:08,751 Epoch[147/300], Step[0700/1252], Avg Loss: 3.4239, Avg Acc: 0.4067
2022-01-17 14:00:28,664 Epoch[147/300], Step[0750/1252], Avg Loss: 3.4273, Avg Acc: 0.4076
2022-01-17 14:01:48,592 Epoch[147/300], Step[0800/1252], Avg Loss: 3.4341, Avg Acc: 0.4069
2022-01-17 14:03:09,204 Epoch[147/300], Step[0850/1252], Avg Loss: 3.4381, Avg Acc: 0.4069
2022-01-17 14:04:28,969 Epoch[147/300], Step[0900/1252], Avg Loss: 3.4348, Avg Acc: 0.4088
2022-01-17 14:05:49,087 Epoch[147/300], Step[0950/1252], Avg Loss: 3.4407, Avg Acc: 0.4087
2022-01-17 14:07:09,937 Epoch[147/300], Step[1000/1252], Avg Loss: 3.4405, Avg Acc: 0.4087
2022-01-17 14:08:30,320 Epoch[147/300], Step[1050/1252], Avg Loss: 3.4428, Avg Acc: 0.4096
2022-01-17 14:09:50,212 Epoch[147/300], Step[1100/1252], Avg Loss: 3.4418, Avg Acc: 0.4092
2022-01-17 14:11:10,124 Epoch[147/300], Step[1150/1252], Avg Loss: 3.4428, Avg Acc: 0.4093
2022-01-17 14:12:30,978 Epoch[147/300], Step[1200/1252], Avg Loss: 3.4427, Avg Acc: 0.4088
2022-01-17 14:13:50,331 Epoch[147/300], Step[1250/1252], Avg Loss: 3.4425, Avg Acc: 0.4082
2022-01-17 14:13:57,213 ----- Epoch[147/300], Train Loss: 3.4425, Train Acc: 0.4082, time: 2109.84, Best Val(epoch146) Acc@1: 0.7210
2022-01-17 14:13:57,213 Now training epoch 148. LR=0.000571
2022-01-17 14:15:33,161 Epoch[148/300], Step[0000/1252], Avg Loss: 3.6098, Avg Acc: 0.0869
2022-01-17 14:16:52,959 Epoch[148/300], Step[0050/1252], Avg Loss: 3.5003, Avg Acc: 0.3857
2022-01-17 14:18:12,152 Epoch[148/300], Step[0100/1252], Avg Loss: 3.4467, Avg Acc: 0.3965
2022-01-17 14:19:32,810 Epoch[148/300], Step[0150/1252], Avg Loss: 3.4408, Avg Acc: 0.3959
2022-01-17 14:20:51,564 Epoch[148/300], Step[0200/1252], Avg Loss: 3.4310, Avg Acc: 0.4019
2022-01-17 14:22:12,240 Epoch[148/300], Step[0250/1252], Avg Loss: 3.4500, Avg Acc: 0.4005
2022-01-17 14:23:32,955 Epoch[148/300], Step[0300/1252], Avg Loss: 3.4410, Avg Acc: 0.3989
2022-01-17 14:24:52,494 Epoch[148/300], Step[0350/1252], Avg Loss: 3.4360, Avg Acc: 0.4044
2022-01-17 14:26:11,884 Epoch[148/300], Step[0400/1252], Avg Loss: 3.4395, Avg Acc: 0.4031
2022-01-17 14:27:31,552 Epoch[148/300], Step[0450/1252], Avg Loss: 3.4396, Avg Acc: 0.4018
2022-01-17 14:28:51,214 Epoch[148/300], Step[0500/1252], Avg Loss: 3.4398, Avg Acc: 0.4042
2022-01-17 14:30:11,138 Epoch[148/300], Step[0550/1252], Avg Loss: 3.4368, Avg Acc: 0.4058
2022-01-17 14:31:31,722 Epoch[148/300], Step[0600/1252], Avg Loss: 3.4344, Avg Acc: 0.4082
2022-01-17 14:32:52,420 Epoch[148/300], Step[0650/1252], Avg Loss: 3.4358, Avg Acc: 0.4071
2022-01-17 14:34:12,929 Epoch[148/300], Step[0700/1252], Avg Loss: 3.4344, Avg Acc: 0.4063
2022-01-17 14:35:33,923 Epoch[148/300], Step[0750/1252], Avg Loss: 3.4350, Avg Acc: 0.4058
2022-01-17 14:36:54,523 Epoch[148/300], Step[0800/1252], Avg Loss: 3.4360, Avg Acc: 0.4040
2022-01-17 14:38:14,699 Epoch[148/300], Step[0850/1252], Avg Loss: 3.4363, Avg Acc: 0.4044
2022-01-17 14:39:34,737 Epoch[148/300], Step[0900/1252], Avg Loss: 3.4343, Avg Acc: 0.4047
2022-01-17 14:40:55,363 Epoch[148/300], Step[0950/1252], Avg Loss: 3.4330, Avg Acc: 0.4048
2022-01-17 14:42:15,662 Epoch[148/300], Step[1000/1252], Avg Loss: 3.4362, Avg Acc: 0.4039
2022-01-17 14:43:36,592 Epoch[148/300], Step[1050/1252], Avg Loss: 3.4408, Avg Acc: 0.4031
2022-01-17 14:44:57,182 Epoch[148/300], Step[1100/1252], Avg Loss: 3.4404, Avg Acc: 0.4047
2022-01-17 14:46:17,211 Epoch[148/300], Step[1150/1252], Avg Loss: 3.4393, Avg Acc: 0.4043
2022-01-17 14:47:37,743 Epoch[148/300], Step[1200/1252], Avg Loss: 3.4411, Avg Acc: 0.4039
2022-01-17 14:48:57,319 Epoch[148/300], Step[1250/1252], Avg Loss: 3.4401, Avg Acc: 0.4043
2022-01-17 14:49:02,780 ----- Epoch[148/300], Train Loss: 3.4402, Train Acc: 0.4043, time: 2105.56, Best Val(epoch146) Acc@1: 0.7210
2022-01-17 14:49:02,780 ----- Validation after Epoch: 148
2022-01-17 14:50:23,871 Val Step[0000/1563], Avg Loss: 0.9781, Avg Acc@1: 0.8125, Avg Acc@5: 0.9375
2022-01-17 14:50:25,601 Val Step[0050/1563], Avg Loss: 1.2220, Avg Acc@1: 0.7341, Avg Acc@5: 0.9185
2022-01-17 14:50:27,156 Val Step[0100/1563], Avg Loss: 1.2268, Avg Acc@1: 0.7333, Avg Acc@5: 0.9143
2022-01-17 14:50:28,720 Val Step[0150/1563], Avg Loss: 1.2332, Avg Acc@1: 0.7301, Avg Acc@5: 0.9108
2022-01-17 14:50:30,442 Val Step[0200/1563], Avg Loss: 1.2308, Avg Acc@1: 0.7303, Avg Acc@5: 0.9114
2022-01-17 14:50:32,054 Val Step[0250/1563], Avg Loss: 1.2249, Avg Acc@1: 0.7283, Avg Acc@5: 0.9104
2022-01-17 14:50:33,835 Val Step[0300/1563], Avg Loss: 1.2253, Avg Acc@1: 0.7279, Avg Acc@5: 0.9095
2022-01-17 14:50:35,504 Val Step[0350/1563], Avg Loss: 1.2286, Avg Acc@1: 0.7288, Avg Acc@5: 0.9093
2022-01-17 14:50:37,204 Val Step[0400/1563], Avg Loss: 1.2279, Avg Acc@1: 0.7280, Avg Acc@5: 0.9098
2022-01-17 14:50:38,814 Val Step[0450/1563], Avg Loss: 1.2352, Avg Acc@1: 0.7260, Avg Acc@5: 0.9094
2022-01-17 14:50:40,424 Val Step[0500/1563], Avg Loss: 1.2362, Avg Acc@1: 0.7259, Avg Acc@5: 0.9096
2022-01-17 14:50:42,022 Val Step[0550/1563], Avg Loss: 1.2397, Avg Acc@1: 0.7245, Avg Acc@5: 0.9099
2022-01-17 14:50:43,687 Val Step[0600/1563], Avg Loss: 1.2376, Avg Acc@1: 0.7244, Avg Acc@5: 0.9104
2022-01-17 14:50:45,434 Val Step[0650/1563], Avg Loss: 1.2377, Avg Acc@1: 0.7242, Avg Acc@5: 0.9108
2022-01-17 14:50:47,008 Val Step[0700/1563], Avg Loss: 1.2359, Avg Acc@1: 0.7241, Avg Acc@5: 0.9120
2022-01-17 14:50:48,600 Val Step[0750/1563], Avg Loss: 1.2415, Avg Acc@1: 0.7224, Avg Acc@5: 0.9115
2022-01-17 14:50:50,264 Val Step[0800/1563], Avg Loss: 1.2419, Avg Acc@1: 0.7229, Avg Acc@5: 0.9117
2022-01-17 14:50:52,055 Val Step[0850/1563], Avg Loss: 1.2433, Avg Acc@1: 0.7228, Avg Acc@5: 0.9115
2022-01-17 14:50:53,933 Val Step[0900/1563], Avg Loss: 1.2393, Avg Acc@1: 0.7235, Avg Acc@5: 0.9121
2022-01-17 14:50:55,622 Val Step[0950/1563], Avg Loss: 1.2394, Avg Acc@1: 0.7236, Avg Acc@5: 0.9123
2022-01-17 14:50:57,181 Val Step[1000/1563], Avg Loss: 1.2391, Avg Acc@1: 0.7238, Avg Acc@5: 0.9120
2022-01-17 14:50:58,751 Val Step[1050/1563], Avg Loss: 1.2412, Avg Acc@1: 0.7231, Avg Acc@5: 0.9118
2022-01-17 14:51:00,435 Val Step[1100/1563], Avg Loss: 1.2418, Avg Acc@1: 0.7230, Avg Acc@5: 0.9118
2022-01-17 14:51:01,959 Val Step[1150/1563], Avg Loss: 1.2410, Avg Acc@1: 0.7226, Avg Acc@5: 0.9121
2022-01-17 14:51:03,596 Val Step[1200/1563], Avg Loss: 1.2403, Avg Acc@1: 0.7229, Avg Acc@5: 0.9121
2022-01-17 14:51:05,342 Val Step[1250/1563], Avg Loss: 1.2400, Avg Acc@1: 0.7227, Avg Acc@5: 0.9122
2022-01-17 14:51:06,972 Val Step[1300/1563], Avg Loss: 1.2422, Avg Acc@1: 0.7231, Avg Acc@5: 0.9121
2022-01-17 14:51:08,573 Val Step[1350/1563], Avg Loss: 1.2423, Avg Acc@1: 0.7229, Avg Acc@5: 0.9119
2022-01-17 14:51:10,146 Val Step[1400/1563], Avg Loss: 1.2415, Avg Acc@1: 0.7231, Avg Acc@5: 0.9117
2022-01-17 14:51:11,710 Val Step[1450/1563], Avg Loss: 1.2417, Avg Acc@1: 0.7227, Avg Acc@5: 0.9114
2022-01-17 14:51:13,320 Val Step[1500/1563], Avg Loss: 1.2412, Avg Acc@1: 0.7231, Avg Acc@5: 0.9115
2022-01-17 14:51:14,863 Val Step[1550/1563], Avg Loss: 1.2415, Avg Acc@1: 0.7227, Avg Acc@5: 0.9115
2022-01-17 14:51:16,757 ----- Epoch[148/300], Validation Loss: 1.2413, Validation Acc@1: 0.7227, Validation Acc@5: 0.9116, time: 133.97
2022-01-17 14:51:17,823 the pre best model acc:0.7210, at epoch 146
2022-01-17 14:51:18,115 current best model acc:0.7227, at epoch 148
2022-01-17 14:51:18,115 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 14:51:18,115 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 14:51:18,115 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 14:51:18,115 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 14:51:18,115 Now training epoch 149. LR=0.000566
2022-01-17 14:53:01,231 Epoch[149/300], Step[0000/1252], Avg Loss: 3.4808, Avg Acc: 0.5088
2022-01-17 14:54:21,362 Epoch[149/300], Step[0050/1252], Avg Loss: 3.4142, Avg Acc: 0.4006
2022-01-17 14:55:40,541 Epoch[149/300], Step[0100/1252], Avg Loss: 3.3944, Avg Acc: 0.4161
2022-01-17 14:57:01,049 Epoch[149/300], Step[0150/1252], Avg Loss: 3.3985, Avg Acc: 0.4166
2022-01-17 14:58:20,702 Epoch[149/300], Step[0200/1252], Avg Loss: 3.3872, Avg Acc: 0.4198
2022-01-17 14:59:40,572 Epoch[149/300], Step[0250/1252], Avg Loss: 3.3868, Avg Acc: 0.4170
2022-01-17 15:01:01,141 Epoch[149/300], Step[0300/1252], Avg Loss: 3.4058, Avg Acc: 0.4133
2022-01-17 15:02:21,901 Epoch[149/300], Step[0350/1252], Avg Loss: 3.4014, Avg Acc: 0.4136
2022-01-17 15:03:42,411 Epoch[149/300], Step[0400/1252], Avg Loss: 3.4118, Avg Acc: 0.4094
2022-01-17 15:05:03,688 Epoch[149/300], Step[0450/1252], Avg Loss: 3.4110, Avg Acc: 0.4071
2022-01-17 15:06:23,231 Epoch[149/300], Step[0500/1252], Avg Loss: 3.4155, Avg Acc: 0.4079
2022-01-17 15:07:43,239 Epoch[149/300], Step[0550/1252], Avg Loss: 3.4151, Avg Acc: 0.4043
2022-01-17 15:09:04,168 Epoch[149/300], Step[0600/1252], Avg Loss: 3.4161, Avg Acc: 0.4030
2022-01-17 15:10:24,527 Epoch[149/300], Step[0650/1252], Avg Loss: 3.4165, Avg Acc: 0.4027
2022-01-17 15:11:44,186 Epoch[149/300], Step[0700/1252], Avg Loss: 3.4161, Avg Acc: 0.4025
2022-01-17 15:13:04,716 Epoch[149/300], Step[0750/1252], Avg Loss: 3.4162, Avg Acc: 0.4033
2022-01-17 15:14:25,143 Epoch[149/300], Step[0800/1252], Avg Loss: 3.4164, Avg Acc: 0.4038
2022-01-17 15:15:44,559 Epoch[149/300], Step[0850/1252], Avg Loss: 3.4184, Avg Acc: 0.4035
2022-01-17 15:17:05,887 Epoch[149/300], Step[0900/1252], Avg Loss: 3.4204, Avg Acc: 0.4036
2022-01-17 15:18:26,631 Epoch[149/300], Step[0950/1252], Avg Loss: 3.4218, Avg Acc: 0.4035
2022-01-17 15:19:46,442 Epoch[149/300], Step[1000/1252], Avg Loss: 3.4252, Avg Acc: 0.4043
2022-01-17 15:21:07,452 Epoch[149/300], Step[1050/1252], Avg Loss: 3.4261, Avg Acc: 0.4031
2022-01-17 15:22:28,175 Epoch[149/300], Step[1100/1252], Avg Loss: 3.4236, Avg Acc: 0.4027
2022-01-17 15:23:49,408 Epoch[149/300], Step[1150/1252], Avg Loss: 3.4228, Avg Acc: 0.4032
2022-01-17 15:25:10,435 Epoch[149/300], Step[1200/1252], Avg Loss: 3.4212, Avg Acc: 0.4046
2022-01-17 15:26:29,364 Epoch[149/300], Step[1250/1252], Avg Loss: 3.4243, Avg Acc: 0.4045
2022-01-17 15:26:35,871 ----- Epoch[149/300], Train Loss: 3.4243, Train Acc: 0.4045, time: 2117.75, Best Val(epoch148) Acc@1: 0.7227
2022-01-17 15:26:35,872 Now training epoch 150. LR=0.000560
2022-01-17 15:28:23,404 Epoch[150/300], Step[0000/1252], Avg Loss: 3.6591, Avg Acc: 0.2832
2022-01-17 15:29:44,154 Epoch[150/300], Step[0050/1252], Avg Loss: 3.4301, Avg Acc: 0.4070
2022-01-17 15:31:03,154 Epoch[150/300], Step[0100/1252], Avg Loss: 3.4208, Avg Acc: 0.4201
2022-01-17 15:32:23,168 Epoch[150/300], Step[0150/1252], Avg Loss: 3.4206, Avg Acc: 0.4060
2022-01-17 15:33:43,356 Epoch[150/300], Step[0200/1252], Avg Loss: 3.4145, Avg Acc: 0.4131
2022-01-17 15:35:02,318 Epoch[150/300], Step[0250/1252], Avg Loss: 3.4103, Avg Acc: 0.4099
2022-01-17 15:36:21,947 Epoch[150/300], Step[0300/1252], Avg Loss: 3.4077, Avg Acc: 0.4087
2022-01-17 15:37:42,115 Epoch[150/300], Step[0350/1252], Avg Loss: 3.4073, Avg Acc: 0.4084
2022-01-17 15:39:01,430 Epoch[150/300], Step[0400/1252], Avg Loss: 3.4180, Avg Acc: 0.4066
2022-01-17 15:40:20,890 Epoch[150/300], Step[0450/1252], Avg Loss: 3.4212, Avg Acc: 0.4066
2022-01-17 15:41:40,620 Epoch[150/300], Step[0500/1252], Avg Loss: 3.4222, Avg Acc: 0.4060
2022-01-17 15:42:59,883 Epoch[150/300], Step[0550/1252], Avg Loss: 3.4199, Avg Acc: 0.4079
2022-01-17 15:44:20,382 Epoch[150/300], Step[0600/1252], Avg Loss: 3.4226, Avg Acc: 0.4075
2022-01-17 15:45:40,390 Epoch[150/300], Step[0650/1252], Avg Loss: 3.4260, Avg Acc: 0.4070
2022-01-17 15:47:01,407 Epoch[150/300], Step[0700/1252], Avg Loss: 3.4267, Avg Acc: 0.4048
2022-01-17 15:48:21,234 Epoch[150/300], Step[0750/1252], Avg Loss: 3.4268, Avg Acc: 0.4061
2022-01-17 15:49:40,650 Epoch[150/300], Step[0800/1252], Avg Loss: 3.4303, Avg Acc: 0.4049
2022-01-17 15:50:59,762 Epoch[150/300], Step[0850/1252], Avg Loss: 3.4308, Avg Acc: 0.4050
2022-01-17 15:52:19,260 Epoch[150/300], Step[0900/1252], Avg Loss: 3.4301, Avg Acc: 0.4066
2022-01-17 15:53:39,166 Epoch[150/300], Step[0950/1252], Avg Loss: 3.4256, Avg Acc: 0.4064
2022-01-17 15:54:58,924 Epoch[150/300], Step[1000/1252], Avg Loss: 3.4270, Avg Acc: 0.4061
2022-01-17 15:56:19,741 Epoch[150/300], Step[1050/1252], Avg Loss: 3.4282, Avg Acc: 0.4049
2022-01-17 15:57:39,518 Epoch[150/300], Step[1100/1252], Avg Loss: 3.4277, Avg Acc: 0.4041
2022-01-17 15:59:00,041 Epoch[150/300], Step[1150/1252], Avg Loss: 3.4245, Avg Acc: 0.4035
2022-01-17 16:00:18,828 Epoch[150/300], Step[1200/1252], Avg Loss: 3.4235, Avg Acc: 0.4042
2022-01-17 16:01:37,996 Epoch[150/300], Step[1250/1252], Avg Loss: 3.4253, Avg Acc: 0.4034
2022-01-17 16:01:44,187 ----- Epoch[150/300], Train Loss: 3.4253, Train Acc: 0.4034, time: 2108.31, Best Val(epoch148) Acc@1: 0.7227
2022-01-17 16:01:44,187 ----- Validation after Epoch: 150
2022-01-17 16:03:09,147 Val Step[0000/1563], Avg Loss: 0.9884, Avg Acc@1: 0.7500, Avg Acc@5: 0.9688
2022-01-17 16:03:11,214 Val Step[0050/1563], Avg Loss: 1.2373, Avg Acc@1: 0.7169, Avg Acc@5: 0.9087
2022-01-17 16:03:13,035 Val Step[0100/1563], Avg Loss: 1.2408, Avg Acc@1: 0.7203, Avg Acc@5: 0.9124
2022-01-17 16:03:14,658 Val Step[0150/1563], Avg Loss: 1.2461, Avg Acc@1: 0.7202, Avg Acc@5: 0.9100
2022-01-17 16:03:16,278 Val Step[0200/1563], Avg Loss: 1.2376, Avg Acc@1: 0.7237, Avg Acc@5: 0.9108
2022-01-17 16:03:17,970 Val Step[0250/1563], Avg Loss: 1.2299, Avg Acc@1: 0.7260, Avg Acc@5: 0.9105
2022-01-17 16:03:19,560 Val Step[0300/1563], Avg Loss: 1.2321, Avg Acc@1: 0.7254, Avg Acc@5: 0.9085
2022-01-17 16:03:21,238 Val Step[0350/1563], Avg Loss: 1.2358, Avg Acc@1: 0.7250, Avg Acc@5: 0.9082
2022-01-17 16:03:22,985 Val Step[0400/1563], Avg Loss: 1.2354, Avg Acc@1: 0.7242, Avg Acc@5: 0.9084
2022-01-17 16:03:24,658 Val Step[0450/1563], Avg Loss: 1.2405, Avg Acc@1: 0.7222, Avg Acc@5: 0.9088
2022-01-17 16:03:26,250 Val Step[0500/1563], Avg Loss: 1.2430, Avg Acc@1: 0.7214, Avg Acc@5: 0.9091
2022-01-17 16:03:27,850 Val Step[0550/1563], Avg Loss: 1.2455, Avg Acc@1: 0.7211, Avg Acc@5: 0.9091
2022-01-17 16:03:29,484 Val Step[0600/1563], Avg Loss: 1.2440, Avg Acc@1: 0.7216, Avg Acc@5: 0.9099
2022-01-17 16:03:31,132 Val Step[0650/1563], Avg Loss: 1.2436, Avg Acc@1: 0.7229, Avg Acc@5: 0.9102
2022-01-17 16:03:32,862 Val Step[0700/1563], Avg Loss: 1.2397, Avg Acc@1: 0.7236, Avg Acc@5: 0.9108
2022-01-17 16:03:34,647 Val Step[0750/1563], Avg Loss: 1.2446, Avg Acc@1: 0.7221, Avg Acc@5: 0.9104
2022-01-17 16:03:36,300 Val Step[0800/1563], Avg Loss: 1.2434, Avg Acc@1: 0.7228, Avg Acc@5: 0.9102
2022-01-17 16:03:37,928 Val Step[0850/1563], Avg Loss: 1.2446, Avg Acc@1: 0.7223, Avg Acc@5: 0.9100
2022-01-17 16:03:39,649 Val Step[0900/1563], Avg Loss: 1.2415, Avg Acc@1: 0.7228, Avg Acc@5: 0.9102
2022-01-17 16:03:41,251 Val Step[0950/1563], Avg Loss: 1.2409, Avg Acc@1: 0.7230, Avg Acc@5: 0.9105
2022-01-17 16:03:42,791 Val Step[1000/1563], Avg Loss: 1.2412, Avg Acc@1: 0.7234, Avg Acc@5: 0.9106
2022-01-17 16:03:44,448 Val Step[1050/1563], Avg Loss: 1.2434, Avg Acc@1: 0.7223, Avg Acc@5: 0.9104
2022-01-17 16:03:46,168 Val Step[1100/1563], Avg Loss: 1.2431, Avg Acc@1: 0.7226, Avg Acc@5: 0.9106
2022-01-17 16:03:47,845 Val Step[1150/1563], Avg Loss: 1.2415, Avg Acc@1: 0.7232, Avg Acc@5: 0.9108
2022-01-17 16:03:49,607 Val Step[1200/1563], Avg Loss: 1.2397, Avg Acc@1: 0.7237, Avg Acc@5: 0.9112
2022-01-17 16:03:51,365 Val Step[1250/1563], Avg Loss: 1.2396, Avg Acc@1: 0.7236, Avg Acc@5: 0.9111
2022-01-17 16:03:53,141 Val Step[1300/1563], Avg Loss: 1.2427, Avg Acc@1: 0.7231, Avg Acc@5: 0.9106
2022-01-17 16:03:54,911 Val Step[1350/1563], Avg Loss: 1.2431, Avg Acc@1: 0.7226, Avg Acc@5: 0.9106
2022-01-17 16:03:56,502 Val Step[1400/1563], Avg Loss: 1.2435, Avg Acc@1: 0.7223, Avg Acc@5: 0.9104
2022-01-17 16:03:58,066 Val Step[1450/1563], Avg Loss: 1.2429, Avg Acc@1: 0.7228, Avg Acc@5: 0.9102
2022-01-17 16:03:59,831 Val Step[1500/1563], Avg Loss: 1.2428, Avg Acc@1: 0.7227, Avg Acc@5: 0.9103
2022-01-17 16:04:01,489 Val Step[1550/1563], Avg Loss: 1.2427, Avg Acc@1: 0.7227, Avg Acc@5: 0.9101
2022-01-17 16:04:03,568 ----- Epoch[150/300], Validation Loss: 1.2426, Validation Acc@1: 0.7225, Validation Acc@5: 0.9102, time: 139.38
2022-01-17 16:04:03,946 ----- Save model: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-150-Loss-3.421876493307271.pdparams
2022-01-17 16:04:03,946 ----- Save optim: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-150-Loss-3.421876493307271.pdopt
2022-01-17 16:04:03,946 Now training epoch 151. LR=0.000555
2022-01-17 16:05:41,354 Epoch[151/300], Step[0000/1252], Avg Loss: 2.8528, Avg Acc: 0.5107
2022-01-17 16:07:01,532 Epoch[151/300], Step[0050/1252], Avg Loss: 3.4198, Avg Acc: 0.3801
2022-01-17 16:08:22,050 Epoch[151/300], Step[0100/1252], Avg Loss: 3.4163, Avg Acc: 0.3937
2022-01-17 16:09:40,989 Epoch[151/300], Step[0150/1252], Avg Loss: 3.4195, Avg Acc: 0.4019
2022-01-17 16:11:01,055 Epoch[151/300], Step[0200/1252], Avg Loss: 3.4098, Avg Acc: 0.4004
2022-01-17 16:12:21,296 Epoch[151/300], Step[0250/1252], Avg Loss: 3.3909, Avg Acc: 0.4057
2022-01-17 16:13:40,199 Epoch[151/300], Step[0300/1252], Avg Loss: 3.3853, Avg Acc: 0.4066
2022-01-17 16:15:00,022 Epoch[151/300], Step[0350/1252], Avg Loss: 3.3945, Avg Acc: 0.4019
2022-01-17 16:16:19,994 Epoch[151/300], Step[0400/1252], Avg Loss: 3.4000, Avg Acc: 0.4020
2022-01-17 16:17:38,300 Epoch[151/300], Step[0450/1252], Avg Loss: 3.4095, Avg Acc: 0.4008
2022-01-17 16:18:57,549 Epoch[151/300], Step[0500/1252], Avg Loss: 3.4136, Avg Acc: 0.4028
2022-01-17 16:20:17,249 Epoch[151/300], Step[0550/1252], Avg Loss: 3.4130, Avg Acc: 0.4015
2022-01-17 16:21:36,299 Epoch[151/300], Step[0600/1252], Avg Loss: 3.4116, Avg Acc: 0.4027
2022-01-17 16:22:56,166 Epoch[151/300], Step[0650/1252], Avg Loss: 3.4188, Avg Acc: 0.4017
2022-01-17 16:24:15,992 Epoch[151/300], Step[0700/1252], Avg Loss: 3.4173, Avg Acc: 0.4021
2022-01-17 16:25:35,928 Epoch[151/300], Step[0750/1252], Avg Loss: 3.4134, Avg Acc: 0.4028
2022-01-17 16:26:55,353 Epoch[151/300], Step[0800/1252], Avg Loss: 3.4115, Avg Acc: 0.4042
2022-01-17 16:28:15,981 Epoch[151/300], Step[0850/1252], Avg Loss: 3.4160, Avg Acc: 0.4041
2022-01-17 16:29:36,322 Epoch[151/300], Step[0900/1252], Avg Loss: 3.4191, Avg Acc: 0.4028
2022-01-17 16:30:55,600 Epoch[151/300], Step[0950/1252], Avg Loss: 3.4194, Avg Acc: 0.4033
2022-01-17 16:32:14,537 Epoch[151/300], Step[1000/1252], Avg Loss: 3.4192, Avg Acc: 0.4052
2022-01-17 16:33:34,120 Epoch[151/300], Step[1050/1252], Avg Loss: 3.4182, Avg Acc: 0.4050
2022-01-17 16:34:54,385 Epoch[151/300], Step[1100/1252], Avg Loss: 3.4222, Avg Acc: 0.4054
2022-01-17 16:36:14,044 Epoch[151/300], Step[1150/1252], Avg Loss: 3.4237, Avg Acc: 0.4037
2022-01-17 16:37:34,452 Epoch[151/300], Step[1200/1252], Avg Loss: 3.4272, Avg Acc: 0.4035
2022-01-17 16:38:54,245 Epoch[151/300], Step[1250/1252], Avg Loss: 3.4271, Avg Acc: 0.4035
2022-01-17 16:39:00,460 ----- Epoch[151/300], Train Loss: 3.4271, Train Acc: 0.4035, time: 2096.51, Best Val(epoch148) Acc@1: 0.7227
2022-01-17 16:39:00,460 Now training epoch 152. LR=0.000549
2022-01-17 16:40:45,042 Epoch[152/300], Step[0000/1252], Avg Loss: 2.8255, Avg Acc: 0.4961
2022-01-17 16:42:06,056 Epoch[152/300], Step[0050/1252], Avg Loss: 3.4280, Avg Acc: 0.4179
2022-01-17 16:43:26,501 Epoch[152/300], Step[0100/1252], Avg Loss: 3.4030, Avg Acc: 0.4170
2022-01-17 16:44:47,035 Epoch[152/300], Step[0150/1252], Avg Loss: 3.4223, Avg Acc: 0.4134
2022-01-17 16:46:07,127 Epoch[152/300], Step[0200/1252], Avg Loss: 3.4168, Avg Acc: 0.4168
2022-01-17 16:47:27,382 Epoch[152/300], Step[0250/1252], Avg Loss: 3.4216, Avg Acc: 0.4121
2022-01-17 16:48:48,115 Epoch[152/300], Step[0300/1252], Avg Loss: 3.4152, Avg Acc: 0.4135
2022-01-17 16:50:08,344 Epoch[152/300], Step[0350/1252], Avg Loss: 3.4153, Avg Acc: 0.4106
2022-01-17 16:51:28,952 Epoch[152/300], Step[0400/1252], Avg Loss: 3.4209, Avg Acc: 0.4097
2022-01-17 16:52:49,282 Epoch[152/300], Step[0450/1252], Avg Loss: 3.4250, Avg Acc: 0.4086
2022-01-17 16:54:09,169 Epoch[152/300], Step[0500/1252], Avg Loss: 3.4253, Avg Acc: 0.4074
2022-01-17 16:55:28,187 Epoch[152/300], Step[0550/1252], Avg Loss: 3.4232, Avg Acc: 0.4089
2022-01-17 16:56:47,667 Epoch[152/300], Step[0600/1252], Avg Loss: 3.4282, Avg Acc: 0.4070
2022-01-17 16:58:06,765 Epoch[152/300], Step[0650/1252], Avg Loss: 3.4228, Avg Acc: 0.4072
2022-01-17 16:59:26,542 Epoch[152/300], Step[0700/1252], Avg Loss: 3.4220, Avg Acc: 0.4090
2022-01-17 17:00:46,062 Epoch[152/300], Step[0750/1252], Avg Loss: 3.4227, Avg Acc: 0.4074
2022-01-17 17:02:07,082 Epoch[152/300], Step[0800/1252], Avg Loss: 3.4219, Avg Acc: 0.4066
2022-01-17 17:03:27,382 Epoch[152/300], Step[0850/1252], Avg Loss: 3.4192, Avg Acc: 0.4051
2022-01-17 17:04:47,087 Epoch[152/300], Step[0900/1252], Avg Loss: 3.4179, Avg Acc: 0.4050
2022-01-17 17:06:07,646 Epoch[152/300], Step[0950/1252], Avg Loss: 3.4141, Avg Acc: 0.4040
2022-01-17 17:07:28,354 Epoch[152/300], Step[1000/1252], Avg Loss: 3.4154, Avg Acc: 0.4040
2022-01-17 17:08:49,104 Epoch[152/300], Step[1050/1252], Avg Loss: 3.4179, Avg Acc: 0.4024
2022-01-17 17:10:09,901 Epoch[152/300], Step[1100/1252], Avg Loss: 3.4162, Avg Acc: 0.4031
2022-01-17 17:11:29,991 Epoch[152/300], Step[1150/1252], Avg Loss: 3.4172, Avg Acc: 0.4029
2022-01-17 17:12:49,665 Epoch[152/300], Step[1200/1252], Avg Loss: 3.4193, Avg Acc: 0.4037
2022-01-17 17:14:09,041 Epoch[152/300], Step[1250/1252], Avg Loss: 3.4201, Avg Acc: 0.4036
2022-01-17 17:14:15,761 ----- Epoch[152/300], Train Loss: 3.4202, Train Acc: 0.4036, time: 2115.30, Best Val(epoch148) Acc@1: 0.7227
2022-01-17 17:14:15,761 ----- Validation after Epoch: 152
2022-01-17 17:15:47,430 Val Step[0000/1563], Avg Loss: 1.0786, Avg Acc@1: 0.7188, Avg Acc@5: 0.9688
2022-01-17 17:15:49,039 Val Step[0050/1563], Avg Loss: 1.1993, Avg Acc@1: 0.7273, Avg Acc@5: 0.9118
2022-01-17 17:15:50,605 Val Step[0100/1563], Avg Loss: 1.2353, Avg Acc@1: 0.7265, Avg Acc@5: 0.9106
2022-01-17 17:15:52,231 Val Step[0150/1563], Avg Loss: 1.2324, Avg Acc@1: 0.7225, Avg Acc@5: 0.9118
2022-01-17 17:15:53,839 Val Step[0200/1563], Avg Loss: 1.2254, Avg Acc@1: 0.7250, Avg Acc@5: 0.9126
2022-01-17 17:15:55,540 Val Step[0250/1563], Avg Loss: 1.2175, Avg Acc@1: 0.7262, Avg Acc@5: 0.9130
2022-01-17 17:15:57,076 Val Step[0300/1563], Avg Loss: 1.2198, Avg Acc@1: 0.7258, Avg Acc@5: 0.9137
2022-01-17 17:15:58,738 Val Step[0350/1563], Avg Loss: 1.2235, Avg Acc@1: 0.7252, Avg Acc@5: 0.9138
2022-01-17 17:16:00,555 Val Step[0400/1563], Avg Loss: 1.2212, Avg Acc@1: 0.7251, Avg Acc@5: 0.9142
2022-01-17 17:16:02,189 Val Step[0450/1563], Avg Loss: 1.2297, Avg Acc@1: 0.7226, Avg Acc@5: 0.9126
2022-01-17 17:16:03,732 Val Step[0500/1563], Avg Loss: 1.2322, Avg Acc@1: 0.7221, Avg Acc@5: 0.9121
2022-01-17 17:16:05,397 Val Step[0550/1563], Avg Loss: 1.2356, Avg Acc@1: 0.7219, Avg Acc@5: 0.9120
2022-01-17 17:16:07,291 Val Step[0600/1563], Avg Loss: 1.2334, Avg Acc@1: 0.7221, Avg Acc@5: 0.9125
2022-01-17 17:16:09,169 Val Step[0650/1563], Avg Loss: 1.2336, Avg Acc@1: 0.7226, Avg Acc@5: 0.9131
2022-01-17 17:16:10,917 Val Step[0700/1563], Avg Loss: 1.2316, Avg Acc@1: 0.7229, Avg Acc@5: 0.9136
2022-01-17 17:16:12,461 Val Step[0750/1563], Avg Loss: 1.2367, Avg Acc@1: 0.7216, Avg Acc@5: 0.9128
2022-01-17 17:16:14,181 Val Step[0800/1563], Avg Loss: 1.2353, Avg Acc@1: 0.7222, Avg Acc@5: 0.9129
2022-01-17 17:16:16,024 Val Step[0850/1563], Avg Loss: 1.2364, Avg Acc@1: 0.7220, Avg Acc@5: 0.9125
2022-01-17 17:16:17,582 Val Step[0900/1563], Avg Loss: 1.2340, Avg Acc@1: 0.7224, Avg Acc@5: 0.9128
2022-01-17 17:16:19,149 Val Step[0950/1563], Avg Loss: 1.2341, Avg Acc@1: 0.7227, Avg Acc@5: 0.9130
2022-01-17 17:16:20,706 Val Step[1000/1563], Avg Loss: 1.2350, Avg Acc@1: 0.7230, Avg Acc@5: 0.9126
2022-01-17 17:16:22,425 Val Step[1050/1563], Avg Loss: 1.2373, Avg Acc@1: 0.7223, Avg Acc@5: 0.9120
2022-01-17 17:16:24,251 Val Step[1100/1563], Avg Loss: 1.2378, Avg Acc@1: 0.7215, Avg Acc@5: 0.9122
2022-01-17 17:16:26,075 Val Step[1150/1563], Avg Loss: 1.2368, Avg Acc@1: 0.7222, Avg Acc@5: 0.9121
2022-01-17 17:16:27,893 Val Step[1200/1563], Avg Loss: 1.2357, Avg Acc@1: 0.7224, Avg Acc@5: 0.9124
2022-01-17 17:16:29,653 Val Step[1250/1563], Avg Loss: 1.2349, Avg Acc@1: 0.7226, Avg Acc@5: 0.9123
2022-01-17 17:16:31,343 Val Step[1300/1563], Avg Loss: 1.2378, Avg Acc@1: 0.7224, Avg Acc@5: 0.9119
2022-01-17 17:16:32,931 Val Step[1350/1563], Avg Loss: 1.2376, Avg Acc@1: 0.7220, Avg Acc@5: 0.9118
2022-01-17 17:16:34,747 Val Step[1400/1563], Avg Loss: 1.2375, Avg Acc@1: 0.7218, Avg Acc@5: 0.9118
2022-01-17 17:16:36,468 Val Step[1450/1563], Avg Loss: 1.2360, Avg Acc@1: 0.7222, Avg Acc@5: 0.9118
2022-01-17 17:16:38,189 Val Step[1500/1563], Avg Loss: 1.2360, Avg Acc@1: 0.7225, Avg Acc@5: 0.9121
2022-01-17 17:16:40,008 Val Step[1550/1563], Avg Loss: 1.2364, Avg Acc@1: 0.7222, Avg Acc@5: 0.9120
2022-01-17 17:16:42,346 ----- Epoch[152/300], Validation Loss: 1.2365, Validation Acc@1: 0.7221, Validation Acc@5: 0.9121, time: 146.58
2022-01-17 17:16:42,346 Now training epoch 153. LR=0.000544
2022-01-17 17:18:18,545 Epoch[153/300], Step[0000/1252], Avg Loss: 3.2349, Avg Acc: 0.4561
2022-01-17 17:19:38,674 Epoch[153/300], Step[0050/1252], Avg Loss: 3.4387, Avg Acc: 0.4185
2022-01-17 17:20:58,252 Epoch[153/300], Step[0100/1252], Avg Loss: 3.4092, Avg Acc: 0.4181
2022-01-17 17:22:18,542 Epoch[153/300], Step[0150/1252], Avg Loss: 3.4062, Avg Acc: 0.4184
2022-01-17 17:23:38,420 Epoch[153/300], Step[0200/1252], Avg Loss: 3.4208, Avg Acc: 0.4126
2022-01-17 17:24:57,934 Epoch[153/300], Step[0250/1252], Avg Loss: 3.4042, Avg Acc: 0.4163
2022-01-17 17:26:18,294 Epoch[153/300], Step[0300/1252], Avg Loss: 3.4101, Avg Acc: 0.4167
2022-01-17 17:27:39,118 Epoch[153/300], Step[0350/1252], Avg Loss: 3.4090, Avg Acc: 0.4101
2022-01-17 17:28:59,439 Epoch[153/300], Step[0400/1252], Avg Loss: 3.4040, Avg Acc: 0.4111
2022-01-17 17:30:19,971 Epoch[153/300], Step[0450/1252], Avg Loss: 3.4069, Avg Acc: 0.4116
2022-01-17 17:31:39,645 Epoch[153/300], Step[0500/1252], Avg Loss: 3.4098, Avg Acc: 0.4099
2022-01-17 17:33:00,142 Epoch[153/300], Step[0550/1252], Avg Loss: 3.4151, Avg Acc: 0.4078
2022-01-17 17:34:19,857 Epoch[153/300], Step[0600/1252], Avg Loss: 3.4178, Avg Acc: 0.4077
2022-01-17 17:35:40,537 Epoch[153/300], Step[0650/1252], Avg Loss: 3.4155, Avg Acc: 0.4063
2022-01-17 17:37:01,057 Epoch[153/300], Step[0700/1252], Avg Loss: 3.4179, Avg Acc: 0.4060
2022-01-17 17:38:21,424 Epoch[153/300], Step[0750/1252], Avg Loss: 3.4217, Avg Acc: 0.4051
2022-01-17 17:39:41,487 Epoch[153/300], Step[0800/1252], Avg Loss: 3.4200, Avg Acc: 0.4063
2022-01-17 17:41:01,924 Epoch[153/300], Step[0850/1252], Avg Loss: 3.4184, Avg Acc: 0.4058
2022-01-17 17:42:22,350 Epoch[153/300], Step[0900/1252], Avg Loss: 3.4219, Avg Acc: 0.4060
2022-01-17 17:43:41,648 Epoch[153/300], Step[0950/1252], Avg Loss: 3.4248, Avg Acc: 0.4059
2022-01-17 17:45:01,684 Epoch[153/300], Step[1000/1252], Avg Loss: 3.4247, Avg Acc: 0.4043
2022-01-17 17:46:20,322 Epoch[153/300], Step[1050/1252], Avg Loss: 3.4255, Avg Acc: 0.4052
2022-01-17 17:47:40,634 Epoch[153/300], Step[1100/1252], Avg Loss: 3.4230, Avg Acc: 0.4051
2022-01-17 17:49:01,148 Epoch[153/300], Step[1150/1252], Avg Loss: 3.4238, Avg Acc: 0.4043
2022-01-17 17:50:21,353 Epoch[153/300], Step[1200/1252], Avg Loss: 3.4248, Avg Acc: 0.4040
2022-01-17 17:51:41,079 Epoch[153/300], Step[1250/1252], Avg Loss: 3.4268, Avg Acc: 0.4024
2022-01-17 17:51:46,557 ----- Epoch[153/300], Train Loss: 3.4268, Train Acc: 0.4024, time: 2104.21, Best Val(epoch148) Acc@1: 0.7227
2022-01-17 17:51:46,557 Now training epoch 154. LR=0.000538
2022-01-17 17:53:28,960 Epoch[154/300], Step[0000/1252], Avg Loss: 2.8828, Avg Acc: 0.6494
2022-01-17 17:54:48,471 Epoch[154/300], Step[0050/1252], Avg Loss: 3.2864, Avg Acc: 0.4265
2022-01-17 17:56:08,170 Epoch[154/300], Step[0100/1252], Avg Loss: 3.3118, Avg Acc: 0.4303
2022-01-17 17:57:29,070 Epoch[154/300], Step[0150/1252], Avg Loss: 3.3189, Avg Acc: 0.4193
2022-01-17 17:58:48,227 Epoch[154/300], Step[0200/1252], Avg Loss: 3.3298, Avg Acc: 0.4205
2022-01-17 18:00:08,352 Epoch[154/300], Step[0250/1252], Avg Loss: 3.3503, Avg Acc: 0.4151
2022-01-17 18:01:27,849 Epoch[154/300], Step[0300/1252], Avg Loss: 3.3533, Avg Acc: 0.4132
2022-01-17 18:02:47,396 Epoch[154/300], Step[0350/1252], Avg Loss: 3.3686, Avg Acc: 0.4155
2022-01-17 18:04:06,536 Epoch[154/300], Step[0400/1252], Avg Loss: 3.3712, Avg Acc: 0.4176
2022-01-17 18:05:26,808 Epoch[154/300], Step[0450/1252], Avg Loss: 3.3739, Avg Acc: 0.4148
2022-01-17 18:06:46,195 Epoch[154/300], Step[0500/1252], Avg Loss: 3.3770, Avg Acc: 0.4120
2022-01-17 18:08:06,503 Epoch[154/300], Step[0550/1252], Avg Loss: 3.3798, Avg Acc: 0.4124
2022-01-17 18:09:27,283 Epoch[154/300], Step[0600/1252], Avg Loss: 3.3864, Avg Acc: 0.4108
2022-01-17 18:10:46,265 Epoch[154/300], Step[0650/1252], Avg Loss: 3.3848, Avg Acc: 0.4119
2022-01-17 18:12:07,866 Epoch[154/300], Step[0700/1252], Avg Loss: 3.3883, Avg Acc: 0.4099
2022-01-17 18:13:28,284 Epoch[154/300], Step[0750/1252], Avg Loss: 3.3865, Avg Acc: 0.4097
2022-01-17 18:14:48,697 Epoch[154/300], Step[0800/1252], Avg Loss: 3.3905, Avg Acc: 0.4097
2022-01-17 18:16:09,399 Epoch[154/300], Step[0850/1252], Avg Loss: 3.3937, Avg Acc: 0.4079
2022-01-17 18:17:29,859 Epoch[154/300], Step[0900/1252], Avg Loss: 3.3931, Avg Acc: 0.4071
2022-01-17 18:18:49,531 Epoch[154/300], Step[0950/1252], Avg Loss: 3.3936, Avg Acc: 0.4082
2022-01-17 18:20:09,283 Epoch[154/300], Step[1000/1252], Avg Loss: 3.3954, Avg Acc: 0.4087
2022-01-17 18:21:30,107 Epoch[154/300], Step[1050/1252], Avg Loss: 3.3971, Avg Acc: 0.4075
2022-01-17 18:22:49,139 Epoch[154/300], Step[1100/1252], Avg Loss: 3.3971, Avg Acc: 0.4084
2022-01-17 18:24:08,487 Epoch[154/300], Step[1150/1252], Avg Loss: 3.3985, Avg Acc: 0.4077
2022-01-17 18:25:29,054 Epoch[154/300], Step[1200/1252], Avg Loss: 3.4009, Avg Acc: 0.4077
2022-01-17 18:26:48,288 Epoch[154/300], Step[1250/1252], Avg Loss: 3.4019, Avg Acc: 0.4062
2022-01-17 18:26:53,829 ----- Epoch[154/300], Train Loss: 3.4018, Train Acc: 0.4062, time: 2107.27, Best Val(epoch148) Acc@1: 0.7227
2022-01-17 18:26:53,829 ----- Validation after Epoch: 154
2022-01-17 18:28:20,746 Val Step[0000/1563], Avg Loss: 1.0163, Avg Acc@1: 0.7188, Avg Acc@5: 0.9688
2022-01-17 18:28:22,393 Val Step[0050/1563], Avg Loss: 1.1904, Avg Acc@1: 0.7255, Avg Acc@5: 0.9252
2022-01-17 18:28:24,006 Val Step[0100/1563], Avg Loss: 1.1930, Avg Acc@1: 0.7231, Avg Acc@5: 0.9239
2022-01-17 18:28:25,749 Val Step[0150/1563], Avg Loss: 1.2088, Avg Acc@1: 0.7216, Avg Acc@5: 0.9176
2022-01-17 18:28:27,406 Val Step[0200/1563], Avg Loss: 1.2103, Avg Acc@1: 0.7233, Avg Acc@5: 0.9168
2022-01-17 18:28:29,180 Val Step[0250/1563], Avg Loss: 1.1996, Avg Acc@1: 0.7242, Avg Acc@5: 0.9168
2022-01-17 18:28:30,765 Val Step[0300/1563], Avg Loss: 1.2033, Avg Acc@1: 0.7239, Avg Acc@5: 0.9150
2022-01-17 18:28:32,342 Val Step[0350/1563], Avg Loss: 1.2108, Avg Acc@1: 0.7225, Avg Acc@5: 0.9148
2022-01-17 18:28:33,925 Val Step[0400/1563], Avg Loss: 1.2094, Avg Acc@1: 0.7232, Avg Acc@5: 0.9144
2022-01-17 18:28:35,472 Val Step[0450/1563], Avg Loss: 1.2145, Avg Acc@1: 0.7212, Avg Acc@5: 0.9137
2022-01-17 18:28:37,136 Val Step[0500/1563], Avg Loss: 1.2182, Avg Acc@1: 0.7202, Avg Acc@5: 0.9139
2022-01-17 18:28:38,786 Val Step[0550/1563], Avg Loss: 1.2191, Avg Acc@1: 0.7201, Avg Acc@5: 0.9141
2022-01-17 18:28:40,380 Val Step[0600/1563], Avg Loss: 1.2185, Avg Acc@1: 0.7200, Avg Acc@5: 0.9152
2022-01-17 18:28:42,002 Val Step[0650/1563], Avg Loss: 1.2212, Avg Acc@1: 0.7204, Avg Acc@5: 0.9144
2022-01-17 18:28:43,650 Val Step[0700/1563], Avg Loss: 1.2172, Avg Acc@1: 0.7224, Avg Acc@5: 0.9150
2022-01-17 18:28:45,203 Val Step[0750/1563], Avg Loss: 1.2236, Avg Acc@1: 0.7210, Avg Acc@5: 0.9144
2022-01-17 18:28:46,876 Val Step[0800/1563], Avg Loss: 1.2227, Avg Acc@1: 0.7215, Avg Acc@5: 0.9140
2022-01-17 18:28:48,597 Val Step[0850/1563], Avg Loss: 1.2239, Avg Acc@1: 0.7206, Avg Acc@5: 0.9140
2022-01-17 18:28:50,494 Val Step[0900/1563], Avg Loss: 1.2191, Avg Acc@1: 0.7219, Avg Acc@5: 0.9146
2022-01-17 18:28:52,040 Val Step[0950/1563], Avg Loss: 1.2181, Avg Acc@1: 0.7228, Avg Acc@5: 0.9147
2022-01-17 18:28:53,591 Val Step[1000/1563], Avg Loss: 1.2199, Avg Acc@1: 0.7231, Avg Acc@5: 0.9143
2022-01-17 18:28:55,213 Val Step[1050/1563], Avg Loss: 1.2211, Avg Acc@1: 0.7224, Avg Acc@5: 0.9140
2022-01-17 18:28:56,831 Val Step[1100/1563], Avg Loss: 1.2210, Avg Acc@1: 0.7220, Avg Acc@5: 0.9136
2022-01-17 18:28:58,402 Val Step[1150/1563], Avg Loss: 1.2189, Avg Acc@1: 0.7224, Avg Acc@5: 0.9138
2022-01-17 18:29:00,124 Val Step[1200/1563], Avg Loss: 1.2185, Avg Acc@1: 0.7226, Avg Acc@5: 0.9139
2022-01-17 18:29:01,745 Val Step[1250/1563], Avg Loss: 1.2176, Avg Acc@1: 0.7225, Avg Acc@5: 0.9141
2022-01-17 18:29:03,428 Val Step[1300/1563], Avg Loss: 1.2200, Avg Acc@1: 0.7227, Avg Acc@5: 0.9137
2022-01-17 18:29:05,186 Val Step[1350/1563], Avg Loss: 1.2205, Avg Acc@1: 0.7225, Avg Acc@5: 0.9134
2022-01-17 18:29:06,805 Val Step[1400/1563], Avg Loss: 1.2201, Avg Acc@1: 0.7227, Avg Acc@5: 0.9133
2022-01-17 18:29:08,385 Val Step[1450/1563], Avg Loss: 1.2196, Avg Acc@1: 0.7228, Avg Acc@5: 0.9131
2022-01-17 18:29:10,066 Val Step[1500/1563], Avg Loss: 1.2187, Avg Acc@1: 0.7231, Avg Acc@5: 0.9134
2022-01-17 18:29:11,800 Val Step[1550/1563], Avg Loss: 1.2198, Avg Acc@1: 0.7227, Avg Acc@5: 0.9132
2022-01-17 18:29:13,902 ----- Epoch[154/300], Validation Loss: 1.2195, Validation Acc@1: 0.7227, Validation Acc@5: 0.9134, time: 140.07
2022-01-17 18:29:14,978 the pre best model acc:0.7227, at epoch 148
2022-01-17 18:29:15,270 current best model acc:0.7227, at epoch 154
2022-01-17 18:29:15,270 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 18:29:15,270 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 18:29:15,270 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 18:29:15,270 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 18:29:15,271 Now training epoch 155. LR=0.000533
2022-01-17 18:30:53,365 Epoch[155/300], Step[0000/1252], Avg Loss: 3.3383, Avg Acc: 0.3330
2022-01-17 18:32:12,883 Epoch[155/300], Step[0050/1252], Avg Loss: 3.4738, Avg Acc: 0.3917
2022-01-17 18:33:32,619 Epoch[155/300], Step[0100/1252], Avg Loss: 3.4521, Avg Acc: 0.4042
2022-01-17 18:34:51,451 Epoch[155/300], Step[0150/1252], Avg Loss: 3.4215, Avg Acc: 0.4035
2022-01-17 18:36:11,751 Epoch[155/300], Step[0200/1252], Avg Loss: 3.4087, Avg Acc: 0.4082
2022-01-17 18:37:30,929 Epoch[155/300], Step[0250/1252], Avg Loss: 3.4031, Avg Acc: 0.4175
2022-01-17 18:38:51,096 Epoch[155/300], Step[0300/1252], Avg Loss: 3.4142, Avg Acc: 0.4082
2022-01-17 18:40:09,973 Epoch[155/300], Step[0350/1252], Avg Loss: 3.4051, Avg Acc: 0.4108
2022-01-17 18:41:29,543 Epoch[155/300], Step[0400/1252], Avg Loss: 3.4137, Avg Acc: 0.4094
2022-01-17 18:42:49,240 Epoch[155/300], Step[0450/1252], Avg Loss: 3.4182, Avg Acc: 0.4066
2022-01-17 18:44:08,679 Epoch[155/300], Step[0500/1252], Avg Loss: 3.4178, Avg Acc: 0.4071
2022-01-17 18:45:29,235 Epoch[155/300], Step[0550/1252], Avg Loss: 3.4176, Avg Acc: 0.4048
2022-01-17 18:46:49,625 Epoch[155/300], Step[0600/1252], Avg Loss: 3.4152, Avg Acc: 0.4046
2022-01-17 18:48:10,480 Epoch[155/300], Step[0650/1252], Avg Loss: 3.4152, Avg Acc: 0.4037
2022-01-17 18:49:30,913 Epoch[155/300], Step[0700/1252], Avg Loss: 3.4142, Avg Acc: 0.4022
2022-01-17 18:50:51,264 Epoch[155/300], Step[0750/1252], Avg Loss: 3.4181, Avg Acc: 0.4010
2022-01-17 18:52:11,224 Epoch[155/300], Step[0800/1252], Avg Loss: 3.4192, Avg Acc: 0.4038
2022-01-17 18:53:31,041 Epoch[155/300], Step[0850/1252], Avg Loss: 3.4200, Avg Acc: 0.4043
2022-01-17 18:54:50,962 Epoch[155/300], Step[0900/1252], Avg Loss: 3.4204, Avg Acc: 0.4034
2022-01-17 18:56:10,729 Epoch[155/300], Step[0950/1252], Avg Loss: 3.4211, Avg Acc: 0.4030
2022-01-17 18:57:31,133 Epoch[155/300], Step[1000/1252], Avg Loss: 3.4181, Avg Acc: 0.4030
2022-01-17 18:58:50,621 Epoch[155/300], Step[1050/1252], Avg Loss: 3.4202, Avg Acc: 0.4034
2022-01-17 19:00:10,582 Epoch[155/300], Step[1100/1252], Avg Loss: 3.4218, Avg Acc: 0.4021
2022-01-17 19:01:30,150 Epoch[155/300], Step[1150/1252], Avg Loss: 3.4210, Avg Acc: 0.4030
2022-01-17 19:02:51,177 Epoch[155/300], Step[1200/1252], Avg Loss: 3.4245, Avg Acc: 0.4033
2022-01-17 19:04:10,913 Epoch[155/300], Step[1250/1252], Avg Loss: 3.4208, Avg Acc: 0.4043
2022-01-17 19:04:18,756 ----- Epoch[155/300], Train Loss: 3.4208, Train Acc: 0.4043, time: 2103.48, Best Val(epoch154) Acc@1: 0.7227
2022-01-17 19:04:18,757 Now training epoch 156. LR=0.000527
2022-01-17 19:06:02,136 Epoch[156/300], Step[0000/1252], Avg Loss: 3.6840, Avg Acc: 0.2139
2022-01-17 19:07:22,461 Epoch[156/300], Step[0050/1252], Avg Loss: 3.3731, Avg Acc: 0.3991
2022-01-17 19:08:41,816 Epoch[156/300], Step[0100/1252], Avg Loss: 3.3808, Avg Acc: 0.4166
2022-01-17 19:10:02,772 Epoch[156/300], Step[0150/1252], Avg Loss: 3.3913, Avg Acc: 0.4020
2022-01-17 19:11:23,941 Epoch[156/300], Step[0200/1252], Avg Loss: 3.3821, Avg Acc: 0.3962
2022-01-17 19:12:43,869 Epoch[156/300], Step[0250/1252], Avg Loss: 3.3889, Avg Acc: 0.4017
2022-01-17 19:14:03,280 Epoch[156/300], Step[0300/1252], Avg Loss: 3.3978, Avg Acc: 0.4050
2022-01-17 19:15:22,590 Epoch[156/300], Step[0350/1252], Avg Loss: 3.3876, Avg Acc: 0.4115
2022-01-17 19:16:42,825 Epoch[156/300], Step[0400/1252], Avg Loss: 3.3903, Avg Acc: 0.4146
2022-01-17 19:18:03,064 Epoch[156/300], Step[0450/1252], Avg Loss: 3.3837, Avg Acc: 0.4163
2022-01-17 19:19:23,055 Epoch[156/300], Step[0500/1252], Avg Loss: 3.3869, Avg Acc: 0.4155
2022-01-17 19:20:43,551 Epoch[156/300], Step[0550/1252], Avg Loss: 3.3897, Avg Acc: 0.4118
2022-01-17 19:22:04,056 Epoch[156/300], Step[0600/1252], Avg Loss: 3.3907, Avg Acc: 0.4112
2022-01-17 19:23:25,009 Epoch[156/300], Step[0650/1252], Avg Loss: 3.3899, Avg Acc: 0.4120
2022-01-17 19:24:45,167 Epoch[156/300], Step[0700/1252], Avg Loss: 3.3835, Avg Acc: 0.4136
2022-01-17 19:26:05,161 Epoch[156/300], Step[0750/1252], Avg Loss: 3.3845, Avg Acc: 0.4129
2022-01-17 19:27:25,428 Epoch[156/300], Step[0800/1252], Avg Loss: 3.3847, Avg Acc: 0.4120
2022-01-17 19:28:45,081 Epoch[156/300], Step[0850/1252], Avg Loss: 3.3890, Avg Acc: 0.4101
2022-01-17 19:30:05,929 Epoch[156/300], Step[0900/1252], Avg Loss: 3.3886, Avg Acc: 0.4086
2022-01-17 19:31:26,281 Epoch[156/300], Step[0950/1252], Avg Loss: 3.3879, Avg Acc: 0.4085
2022-01-17 19:32:46,540 Epoch[156/300], Step[1000/1252], Avg Loss: 3.3863, Avg Acc: 0.4100
2022-01-17 19:34:06,760 Epoch[156/300], Step[1050/1252], Avg Loss: 3.3904, Avg Acc: 0.4100
2022-01-17 19:35:26,910 Epoch[156/300], Step[1100/1252], Avg Loss: 3.3930, Avg Acc: 0.4088
2022-01-17 19:36:46,994 Epoch[156/300], Step[1150/1252], Avg Loss: 3.3925, Avg Acc: 0.4102
2022-01-17 19:38:07,047 Epoch[156/300], Step[1200/1252], Avg Loss: 3.3936, Avg Acc: 0.4085
2022-01-17 19:39:26,160 Epoch[156/300], Step[1250/1252], Avg Loss: 3.3966, Avg Acc: 0.4080
2022-01-17 19:39:31,743 ----- Epoch[156/300], Train Loss: 3.3967, Train Acc: 0.4080, time: 2112.98, Best Val(epoch154) Acc@1: 0.7227
2022-01-17 19:39:31,743 ----- Validation after Epoch: 156
2022-01-17 19:41:00,811 Val Step[0000/1563], Avg Loss: 1.0532, Avg Acc@1: 0.7812, Avg Acc@5: 0.9062
2022-01-17 19:41:02,559 Val Step[0050/1563], Avg Loss: 1.2257, Avg Acc@1: 0.7384, Avg Acc@5: 0.9161
2022-01-17 19:41:04,623 Val Step[0100/1563], Avg Loss: 1.2468, Avg Acc@1: 0.7308, Avg Acc@5: 0.9171
2022-01-17 19:41:06,281 Val Step[0150/1563], Avg Loss: 1.2459, Avg Acc@1: 0.7314, Avg Acc@5: 0.9156
2022-01-17 19:41:07,835 Val Step[0200/1563], Avg Loss: 1.2440, Avg Acc@1: 0.7309, Avg Acc@5: 0.9154
2022-01-17 19:41:09,466 Val Step[0250/1563], Avg Loss: 1.2373, Avg Acc@1: 0.7311, Avg Acc@5: 0.9147
2022-01-17 19:41:11,014 Val Step[0300/1563], Avg Loss: 1.2370, Avg Acc@1: 0.7307, Avg Acc@5: 0.9132
2022-01-17 19:41:12,533 Val Step[0350/1563], Avg Loss: 1.2444, Avg Acc@1: 0.7290, Avg Acc@5: 0.9126
2022-01-17 19:41:14,129 Val Step[0400/1563], Avg Loss: 1.2408, Avg Acc@1: 0.7301, Avg Acc@5: 0.9138
2022-01-17 19:41:15,839 Val Step[0450/1563], Avg Loss: 1.2479, Avg Acc@1: 0.7271, Avg Acc@5: 0.9135
2022-01-17 19:41:17,466 Val Step[0500/1563], Avg Loss: 1.2503, Avg Acc@1: 0.7265, Avg Acc@5: 0.9132
2022-01-17 19:41:19,129 Val Step[0550/1563], Avg Loss: 1.2509, Avg Acc@1: 0.7250, Avg Acc@5: 0.9142
2022-01-17 19:41:20,729 Val Step[0600/1563], Avg Loss: 1.2497, Avg Acc@1: 0.7249, Avg Acc@5: 0.9151
2022-01-17 19:41:22,269 Val Step[0650/1563], Avg Loss: 1.2496, Avg Acc@1: 0.7249, Avg Acc@5: 0.9155
2022-01-17 19:41:23,887 Val Step[0700/1563], Avg Loss: 1.2470, Avg Acc@1: 0.7256, Avg Acc@5: 0.9158
2022-01-17 19:41:25,474 Val Step[0750/1563], Avg Loss: 1.2517, Avg Acc@1: 0.7247, Avg Acc@5: 0.9152
2022-01-17 19:41:27,060 Val Step[0800/1563], Avg Loss: 1.2504, Avg Acc@1: 0.7255, Avg Acc@5: 0.9155
2022-01-17 19:41:28,696 Val Step[0850/1563], Avg Loss: 1.2524, Avg Acc@1: 0.7251, Avg Acc@5: 0.9151
2022-01-17 19:41:30,384 Val Step[0900/1563], Avg Loss: 1.2488, Avg Acc@1: 0.7260, Avg Acc@5: 0.9152
2022-01-17 19:41:32,107 Val Step[0950/1563], Avg Loss: 1.2497, Avg Acc@1: 0.7256, Avg Acc@5: 0.9154
2022-01-17 19:41:34,058 Val Step[1000/1563], Avg Loss: 1.2502, Avg Acc@1: 0.7256, Avg Acc@5: 0.9153
2022-01-17 19:41:35,654 Val Step[1050/1563], Avg Loss: 1.2520, Avg Acc@1: 0.7249, Avg Acc@5: 0.9148
2022-01-17 19:41:37,421 Val Step[1100/1563], Avg Loss: 1.2531, Avg Acc@1: 0.7246, Avg Acc@5: 0.9143
2022-01-17 19:41:39,178 Val Step[1150/1563], Avg Loss: 1.2532, Avg Acc@1: 0.7247, Avg Acc@5: 0.9140
2022-01-17 19:41:40,744 Val Step[1200/1563], Avg Loss: 1.2519, Avg Acc@1: 0.7250, Avg Acc@5: 0.9140
2022-01-17 19:41:42,490 Val Step[1250/1563], Avg Loss: 1.2506, Avg Acc@1: 0.7252, Avg Acc@5: 0.9142
2022-01-17 19:41:44,400 Val Step[1300/1563], Avg Loss: 1.2525, Avg Acc@1: 0.7255, Avg Acc@5: 0.9140
2022-01-17 19:41:46,111 Val Step[1350/1563], Avg Loss: 1.2531, Avg Acc@1: 0.7254, Avg Acc@5: 0.9138
2022-01-17 19:41:47,825 Val Step[1400/1563], Avg Loss: 1.2528, Avg Acc@1: 0.7252, Avg Acc@5: 0.9137
2022-01-17 19:41:49,391 Val Step[1450/1563], Avg Loss: 1.2523, Avg Acc@1: 0.7257, Avg Acc@5: 0.9135
2022-01-17 19:41:50,986 Val Step[1500/1563], Avg Loss: 1.2516, Avg Acc@1: 0.7262, Avg Acc@5: 0.9140
2022-01-17 19:41:52,501 Val Step[1550/1563], Avg Loss: 1.2521, Avg Acc@1: 0.7259, Avg Acc@5: 0.9140
2022-01-17 19:41:54,660 ----- Epoch[156/300], Validation Loss: 1.2520, Validation Acc@1: 0.7259, Validation Acc@5: 0.9140, time: 142.91
2022-01-17 19:41:55,758 the pre best model acc:0.7227, at epoch 154
2022-01-17 19:41:56,050 current best model acc:0.7259, at epoch 156
2022-01-17 19:41:56,050 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 19:41:56,051 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 19:41:56,051 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-17 19:41:56,051 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-17 19:41:56,051 Now training epoch 157. LR=0.000522
2022-01-17 19:43:39,091 Epoch[157/300], Step[0000/1252], Avg Loss: 3.4461, Avg Acc: 0.5410
2022-01-17 19:44:59,297 Epoch[157/300], Step[0050/1252], Avg Loss: 3.3448, Avg Acc: 0.4480
2022-01-17 19:46:19,825 Epoch[157/300], Step[0100/1252], Avg Loss: 3.3820, Avg Acc: 0.4135
2022-01-17 19:47:40,202 Epoch[157/300], Step[0150/1252], Avg Loss: 3.3946, Avg Acc: 0.4102
2022-01-17 19:49:01,551 Epoch[157/300], Step[0200/1252], Avg Loss: 3.3942, Avg Acc: 0.4155
2022-01-17 19:50:21,623 Epoch[157/300], Step[0250/1252], Avg Loss: 3.3969, Avg Acc: 0.4153
2022-01-17 19:51:42,754 Epoch[157/300], Step[0300/1252], Avg Loss: 3.4031, Avg Acc: 0.4109
2022-01-17 19:53:02,173 Epoch[157/300], Step[0350/1252], Avg Loss: 3.4102, Avg Acc: 0.4128
2022-01-17 19:54:21,903 Epoch[157/300], Step[0400/1252], Avg Loss: 3.4151, Avg Acc: 0.4139
2022-01-17 19:55:42,134 Epoch[157/300], Step[0450/1252], Avg Loss: 3.4158, Avg Acc: 0.4112
2022-01-17 19:57:01,504 Epoch[157/300], Step[0500/1252], Avg Loss: 3.4100, Avg Acc: 0.4133
2022-01-17 19:58:22,269 Epoch[157/300], Step[0550/1252], Avg Loss: 3.4044, Avg Acc: 0.4147
2022-01-17 19:59:42,095 Epoch[157/300], Step[0600/1252], Avg Loss: 3.4044, Avg Acc: 0.4151
2022-01-17 20:01:02,687 Epoch[157/300], Step[0650/1252], Avg Loss: 3.3988, Avg Acc: 0.4161
2022-01-17 20:02:23,357 Epoch[157/300], Step[0700/1252], Avg Loss: 3.3946, Avg Acc: 0.4152
2022-01-17 20:03:43,872 Epoch[157/300], Step[0750/1252], Avg Loss: 3.3902, Avg Acc: 0.4156
2022-01-17 20:05:04,738 Epoch[157/300], Step[0800/1252], Avg Loss: 3.3920, Avg Acc: 0.4140
2022-01-17 20:06:25,831 Epoch[157/300], Step[0850/1252], Avg Loss: 3.3941, Avg Acc: 0.4134
2022-01-17 20:07:46,861 Epoch[157/300], Step[0900/1252], Avg Loss: 3.3934, Avg Acc: 0.4127
2022-01-17 20:09:07,074 Epoch[157/300], Step[0950/1252], Avg Loss: 3.3982, Avg Acc: 0.4115
2022-01-17 20:10:27,790 Epoch[157/300], Step[1000/1252], Avg Loss: 3.3984, Avg Acc: 0.4102
2022-01-17 20:11:47,988 Epoch[157/300], Step[1050/1252], Avg Loss: 3.4024, Avg Acc: 0.4095
2022-01-17 20:13:09,566 Epoch[157/300], Step[1100/1252], Avg Loss: 3.4001, Avg Acc: 0.4087
2022-01-17 20:14:30,300 Epoch[157/300], Step[1150/1252], Avg Loss: 3.3996, Avg Acc: 0.4075
2022-01-17 20:15:51,736 Epoch[157/300], Step[1200/1252], Avg Loss: 3.3993, Avg Acc: 0.4070
2022-01-17 20:17:10,947 Epoch[157/300], Step[1250/1252], Avg Loss: 3.3973, Avg Acc: 0.4065
2022-01-17 20:17:17,527 ----- Epoch[157/300], Train Loss: 3.3973, Train Acc: 0.4065, time: 2121.47, Best Val(epoch156) Acc@1: 0.7259
2022-01-17 20:17:17,527 Now training epoch 158. LR=0.000516
2022-01-17 20:19:03,334 Epoch[158/300], Step[0000/1252], Avg Loss: 3.2843, Avg Acc: 0.5537
2022-01-17 20:20:23,108 Epoch[158/300], Step[0050/1252], Avg Loss: 3.3604, Avg Acc: 0.4333
2022-01-17 20:21:43,151 Epoch[158/300], Step[0100/1252], Avg Loss: 3.3528, Avg Acc: 0.4109
2022-01-17 20:23:02,712 Epoch[158/300], Step[0150/1252], Avg Loss: 3.3526, Avg Acc: 0.3983
2022-01-17 20:24:22,721 Epoch[158/300], Step[0200/1252], Avg Loss: 3.3630, Avg Acc: 0.3940
2022-01-17 20:25:42,577 Epoch[158/300], Step[0250/1252], Avg Loss: 3.3715, Avg Acc: 0.4009
2022-01-17 20:27:02,669 Epoch[158/300], Step[0300/1252], Avg Loss: 3.3775, Avg Acc: 0.4068
2022-01-17 20:28:22,103 Epoch[158/300], Step[0350/1252], Avg Loss: 3.3816, Avg Acc: 0.4027
2022-01-17 20:29:42,003 Epoch[158/300], Step[0400/1252], Avg Loss: 3.3832, Avg Acc: 0.3988
2022-01-17 20:31:02,533 Epoch[158/300], Step[0450/1252], Avg Loss: 3.3894, Avg Acc: 0.3986
2022-01-17 20:32:22,845 Epoch[158/300], Step[0500/1252], Avg Loss: 3.3974, Avg Acc: 0.3945
2022-01-17 20:33:43,465 Epoch[158/300], Step[0550/1252], Avg Loss: 3.3951, Avg Acc: 0.3953
2022-01-17 20:35:03,790 Epoch[158/300], Step[0600/1252], Avg Loss: 3.3965, Avg Acc: 0.3946
2022-01-17 20:36:24,101 Epoch[158/300], Step[0650/1252], Avg Loss: 3.3958, Avg Acc: 0.3963
2022-01-17 20:37:44,038 Epoch[158/300], Step[0700/1252], Avg Loss: 3.3982, Avg Acc: 0.3982
2022-01-17 20:39:04,254 Epoch[158/300], Step[0750/1252], Avg Loss: 3.3948, Avg Acc: 0.3995
2022-01-17 20:40:25,239 Epoch[158/300], Step[0800/1252], Avg Loss: 3.3937, Avg Acc: 0.4022
2022-01-17 20:41:45,759 Epoch[158/300], Step[0850/1252], Avg Loss: 3.3938, Avg Acc: 0.4022
2022-01-17 20:43:06,887 Epoch[158/300], Step[0900/1252], Avg Loss: 3.3922, Avg Acc: 0.4036
2022-01-17 20:44:26,911 Epoch[158/300], Step[0950/1252], Avg Loss: 3.3951, Avg Acc: 0.4035
2022-01-17 20:45:47,005 Epoch[158/300], Step[1000/1252], Avg Loss: 3.3937, Avg Acc: 0.4036
2022-01-17 20:47:06,697 Epoch[158/300], Step[1050/1252], Avg Loss: 3.3932, Avg Acc: 0.4033
2022-01-17 20:48:27,141 Epoch[158/300], Step[1100/1252], Avg Loss: 3.3926, Avg Acc: 0.4038
2022-01-17 20:49:48,249 Epoch[158/300], Step[1150/1252], Avg Loss: 3.3922, Avg Acc: 0.4023
2022-01-17 20:51:09,236 Epoch[158/300], Step[1200/1252], Avg Loss: 3.3896, Avg Acc: 0.4024
2022-01-17 20:52:28,974 Epoch[158/300], Step[1250/1252], Avg Loss: 3.3905, Avg Acc: 0.4033
2022-01-17 20:52:34,733 ----- Epoch[158/300], Train Loss: 3.3905, Train Acc: 0.4033, time: 2117.20, Best Val(epoch156) Acc@1: 0.7259
2022-01-17 20:52:34,733 ----- Validation after Epoch: 158
2022-01-17 20:54:08,254 Val Step[0000/1563], Avg Loss: 0.9873, Avg Acc@1: 0.7812, Avg Acc@5: 0.9375
2022-01-17 20:54:09,913 Val Step[0050/1563], Avg Loss: 1.1997, Avg Acc@1: 0.7212, Avg Acc@5: 0.9124
2022-01-17 20:54:11,549 Val Step[0100/1563], Avg Loss: 1.2155, Avg Acc@1: 0.7212, Avg Acc@5: 0.9158
2022-01-17 20:54:13,075 Val Step[0150/1563], Avg Loss: 1.2229, Avg Acc@1: 0.7225, Avg Acc@5: 0.9151
2022-01-17 20:54:14,706 Val Step[0200/1563], Avg Loss: 1.2243, Avg Acc@1: 0.7248, Avg Acc@5: 0.9154
2022-01-17 20:54:16,305 Val Step[0250/1563], Avg Loss: 1.2147, Avg Acc@1: 0.7257, Avg Acc@5: 0.9161
2022-01-17 20:54:17,906 Val Step[0300/1563], Avg Loss: 1.2178, Avg Acc@1: 0.7266, Avg Acc@5: 0.9141
2022-01-17 20:54:19,584 Val Step[0350/1563], Avg Loss: 1.2240, Avg Acc@1: 0.7258, Avg Acc@5: 0.9133
2022-01-17 20:54:21,293 Val Step[0400/1563], Avg Loss: 1.2207, Avg Acc@1: 0.7260, Avg Acc@5: 0.9140
2022-01-17 20:54:22,835 Val Step[0450/1563], Avg Loss: 1.2271, Avg Acc@1: 0.7242, Avg Acc@5: 0.9130
2022-01-17 20:54:24,422 Val Step[0500/1563], Avg Loss: 1.2289, Avg Acc@1: 0.7234, Avg Acc@5: 0.9133
2022-01-17 20:54:26,038 Val Step[0550/1563], Avg Loss: 1.2297, Avg Acc@1: 0.7225, Avg Acc@5: 0.9135
2022-01-17 20:54:27,553 Val Step[0600/1563], Avg Loss: 1.2318, Avg Acc@1: 0.7231, Avg Acc@5: 0.9137
2022-01-17 20:54:29,194 Val Step[0650/1563], Avg Loss: 1.2308, Avg Acc@1: 0.7241, Avg Acc@5: 0.9138
2022-01-17 20:54:30,766 Val Step[0700/1563], Avg Loss: 1.2264, Avg Acc@1: 0.7255, Avg Acc@5: 0.9146
2022-01-17 20:54:32,385 Val Step[0750/1563], Avg Loss: 1.2317, Avg Acc@1: 0.7248, Avg Acc@5: 0.9141
2022-01-17 20:54:34,113 Val Step[0800/1563], Avg Loss: 1.2302, Avg Acc@1: 0.7253, Avg Acc@5: 0.9141
2022-01-17 20:54:35,854 Val Step[0850/1563], Avg Loss: 1.2318, Avg Acc@1: 0.7250, Avg Acc@5: 0.9143
2022-01-17 20:54:37,670 Val Step[0900/1563], Avg Loss: 1.2269, Avg Acc@1: 0.7257, Avg Acc@5: 0.9148
2022-01-17 20:54:39,495 Val Step[0950/1563], Avg Loss: 1.2278, Avg Acc@1: 0.7256, Avg Acc@5: 0.9151
2022-01-17 20:54:41,267 Val Step[1000/1563], Avg Loss: 1.2281, Avg Acc@1: 0.7258, Avg Acc@5: 0.9148
2022-01-17 20:54:42,900 Val Step[1050/1563], Avg Loss: 1.2299, Avg Acc@1: 0.7252, Avg Acc@5: 0.9143
2022-01-17 20:54:44,448 Val Step[1100/1563], Avg Loss: 1.2294, Avg Acc@1: 0.7252, Avg Acc@5: 0.9142
2022-01-17 20:54:46,037 Val Step[1150/1563], Avg Loss: 1.2284, Avg Acc@1: 0.7257, Avg Acc@5: 0.9140
2022-01-17 20:54:47,715 Val Step[1200/1563], Avg Loss: 1.2273, Avg Acc@1: 0.7260, Avg Acc@5: 0.9140
2022-01-17 20:54:49,318 Val Step[1250/1563], Avg Loss: 1.2263, Avg Acc@1: 0.7259, Avg Acc@5: 0.9141
2022-01-17 20:54:50,900 Val Step[1300/1563], Avg Loss: 1.2299, Avg Acc@1: 0.7255, Avg Acc@5: 0.9138
2022-01-17 20:54:52,424 Val Step[1350/1563], Avg Loss: 1.2303, Avg Acc@1: 0.7255, Avg Acc@5: 0.9135
2022-01-17 20:54:53,966 Val Step[1400/1563], Avg Loss: 1.2295, Avg Acc@1: 0.7254, Avg Acc@5: 0.9133
2022-01-17 20:54:55,604 Val Step[1450/1563], Avg Loss: 1.2298, Avg Acc@1: 0.7255, Avg Acc@5: 0.9129
2022-01-17 20:54:57,278 Val Step[1500/1563], Avg Loss: 1.2296, Avg Acc@1: 0.7256, Avg Acc@5: 0.9131
2022-01-17 20:54:58,875 Val Step[1550/1563], Avg Loss: 1.2300, Avg Acc@1: 0.7254, Avg Acc@5: 0.9130
2022-01-17 20:55:01,183 ----- Epoch[158/300], Validation Loss: 1.2300, Validation Acc@1: 0.7252, Validation Acc@5: 0.9131, time: 146.45
2022-01-17 20:55:01,183 Now training epoch 159. LR=0.000511
2022-01-17 20:56:54,749 Epoch[159/300], Step[0000/1252], Avg Loss: 3.5363, Avg Acc: 0.5488
2022-01-17 20:58:14,335 Epoch[159/300], Step[0050/1252], Avg Loss: 3.4755, Avg Acc: 0.4021
2022-01-17 20:59:34,135 Epoch[159/300], Step[0100/1252], Avg Loss: 3.4677, Avg Acc: 0.4038
2022-01-17 21:00:55,774 Epoch[159/300], Step[0150/1252], Avg Loss: 3.4410, Avg Acc: 0.4000
2022-01-17 21:02:15,943 Epoch[159/300], Step[0200/1252], Avg Loss: 3.4458, Avg Acc: 0.4034
2022-01-17 21:03:35,712 Epoch[159/300], Step[0250/1252], Avg Loss: 3.4317, Avg Acc: 0.4089
2022-01-17 21:04:55,227 Epoch[159/300], Step[0300/1252], Avg Loss: 3.4295, Avg Acc: 0.4045
2022-01-17 21:06:15,517 Epoch[159/300], Step[0350/1252], Avg Loss: 3.4140, Avg Acc: 0.4040
2022-01-17 21:07:36,614 Epoch[159/300], Step[0400/1252], Avg Loss: 3.4016, Avg Acc: 0.4090
2022-01-17 21:08:55,515 Epoch[159/300], Step[0450/1252], Avg Loss: 3.4009, Avg Acc: 0.4106
2022-01-17 21:10:16,020 Epoch[159/300], Step[0500/1252], Avg Loss: 3.3971, Avg Acc: 0.4097
2022-01-17 21:11:35,734 Epoch[159/300], Step[0550/1252], Avg Loss: 3.3985, Avg Acc: 0.4088
2022-01-17 21:12:56,224 Epoch[159/300], Step[0600/1252], Avg Loss: 3.4019, Avg Acc: 0.4057
2022-01-17 21:14:16,790 Epoch[159/300], Step[0650/1252], Avg Loss: 3.4048, Avg Acc: 0.4071
2022-01-17 21:15:37,396 Epoch[159/300], Step[0700/1252], Avg Loss: 3.4064, Avg Acc: 0.4072
2022-01-17 21:16:56,553 Epoch[159/300], Step[0750/1252], Avg Loss: 3.4077, Avg Acc: 0.4072
2022-01-17 21:18:16,119 Epoch[159/300], Step[0800/1252], Avg Loss: 3.4098, Avg Acc: 0.4083
2022-01-17 21:19:36,147 Epoch[159/300], Step[0850/1252], Avg Loss: 3.4081, Avg Acc: 0.4094
2022-01-17 21:20:55,792 Epoch[159/300], Step[0900/1252], Avg Loss: 3.4057, Avg Acc: 0.4116
2022-01-17 21:22:16,908 Epoch[159/300], Step[0950/1252], Avg Loss: 3.4060, Avg Acc: 0.4100
2022-01-17 21:23:37,853 Epoch[159/300], Step[1000/1252], Avg Loss: 3.4076, Avg Acc: 0.4099
2022-01-17 21:24:58,588 Epoch[159/300], Step[1050/1252], Avg Loss: 3.4100, Avg Acc: 0.4090
2022-01-17 21:26:18,465 Epoch[159/300], Step[1100/1252], Avg Loss: 3.4098, Avg Acc: 0.4087
2022-01-17 21:27:38,373 Epoch[159/300], Step[1150/1252], Avg Loss: 3.4125, Avg Acc: 0.4093
2022-01-17 21:28:59,148 Epoch[159/300], Step[1200/1252], Avg Loss: 3.4110, Avg Acc: 0.4108
2022-01-17 21:30:17,706 Epoch[159/300], Step[1250/1252], Avg Loss: 3.4097, Avg Acc: 0.4110
2022-01-17 21:30:24,250 ----- Epoch[159/300], Train Loss: 3.4097, Train Acc: 0.4110, time: 2123.06, Best Val(epoch156) Acc@1: 0.7259
2022-01-17 21:30:24,251 Now training epoch 160. LR=0.000505
2022-01-17 21:32:19,286 Epoch[160/300], Step[0000/1252], Avg Loss: 3.3599, Avg Acc: 0.3701
2022-01-17 21:33:38,940 Epoch[160/300], Step[0050/1252], Avg Loss: 3.3428, Avg Acc: 0.4125
2022-01-17 21:34:57,233 Epoch[160/300], Step[0100/1252], Avg Loss: 3.3953, Avg Acc: 0.4117
2022-01-17 21:36:17,392 Epoch[160/300], Step[0150/1252], Avg Loss: 3.4064, Avg Acc: 0.3993
2022-01-17 21:37:37,870 Epoch[160/300], Step[0200/1252], Avg Loss: 3.4055, Avg Acc: 0.4001
2022-01-17 21:38:58,901 Epoch[160/300], Step[0250/1252], Avg Loss: 3.3892, Avg Acc: 0.4049
2022-01-17 21:40:19,010 Epoch[160/300], Step[0300/1252], Avg Loss: 3.3970, Avg Acc: 0.4026
2022-01-17 21:41:39,232 Epoch[160/300], Step[0350/1252], Avg Loss: 3.3985, Avg Acc: 0.3990
2022-01-17 21:42:59,591 Epoch[160/300], Step[0400/1252], Avg Loss: 3.3915, Avg Acc: 0.4024
2022-01-17 21:44:19,484 Epoch[160/300], Step[0450/1252], Avg Loss: 3.3939, Avg Acc: 0.4041
2022-01-17 21:45:38,967 Epoch[160/300], Step[0500/1252], Avg Loss: 3.3960, Avg Acc: 0.4048
2022-01-17 21:46:58,241 Epoch[160/300], Step[0550/1252], Avg Loss: 3.3918, Avg Acc: 0.4043
2022-01-17 21:48:18,809 Epoch[160/300], Step[0600/1252], Avg Loss: 3.3992, Avg Acc: 0.4047
2022-01-17 21:49:40,250 Epoch[160/300], Step[0650/1252], Avg Loss: 3.3999, Avg Acc: 0.4036
2022-01-17 21:51:00,977 Epoch[160/300], Step[0700/1252], Avg Loss: 3.4014, Avg Acc: 0.4047
2022-01-17 21:52:22,027 Epoch[160/300], Step[0750/1252], Avg Loss: 3.4006, Avg Acc: 0.4055
2022-01-17 21:53:42,222 Epoch[160/300], Step[0800/1252], Avg Loss: 3.3996, Avg Acc: 0.4059
2022-01-17 21:55:01,237 Epoch[160/300], Step[0850/1252], Avg Loss: 3.4021, Avg Acc: 0.4061
2022-01-17 21:56:21,413 Epoch[160/300], Step[0900/1252], Avg Loss: 3.4048, Avg Acc: 0.4072
2022-01-17 21:57:41,313 Epoch[160/300], Step[0950/1252], Avg Loss: 3.4024, Avg Acc: 0.4084
2022-01-17 21:59:01,639 Epoch[160/300], Step[1000/1252], Avg Loss: 3.4048, Avg Acc: 0.4088
2022-01-17 22:00:20,228 Epoch[160/300], Step[1050/1252], Avg Loss: 3.4043, Avg Acc: 0.4085
2022-01-17 22:01:40,907 Epoch[160/300], Step[1100/1252], Avg Loss: 3.4050, Avg Acc: 0.4069
2022-01-17 22:03:01,574 Epoch[160/300], Step[1150/1252], Avg Loss: 3.4057, Avg Acc: 0.4060
2022-01-17 22:04:22,293 Epoch[160/300], Step[1200/1252], Avg Loss: 3.4034, Avg Acc: 0.4069
2022-01-17 22:05:41,941 Epoch[160/300], Step[1250/1252], Avg Loss: 3.4032, Avg Acc: 0.4065
2022-01-17 22:05:48,581 ----- Epoch[160/300], Train Loss: 3.4032, Train Acc: 0.4064, time: 2124.33, Best Val(epoch156) Acc@1: 0.7259
2022-01-17 22:05:48,581 ----- Validation after Epoch: 160
2022-01-17 22:07:20,967 Val Step[0000/1563], Avg Loss: 0.8446, Avg Acc@1: 0.8125, Avg Acc@5: 1.0000
2022-01-17 22:07:22,526 Val Step[0050/1563], Avg Loss: 1.1742, Avg Acc@1: 0.7292, Avg Acc@5: 0.9136
2022-01-17 22:07:24,125 Val Step[0100/1563], Avg Loss: 1.1970, Avg Acc@1: 0.7268, Avg Acc@5: 0.9158
2022-01-17 22:07:25,783 Val Step[0150/1563], Avg Loss: 1.2018, Avg Acc@1: 0.7285, Avg Acc@5: 0.9145
2022-01-17 22:07:27,342 Val Step[0200/1563], Avg Loss: 1.2043, Avg Acc@1: 0.7310, Avg Acc@5: 0.9139
2022-01-17 22:07:28,928 Val Step[0250/1563], Avg Loss: 1.1983, Avg Acc@1: 0.7307, Avg Acc@5: 0.9135
2022-01-17 22:07:30,559 Val Step[0300/1563], Avg Loss: 1.2020, Avg Acc@1: 0.7311, Avg Acc@5: 0.9125
2022-01-17 22:07:32,180 Val Step[0350/1563], Avg Loss: 1.2091, Avg Acc@1: 0.7301, Avg Acc@5: 0.9124
2022-01-17 22:07:33,716 Val Step[0400/1563], Avg Loss: 1.2053, Avg Acc@1: 0.7300, Avg Acc@5: 0.9137
2022-01-17 22:07:35,240 Val Step[0450/1563], Avg Loss: 1.2106, Avg Acc@1: 0.7264, Avg Acc@5: 0.9136
2022-01-17 22:07:36,835 Val Step[0500/1563], Avg Loss: 1.2121, Avg Acc@1: 0.7257, Avg Acc@5: 0.9139
2022-01-17 22:07:38,398 Val Step[0550/1563], Avg Loss: 1.2107, Avg Acc@1: 0.7257, Avg Acc@5: 0.9148
2022-01-17 22:07:39,968 Val Step[0600/1563], Avg Loss: 1.2104, Avg Acc@1: 0.7256, Avg Acc@5: 0.9145
2022-01-17 22:07:41,799 Val Step[0650/1563], Avg Loss: 1.2103, Avg Acc@1: 0.7259, Avg Acc@5: 0.9145
2022-01-17 22:07:43,753 Val Step[0700/1563], Avg Loss: 1.2073, Avg Acc@1: 0.7261, Avg Acc@5: 0.9152
2022-01-17 22:07:45,483 Val Step[0750/1563], Avg Loss: 1.2135, Avg Acc@1: 0.7246, Avg Acc@5: 0.9144
2022-01-17 22:07:47,058 Val Step[0800/1563], Avg Loss: 1.2124, Avg Acc@1: 0.7261, Avg Acc@5: 0.9145
2022-01-17 22:07:48,679 Val Step[0850/1563], Avg Loss: 1.2132, Avg Acc@1: 0.7264, Avg Acc@5: 0.9143
2022-01-17 22:07:50,292 Val Step[0900/1563], Avg Loss: 1.2108, Avg Acc@1: 0.7263, Avg Acc@5: 0.9147
2022-01-17 22:07:51,921 Val Step[0950/1563], Avg Loss: 1.2113, Avg Acc@1: 0.7267, Avg Acc@5: 0.9147
2022-01-17 22:07:53,595 Val Step[1000/1563], Avg Loss: 1.2112, Avg Acc@1: 0.7267, Avg Acc@5: 0.9148
2022-01-17 22:07:55,129 Val Step[1050/1563], Avg Loss: 1.2125, Avg Acc@1: 0.7257, Avg Acc@5: 0.9147
2022-01-17 22:07:56,686 Val Step[1100/1563], Avg Loss: 1.2120, Avg Acc@1: 0.7252, Avg Acc@5: 0.9147
2022-01-17 22:07:58,372 Val Step[1150/1563], Avg Loss: 1.2107, Avg Acc@1: 0.7258, Avg Acc@5: 0.9149
2022-01-17 22:08:00,046 Val Step[1200/1563], Avg Loss: 1.2098, Avg Acc@1: 0.7262, Avg Acc@5: 0.9149
2022-01-17 22:08:01,779 Val Step[1250/1563], Avg Loss: 1.2100, Avg Acc@1: 0.7261, Avg Acc@5: 0.9146
2022-01-17 22:08:03,539 Val Step[1300/1563], Avg Loss: 1.2122, Avg Acc@1: 0.7262, Avg Acc@5: 0.9140
2022-01-17 22:08:05,308 Val Step[1350/1563], Avg Loss: 1.2125, Avg Acc@1: 0.7256, Avg Acc@5: 0.9139
2022-01-17 22:08:06,921 Val Step[1400/1563], Avg Loss: 1.2128, Avg Acc@1: 0.7252, Avg Acc@5: 0.9137
2022-01-17 22:08:08,556 Val Step[1450/1563], Avg Loss: 1.2127, Avg Acc@1: 0.7256, Avg Acc@5: 0.9135
2022-01-17 22:08:10,163 Val Step[1500/1563], Avg Loss: 1.2123, Avg Acc@1: 0.7257, Avg Acc@5: 0.9136
2022-01-17 22:08:11,725 Val Step[1550/1563], Avg Loss: 1.2127, Avg Acc@1: 0.7256, Avg Acc@5: 0.9136
2022-01-17 22:08:13,802 ----- Epoch[160/300], Validation Loss: 1.2129, Validation Acc@1: 0.7255, Validation Acc@5: 0.9138, time: 145.22
2022-01-17 22:08:14,158 ----- Save model: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-160-Loss-3.3763265611259743.pdparams
2022-01-17 22:08:14,158 ----- Save optim: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-160-Loss-3.3763265611259743.pdopt
2022-01-17 22:08:14,158 Now training epoch 161. LR=0.000499
2022-01-17 22:10:06,624 Epoch[161/300], Step[0000/1252], Avg Loss: 3.1878, Avg Acc: 0.6084
2022-01-17 22:11:26,915 Epoch[161/300], Step[0050/1252], Avg Loss: 3.3737, Avg Acc: 0.4233
2022-01-17 22:12:48,180 Epoch[161/300], Step[0100/1252], Avg Loss: 3.3803, Avg Acc: 0.4221
2022-01-17 22:14:09,218 Epoch[161/300], Step[0150/1252], Avg Loss: 3.3912, Avg Acc: 0.4158
2022-01-17 22:15:29,205 Epoch[161/300], Step[0200/1252], Avg Loss: 3.3771, Avg Acc: 0.4168
2022-01-17 22:16:49,137 Epoch[161/300], Step[0250/1252], Avg Loss: 3.3880, Avg Acc: 0.4143
2022-01-17 22:18:08,709 Epoch[161/300], Step[0300/1252], Avg Loss: 3.3993, Avg Acc: 0.4156
2022-01-17 22:19:29,619 Epoch[161/300], Step[0350/1252], Avg Loss: 3.3970, Avg Acc: 0.4146
2022-01-17 22:20:50,410 Epoch[161/300], Step[0400/1252], Avg Loss: 3.3947, Avg Acc: 0.4115
2022-01-17 22:22:10,284 Epoch[161/300], Step[0450/1252], Avg Loss: 3.3974, Avg Acc: 0.4120
2022-01-17 22:23:31,355 Epoch[161/300], Step[0500/1252], Avg Loss: 3.3951, Avg Acc: 0.4122
2022-01-17 22:24:50,813 Epoch[161/300], Step[0550/1252], Avg Loss: 3.3981, Avg Acc: 0.4127
2022-01-17 22:26:11,392 Epoch[161/300], Step[0600/1252], Avg Loss: 3.3997, Avg Acc: 0.4111
2022-01-17 22:27:32,233 Epoch[161/300], Step[0650/1252], Avg Loss: 3.3997, Avg Acc: 0.4101
2022-01-17 22:28:52,866 Epoch[161/300], Step[0700/1252], Avg Loss: 3.3989, Avg Acc: 0.4097
2022-01-17 22:30:12,796 Epoch[161/300], Step[0750/1252], Avg Loss: 3.3980, Avg Acc: 0.4106
2022-01-17 22:31:34,094 Epoch[161/300], Step[0800/1252], Avg Loss: 3.3997, Avg Acc: 0.4121
2022-01-17 22:32:55,239 Epoch[161/300], Step[0850/1252], Avg Loss: 3.4050, Avg Acc: 0.4121
2022-01-17 22:34:15,645 Epoch[161/300], Step[0900/1252], Avg Loss: 3.4027, Avg Acc: 0.4117
2022-01-17 22:35:36,041 Epoch[161/300], Step[0950/1252], Avg Loss: 3.4045, Avg Acc: 0.4104
2022-01-17 22:36:56,263 Epoch[161/300], Step[1000/1252], Avg Loss: 3.3993, Avg Acc: 0.4121
2022-01-17 22:38:17,145 Epoch[161/300], Step[1050/1252], Avg Loss: 3.3967, Avg Acc: 0.4130
2022-01-17 22:39:38,113 Epoch[161/300], Step[1100/1252], Avg Loss: 3.3979, Avg Acc: 0.4132
2022-01-17 22:40:58,442 Epoch[161/300], Step[1150/1252], Avg Loss: 3.3993, Avg Acc: 0.4132
2022-01-17 22:42:20,106 Epoch[161/300], Step[1200/1252], Avg Loss: 3.4005, Avg Acc: 0.4128
2022-01-17 22:43:39,747 Epoch[161/300], Step[1250/1252], Avg Loss: 3.3966, Avg Acc: 0.4134
2022-01-17 22:43:45,667 ----- Epoch[161/300], Train Loss: 3.3966, Train Acc: 0.4134, time: 2131.50, Best Val(epoch156) Acc@1: 0.7259
2022-01-17 22:43:45,667 Now training epoch 162. LR=0.000494
2022-01-17 22:45:33,244 Epoch[162/300], Step[0000/1252], Avg Loss: 3.0449, Avg Acc: 0.2803
2022-01-17 22:46:52,752 Epoch[162/300], Step[0050/1252], Avg Loss: 3.3060, Avg Acc: 0.4152
2022-01-17 22:48:12,410 Epoch[162/300], Step[0100/1252], Avg Loss: 3.3208, Avg Acc: 0.4170
2022-01-17 22:49:32,297 Epoch[162/300], Step[0150/1252], Avg Loss: 3.3310, Avg Acc: 0.4182
2022-01-17 22:50:51,195 Epoch[162/300], Step[0200/1252], Avg Loss: 3.3324, Avg Acc: 0.4162
2022-01-17 22:52:11,471 Epoch[162/300], Step[0250/1252], Avg Loss: 3.3380, Avg Acc: 0.4171
2022-01-17 22:53:30,511 Epoch[162/300], Step[0300/1252], Avg Loss: 3.3451, Avg Acc: 0.4177
2022-01-17 22:54:50,050 Epoch[162/300], Step[0350/1252], Avg Loss: 3.3538, Avg Acc: 0.4156
2022-01-17 22:56:09,937 Epoch[162/300], Step[0400/1252], Avg Loss: 3.3610, Avg Acc: 0.4162
2022-01-17 22:57:29,424 Epoch[162/300], Step[0450/1252], Avg Loss: 3.3635, Avg Acc: 0.4195
2022-01-17 22:58:49,189 Epoch[162/300], Step[0500/1252], Avg Loss: 3.3694, Avg Acc: 0.4156
2022-01-17 23:00:09,648 Epoch[162/300], Step[0550/1252], Avg Loss: 3.3662, Avg Acc: 0.4189
2022-01-17 23:01:28,436 Epoch[162/300], Step[0600/1252], Avg Loss: 3.3686, Avg Acc: 0.4223
2022-01-17 23:02:47,998 Epoch[162/300], Step[0650/1252], Avg Loss: 3.3706, Avg Acc: 0.4214
2022-01-17 23:04:08,232 Epoch[162/300], Step[0700/1252], Avg Loss: 3.3701, Avg Acc: 0.4202
2022-01-17 23:05:27,439 Epoch[162/300], Step[0750/1252], Avg Loss: 3.3757, Avg Acc: 0.4185
2022-01-17 23:06:47,658 Epoch[162/300], Step[0800/1252], Avg Loss: 3.3740, Avg Acc: 0.4187
2022-01-17 23:08:07,869 Epoch[162/300], Step[0850/1252], Avg Loss: 3.3736, Avg Acc: 0.4179
2022-01-17 23:09:28,137 Epoch[162/300], Step[0900/1252], Avg Loss: 3.3756, Avg Acc: 0.4172
2022-01-17 23:10:48,882 Epoch[162/300], Step[0950/1252], Avg Loss: 3.3782, Avg Acc: 0.4177
2022-01-17 23:12:09,132 Epoch[162/300], Step[1000/1252], Avg Loss: 3.3782, Avg Acc: 0.4181
2022-01-17 23:13:29,936 Epoch[162/300], Step[1050/1252], Avg Loss: 3.3776, Avg Acc: 0.4175
2022-01-17 23:14:50,629 Epoch[162/300], Step[1100/1252], Avg Loss: 3.3760, Avg Acc: 0.4171
2022-01-17 23:16:10,635 Epoch[162/300], Step[1150/1252], Avg Loss: 3.3744, Avg Acc: 0.4152
2022-01-17 23:17:30,678 Epoch[162/300], Step[1200/1252], Avg Loss: 3.3734, Avg Acc: 0.4143
2022-01-17 23:18:49,953 Epoch[162/300], Step[1250/1252], Avg Loss: 3.3724, Avg Acc: 0.4148
2022-01-17 23:18:55,480 ----- Epoch[162/300], Train Loss: 3.3724, Train Acc: 0.4148, time: 2109.81, Best Val(epoch156) Acc@1: 0.7259
2022-01-17 23:18:55,480 ----- Validation after Epoch: 162
2022-01-17 23:20:19,276 Val Step[0000/1563], Avg Loss: 0.8864, Avg Acc@1: 0.8125, Avg Acc@5: 0.9375
2022-01-17 23:20:21,161 Val Step[0050/1563], Avg Loss: 1.2293, Avg Acc@1: 0.7194, Avg Acc@5: 0.9007
2022-01-17 23:20:23,034 Val Step[0100/1563], Avg Loss: 1.2293, Avg Acc@1: 0.7249, Avg Acc@5: 0.9062
2022-01-17 23:20:24,623 Val Step[0150/1563], Avg Loss: 1.2273, Avg Acc@1: 0.7264, Avg Acc@5: 0.9067
2022-01-17 23:20:26,175 Val Step[0200/1563], Avg Loss: 1.2228, Avg Acc@1: 0.7261, Avg Acc@5: 0.9103
2022-01-17 23:20:27,879 Val Step[0250/1563], Avg Loss: 1.2126, Avg Acc@1: 0.7271, Avg Acc@5: 0.9110
2022-01-17 23:20:29,651 Val Step[0300/1563], Avg Loss: 1.2129, Avg Acc@1: 0.7274, Avg Acc@5: 0.9101
2022-01-17 23:20:31,203 Val Step[0350/1563], Avg Loss: 1.2198, Avg Acc@1: 0.7256, Avg Acc@5: 0.9091
2022-01-17 23:20:32,859 Val Step[0400/1563], Avg Loss: 1.2128, Avg Acc@1: 0.7269, Avg Acc@5: 0.9103
2022-01-17 23:20:34,452 Val Step[0450/1563], Avg Loss: 1.2205, Avg Acc@1: 0.7244, Avg Acc@5: 0.9092
2022-01-17 23:20:35,988 Val Step[0500/1563], Avg Loss: 1.2238, Avg Acc@1: 0.7229, Avg Acc@5: 0.9094
2022-01-17 23:20:37,524 Val Step[0550/1563], Avg Loss: 1.2210, Avg Acc@1: 0.7244, Avg Acc@5: 0.9102
2022-01-17 23:20:39,110 Val Step[0600/1563], Avg Loss: 1.2215, Avg Acc@1: 0.7243, Avg Acc@5: 0.9107
2022-01-17 23:20:40,696 Val Step[0650/1563], Avg Loss: 1.2208, Avg Acc@1: 0.7250, Avg Acc@5: 0.9109
2022-01-17 23:20:42,298 Val Step[0700/1563], Avg Loss: 1.2179, Avg Acc@1: 0.7254, Avg Acc@5: 0.9118
2022-01-17 23:20:43,964 Val Step[0750/1563], Avg Loss: 1.2251, Avg Acc@1: 0.7241, Avg Acc@5: 0.9111
2022-01-17 23:20:45,589 Val Step[0800/1563], Avg Loss: 1.2242, Avg Acc@1: 0.7251, Avg Acc@5: 0.9110
2022-01-17 23:20:47,164 Val Step[0850/1563], Avg Loss: 1.2252, Avg Acc@1: 0.7253, Avg Acc@5: 0.9110
2022-01-17 23:20:48,805 Val Step[0900/1563], Avg Loss: 1.2221, Avg Acc@1: 0.7254, Avg Acc@5: 0.9116
2022-01-17 23:20:50,361 Val Step[0950/1563], Avg Loss: 1.2215, Avg Acc@1: 0.7255, Avg Acc@5: 0.9120
2022-01-17 23:20:52,018 Val Step[1000/1563], Avg Loss: 1.2220, Avg Acc@1: 0.7254, Avg Acc@5: 0.9119
2022-01-17 23:20:53,854 Val Step[1050/1563], Avg Loss: 1.2241, Avg Acc@1: 0.7245, Avg Acc@5: 0.9117
2022-01-17 23:20:55,511 Val Step[1100/1563], Avg Loss: 1.2245, Avg Acc@1: 0.7244, Avg Acc@5: 0.9119
2022-01-17 23:20:57,184 Val Step[1150/1563], Avg Loss: 1.2229, Avg Acc@1: 0.7246, Avg Acc@5: 0.9120
2022-01-17 23:20:58,901 Val Step[1200/1563], Avg Loss: 1.2216, Avg Acc@1: 0.7250, Avg Acc@5: 0.9121
2022-01-17 23:21:00,496 Val Step[1250/1563], Avg Loss: 1.2210, Avg Acc@1: 0.7249, Avg Acc@5: 0.9124
2022-01-17 23:21:02,150 Val Step[1300/1563], Avg Loss: 1.2242, Avg Acc@1: 0.7249, Avg Acc@5: 0.9121
2022-01-17 23:21:03,779 Val Step[1350/1563], Avg Loss: 1.2251, Avg Acc@1: 0.7245, Avg Acc@5: 0.9123
2022-01-17 23:21:05,495 Val Step[1400/1563], Avg Loss: 1.2247, Avg Acc@1: 0.7245, Avg Acc@5: 0.9121
2022-01-17 23:21:07,105 Val Step[1450/1563], Avg Loss: 1.2242, Avg Acc@1: 0.7248, Avg Acc@5: 0.9122
2022-01-17 23:21:08,699 Val Step[1500/1563], Avg Loss: 1.2246, Avg Acc@1: 0.7251, Avg Acc@5: 0.9123
2022-01-17 23:21:10,562 Val Step[1550/1563], Avg Loss: 1.2247, Avg Acc@1: 0.7253, Avg Acc@5: 0.9122
2022-01-17 23:21:12,664 ----- Epoch[162/300], Validation Loss: 1.2244, Validation Acc@1: 0.7254, Validation Acc@5: 0.9124, time: 137.18
2022-01-17 23:21:12,664 Now training epoch 163. LR=0.000488
2022-01-17 23:22:54,939 Epoch[163/300], Step[0000/1252], Avg Loss: 3.9066, Avg Acc: 0.3857
2022-01-17 23:24:14,058 Epoch[163/300], Step[0050/1252], Avg Loss: 3.4404, Avg Acc: 0.3988
2022-01-17 23:25:33,285 Epoch[163/300], Step[0100/1252], Avg Loss: 3.4391, Avg Acc: 0.4038
2022-01-17 23:26:53,668 Epoch[163/300], Step[0150/1252], Avg Loss: 3.4225, Avg Acc: 0.4069
2022-01-17 23:28:13,676 Epoch[163/300], Step[0200/1252], Avg Loss: 3.4029, Avg Acc: 0.4111
2022-01-17 23:29:33,399 Epoch[163/300], Step[0250/1252], Avg Loss: 3.4057, Avg Acc: 0.4118
2022-01-17 23:30:52,430 Epoch[163/300], Step[0300/1252], Avg Loss: 3.3995, Avg Acc: 0.4083
2022-01-17 23:32:12,235 Epoch[163/300], Step[0350/1252], Avg Loss: 3.3866, Avg Acc: 0.4113
2022-01-17 23:33:32,642 Epoch[163/300], Step[0400/1252], Avg Loss: 3.3899, Avg Acc: 0.4121
2022-01-17 23:34:52,639 Epoch[163/300], Step[0450/1252], Avg Loss: 3.3862, Avg Acc: 0.4090
2022-01-17 23:36:13,494 Epoch[163/300], Step[0500/1252], Avg Loss: 3.3893, Avg Acc: 0.4086
2022-01-17 23:37:33,000 Epoch[163/300], Step[0550/1252], Avg Loss: 3.3898, Avg Acc: 0.4090
2022-01-17 23:38:52,922 Epoch[163/300], Step[0600/1252], Avg Loss: 3.3900, Avg Acc: 0.4087
2022-01-17 23:40:12,253 Epoch[163/300], Step[0650/1252], Avg Loss: 3.3857, Avg Acc: 0.4103
2022-01-17 23:41:33,526 Epoch[163/300], Step[0700/1252], Avg Loss: 3.3897, Avg Acc: 0.4107
2022-01-17 23:42:54,184 Epoch[163/300], Step[0750/1252], Avg Loss: 3.3907, Avg Acc: 0.4115
2022-01-17 23:44:14,476 Epoch[163/300], Step[0800/1252], Avg Loss: 3.3909, Avg Acc: 0.4110
2022-01-17 23:45:34,884 Epoch[163/300], Step[0850/1252], Avg Loss: 3.3944, Avg Acc: 0.4107
2022-01-17 23:46:55,690 Epoch[163/300], Step[0900/1252], Avg Loss: 3.3933, Avg Acc: 0.4091
2022-01-17 23:48:15,902 Epoch[163/300], Step[0950/1252], Avg Loss: 3.3956, Avg Acc: 0.4091
2022-01-17 23:49:36,802 Epoch[163/300], Step[1000/1252], Avg Loss: 3.3987, Avg Acc: 0.4093
2022-01-17 23:50:56,930 Epoch[163/300], Step[1050/1252], Avg Loss: 3.3950, Avg Acc: 0.4088
2022-01-17 23:52:17,073 Epoch[163/300], Step[1100/1252], Avg Loss: 3.3927, Avg Acc: 0.4111
2022-01-17 23:53:38,505 Epoch[163/300], Step[1150/1252], Avg Loss: 3.3908, Avg Acc: 0.4110
2022-01-17 23:54:59,223 Epoch[163/300], Step[1200/1252], Avg Loss: 3.3903, Avg Acc: 0.4107
2022-01-17 23:56:18,713 Epoch[163/300], Step[1250/1252], Avg Loss: 3.3934, Avg Acc: 0.4101
2022-01-17 23:56:25,086 ----- Epoch[163/300], Train Loss: 3.3934, Train Acc: 0.4101, time: 2112.42, Best Val(epoch156) Acc@1: 0.7259
2022-01-17 23:56:25,086 Now training epoch 164. LR=0.000483
