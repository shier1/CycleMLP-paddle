2022-01-12 13:03:18,449 
AMP: False
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RE_COUNT: 1
  RE_MODE: pixel
  RE_PROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 256
  BATCH_SIZE_EVAL: 8
  CROP_PCT: 0.9
  DATASET: imagenet2012
  DATA_PATH: /root/paddlejob/workspace/train_data/datasets/Light_ILSVRC2012
  IMAGE_SIZE: 224
  NUM_WORKERS: 16
EVAL: False
LOCAL_RANK: 0
MODEL:
  MIXER:
    EMBED_DIMS: [64, 128, 320, 512]
    LAYERS: [2, 2, 4, 2]
    MLP_RATIOS: [4, 4, 4, 4]
    TRANSITIONS: [True, True, True, True]
  NAME: cyclemlp_b1
  NUM_CLASSES: 1000
  PRETRAINED: None
  RESUME: None
  TYPE: CycleMLP
NGPUS: 4
REPORT_FREQ: 50
SAVE: /root/paddlejob/workspace/output//train
SAVE_FREQ: 10
SEED: 0
TAG: default
TRAIN:
  ACCUM_ITER: 1
  AUTO_AUGMENT: True
  BASE_LR: 0.0005
  COLOR_JITTER: 0.4
  CUTMIX_ALPHA: 1.0
  CUTMIX_MINMAX: None
  END_LR: 5e-06
  GRAD_CLIP: 5.0
  LAST_EPOCH: 0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    MILESTONES: 30, 60, 90
    NAME: warmupcosine
  MIXUP_ALPHA: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  NUM_EPOCHS: 300
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: AdamW
  RANDOM_ERASE_COUNT: 1
  RANDOM_ERASE_MODE: pixel
  RANDOM_ERASE_PROB: 0.25
  RANDOM_ERASE_SPLIT: False
  SMOOTHING: 0.1
  WARMUP_EPOCHS: 20
  WARMUP_START_LR: 5e-07
  WEIGHT_DECAY: 0.05
VALIDATE_FREQ: 2
VALIDATION:
  REQUIREMENTS: 0.789
2022-01-12 13:03:18,449 ----- world_size = 4, local_rank = 0
2022-01-12 13:03:19,197 ----- Total # of train batch (single gpu): 1252
2022-01-12 13:03:19,197 ----- Total # of val batch (single gpu): 1563
2022-01-12 13:03:19,199 Start training from epoch 1.
2022-01-12 13:03:19,199 Now training epoch 1. LR=0.000051
2022-01-12 13:05:08,239 Epoch[001/300], Step[0000/1252], Avg Loss: 6.9295, Avg Acc: 0.0020
2022-01-12 13:06:35,884 Epoch[001/300], Step[0050/1252], Avg Loss: 6.8880, Avg Acc: 0.0022
2022-01-12 13:08:01,588 Epoch[001/300], Step[0100/1252], Avg Loss: 6.8634, Avg Acc: 0.0028
2022-01-12 13:09:27,189 Epoch[001/300], Step[0150/1252], Avg Loss: 6.8440, Avg Acc: 0.0030
2022-01-12 13:10:54,565 Epoch[001/300], Step[0200/1252], Avg Loss: 6.8297, Avg Acc: 0.0034
2022-01-12 13:12:22,162 Epoch[001/300], Step[0250/1252], Avg Loss: 6.8189, Avg Acc: 0.0037
2022-01-12 13:13:48,352 Epoch[001/300], Step[0300/1252], Avg Loss: 6.8108, Avg Acc: 0.0040
2022-01-12 13:15:15,930 Epoch[001/300], Step[0350/1252], Avg Loss: 6.8048, Avg Acc: 0.0044
2022-01-12 13:16:43,105 Epoch[001/300], Step[0400/1252], Avg Loss: 6.7987, Avg Acc: 0.0045
2022-01-12 13:18:09,963 Epoch[001/300], Step[0450/1252], Avg Loss: 6.7930, Avg Acc: 0.0047
2022-01-12 13:19:36,250 Epoch[001/300], Step[0500/1252], Avg Loss: 6.7873, Avg Acc: 0.0048
2022-01-12 13:21:01,734 Epoch[001/300], Step[0550/1252], Avg Loss: 6.7813, Avg Acc: 0.0050
2022-01-12 13:22:28,550 Epoch[001/300], Step[0600/1252], Avg Loss: 6.7755, Avg Acc: 0.0051
2022-01-12 13:23:56,149 Epoch[001/300], Step[0650/1252], Avg Loss: 6.7696, Avg Acc: 0.0052
2022-01-12 13:25:23,248 Epoch[001/300], Step[0700/1252], Avg Loss: 6.7630, Avg Acc: 0.0054
2022-01-12 13:26:51,421 Epoch[001/300], Step[0750/1252], Avg Loss: 6.7559, Avg Acc: 0.0057
2022-01-12 13:28:18,346 Epoch[001/300], Step[0800/1252], Avg Loss: 6.7488, Avg Acc: 0.0059
2022-01-12 13:29:47,009 Epoch[001/300], Step[0850/1252], Avg Loss: 6.7411, Avg Acc: 0.0061
2022-01-12 13:31:15,252 Epoch[001/300], Step[0900/1252], Avg Loss: 6.7335, Avg Acc: 0.0063
2022-01-12 13:32:43,990 Epoch[001/300], Step[0950/1252], Avg Loss: 6.7257, Avg Acc: 0.0066
2022-01-12 13:34:12,172 Epoch[001/300], Step[1000/1252], Avg Loss: 6.7178, Avg Acc: 0.0069
2022-01-12 13:35:39,597 Epoch[001/300], Step[1050/1252], Avg Loss: 6.7100, Avg Acc: 0.0071
2022-01-12 13:37:05,724 Epoch[001/300], Step[1100/1252], Avg Loss: 6.7020, Avg Acc: 0.0073
2022-01-12 13:38:31,019 Epoch[001/300], Step[1150/1252], Avg Loss: 6.6948, Avg Acc: 0.0075
2022-01-12 13:39:57,444 Epoch[001/300], Step[1200/1252], Avg Loss: 6.6870, Avg Acc: 0.0078
2022-01-12 13:41:23,522 Epoch[001/300], Step[1250/1252], Avg Loss: 6.6796, Avg Acc: 0.0080
2022-01-12 13:41:30,191 ----- Epoch[001/300], Train Loss: 6.6796, Train Acc: 0.0080, time: 2290.99
2022-01-12 13:41:30,191 Now training epoch 2. LR=0.000101
2022-01-12 13:43:08,858 Epoch[002/300], Step[0000/1252], Avg Loss: 6.5035, Avg Acc: 0.0088
2022-01-12 13:44:33,939 Epoch[002/300], Step[0050/1252], Avg Loss: 6.5124, Avg Acc: 0.0130
2022-01-12 13:45:58,514 Epoch[002/300], Step[0100/1252], Avg Loss: 6.5023, Avg Acc: 0.0129
2022-01-12 13:47:22,715 Epoch[002/300], Step[0150/1252], Avg Loss: 6.4951, Avg Acc: 0.0140
2022-01-12 13:48:47,807 Epoch[002/300], Step[0200/1252], Avg Loss: 6.4851, Avg Acc: 0.0147
2022-01-12 13:50:13,286 Epoch[002/300], Step[0250/1252], Avg Loss: 6.4737, Avg Acc: 0.0154
2022-01-12 13:51:36,774 Epoch[002/300], Step[0300/1252], Avg Loss: 6.4656, Avg Acc: 0.0160
2022-01-12 13:53:02,268 Epoch[002/300], Step[0350/1252], Avg Loss: 6.4564, Avg Acc: 0.0162
2022-01-12 13:54:27,458 Epoch[002/300], Step[0400/1252], Avg Loss: 6.4482, Avg Acc: 0.0166
2022-01-12 13:55:52,401 Epoch[002/300], Step[0450/1252], Avg Loss: 6.4412, Avg Acc: 0.0168
2022-01-12 13:57:17,520 Epoch[002/300], Step[0500/1252], Avg Loss: 6.4322, Avg Acc: 0.0170
2022-01-12 13:58:42,573 Epoch[002/300], Step[0550/1252], Avg Loss: 6.4250, Avg Acc: 0.0173
2022-01-12 14:00:07,415 Epoch[002/300], Step[0600/1252], Avg Loss: 6.4170, Avg Acc: 0.0178
2022-01-12 14:01:34,038 Epoch[002/300], Step[0650/1252], Avg Loss: 6.4077, Avg Acc: 0.0181
2022-01-12 14:02:58,280 Epoch[002/300], Step[0700/1252], Avg Loss: 6.3980, Avg Acc: 0.0186
2022-01-12 14:04:24,113 Epoch[002/300], Step[0750/1252], Avg Loss: 6.3906, Avg Acc: 0.0190
2022-01-12 14:05:49,067 Epoch[002/300], Step[0800/1252], Avg Loss: 6.3855, Avg Acc: 0.0192
2022-01-12 14:07:15,701 Epoch[002/300], Step[0850/1252], Avg Loss: 6.3790, Avg Acc: 0.0194
2022-01-12 14:08:41,000 Epoch[002/300], Step[0900/1252], Avg Loss: 6.3737, Avg Acc: 0.0198
2022-01-12 14:10:05,798 Epoch[002/300], Step[0950/1252], Avg Loss: 6.3691, Avg Acc: 0.0201
2022-01-12 14:11:31,445 Epoch[002/300], Step[1000/1252], Avg Loss: 6.3629, Avg Acc: 0.0203
2022-01-12 14:12:57,951 Epoch[002/300], Step[1050/1252], Avg Loss: 6.3563, Avg Acc: 0.0206
2022-01-12 14:14:24,699 Epoch[002/300], Step[1100/1252], Avg Loss: 6.3514, Avg Acc: 0.0209
2022-01-12 14:15:50,997 Epoch[002/300], Step[1150/1252], Avg Loss: 6.3456, Avg Acc: 0.0212
2022-01-12 14:17:17,985 Epoch[002/300], Step[1200/1252], Avg Loss: 6.3404, Avg Acc: 0.0216
2022-01-12 14:18:43,604 Epoch[002/300], Step[1250/1252], Avg Loss: 6.3352, Avg Acc: 0.0220
2022-01-12 14:18:50,706 ----- Epoch[002/300], Train Loss: 6.3352, Train Acc: 0.0220, time: 2240.51
2022-01-12 14:18:50,706 ----- Validation after Epoch: 2
2022-01-12 14:19:59,201 Val Step[0000/1563], Avg Loss: 5.3230, Avg Acc@1: 0.0938, Avg Acc@5: 0.1875
2022-01-12 14:20:01,063 Val Step[0050/1563], Avg Loss: 5.4970, Avg Acc@1: 0.0674, Avg Acc@5: 0.1722
2022-01-12 14:20:02,928 Val Step[0100/1563], Avg Loss: 5.4892, Avg Acc@1: 0.0668, Avg Acc@5: 0.1838
2022-01-12 14:20:04,808 Val Step[0150/1563], Avg Loss: 5.4828, Avg Acc@1: 0.0673, Avg Acc@5: 0.1879
2022-01-12 14:20:06,736 Val Step[0200/1563], Avg Loss: 5.4890, Avg Acc@1: 0.0645, Avg Acc@5: 0.1855
2022-01-12 14:20:08,532 Val Step[0250/1563], Avg Loss: 5.4727, Avg Acc@1: 0.0667, Avg Acc@5: 0.1901
2022-01-12 14:20:10,402 Val Step[0300/1563], Avg Loss: 5.4795, Avg Acc@1: 0.0655, Avg Acc@5: 0.1880
2022-01-12 14:20:12,360 Val Step[0350/1563], Avg Loss: 5.4753, Avg Acc@1: 0.0662, Avg Acc@5: 0.1881
2022-01-12 14:20:14,218 Val Step[0400/1563], Avg Loss: 5.4752, Avg Acc@1: 0.0656, Avg Acc@5: 0.1877
2022-01-12 14:20:16,063 Val Step[0450/1563], Avg Loss: 5.4772, Avg Acc@1: 0.0650, Avg Acc@5: 0.1855
2022-01-12 14:20:17,874 Val Step[0500/1563], Avg Loss: 5.4827, Avg Acc@1: 0.0643, Avg Acc@5: 0.1843
2022-01-12 14:20:19,776 Val Step[0550/1563], Avg Loss: 5.4810, Avg Acc@1: 0.0642, Avg Acc@5: 0.1844
2022-01-12 14:20:21,630 Val Step[0600/1563], Avg Loss: 5.4799, Avg Acc@1: 0.0645, Avg Acc@5: 0.1843
2022-01-12 14:20:23,460 Val Step[0650/1563], Avg Loss: 5.4817, Avg Acc@1: 0.0640, Avg Acc@5: 0.1835
2022-01-12 14:20:25,290 Val Step[0700/1563], Avg Loss: 5.4804, Avg Acc@1: 0.0636, Avg Acc@5: 0.1835
2022-01-12 14:20:27,264 Val Step[0750/1563], Avg Loss: 5.4812, Avg Acc@1: 0.0637, Avg Acc@5: 0.1838
2022-01-12 14:20:29,127 Val Step[0800/1563], Avg Loss: 5.4820, Avg Acc@1: 0.0639, Avg Acc@5: 0.1838
2022-01-12 14:20:30,942 Val Step[0850/1563], Avg Loss: 5.4814, Avg Acc@1: 0.0638, Avg Acc@5: 0.1842
2022-01-12 14:20:32,739 Val Step[0900/1563], Avg Loss: 5.4791, Avg Acc@1: 0.0645, Avg Acc@5: 0.1854
2022-01-12 14:20:34,519 Val Step[0950/1563], Avg Loss: 5.4784, Avg Acc@1: 0.0651, Avg Acc@5: 0.1858
2022-01-12 14:20:36,323 Val Step[1000/1563], Avg Loss: 5.4801, Avg Acc@1: 0.0652, Avg Acc@5: 0.1855
2022-01-12 14:20:38,173 Val Step[1050/1563], Avg Loss: 5.4816, Avg Acc@1: 0.0648, Avg Acc@5: 0.1843
2022-01-12 14:20:40,029 Val Step[1100/1563], Avg Loss: 5.4796, Avg Acc@1: 0.0652, Avg Acc@5: 0.1848
2022-01-12 14:20:41,833 Val Step[1150/1563], Avg Loss: 5.4769, Avg Acc@1: 0.0654, Avg Acc@5: 0.1849
2022-01-12 14:20:43,618 Val Step[1200/1563], Avg Loss: 5.4763, Avg Acc@1: 0.0658, Avg Acc@5: 0.1850
2022-01-12 14:20:45,434 Val Step[1250/1563], Avg Loss: 5.4760, Avg Acc@1: 0.0655, Avg Acc@5: 0.1849
2022-01-12 14:20:47,228 Val Step[1300/1563], Avg Loss: 5.4779, Avg Acc@1: 0.0651, Avg Acc@5: 0.1846
2022-01-12 14:20:49,035 Val Step[1350/1563], Avg Loss: 5.4778, Avg Acc@1: 0.0649, Avg Acc@5: 0.1844
2022-01-12 14:20:50,845 Val Step[1400/1563], Avg Loss: 5.4778, Avg Acc@1: 0.0650, Avg Acc@5: 0.1844
2022-01-12 14:20:52,663 Val Step[1450/1563], Avg Loss: 5.4773, Avg Acc@1: 0.0649, Avg Acc@5: 0.1845
2022-01-12 14:20:54,454 Val Step[1500/1563], Avg Loss: 5.4766, Avg Acc@1: 0.0654, Avg Acc@5: 0.1846
2022-01-12 14:20:56,194 Val Step[1550/1563], Avg Loss: 5.4764, Avg Acc@1: 0.0651, Avg Acc@5: 0.1846
2022-01-12 14:20:58,186 ----- Epoch[002/300], Validation Loss: 5.4769, Validation Acc@1: 0.0651, Validation Acc@5: 0.1847, time: 127.48
2022-01-12 14:20:58,826 the pre best model acc:0.0000, at epoch 0
2022-01-12 14:20:58,827 current best model acc:0.0651, at epoch 2
2022-01-12 14:20:58,827 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-12 14:20:58,827 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-12 14:20:58,827 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-12 14:20:58,827 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-12 14:20:58,827 Now training epoch 3. LR=0.000151
2022-01-12 14:22:42,374 Epoch[003/300], Step[0000/1252], Avg Loss: 6.1488, Avg Acc: 0.0439
2022-01-12 14:24:07,497 Epoch[003/300], Step[0050/1252], Avg Loss: 6.1994, Avg Acc: 0.0306
2022-01-12 14:25:31,455 Epoch[003/300], Step[0100/1252], Avg Loss: 6.1945, Avg Acc: 0.0315
2022-01-12 14:26:55,500 Epoch[003/300], Step[0150/1252], Avg Loss: 6.1923, Avg Acc: 0.0307
2022-01-12 14:28:20,471 Epoch[003/300], Step[0200/1252], Avg Loss: 6.1937, Avg Acc: 0.0307
2022-01-12 14:29:46,914 Epoch[003/300], Step[0250/1252], Avg Loss: 6.1871, Avg Acc: 0.0309
2022-01-12 14:31:13,041 Epoch[003/300], Step[0300/1252], Avg Loss: 6.1796, Avg Acc: 0.0313
2022-01-12 14:32:39,749 Epoch[003/300], Step[0350/1252], Avg Loss: 6.1750, Avg Acc: 0.0318
2022-01-12 14:34:05,505 Epoch[003/300], Step[0400/1252], Avg Loss: 6.1701, Avg Acc: 0.0323
2022-01-12 14:35:32,422 Epoch[003/300], Step[0450/1252], Avg Loss: 6.1647, Avg Acc: 0.0325
2022-01-12 14:36:59,160 Epoch[003/300], Step[0500/1252], Avg Loss: 6.1591, Avg Acc: 0.0327
2022-01-12 14:38:25,953 Epoch[003/300], Step[0550/1252], Avg Loss: 6.1569, Avg Acc: 0.0331
2022-01-12 14:39:52,827 Epoch[003/300], Step[0600/1252], Avg Loss: 6.1521, Avg Acc: 0.0336
2022-01-12 14:41:19,827 Epoch[003/300], Step[0650/1252], Avg Loss: 6.1463, Avg Acc: 0.0340
2022-01-12 14:42:46,500 Epoch[003/300], Step[0700/1252], Avg Loss: 6.1421, Avg Acc: 0.0340
2022-01-12 14:44:13,247 Epoch[003/300], Step[0750/1252], Avg Loss: 6.1385, Avg Acc: 0.0343
2022-01-12 14:45:39,423 Epoch[003/300], Step[0800/1252], Avg Loss: 6.1337, Avg Acc: 0.0349
2022-01-12 14:47:05,213 Epoch[003/300], Step[0850/1252], Avg Loss: 6.1288, Avg Acc: 0.0353
2022-01-12 14:48:31,518 Epoch[003/300], Step[0900/1252], Avg Loss: 6.1245, Avg Acc: 0.0353
2022-01-12 14:49:57,470 Epoch[003/300], Step[0950/1252], Avg Loss: 6.1187, Avg Acc: 0.0358
2022-01-12 14:51:23,758 Epoch[003/300], Step[1000/1252], Avg Loss: 6.1131, Avg Acc: 0.0363
2022-01-12 14:52:50,666 Epoch[003/300], Step[1050/1252], Avg Loss: 6.1080, Avg Acc: 0.0367
2022-01-12 14:54:17,558 Epoch[003/300], Step[1100/1252], Avg Loss: 6.1029, Avg Acc: 0.0371
2022-01-12 14:55:43,283 Epoch[003/300], Step[1150/1252], Avg Loss: 6.0999, Avg Acc: 0.0374
2022-01-12 14:57:09,295 Epoch[003/300], Step[1200/1252], Avg Loss: 6.0940, Avg Acc: 0.0379
2022-01-12 14:58:37,012 Epoch[003/300], Step[1250/1252], Avg Loss: 6.0911, Avg Acc: 0.0380
2022-01-12 14:58:44,150 ----- Epoch[003/300], Train Loss: 6.0911, Train Acc: 0.0380, time: 2265.32, Best Val(epoch2) Acc@1: 0.0651
2022-01-12 14:58:44,150 Now training epoch 4. LR=0.000201
2022-01-12 15:00:27,982 Epoch[004/300], Step[0000/1252], Avg Loss: 5.5759, Avg Acc: 0.0742
2022-01-12 15:01:52,616 Epoch[004/300], Step[0050/1252], Avg Loss: 6.0038, Avg Acc: 0.0479
2022-01-12 15:03:17,168 Epoch[004/300], Step[0100/1252], Avg Loss: 5.9983, Avg Acc: 0.0481
2022-01-12 15:04:41,960 Epoch[004/300], Step[0150/1252], Avg Loss: 5.9803, Avg Acc: 0.0506
2022-01-12 15:06:07,317 Epoch[004/300], Step[0200/1252], Avg Loss: 5.9744, Avg Acc: 0.0493
2022-01-12 15:07:32,035 Epoch[004/300], Step[0250/1252], Avg Loss: 5.9656, Avg Acc: 0.0486
2022-01-12 15:08:56,765 Epoch[004/300], Step[0300/1252], Avg Loss: 5.9638, Avg Acc: 0.0497
2022-01-12 15:10:21,384 Epoch[004/300], Step[0350/1252], Avg Loss: 5.9514, Avg Acc: 0.0509
2022-01-12 15:11:46,423 Epoch[004/300], Step[0400/1252], Avg Loss: 5.9435, Avg Acc: 0.0515
2022-01-12 15:13:11,113 Epoch[004/300], Step[0450/1252], Avg Loss: 5.9407, Avg Acc: 0.0516
2022-01-12 15:14:36,103 Epoch[004/300], Step[0500/1252], Avg Loss: 5.9399, Avg Acc: 0.0518
2022-01-12 15:16:01,256 Epoch[004/300], Step[0550/1252], Avg Loss: 5.9328, Avg Acc: 0.0525
2022-01-12 15:17:27,272 Epoch[004/300], Step[0600/1252], Avg Loss: 5.9285, Avg Acc: 0.0530
2022-01-12 15:18:52,089 Epoch[004/300], Step[0650/1252], Avg Loss: 5.9253, Avg Acc: 0.0533
2022-01-12 15:20:18,736 Epoch[004/300], Step[0700/1252], Avg Loss: 5.9225, Avg Acc: 0.0536
2022-01-12 15:21:44,851 Epoch[004/300], Step[0750/1252], Avg Loss: 5.9189, Avg Acc: 0.0537
2022-01-12 15:23:11,003 Epoch[004/300], Step[0800/1252], Avg Loss: 5.9162, Avg Acc: 0.0538
2022-01-12 15:24:36,721 Epoch[004/300], Step[0850/1252], Avg Loss: 5.9140, Avg Acc: 0.0539
2022-01-12 15:26:03,292 Epoch[004/300], Step[0900/1252], Avg Loss: 5.9088, Avg Acc: 0.0545
2022-01-12 15:27:30,100 Epoch[004/300], Step[0950/1252], Avg Loss: 5.9047, Avg Acc: 0.0550
2022-01-12 15:28:54,887 Epoch[004/300], Step[1000/1252], Avg Loss: 5.8991, Avg Acc: 0.0554
2022-01-12 15:30:20,478 Epoch[004/300], Step[1050/1252], Avg Loss: 5.8942, Avg Acc: 0.0561
2022-01-12 15:31:46,823 Epoch[004/300], Step[1100/1252], Avg Loss: 5.8887, Avg Acc: 0.0567
2022-01-12 15:33:12,290 Epoch[004/300], Step[1150/1252], Avg Loss: 5.8822, Avg Acc: 0.0573
2022-01-12 15:34:39,260 Epoch[004/300], Step[1200/1252], Avg Loss: 5.8772, Avg Acc: 0.0579
2022-01-12 15:36:06,575 Epoch[004/300], Step[1250/1252], Avg Loss: 5.8729, Avg Acc: 0.0583
2022-01-12 15:36:13,709 ----- Epoch[004/300], Train Loss: 5.8729, Train Acc: 0.0583, time: 2249.55, Best Val(epoch2) Acc@1: 0.0651
2022-01-12 15:36:13,709 ----- Validation after Epoch: 4
2022-01-12 15:37:25,534 Val Step[0000/1563], Avg Loss: 4.4134, Avg Acc@1: 0.2188, Avg Acc@5: 0.4062
2022-01-12 15:37:27,352 Val Step[0050/1563], Avg Loss: 4.5705, Avg Acc@1: 0.1599, Avg Acc@5: 0.3444
2022-01-12 15:37:29,152 Val Step[0100/1563], Avg Loss: 4.5410, Avg Acc@1: 0.1631, Avg Acc@5: 0.3512
2022-01-12 15:37:31,002 Val Step[0150/1563], Avg Loss: 4.5409, Avg Acc@1: 0.1616, Avg Acc@5: 0.3522
2022-01-12 15:37:32,837 Val Step[0200/1563], Avg Loss: 4.5480, Avg Acc@1: 0.1586, Avg Acc@5: 0.3511
2022-01-12 15:37:34,670 Val Step[0250/1563], Avg Loss: 4.5219, Avg Acc@1: 0.1616, Avg Acc@5: 0.3574
2022-01-12 15:37:36,475 Val Step[0300/1563], Avg Loss: 4.5238, Avg Acc@1: 0.1612, Avg Acc@5: 0.3572
2022-01-12 15:37:38,340 Val Step[0350/1563], Avg Loss: 4.5224, Avg Acc@1: 0.1621, Avg Acc@5: 0.3574
2022-01-12 15:37:40,404 Val Step[0400/1563], Avg Loss: 4.5197, Avg Acc@1: 0.1633, Avg Acc@5: 0.3596
2022-01-12 15:37:42,492 Val Step[0450/1563], Avg Loss: 4.5248, Avg Acc@1: 0.1620, Avg Acc@5: 0.3571
2022-01-12 15:37:44,559 Val Step[0500/1563], Avg Loss: 4.5343, Avg Acc@1: 0.1598, Avg Acc@5: 0.3550
2022-01-12 15:37:46,644 Val Step[0550/1563], Avg Loss: 4.5327, Avg Acc@1: 0.1601, Avg Acc@5: 0.3571
2022-01-12 15:37:48,717 Val Step[0600/1563], Avg Loss: 4.5288, Avg Acc@1: 0.1598, Avg Acc@5: 0.3575
2022-01-12 15:37:50,776 Val Step[0650/1563], Avg Loss: 4.5288, Avg Acc@1: 0.1592, Avg Acc@5: 0.3564
2022-01-12 15:37:52,807 Val Step[0700/1563], Avg Loss: 4.5304, Avg Acc@1: 0.1584, Avg Acc@5: 0.3562
2022-01-12 15:37:54,616 Val Step[0750/1563], Avg Loss: 4.5287, Avg Acc@1: 0.1588, Avg Acc@5: 0.3557
2022-01-12 15:37:56,539 Val Step[0800/1563], Avg Loss: 4.5280, Avg Acc@1: 0.1584, Avg Acc@5: 0.3564
2022-01-12 15:37:58,554 Val Step[0850/1563], Avg Loss: 4.5279, Avg Acc@1: 0.1585, Avg Acc@5: 0.3563
2022-01-12 15:38:00,519 Val Step[0900/1563], Avg Loss: 4.5236, Avg Acc@1: 0.1601, Avg Acc@5: 0.3571
2022-01-12 15:38:02,391 Val Step[0950/1563], Avg Loss: 4.5247, Avg Acc@1: 0.1598, Avg Acc@5: 0.3569
2022-01-12 15:38:04,281 Val Step[1000/1563], Avg Loss: 4.5281, Avg Acc@1: 0.1596, Avg Acc@5: 0.3562
2022-01-12 15:38:06,106 Val Step[1050/1563], Avg Loss: 4.5298, Avg Acc@1: 0.1597, Avg Acc@5: 0.3555
2022-01-12 15:38:08,346 Val Step[1100/1563], Avg Loss: 4.5272, Avg Acc@1: 0.1601, Avg Acc@5: 0.3566
2022-01-12 15:38:10,490 Val Step[1150/1563], Avg Loss: 4.5249, Avg Acc@1: 0.1604, Avg Acc@5: 0.3569
2022-01-12 15:38:12,558 Val Step[1200/1563], Avg Loss: 4.5246, Avg Acc@1: 0.1600, Avg Acc@5: 0.3564
2022-01-12 15:38:14,599 Val Step[1250/1563], Avg Loss: 4.5239, Avg Acc@1: 0.1599, Avg Acc@5: 0.3568
2022-01-12 15:38:16,644 Val Step[1300/1563], Avg Loss: 4.5255, Avg Acc@1: 0.1600, Avg Acc@5: 0.3563
2022-01-12 15:38:18,713 Val Step[1350/1563], Avg Loss: 4.5250, Avg Acc@1: 0.1594, Avg Acc@5: 0.3558
2022-01-12 15:38:20,797 Val Step[1400/1563], Avg Loss: 4.5242, Avg Acc@1: 0.1596, Avg Acc@5: 0.3551
2022-01-12 15:38:22,863 Val Step[1450/1563], Avg Loss: 4.5238, Avg Acc@1: 0.1595, Avg Acc@5: 0.3544
2022-01-12 15:38:24,912 Val Step[1500/1563], Avg Loss: 4.5239, Avg Acc@1: 0.1596, Avg Acc@5: 0.3545
2022-01-12 15:38:26,923 Val Step[1550/1563], Avg Loss: 4.5230, Avg Acc@1: 0.1599, Avg Acc@5: 0.3548
2022-01-12 15:38:28,880 ----- Epoch[004/300], Validation Loss: 4.5234, Validation Acc@1: 0.1597, Validation Acc@5: 0.3547, time: 135.17
2022-01-12 15:38:30,253 the pre best model acc:0.0651, at epoch 2
2022-01-12 15:38:30,536 current best model acc:0.1597, at epoch 4
2022-01-12 15:38:30,536 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-12 15:38:30,536 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-12 15:38:30,536 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-12 15:38:30,536 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-12 15:38:30,536 Now training epoch 5. LR=0.000251
2022-01-12 15:40:17,310 Epoch[005/300], Step[0000/1252], Avg Loss: 5.4426, Avg Acc: 0.1133
2022-01-12 15:41:42,406 Epoch[005/300], Step[0050/1252], Avg Loss: 5.7826, Avg Acc: 0.0634
2022-01-12 15:43:05,625 Epoch[005/300], Step[0100/1252], Avg Loss: 5.7664, Avg Acc: 0.0719
2022-01-12 15:44:31,237 Epoch[005/300], Step[0150/1252], Avg Loss: 5.7424, Avg Acc: 0.0746
2022-01-12 15:45:56,382 Epoch[005/300], Step[0200/1252], Avg Loss: 5.7388, Avg Acc: 0.0727
2022-01-12 15:47:22,037 Epoch[005/300], Step[0250/1252], Avg Loss: 5.7389, Avg Acc: 0.0730
2022-01-12 15:48:48,487 Epoch[005/300], Step[0300/1252], Avg Loss: 5.7355, Avg Acc: 0.0730
2022-01-12 15:50:14,630 Epoch[005/300], Step[0350/1252], Avg Loss: 5.7308, Avg Acc: 0.0729
2022-01-12 15:51:40,693 Epoch[005/300], Step[0400/1252], Avg Loss: 5.7294, Avg Acc: 0.0741
2022-01-12 15:53:07,250 Epoch[005/300], Step[0450/1252], Avg Loss: 5.7261, Avg Acc: 0.0744
2022-01-12 15:54:32,062 Epoch[005/300], Step[0500/1252], Avg Loss: 5.7190, Avg Acc: 0.0748
2022-01-12 15:55:56,896 Epoch[005/300], Step[0550/1252], Avg Loss: 5.7129, Avg Acc: 0.0751
2022-01-12 15:57:23,964 Epoch[005/300], Step[0600/1252], Avg Loss: 5.7109, Avg Acc: 0.0752
2022-01-12 15:58:48,967 Epoch[005/300], Step[0650/1252], Avg Loss: 5.7055, Avg Acc: 0.0750
2022-01-12 16:00:15,236 Epoch[005/300], Step[0700/1252], Avg Loss: 5.7012, Avg Acc: 0.0754
2022-01-12 16:01:41,207 Epoch[005/300], Step[0750/1252], Avg Loss: 5.6942, Avg Acc: 0.0764
2022-01-12 16:03:07,366 Epoch[005/300], Step[0800/1252], Avg Loss: 5.6864, Avg Acc: 0.0771
2022-01-12 16:04:33,646 Epoch[005/300], Step[0850/1252], Avg Loss: 5.6819, Avg Acc: 0.0777
2022-01-12 16:05:59,271 Epoch[005/300], Step[0900/1252], Avg Loss: 5.6789, Avg Acc: 0.0780
2022-01-12 16:07:24,788 Epoch[005/300], Step[0950/1252], Avg Loss: 5.6748, Avg Acc: 0.0786
2022-01-12 16:08:50,558 Epoch[005/300], Step[1000/1252], Avg Loss: 5.6727, Avg Acc: 0.0788
2022-01-12 16:10:16,814 Epoch[005/300], Step[1050/1252], Avg Loss: 5.6654, Avg Acc: 0.0797
2022-01-12 16:11:43,297 Epoch[005/300], Step[1100/1252], Avg Loss: 5.6611, Avg Acc: 0.0802
2022-01-12 16:13:09,481 Epoch[005/300], Step[1150/1252], Avg Loss: 5.6563, Avg Acc: 0.0807
2022-01-12 16:14:35,877 Epoch[005/300], Step[1200/1252], Avg Loss: 5.6519, Avg Acc: 0.0812
2022-01-12 16:16:03,686 Epoch[005/300], Step[1250/1252], Avg Loss: 5.6487, Avg Acc: 0.0814
2022-01-12 16:16:10,821 ----- Epoch[005/300], Train Loss: 5.6487, Train Acc: 0.0814, time: 2260.28, Best Val(epoch4) Acc@1: 0.1597
2022-01-12 16:16:10,821 Now training epoch 6. LR=0.000301
2022-01-12 16:17:56,796 Epoch[006/300], Step[0000/1252], Avg Loss: 5.7305, Avg Acc: 0.0195
2022-01-12 16:19:21,582 Epoch[006/300], Step[0050/1252], Avg Loss: 5.6142, Avg Acc: 0.0893
2022-01-12 16:20:46,642 Epoch[006/300], Step[0100/1252], Avg Loss: 5.5830, Avg Acc: 0.0934
2022-01-12 16:22:12,509 Epoch[006/300], Step[0150/1252], Avg Loss: 5.5795, Avg Acc: 0.0941
2022-01-12 16:23:37,546 Epoch[006/300], Step[0200/1252], Avg Loss: 5.5708, Avg Acc: 0.0919
2022-01-12 16:25:03,380 Epoch[006/300], Step[0250/1252], Avg Loss: 5.5556, Avg Acc: 0.0934
2022-01-12 16:26:29,088 Epoch[006/300], Step[0300/1252], Avg Loss: 5.5562, Avg Acc: 0.0941
2022-01-12 16:27:54,438 Epoch[006/300], Step[0350/1252], Avg Loss: 5.5462, Avg Acc: 0.0945
2022-01-12 16:29:18,989 Epoch[006/300], Step[0400/1252], Avg Loss: 5.5406, Avg Acc: 0.0953
2022-01-12 16:30:44,162 Epoch[006/300], Step[0450/1252], Avg Loss: 5.5281, Avg Acc: 0.0974
2022-01-12 16:32:10,480 Epoch[006/300], Step[0500/1252], Avg Loss: 5.5223, Avg Acc: 0.0974
2022-01-12 16:33:35,895 Epoch[006/300], Step[0550/1252], Avg Loss: 5.5187, Avg Acc: 0.0974
2022-01-12 16:35:01,372 Epoch[006/300], Step[0600/1252], Avg Loss: 5.5093, Avg Acc: 0.0977
2022-01-12 16:36:27,066 Epoch[006/300], Step[0650/1252], Avg Loss: 5.5045, Avg Acc: 0.0982
2022-01-12 16:37:53,830 Epoch[006/300], Step[0700/1252], Avg Loss: 5.4983, Avg Acc: 0.0992
2022-01-12 16:39:19,472 Epoch[006/300], Step[0750/1252], Avg Loss: 5.4906, Avg Acc: 0.1000
2022-01-12 16:40:42,930 Epoch[006/300], Step[0800/1252], Avg Loss: 5.4862, Avg Acc: 0.1011
2022-01-12 16:42:08,118 Epoch[006/300], Step[0850/1252], Avg Loss: 5.4829, Avg Acc: 0.1016
2022-01-12 16:43:34,549 Epoch[006/300], Step[0900/1252], Avg Loss: 5.4799, Avg Acc: 0.1021
2022-01-12 16:45:01,293 Epoch[006/300], Step[0950/1252], Avg Loss: 5.4721, Avg Acc: 0.1026
2022-01-12 16:46:27,620 Epoch[006/300], Step[1000/1252], Avg Loss: 5.4673, Avg Acc: 0.1029
2022-01-12 16:47:54,505 Epoch[006/300], Step[1050/1252], Avg Loss: 5.4629, Avg Acc: 0.1028
2022-01-12 16:49:21,219 Epoch[006/300], Step[1100/1252], Avg Loss: 5.4572, Avg Acc: 0.1034
2022-01-12 16:50:46,888 Epoch[006/300], Step[1150/1252], Avg Loss: 5.4516, Avg Acc: 0.1040
2022-01-12 16:52:13,804 Epoch[006/300], Step[1200/1252], Avg Loss: 5.4469, Avg Acc: 0.1048
2022-01-12 16:53:40,955 Epoch[006/300], Step[1250/1252], Avg Loss: 5.4451, Avg Acc: 0.1057
2022-01-12 16:53:47,972 ----- Epoch[006/300], Train Loss: 5.4451, Train Acc: 0.1057, time: 2257.15, Best Val(epoch4) Acc@1: 0.1597
2022-01-12 16:53:47,972 ----- Validation after Epoch: 6
2022-01-12 16:55:03,533 Val Step[0000/1563], Avg Loss: 3.6726, Avg Acc@1: 0.3750, Avg Acc@5: 0.5625
2022-01-12 16:55:05,336 Val Step[0050/1563], Avg Loss: 3.8195, Avg Acc@1: 0.2506, Avg Acc@5: 0.4847
2022-01-12 16:55:07,190 Val Step[0100/1563], Avg Loss: 3.7927, Avg Acc@1: 0.2497, Avg Acc@5: 0.4916
2022-01-12 16:55:08,997 Val Step[0150/1563], Avg Loss: 3.7804, Avg Acc@1: 0.2546, Avg Acc@5: 0.4942
2022-01-12 16:55:10,790 Val Step[0200/1563], Avg Loss: 3.7758, Avg Acc@1: 0.2558, Avg Acc@5: 0.4974
2022-01-12 16:55:12,567 Val Step[0250/1563], Avg Loss: 3.7510, Avg Acc@1: 0.2592, Avg Acc@5: 0.5034
2022-01-12 16:55:14,344 Val Step[0300/1563], Avg Loss: 3.7515, Avg Acc@1: 0.2593, Avg Acc@5: 0.5040
2022-01-12 16:55:16,169 Val Step[0350/1563], Avg Loss: 3.7493, Avg Acc@1: 0.2597, Avg Acc@5: 0.5044
2022-01-12 16:55:17,958 Val Step[0400/1563], Avg Loss: 3.7475, Avg Acc@1: 0.2595, Avg Acc@5: 0.5052
2022-01-12 16:55:19,726 Val Step[0450/1563], Avg Loss: 3.7531, Avg Acc@1: 0.2597, Avg Acc@5: 0.5033
2022-01-12 16:55:21,505 Val Step[0500/1563], Avg Loss: 3.7614, Avg Acc@1: 0.2585, Avg Acc@5: 0.5016
2022-01-12 16:55:23,320 Val Step[0550/1563], Avg Loss: 3.7633, Avg Acc@1: 0.2580, Avg Acc@5: 0.5015
2022-01-12 16:55:25,106 Val Step[0600/1563], Avg Loss: 3.7632, Avg Acc@1: 0.2579, Avg Acc@5: 0.5008
2022-01-12 16:55:26,993 Val Step[0650/1563], Avg Loss: 3.7635, Avg Acc@1: 0.2566, Avg Acc@5: 0.5002
2022-01-12 16:55:28,922 Val Step[0700/1563], Avg Loss: 3.7655, Avg Acc@1: 0.2553, Avg Acc@5: 0.4997
2022-01-12 16:55:30,797 Val Step[0750/1563], Avg Loss: 3.7676, Avg Acc@1: 0.2561, Avg Acc@5: 0.4994
2022-01-12 16:55:32,573 Val Step[0800/1563], Avg Loss: 3.7680, Avg Acc@1: 0.2561, Avg Acc@5: 0.4997
2022-01-12 16:55:34,379 Val Step[0850/1563], Avg Loss: 3.7659, Avg Acc@1: 0.2559, Avg Acc@5: 0.4999
2022-01-12 16:55:36,279 Val Step[0900/1563], Avg Loss: 3.7637, Avg Acc@1: 0.2569, Avg Acc@5: 0.5003
2022-01-12 16:55:38,085 Val Step[0950/1563], Avg Loss: 3.7635, Avg Acc@1: 0.2569, Avg Acc@5: 0.4996
2022-01-12 16:55:40,001 Val Step[1000/1563], Avg Loss: 3.7647, Avg Acc@1: 0.2571, Avg Acc@5: 0.4991
2022-01-12 16:55:41,857 Val Step[1050/1563], Avg Loss: 3.7679, Avg Acc@1: 0.2570, Avg Acc@5: 0.4978
2022-01-12 16:55:43,651 Val Step[1100/1563], Avg Loss: 3.7651, Avg Acc@1: 0.2570, Avg Acc@5: 0.4983
2022-01-12 16:55:45,475 Val Step[1150/1563], Avg Loss: 3.7627, Avg Acc@1: 0.2575, Avg Acc@5: 0.4991
2022-01-12 16:55:47,300 Val Step[1200/1563], Avg Loss: 3.7636, Avg Acc@1: 0.2575, Avg Acc@5: 0.4985
2022-01-12 16:55:49,162 Val Step[1250/1563], Avg Loss: 3.7620, Avg Acc@1: 0.2576, Avg Acc@5: 0.4986
2022-01-12 16:55:51,071 Val Step[1300/1563], Avg Loss: 3.7651, Avg Acc@1: 0.2572, Avg Acc@5: 0.4982
2022-01-12 16:55:52,892 Val Step[1350/1563], Avg Loss: 3.7655, Avg Acc@1: 0.2566, Avg Acc@5: 0.4979
2022-01-12 16:55:54,665 Val Step[1400/1563], Avg Loss: 3.7660, Avg Acc@1: 0.2562, Avg Acc@5: 0.4969
2022-01-12 16:55:56,502 Val Step[1450/1563], Avg Loss: 3.7662, Avg Acc@1: 0.2564, Avg Acc@5: 0.4968
2022-01-12 16:55:58,564 Val Step[1500/1563], Avg Loss: 3.7662, Avg Acc@1: 0.2570, Avg Acc@5: 0.4971
2022-01-12 16:56:00,583 Val Step[1550/1563], Avg Loss: 3.7651, Avg Acc@1: 0.2576, Avg Acc@5: 0.4974
2022-01-12 16:56:02,944 ----- Epoch[006/300], Validation Loss: 3.7654, Validation Acc@1: 0.2575, Validation Acc@5: 0.4974, time: 134.97
2022-01-12 16:56:04,306 the pre best model acc:0.1597, at epoch 4
2022-01-12 16:56:04,536 current best model acc:0.2575, at epoch 6
2022-01-12 16:56:04,536 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-12 16:56:04,536 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-12 16:56:04,536 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-12 16:56:04,536 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-12 16:56:04,536 Now training epoch 7. LR=0.000351
2022-01-12 16:57:45,523 Epoch[007/300], Step[0000/1252], Avg Loss: 4.9940, Avg Acc: 0.0791
2022-01-12 16:59:10,721 Epoch[007/300], Step[0050/1252], Avg Loss: 5.3669, Avg Acc: 0.1159
2022-01-12 17:00:35,550 Epoch[007/300], Step[0100/1252], Avg Loss: 5.3452, Avg Acc: 0.1179
2022-01-12 17:02:00,829 Epoch[007/300], Step[0150/1252], Avg Loss: 5.3367, Avg Acc: 0.1171
2022-01-12 17:03:26,767 Epoch[007/300], Step[0200/1252], Avg Loss: 5.3265, Avg Acc: 0.1198
2022-01-12 17:04:52,544 Epoch[007/300], Step[0250/1252], Avg Loss: 5.3232, Avg Acc: 0.1210
2022-01-12 17:06:19,297 Epoch[007/300], Step[0300/1252], Avg Loss: 5.3177, Avg Acc: 0.1208
2022-01-12 17:07:45,607 Epoch[007/300], Step[0350/1252], Avg Loss: 5.3110, Avg Acc: 0.1219
2022-01-12 17:09:11,086 Epoch[007/300], Step[0400/1252], Avg Loss: 5.3176, Avg Acc: 0.1219
2022-01-12 17:10:36,750 Epoch[007/300], Step[0450/1252], Avg Loss: 5.3130, Avg Acc: 0.1226
2022-01-12 17:12:02,559 Epoch[007/300], Step[0500/1252], Avg Loss: 5.3056, Avg Acc: 0.1237
2022-01-12 17:13:27,312 Epoch[007/300], Step[0550/1252], Avg Loss: 5.3026, Avg Acc: 0.1240
2022-01-12 17:14:53,226 Epoch[007/300], Step[0600/1252], Avg Loss: 5.2995, Avg Acc: 0.1250
2022-01-12 17:16:19,625 Epoch[007/300], Step[0650/1252], Avg Loss: 5.2920, Avg Acc: 0.1264
2022-01-12 17:17:45,437 Epoch[007/300], Step[0700/1252], Avg Loss: 5.2879, Avg Acc: 0.1267
2022-01-12 17:19:10,283 Epoch[007/300], Step[0750/1252], Avg Loss: 5.2852, Avg Acc: 0.1273
2022-01-12 17:20:35,460 Epoch[007/300], Step[0800/1252], Avg Loss: 5.2823, Avg Acc: 0.1279
2022-01-12 17:22:00,964 Epoch[007/300], Step[0850/1252], Avg Loss: 5.2805, Avg Acc: 0.1283
2022-01-12 17:23:25,212 Epoch[007/300], Step[0900/1252], Avg Loss: 5.2745, Avg Acc: 0.1287
2022-01-12 17:24:51,214 Epoch[007/300], Step[0950/1252], Avg Loss: 5.2706, Avg Acc: 0.1292
2022-01-12 17:26:14,917 Epoch[007/300], Step[1000/1252], Avg Loss: 5.2666, Avg Acc: 0.1301
2022-01-12 17:27:40,052 Epoch[007/300], Step[1050/1252], Avg Loss: 5.2634, Avg Acc: 0.1306
2022-01-12 17:29:04,140 Epoch[007/300], Step[1100/1252], Avg Loss: 5.2626, Avg Acc: 0.1306
2022-01-12 17:30:29,863 Epoch[007/300], Step[1150/1252], Avg Loss: 5.2605, Avg Acc: 0.1309
2022-01-12 17:31:54,122 Epoch[007/300], Step[1200/1252], Avg Loss: 5.2557, Avg Acc: 0.1316
2022-01-12 17:33:20,263 Epoch[007/300], Step[1250/1252], Avg Loss: 5.2518, Avg Acc: 0.1322
2022-01-12 17:33:27,326 ----- Epoch[007/300], Train Loss: 5.2518, Train Acc: 0.1322, time: 2242.79, Best Val(epoch6) Acc@1: 0.2575
2022-01-12 17:33:27,326 Now training epoch 8. LR=0.000401
2022-01-12 17:35:13,763 Epoch[008/300], Step[0000/1252], Avg Loss: 4.9014, Avg Acc: 0.1387
2022-01-12 17:36:37,379 Epoch[008/300], Step[0050/1252], Avg Loss: 5.1593, Avg Acc: 0.1587
2022-01-12 17:38:02,693 Epoch[008/300], Step[0100/1252], Avg Loss: 5.1772, Avg Acc: 0.1454
2022-01-12 17:39:27,473 Epoch[008/300], Step[0150/1252], Avg Loss: 5.1620, Avg Acc: 0.1429
2022-01-12 17:40:52,284 Epoch[008/300], Step[0200/1252], Avg Loss: 5.1508, Avg Acc: 0.1432
2022-01-12 17:42:17,972 Epoch[008/300], Step[0250/1252], Avg Loss: 5.1376, Avg Acc: 0.1445
2022-01-12 17:43:43,502 Epoch[008/300], Step[0300/1252], Avg Loss: 5.1278, Avg Acc: 0.1470
2022-01-12 17:45:08,919 Epoch[008/300], Step[0350/1252], Avg Loss: 5.1221, Avg Acc: 0.1478
2022-01-12 17:46:34,643 Epoch[008/300], Step[0400/1252], Avg Loss: 5.1229, Avg Acc: 0.1490
2022-01-12 17:48:00,544 Epoch[008/300], Step[0450/1252], Avg Loss: 5.1173, Avg Acc: 0.1507
2022-01-12 17:49:25,762 Epoch[008/300], Step[0500/1252], Avg Loss: 5.1139, Avg Acc: 0.1512
2022-01-12 17:50:51,522 Epoch[008/300], Step[0550/1252], Avg Loss: 5.1105, Avg Acc: 0.1499
2022-01-12 17:52:16,267 Epoch[008/300], Step[0600/1252], Avg Loss: 5.1095, Avg Acc: 0.1515
2022-01-12 17:53:42,430 Epoch[008/300], Step[0650/1252], Avg Loss: 5.1112, Avg Acc: 0.1520
2022-01-12 17:55:08,653 Epoch[008/300], Step[0700/1252], Avg Loss: 5.1093, Avg Acc: 0.1525
2022-01-12 17:56:34,322 Epoch[008/300], Step[0750/1252], Avg Loss: 5.1052, Avg Acc: 0.1533
2022-01-12 17:57:59,815 Epoch[008/300], Step[0800/1252], Avg Loss: 5.0975, Avg Acc: 0.1544
2022-01-12 17:59:25,615 Epoch[008/300], Step[0850/1252], Avg Loss: 5.0941, Avg Acc: 0.1551
2022-01-12 18:00:51,363 Epoch[008/300], Step[0900/1252], Avg Loss: 5.0916, Avg Acc: 0.1554
2022-01-12 18:02:17,805 Epoch[008/300], Step[0950/1252], Avg Loss: 5.0902, Avg Acc: 0.1553
2022-01-12 18:03:42,271 Epoch[008/300], Step[1000/1252], Avg Loss: 5.0888, Avg Acc: 0.1553
2022-01-12 18:05:07,821 Epoch[008/300], Step[1050/1252], Avg Loss: 5.0862, Avg Acc: 0.1550
2022-01-12 18:06:33,032 Epoch[008/300], Step[1100/1252], Avg Loss: 5.0814, Avg Acc: 0.1563
2022-01-12 18:07:58,316 Epoch[008/300], Step[1150/1252], Avg Loss: 5.0777, Avg Acc: 0.1566
2022-01-12 18:09:25,080 Epoch[008/300], Step[1200/1252], Avg Loss: 5.0722, Avg Acc: 0.1570
2022-01-12 18:10:50,360 Epoch[008/300], Step[1250/1252], Avg Loss: 5.0663, Avg Acc: 0.1581
2022-01-12 18:10:56,869 ----- Epoch[008/300], Train Loss: 5.0663, Train Acc: 0.1581, time: 2249.54, Best Val(epoch6) Acc@1: 0.2575
2022-01-12 18:10:56,869 ----- Validation after Epoch: 8
2022-01-12 18:12:39,063 Val Step[0000/1563], Avg Loss: 3.0583, Avg Acc@1: 0.3438, Avg Acc@5: 0.6250
2022-01-12 18:12:40,875 Val Step[0050/1563], Avg Loss: 3.1043, Avg Acc@1: 0.3646, Avg Acc@5: 0.6225
2022-01-12 18:12:42,762 Val Step[0100/1563], Avg Loss: 3.0913, Avg Acc@1: 0.3666, Avg Acc@5: 0.6253
2022-01-12 18:12:44,600 Val Step[0150/1563], Avg Loss: 3.0864, Avg Acc@1: 0.3702, Avg Acc@5: 0.6254
2022-01-12 18:12:46,410 Val Step[0200/1563], Avg Loss: 3.0906, Avg Acc@1: 0.3696, Avg Acc@5: 0.6262
2022-01-12 18:12:48,309 Val Step[0250/1563], Avg Loss: 3.0686, Avg Acc@1: 0.3723, Avg Acc@5: 0.6286
2022-01-12 18:12:50,183 Val Step[0300/1563], Avg Loss: 3.0687, Avg Acc@1: 0.3718, Avg Acc@5: 0.6310
2022-01-12 18:12:51,976 Val Step[0350/1563], Avg Loss: 3.0689, Avg Acc@1: 0.3715, Avg Acc@5: 0.6318
2022-01-12 18:12:53,769 Val Step[0400/1563], Avg Loss: 3.0698, Avg Acc@1: 0.3723, Avg Acc@5: 0.6320
2022-01-12 18:12:55,571 Val Step[0450/1563], Avg Loss: 3.0758, Avg Acc@1: 0.3699, Avg Acc@5: 0.6300
2022-01-12 18:12:57,396 Val Step[0500/1563], Avg Loss: 3.0811, Avg Acc@1: 0.3685, Avg Acc@5: 0.6279
2022-01-12 18:12:59,222 Val Step[0550/1563], Avg Loss: 3.0881, Avg Acc@1: 0.3658, Avg Acc@5: 0.6266
2022-01-12 18:13:01,133 Val Step[0600/1563], Avg Loss: 3.0896, Avg Acc@1: 0.3662, Avg Acc@5: 0.6265
2022-01-12 18:13:02,936 Val Step[0650/1563], Avg Loss: 3.0911, Avg Acc@1: 0.3651, Avg Acc@5: 0.6262
2022-01-12 18:13:04,924 Val Step[0700/1563], Avg Loss: 3.0929, Avg Acc@1: 0.3650, Avg Acc@5: 0.6255
2022-01-12 18:13:07,156 Val Step[0750/1563], Avg Loss: 3.0975, Avg Acc@1: 0.3646, Avg Acc@5: 0.6255
2022-01-12 18:13:09,237 Val Step[0800/1563], Avg Loss: 3.0959, Avg Acc@1: 0.3642, Avg Acc@5: 0.6257
2022-01-12 18:13:11,297 Val Step[0850/1563], Avg Loss: 3.0937, Avg Acc@1: 0.3645, Avg Acc@5: 0.6256
2022-01-12 18:13:13,463 Val Step[0900/1563], Avg Loss: 3.0900, Avg Acc@1: 0.3655, Avg Acc@5: 0.6265
2022-01-12 18:13:15,548 Val Step[0950/1563], Avg Loss: 3.0902, Avg Acc@1: 0.3657, Avg Acc@5: 0.6261
2022-01-12 18:13:17,594 Val Step[1000/1563], Avg Loss: 3.0908, Avg Acc@1: 0.3654, Avg Acc@5: 0.6254
2022-01-12 18:13:19,635 Val Step[1050/1563], Avg Loss: 3.0938, Avg Acc@1: 0.3644, Avg Acc@5: 0.6248
2022-01-12 18:13:21,658 Val Step[1100/1563], Avg Loss: 3.0902, Avg Acc@1: 0.3646, Avg Acc@5: 0.6253
2022-01-12 18:13:23,688 Val Step[1150/1563], Avg Loss: 3.0893, Avg Acc@1: 0.3650, Avg Acc@5: 0.6254
2022-01-12 18:13:25,744 Val Step[1200/1563], Avg Loss: 3.0888, Avg Acc@1: 0.3648, Avg Acc@5: 0.6259
2022-01-12 18:13:27,840 Val Step[1250/1563], Avg Loss: 3.0862, Avg Acc@1: 0.3649, Avg Acc@5: 0.6261
2022-01-12 18:13:29,942 Val Step[1300/1563], Avg Loss: 3.0903, Avg Acc@1: 0.3643, Avg Acc@5: 0.6252
2022-01-12 18:13:31,989 Val Step[1350/1563], Avg Loss: 3.0900, Avg Acc@1: 0.3644, Avg Acc@5: 0.6246
2022-01-12 18:13:34,038 Val Step[1400/1563], Avg Loss: 3.0898, Avg Acc@1: 0.3638, Avg Acc@5: 0.6242
2022-01-12 18:13:36,118 Val Step[1450/1563], Avg Loss: 3.0908, Avg Acc@1: 0.3639, Avg Acc@5: 0.6236
2022-01-12 18:13:38,272 Val Step[1500/1563], Avg Loss: 3.0904, Avg Acc@1: 0.3642, Avg Acc@5: 0.6237
2022-01-12 18:13:40,309 Val Step[1550/1563], Avg Loss: 3.0894, Avg Acc@1: 0.3647, Avg Acc@5: 0.6237
2022-01-12 18:13:42,277 ----- Epoch[008/300], Validation Loss: 3.0892, Validation Acc@1: 0.3649, Validation Acc@5: 0.6237, time: 165.41
2022-01-12 18:13:43,593 the pre best model acc:0.2575, at epoch 6
2022-01-12 18:13:43,872 current best model acc:0.3649, at epoch 8
2022-01-12 18:13:43,873 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-12 18:13:43,873 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-12 18:13:43,873 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-12 18:13:43,873 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-12 18:13:43,873 Now training epoch 9. LR=0.000451
2022-01-12 18:15:28,967 Epoch[009/300], Step[0000/1252], Avg Loss: 5.1849, Avg Acc: 0.1084
2022-01-12 18:16:53,282 Epoch[009/300], Step[0050/1252], Avg Loss: 5.0443, Avg Acc: 0.1655
2022-01-12 18:18:17,688 Epoch[009/300], Step[0100/1252], Avg Loss: 5.0180, Avg Acc: 0.1696
2022-01-12 18:19:42,331 Epoch[009/300], Step[0150/1252], Avg Loss: 4.9990, Avg Acc: 0.1709
2022-01-12 18:21:04,949 Epoch[009/300], Step[0200/1252], Avg Loss: 4.9956, Avg Acc: 0.1690
2022-01-12 18:22:29,792 Epoch[009/300], Step[0250/1252], Avg Loss: 4.9864, Avg Acc: 0.1689
2022-01-12 18:23:54,270 Epoch[009/300], Step[0300/1252], Avg Loss: 4.9844, Avg Acc: 0.1709
2022-01-12 18:25:20,151 Epoch[009/300], Step[0350/1252], Avg Loss: 4.9857, Avg Acc: 0.1709
2022-01-12 18:26:46,024 Epoch[009/300], Step[0400/1252], Avg Loss: 4.9853, Avg Acc: 0.1703
2022-01-12 18:28:11,930 Epoch[009/300], Step[0450/1252], Avg Loss: 4.9774, Avg Acc: 0.1708
2022-01-12 18:29:36,992 Epoch[009/300], Step[0500/1252], Avg Loss: 4.9736, Avg Acc: 0.1705
2022-01-12 18:31:03,143 Epoch[009/300], Step[0550/1252], Avg Loss: 4.9664, Avg Acc: 0.1718
2022-01-12 18:32:28,745 Epoch[009/300], Step[0600/1252], Avg Loss: 4.9642, Avg Acc: 0.1734
2022-01-12 18:33:55,530 Epoch[009/300], Step[0650/1252], Avg Loss: 4.9622, Avg Acc: 0.1722
2022-01-12 18:35:21,569 Epoch[009/300], Step[0700/1252], Avg Loss: 4.9617, Avg Acc: 0.1720
2022-01-12 18:36:47,887 Epoch[009/300], Step[0750/1252], Avg Loss: 4.9582, Avg Acc: 0.1734
2022-01-12 18:38:13,284 Epoch[009/300], Step[0800/1252], Avg Loss: 4.9560, Avg Acc: 0.1750
2022-01-12 18:39:39,134 Epoch[009/300], Step[0850/1252], Avg Loss: 4.9549, Avg Acc: 0.1754
2022-01-12 18:41:05,254 Epoch[009/300], Step[0900/1252], Avg Loss: 4.9505, Avg Acc: 0.1768
2022-01-12 18:42:31,721 Epoch[009/300], Step[0950/1252], Avg Loss: 4.9441, Avg Acc: 0.1778
2022-01-12 18:43:58,273 Epoch[009/300], Step[1000/1252], Avg Loss: 4.9428, Avg Acc: 0.1777
2022-01-12 18:45:24,860 Epoch[009/300], Step[1050/1252], Avg Loss: 4.9380, Avg Acc: 0.1787
2022-01-12 18:46:51,568 Epoch[009/300], Step[1100/1252], Avg Loss: 4.9339, Avg Acc: 0.1792
2022-01-12 18:48:18,304 Epoch[009/300], Step[1150/1252], Avg Loss: 4.9300, Avg Acc: 0.1793
2022-01-12 18:49:43,807 Epoch[009/300], Step[1200/1252], Avg Loss: 4.9255, Avg Acc: 0.1796
2022-01-12 18:51:11,853 Epoch[009/300], Step[1250/1252], Avg Loss: 4.9208, Avg Acc: 0.1805
2022-01-12 18:51:19,236 ----- Epoch[009/300], Train Loss: 4.9208, Train Acc: 0.1805, time: 2255.36, Best Val(epoch8) Acc@1: 0.3649
2022-01-12 18:51:19,236 Now training epoch 10. LR=0.000501
2022-01-12 18:53:05,808 Epoch[010/300], Step[0000/1252], Avg Loss: 5.1403, Avg Acc: 0.2539
2022-01-12 18:54:30,560 Epoch[010/300], Step[0050/1252], Avg Loss: 4.9250, Avg Acc: 0.1747
2022-01-12 18:55:56,436 Epoch[010/300], Step[0100/1252], Avg Loss: 4.9069, Avg Acc: 0.1823
2022-01-12 18:57:22,785 Epoch[010/300], Step[0150/1252], Avg Loss: 4.8889, Avg Acc: 0.1837
2022-01-12 18:58:48,046 Epoch[010/300], Step[0200/1252], Avg Loss: 4.8911, Avg Acc: 0.1808
2022-01-12 19:00:12,335 Epoch[010/300], Step[0250/1252], Avg Loss: 4.8838, Avg Acc: 0.1837
2022-01-12 19:01:37,550 Epoch[010/300], Step[0300/1252], Avg Loss: 4.8749, Avg Acc: 0.1832
2022-01-12 19:03:02,796 Epoch[010/300], Step[0350/1252], Avg Loss: 4.8671, Avg Acc: 0.1856
2022-01-12 19:04:28,672 Epoch[010/300], Step[0400/1252], Avg Loss: 4.8612, Avg Acc: 0.1861
2022-01-12 19:05:54,444 Epoch[010/300], Step[0450/1252], Avg Loss: 4.8576, Avg Acc: 0.1878
2022-01-12 19:07:19,819 Epoch[010/300], Step[0500/1252], Avg Loss: 4.8510, Avg Acc: 0.1899
2022-01-12 19:08:44,139 Epoch[010/300], Step[0550/1252], Avg Loss: 4.8469, Avg Acc: 0.1914
2022-01-12 19:10:09,906 Epoch[010/300], Step[0600/1252], Avg Loss: 4.8436, Avg Acc: 0.1918
2022-01-12 19:11:35,780 Epoch[010/300], Step[0650/1252], Avg Loss: 4.8447, Avg Acc: 0.1909
2022-01-12 19:13:01,084 Epoch[010/300], Step[0700/1252], Avg Loss: 4.8437, Avg Acc: 0.1912
2022-01-12 19:14:26,504 Epoch[010/300], Step[0750/1252], Avg Loss: 4.8359, Avg Acc: 0.1905
2022-01-12 19:15:51,141 Epoch[010/300], Step[0800/1252], Avg Loss: 4.8324, Avg Acc: 0.1921
2022-01-12 19:17:15,086 Epoch[010/300], Step[0850/1252], Avg Loss: 4.8270, Avg Acc: 0.1931
2022-01-12 19:18:40,773 Epoch[010/300], Step[0900/1252], Avg Loss: 4.8235, Avg Acc: 0.1943
2022-01-12 19:20:07,252 Epoch[010/300], Step[0950/1252], Avg Loss: 4.8202, Avg Acc: 0.1943
2022-01-12 19:21:33,499 Epoch[010/300], Step[1000/1252], Avg Loss: 4.8184, Avg Acc: 0.1934
2022-01-12 19:22:59,427 Epoch[010/300], Step[1050/1252], Avg Loss: 4.8155, Avg Acc: 0.1933
2022-01-12 19:24:25,034 Epoch[010/300], Step[1100/1252], Avg Loss: 4.8126, Avg Acc: 0.1937
2022-01-12 19:25:51,102 Epoch[010/300], Step[1150/1252], Avg Loss: 4.8091, Avg Acc: 0.1941
2022-01-12 19:27:16,200 Epoch[010/300], Step[1200/1252], Avg Loss: 4.8076, Avg Acc: 0.1950
2022-01-12 19:28:42,975 Epoch[010/300], Step[1250/1252], Avg Loss: 4.8060, Avg Acc: 0.1954
2022-01-12 19:28:50,598 ----- Epoch[010/300], Train Loss: 4.8060, Train Acc: 0.1954, time: 2251.36, Best Val(epoch8) Acc@1: 0.3649
2022-01-12 19:28:50,599 ----- Validation after Epoch: 10
2022-01-12 19:30:12,527 Val Step[0000/1563], Avg Loss: 2.4742, Avg Acc@1: 0.4375, Avg Acc@5: 0.6562
2022-01-12 19:30:14,384 Val Step[0050/1563], Avg Loss: 2.7132, Avg Acc@1: 0.4295, Avg Acc@5: 0.6942
2022-01-12 19:30:16,331 Val Step[0100/1563], Avg Loss: 2.7313, Avg Acc@1: 0.4248, Avg Acc@5: 0.6928
2022-01-12 19:30:18,264 Val Step[0150/1563], Avg Loss: 2.7294, Avg Acc@1: 0.4259, Avg Acc@5: 0.6892
2022-01-12 19:30:20,267 Val Step[0200/1563], Avg Loss: 2.7343, Avg Acc@1: 0.4246, Avg Acc@5: 0.6880
2022-01-12 19:30:22,204 Val Step[0250/1563], Avg Loss: 2.7206, Avg Acc@1: 0.4288, Avg Acc@5: 0.6877
2022-01-12 19:30:24,057 Val Step[0300/1563], Avg Loss: 2.7176, Avg Acc@1: 0.4319, Avg Acc@5: 0.6907
2022-01-12 19:30:25,933 Val Step[0350/1563], Avg Loss: 2.7178, Avg Acc@1: 0.4322, Avg Acc@5: 0.6893
2022-01-12 19:30:27,823 Val Step[0400/1563], Avg Loss: 2.7153, Avg Acc@1: 0.4330, Avg Acc@5: 0.6908
2022-01-12 19:30:29,734 Val Step[0450/1563], Avg Loss: 2.7166, Avg Acc@1: 0.4319, Avg Acc@5: 0.6919
2022-01-12 19:30:31,671 Val Step[0500/1563], Avg Loss: 2.7213, Avg Acc@1: 0.4306, Avg Acc@5: 0.6908
2022-01-12 19:30:33,604 Val Step[0550/1563], Avg Loss: 2.7238, Avg Acc@1: 0.4281, Avg Acc@5: 0.6911
2022-01-12 19:30:35,530 Val Step[0600/1563], Avg Loss: 2.7262, Avg Acc@1: 0.4279, Avg Acc@5: 0.6901
2022-01-12 19:30:37,401 Val Step[0650/1563], Avg Loss: 2.7255, Avg Acc@1: 0.4274, Avg Acc@5: 0.6901
2022-01-12 19:30:39,247 Val Step[0700/1563], Avg Loss: 2.7275, Avg Acc@1: 0.4261, Avg Acc@5: 0.6899
2022-01-12 19:30:41,083 Val Step[0750/1563], Avg Loss: 2.7319, Avg Acc@1: 0.4260, Avg Acc@5: 0.6887
2022-01-12 19:30:42,918 Val Step[0800/1563], Avg Loss: 2.7328, Avg Acc@1: 0.4254, Avg Acc@5: 0.6887
2022-01-12 19:30:44,798 Val Step[0850/1563], Avg Loss: 2.7316, Avg Acc@1: 0.4264, Avg Acc@5: 0.6889
2022-01-12 19:30:46,742 Val Step[0900/1563], Avg Loss: 2.7284, Avg Acc@1: 0.4273, Avg Acc@5: 0.6895
2022-01-12 19:30:48,922 Val Step[0950/1563], Avg Loss: 2.7281, Avg Acc@1: 0.4271, Avg Acc@5: 0.6894
2022-01-12 19:30:51,109 Val Step[1000/1563], Avg Loss: 2.7278, Avg Acc@1: 0.4271, Avg Acc@5: 0.6896
2022-01-12 19:30:53,264 Val Step[1050/1563], Avg Loss: 2.7333, Avg Acc@1: 0.4263, Avg Acc@5: 0.6887
2022-01-12 19:30:55,361 Val Step[1100/1563], Avg Loss: 2.7318, Avg Acc@1: 0.4264, Avg Acc@5: 0.6893
2022-01-12 19:30:57,428 Val Step[1150/1563], Avg Loss: 2.7283, Avg Acc@1: 0.4275, Avg Acc@5: 0.6898
2022-01-12 19:30:59,485 Val Step[1200/1563], Avg Loss: 2.7267, Avg Acc@1: 0.4275, Avg Acc@5: 0.6902
2022-01-12 19:31:01,531 Val Step[1250/1563], Avg Loss: 2.7249, Avg Acc@1: 0.4273, Avg Acc@5: 0.6902
2022-01-12 19:31:03,577 Val Step[1300/1563], Avg Loss: 2.7288, Avg Acc@1: 0.4263, Avg Acc@5: 0.6895
2022-01-12 19:31:05,623 Val Step[1350/1563], Avg Loss: 2.7287, Avg Acc@1: 0.4260, Avg Acc@5: 0.6894
2022-01-12 19:31:07,453 Val Step[1400/1563], Avg Loss: 2.7286, Avg Acc@1: 0.4261, Avg Acc@5: 0.6889
2022-01-12 19:31:09,342 Val Step[1450/1563], Avg Loss: 2.7291, Avg Acc@1: 0.4259, Avg Acc@5: 0.6886
2022-01-12 19:31:11,196 Val Step[1500/1563], Avg Loss: 2.7285, Avg Acc@1: 0.4263, Avg Acc@5: 0.6883
2022-01-12 19:31:12,920 Val Step[1550/1563], Avg Loss: 2.7279, Avg Acc@1: 0.4267, Avg Acc@5: 0.6884
2022-01-12 19:31:14,781 ----- Epoch[010/300], Validation Loss: 2.7281, Validation Acc@1: 0.4268, Validation Acc@5: 0.6884, time: 144.18
2022-01-12 19:31:16,104 the pre best model acc:0.3649, at epoch 8
2022-01-12 19:31:16,371 current best model acc:0.4268, at epoch 10
2022-01-12 19:31:16,372 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-12 19:31:16,372 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-12 19:31:16,372 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-12 19:31:16,372 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-12 19:31:17,014 ----- Save model: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-10-Loss-4.776624126235904.pdparams
2022-01-12 19:31:17,015 ----- Save optim: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-10-Loss-4.776624126235904.pdopt
2022-01-12 19:31:17,015 Now training epoch 11. LR=0.000550
2022-01-12 19:33:05,115 Epoch[011/300], Step[0000/1252], Avg Loss: 5.0297, Avg Acc: 0.2324
2022-01-12 19:34:31,866 Epoch[011/300], Step[0050/1252], Avg Loss: 4.7603, Avg Acc: 0.2025
2022-01-12 19:35:57,416 Epoch[011/300], Step[0100/1252], Avg Loss: 4.7654, Avg Acc: 0.1955
2022-01-12 19:37:23,017 Epoch[011/300], Step[0150/1252], Avg Loss: 4.7644, Avg Acc: 0.1947
2022-01-12 19:38:48,469 Epoch[011/300], Step[0200/1252], Avg Loss: 4.7687, Avg Acc: 0.1991
2022-01-12 19:40:14,340 Epoch[011/300], Step[0250/1252], Avg Loss: 4.7519, Avg Acc: 0.2001
2022-01-12 19:41:39,685 Epoch[011/300], Step[0300/1252], Avg Loss: 4.7466, Avg Acc: 0.2002
2022-01-12 19:43:04,942 Epoch[011/300], Step[0350/1252], Avg Loss: 4.7359, Avg Acc: 0.2023
2022-01-12 19:44:30,242 Epoch[011/300], Step[0400/1252], Avg Loss: 4.7422, Avg Acc: 0.2026
2022-01-12 19:45:55,251 Epoch[011/300], Step[0450/1252], Avg Loss: 4.7362, Avg Acc: 0.2052
2022-01-12 19:47:20,519 Epoch[011/300], Step[0500/1252], Avg Loss: 4.7318, Avg Acc: 0.2059
2022-01-12 19:48:44,587 Epoch[011/300], Step[0550/1252], Avg Loss: 4.7281, Avg Acc: 0.2073
2022-01-12 19:50:10,501 Epoch[011/300], Step[0600/1252], Avg Loss: 4.7266, Avg Acc: 0.2072
2022-01-12 19:51:35,760 Epoch[011/300], Step[0650/1252], Avg Loss: 4.7225, Avg Acc: 0.2069
2022-01-12 19:53:00,878 Epoch[011/300], Step[0700/1252], Avg Loss: 4.7212, Avg Acc: 0.2079
2022-01-12 19:54:26,464 Epoch[011/300], Step[0750/1252], Avg Loss: 4.7240, Avg Acc: 0.2075
2022-01-12 19:55:51,913 Epoch[011/300], Step[0800/1252], Avg Loss: 4.7178, Avg Acc: 0.2088
2022-01-12 19:57:18,451 Epoch[011/300], Step[0850/1252], Avg Loss: 4.7162, Avg Acc: 0.2089
2022-01-12 19:58:45,321 Epoch[011/300], Step[0900/1252], Avg Loss: 4.7122, Avg Acc: 0.2094
2022-01-12 20:00:12,107 Epoch[011/300], Step[0950/1252], Avg Loss: 4.7127, Avg Acc: 0.2087
2022-01-12 20:01:37,902 Epoch[011/300], Step[1000/1252], Avg Loss: 4.7081, Avg Acc: 0.2100
2022-01-12 20:03:03,384 Epoch[011/300], Step[1050/1252], Avg Loss: 4.7019, Avg Acc: 0.2112
2022-01-12 20:04:29,647 Epoch[011/300], Step[1100/1252], Avg Loss: 4.6960, Avg Acc: 0.2124
2022-01-12 20:05:53,832 Epoch[011/300], Step[1150/1252], Avg Loss: 4.6918, Avg Acc: 0.2134
2022-01-12 20:07:20,028 Epoch[011/300], Step[1200/1252], Avg Loss: 4.6903, Avg Acc: 0.2137
2022-01-12 20:08:45,486 Epoch[011/300], Step[1250/1252], Avg Loss: 4.6888, Avg Acc: 0.2149
2022-01-12 20:08:52,564 ----- Epoch[011/300], Train Loss: 4.6888, Train Acc: 0.2149, time: 2255.54, Best Val(epoch10) Acc@1: 0.4268
2022-01-12 20:08:52,564 Now training epoch 12. LR=0.000600
2022-01-12 20:10:46,005 Epoch[012/300], Step[0000/1252], Avg Loss: 4.3368, Avg Acc: 0.1416
2022-01-12 20:12:11,048 Epoch[012/300], Step[0050/1252], Avg Loss: 4.5992, Avg Acc: 0.2235
2022-01-12 20:13:36,399 Epoch[012/300], Step[0100/1252], Avg Loss: 4.6071, Avg Acc: 0.2300
2022-01-12 20:15:02,432 Epoch[012/300], Step[0150/1252], Avg Loss: 4.6056, Avg Acc: 0.2253
2022-01-12 20:16:28,712 Epoch[012/300], Step[0200/1252], Avg Loss: 4.5991, Avg Acc: 0.2246
2022-01-12 20:17:54,538 Epoch[012/300], Step[0250/1252], Avg Loss: 4.6022, Avg Acc: 0.2243
2022-01-12 20:19:20,078 Epoch[012/300], Step[0300/1252], Avg Loss: 4.6001, Avg Acc: 0.2259
2022-01-12 20:20:46,128 Epoch[012/300], Step[0350/1252], Avg Loss: 4.6036, Avg Acc: 0.2255
2022-01-12 20:22:11,691 Epoch[012/300], Step[0400/1252], Avg Loss: 4.6062, Avg Acc: 0.2251
2022-01-12 20:23:37,537 Epoch[012/300], Step[0450/1252], Avg Loss: 4.6107, Avg Acc: 0.2243
2022-01-12 20:25:02,371 Epoch[012/300], Step[0500/1252], Avg Loss: 4.6084, Avg Acc: 0.2241
2022-01-12 20:26:27,572 Epoch[012/300], Step[0550/1252], Avg Loss: 4.6024, Avg Acc: 0.2249
2022-01-12 20:27:52,873 Epoch[012/300], Step[0600/1252], Avg Loss: 4.6048, Avg Acc: 0.2258
2022-01-12 20:29:18,664 Epoch[012/300], Step[0650/1252], Avg Loss: 4.6052, Avg Acc: 0.2267
2022-01-12 20:30:45,180 Epoch[012/300], Step[0700/1252], Avg Loss: 4.6071, Avg Acc: 0.2262
2022-01-12 20:32:11,546 Epoch[012/300], Step[0750/1252], Avg Loss: 4.6097, Avg Acc: 0.2251
2022-01-12 20:33:37,609 Epoch[012/300], Step[0800/1252], Avg Loss: 4.6078, Avg Acc: 0.2250
2022-01-12 20:35:03,078 Epoch[012/300], Step[0850/1252], Avg Loss: 4.6081, Avg Acc: 0.2249
2022-01-12 20:36:28,531 Epoch[012/300], Step[0900/1252], Avg Loss: 4.6060, Avg Acc: 0.2255
2022-01-12 20:37:54,464 Epoch[012/300], Step[0950/1252], Avg Loss: 4.5997, Avg Acc: 0.2260
2022-01-12 20:39:21,176 Epoch[012/300], Step[1000/1252], Avg Loss: 4.6004, Avg Acc: 0.2265
2022-01-12 20:40:46,925 Epoch[012/300], Step[1050/1252], Avg Loss: 4.5981, Avg Acc: 0.2269
2022-01-12 20:42:13,518 Epoch[012/300], Step[1100/1252], Avg Loss: 4.5991, Avg Acc: 0.2266
2022-01-12 20:43:40,033 Epoch[012/300], Step[1150/1252], Avg Loss: 4.5978, Avg Acc: 0.2264
2022-01-12 20:45:05,899 Epoch[012/300], Step[1200/1252], Avg Loss: 4.5930, Avg Acc: 0.2271
2022-01-12 20:46:33,192 Epoch[012/300], Step[1250/1252], Avg Loss: 4.5929, Avg Acc: 0.2276
2022-01-12 20:46:40,262 ----- Epoch[012/300], Train Loss: 4.5929, Train Acc: 0.2276, time: 2267.69, Best Val(epoch10) Acc@1: 0.4268
2022-01-12 20:46:40,262 ----- Validation after Epoch: 12
2022-01-12 20:47:55,014 Val Step[0000/1563], Avg Loss: 2.3457, Avg Acc@1: 0.5625, Avg Acc@5: 0.7188
2022-01-12 20:47:56,921 Val Step[0050/1563], Avg Loss: 2.4510, Avg Acc@1: 0.4755, Avg Acc@5: 0.7408
2022-01-12 20:47:58,792 Val Step[0100/1563], Avg Loss: 2.4711, Avg Acc@1: 0.4718, Avg Acc@5: 0.7376
2022-01-12 20:48:00,610 Val Step[0150/1563], Avg Loss: 2.4523, Avg Acc@1: 0.4770, Avg Acc@5: 0.7407
2022-01-12 20:48:02,408 Val Step[0200/1563], Avg Loss: 2.4554, Avg Acc@1: 0.4778, Avg Acc@5: 0.7388
2022-01-12 20:48:04,375 Val Step[0250/1563], Avg Loss: 2.4361, Avg Acc@1: 0.4844, Avg Acc@5: 0.7432
2022-01-12 20:48:06,319 Val Step[0300/1563], Avg Loss: 2.4292, Avg Acc@1: 0.4865, Avg Acc@5: 0.7441
2022-01-12 20:48:08,304 Val Step[0350/1563], Avg Loss: 2.4354, Avg Acc@1: 0.4887, Avg Acc@5: 0.7420
2022-01-12 20:48:10,189 Val Step[0400/1563], Avg Loss: 2.4358, Avg Acc@1: 0.4886, Avg Acc@5: 0.7428
2022-01-12 20:48:12,073 Val Step[0450/1563], Avg Loss: 2.4410, Avg Acc@1: 0.4865, Avg Acc@5: 0.7404
2022-01-12 20:48:13,923 Val Step[0500/1563], Avg Loss: 2.4417, Avg Acc@1: 0.4847, Avg Acc@5: 0.7382
2022-01-12 20:48:15,772 Val Step[0550/1563], Avg Loss: 2.4423, Avg Acc@1: 0.4851, Avg Acc@5: 0.7381
2022-01-12 20:48:17,611 Val Step[0600/1563], Avg Loss: 2.4439, Avg Acc@1: 0.4846, Avg Acc@5: 0.7380
2022-01-12 20:48:19,499 Val Step[0650/1563], Avg Loss: 2.4447, Avg Acc@1: 0.4839, Avg Acc@5: 0.7371
2022-01-12 20:48:21,369 Val Step[0700/1563], Avg Loss: 2.4434, Avg Acc@1: 0.4834, Avg Acc@5: 0.7377
2022-01-12 20:48:23,266 Val Step[0750/1563], Avg Loss: 2.4511, Avg Acc@1: 0.4817, Avg Acc@5: 0.7359
2022-01-12 20:48:25,212 Val Step[0800/1563], Avg Loss: 2.4511, Avg Acc@1: 0.4821, Avg Acc@5: 0.7361
2022-01-12 20:48:27,111 Val Step[0850/1563], Avg Loss: 2.4506, Avg Acc@1: 0.4820, Avg Acc@5: 0.7365
2022-01-12 20:48:28,946 Val Step[0900/1563], Avg Loss: 2.4471, Avg Acc@1: 0.4830, Avg Acc@5: 0.7364
2022-01-12 20:48:30,764 Val Step[0950/1563], Avg Loss: 2.4468, Avg Acc@1: 0.4828, Avg Acc@5: 0.7368
2022-01-12 20:48:32,653 Val Step[1000/1563], Avg Loss: 2.4465, Avg Acc@1: 0.4823, Avg Acc@5: 0.7368
2022-01-12 20:48:34,539 Val Step[1050/1563], Avg Loss: 2.4514, Avg Acc@1: 0.4813, Avg Acc@5: 0.7357
2022-01-12 20:48:36,397 Val Step[1100/1563], Avg Loss: 2.4506, Avg Acc@1: 0.4815, Avg Acc@5: 0.7357
2022-01-12 20:48:38,199 Val Step[1150/1563], Avg Loss: 2.4483, Avg Acc@1: 0.4816, Avg Acc@5: 0.7359
2022-01-12 20:48:40,110 Val Step[1200/1563], Avg Loss: 2.4486, Avg Acc@1: 0.4814, Avg Acc@5: 0.7361
2022-01-12 20:48:42,024 Val Step[1250/1563], Avg Loss: 2.4466, Avg Acc@1: 0.4816, Avg Acc@5: 0.7364
2022-01-12 20:48:43,966 Val Step[1300/1563], Avg Loss: 2.4501, Avg Acc@1: 0.4807, Avg Acc@5: 0.7356
2022-01-12 20:48:45,768 Val Step[1350/1563], Avg Loss: 2.4500, Avg Acc@1: 0.4804, Avg Acc@5: 0.7352
2022-01-12 20:48:47,572 Val Step[1400/1563], Avg Loss: 2.4500, Avg Acc@1: 0.4799, Avg Acc@5: 0.7353
2022-01-12 20:48:49,478 Val Step[1450/1563], Avg Loss: 2.4507, Avg Acc@1: 0.4799, Avg Acc@5: 0.7350
2022-01-12 20:48:51,365 Val Step[1500/1563], Avg Loss: 2.4500, Avg Acc@1: 0.4803, Avg Acc@5: 0.7355
2022-01-12 20:48:53,088 Val Step[1550/1563], Avg Loss: 2.4504, Avg Acc@1: 0.4804, Avg Acc@5: 0.7354
2022-01-12 20:48:55,019 ----- Epoch[012/300], Validation Loss: 2.4506, Validation Acc@1: 0.4803, Validation Acc@5: 0.7355, time: 134.75
2022-01-12 20:48:56,342 the pre best model acc:0.4268, at epoch 10
2022-01-12 20:48:56,617 current best model acc:0.4803, at epoch 12
2022-01-12 20:48:56,618 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-12 20:48:56,618 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-12 20:48:56,618 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-12 20:48:56,618 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-12 20:48:56,618 Now training epoch 13. LR=0.000650
2022-01-12 20:50:41,349 Epoch[013/300], Step[0000/1252], Avg Loss: 4.5228, Avg Acc: 0.2354
2022-01-12 20:52:06,078 Epoch[013/300], Step[0050/1252], Avg Loss: 4.5323, Avg Acc: 0.2543
2022-01-12 20:53:31,663 Epoch[013/300], Step[0100/1252], Avg Loss: 4.5738, Avg Acc: 0.2239
2022-01-12 20:54:56,481 Epoch[013/300], Step[0150/1252], Avg Loss: 4.5711, Avg Acc: 0.2285
2022-01-12 20:56:21,201 Epoch[013/300], Step[0200/1252], Avg Loss: 4.5612, Avg Acc: 0.2315
2022-01-12 20:57:45,479 Epoch[013/300], Step[0250/1252], Avg Loss: 4.5730, Avg Acc: 0.2281
2022-01-12 20:59:11,275 Epoch[013/300], Step[0300/1252], Avg Loss: 4.5727, Avg Acc: 0.2271
2022-01-12 21:00:35,648 Epoch[013/300], Step[0350/1252], Avg Loss: 4.5767, Avg Acc: 0.2275
2022-01-12 21:02:00,664 Epoch[013/300], Step[0400/1252], Avg Loss: 4.5691, Avg Acc: 0.2299
2022-01-12 21:03:25,907 Epoch[013/300], Step[0450/1252], Avg Loss: 4.5581, Avg Acc: 0.2317
2022-01-12 21:04:51,025 Epoch[013/300], Step[0500/1252], Avg Loss: 4.5540, Avg Acc: 0.2338
2022-01-12 21:06:16,878 Epoch[013/300], Step[0550/1252], Avg Loss: 4.5610, Avg Acc: 0.2316
2022-01-12 21:07:43,550 Epoch[013/300], Step[0600/1252], Avg Loss: 4.5581, Avg Acc: 0.2306
2022-01-12 21:09:09,472 Epoch[013/300], Step[0650/1252], Avg Loss: 4.5580, Avg Acc: 0.2301
2022-01-12 21:10:35,044 Epoch[013/300], Step[0700/1252], Avg Loss: 4.5558, Avg Acc: 0.2304
2022-01-12 21:11:59,637 Epoch[013/300], Step[0750/1252], Avg Loss: 4.5584, Avg Acc: 0.2313
2022-01-12 21:13:25,494 Epoch[013/300], Step[0800/1252], Avg Loss: 4.5529, Avg Acc: 0.2327
2022-01-12 21:14:51,342 Epoch[013/300], Step[0850/1252], Avg Loss: 4.5507, Avg Acc: 0.2330
2022-01-12 21:16:17,505 Epoch[013/300], Step[0900/1252], Avg Loss: 4.5494, Avg Acc: 0.2324
2022-01-12 21:17:43,937 Epoch[013/300], Step[0950/1252], Avg Loss: 4.5457, Avg Acc: 0.2324
2022-01-12 21:19:09,566 Epoch[013/300], Step[1000/1252], Avg Loss: 4.5437, Avg Acc: 0.2336
2022-01-12 21:20:36,014 Epoch[013/300], Step[1050/1252], Avg Loss: 4.5429, Avg Acc: 0.2332
2022-01-12 21:22:01,323 Epoch[013/300], Step[1100/1252], Avg Loss: 4.5427, Avg Acc: 0.2330
2022-01-12 21:23:28,039 Epoch[013/300], Step[1150/1252], Avg Loss: 4.5413, Avg Acc: 0.2338
2022-01-12 21:24:55,076 Epoch[013/300], Step[1200/1252], Avg Loss: 4.5404, Avg Acc: 0.2338
2022-01-12 21:26:20,758 Epoch[013/300], Step[1250/1252], Avg Loss: 4.5368, Avg Acc: 0.2342
2022-01-12 21:26:27,970 ----- Epoch[013/300], Train Loss: 4.5368, Train Acc: 0.2342, time: 2251.35, Best Val(epoch12) Acc@1: 0.4803
2022-01-12 21:26:27,970 Now training epoch 14. LR=0.000700
2022-01-12 21:28:18,876 Epoch[014/300], Step[0000/1252], Avg Loss: 4.7796, Avg Acc: 0.2275
2022-01-12 21:29:44,870 Epoch[014/300], Step[0050/1252], Avg Loss: 4.4608, Avg Acc: 0.2466
2022-01-12 21:31:10,147 Epoch[014/300], Step[0100/1252], Avg Loss: 4.4628, Avg Acc: 0.2432
2022-01-12 21:32:34,838 Epoch[014/300], Step[0150/1252], Avg Loss: 4.4453, Avg Acc: 0.2391
2022-01-12 21:33:59,610 Epoch[014/300], Step[0200/1252], Avg Loss: 4.4567, Avg Acc: 0.2436
2022-01-12 21:35:24,789 Epoch[014/300], Step[0250/1252], Avg Loss: 4.4639, Avg Acc: 0.2424
2022-01-12 21:36:49,213 Epoch[014/300], Step[0300/1252], Avg Loss: 4.4632, Avg Acc: 0.2413
2022-01-12 21:38:15,215 Epoch[014/300], Step[0350/1252], Avg Loss: 4.4676, Avg Acc: 0.2406
2022-01-12 21:39:40,479 Epoch[014/300], Step[0400/1252], Avg Loss: 4.4678, Avg Acc: 0.2439
2022-01-12 21:41:05,804 Epoch[014/300], Step[0450/1252], Avg Loss: 4.4719, Avg Acc: 0.2440
2022-01-12 21:42:31,226 Epoch[014/300], Step[0500/1252], Avg Loss: 4.4732, Avg Acc: 0.2448
2022-01-12 21:43:57,367 Epoch[014/300], Step[0550/1252], Avg Loss: 4.4728, Avg Acc: 0.2462
2022-01-12 21:45:23,588 Epoch[014/300], Step[0600/1252], Avg Loss: 4.4801, Avg Acc: 0.2459
2022-01-12 21:46:48,970 Epoch[014/300], Step[0650/1252], Avg Loss: 4.4731, Avg Acc: 0.2461
2022-01-12 21:48:15,564 Epoch[014/300], Step[0700/1252], Avg Loss: 4.4757, Avg Acc: 0.2458
2022-01-12 21:49:40,431 Epoch[014/300], Step[0750/1252], Avg Loss: 4.4743, Avg Acc: 0.2465
2022-01-12 21:51:05,502 Epoch[014/300], Step[0800/1252], Avg Loss: 4.4753, Avg Acc: 0.2454
2022-01-12 21:52:30,874 Epoch[014/300], Step[0850/1252], Avg Loss: 4.4700, Avg Acc: 0.2448
2022-01-12 21:53:56,179 Epoch[014/300], Step[0900/1252], Avg Loss: 4.4714, Avg Acc: 0.2443
2022-01-12 21:55:21,693 Epoch[014/300], Step[0950/1252], Avg Loss: 4.4674, Avg Acc: 0.2443
2022-01-12 21:56:47,898 Epoch[014/300], Step[1000/1252], Avg Loss: 4.4648, Avg Acc: 0.2446
2022-01-12 21:58:13,395 Epoch[014/300], Step[1050/1252], Avg Loss: 4.4648, Avg Acc: 0.2445
2022-01-12 21:59:37,548 Epoch[014/300], Step[1100/1252], Avg Loss: 4.4625, Avg Acc: 0.2447
2022-01-12 22:01:03,265 Epoch[014/300], Step[1150/1252], Avg Loss: 4.4592, Avg Acc: 0.2457
2022-01-12 22:02:28,461 Epoch[014/300], Step[1200/1252], Avg Loss: 4.4565, Avg Acc: 0.2460
2022-01-12 22:03:55,110 Epoch[014/300], Step[1250/1252], Avg Loss: 4.4536, Avg Acc: 0.2468
2022-01-12 22:04:01,825 ----- Epoch[014/300], Train Loss: 4.4536, Train Acc: 0.2468, time: 2253.85, Best Val(epoch12) Acc@1: 0.4803
2022-01-12 22:04:01,825 ----- Validation after Epoch: 14
2022-01-12 22:05:18,470 Val Step[0000/1563], Avg Loss: 1.9683, Avg Acc@1: 0.5312, Avg Acc@5: 0.7812
2022-01-12 22:05:20,366 Val Step[0050/1563], Avg Loss: 2.2562, Avg Acc@1: 0.5086, Avg Acc@5: 0.7672
2022-01-12 22:05:22,147 Val Step[0100/1563], Avg Loss: 2.2613, Avg Acc@1: 0.5068, Avg Acc@5: 0.7713
2022-01-12 22:05:23,975 Val Step[0150/1563], Avg Loss: 2.2455, Avg Acc@1: 0.5103, Avg Acc@5: 0.7713
2022-01-12 22:05:25,871 Val Step[0200/1563], Avg Loss: 2.2466, Avg Acc@1: 0.5138, Avg Acc@5: 0.7682
2022-01-12 22:05:27,699 Val Step[0250/1563], Avg Loss: 2.2212, Avg Acc@1: 0.5192, Avg Acc@5: 0.7723
2022-01-12 22:05:29,482 Val Step[0300/1563], Avg Loss: 2.2194, Avg Acc@1: 0.5229, Avg Acc@5: 0.7725
2022-01-12 22:05:31,258 Val Step[0350/1563], Avg Loss: 2.2188, Avg Acc@1: 0.5234, Avg Acc@5: 0.7721
2022-01-12 22:05:33,101 Val Step[0400/1563], Avg Loss: 2.2172, Avg Acc@1: 0.5252, Avg Acc@5: 0.7714
2022-01-12 22:05:34,993 Val Step[0450/1563], Avg Loss: 2.2254, Avg Acc@1: 0.5235, Avg Acc@5: 0.7688
2022-01-12 22:05:36,769 Val Step[0500/1563], Avg Loss: 2.2275, Avg Acc@1: 0.5227, Avg Acc@5: 0.7680
2022-01-12 22:05:38,620 Val Step[0550/1563], Avg Loss: 2.2299, Avg Acc@1: 0.5218, Avg Acc@5: 0.7676
2022-01-12 22:05:40,485 Val Step[0600/1563], Avg Loss: 2.2305, Avg Acc@1: 0.5204, Avg Acc@5: 0.7680
2022-01-12 22:05:42,296 Val Step[0650/1563], Avg Loss: 2.2350, Avg Acc@1: 0.5178, Avg Acc@5: 0.7677
2022-01-12 22:05:44,434 Val Step[0700/1563], Avg Loss: 2.2368, Avg Acc@1: 0.5175, Avg Acc@5: 0.7673
2022-01-12 22:05:46,459 Val Step[0750/1563], Avg Loss: 2.2424, Avg Acc@1: 0.5172, Avg Acc@5: 0.7660
2022-01-12 22:05:48,314 Val Step[0800/1563], Avg Loss: 2.2425, Avg Acc@1: 0.5169, Avg Acc@5: 0.7665
2022-01-12 22:05:50,193 Val Step[0850/1563], Avg Loss: 2.2441, Avg Acc@1: 0.5157, Avg Acc@5: 0.7659
2022-01-12 22:05:52,127 Val Step[0900/1563], Avg Loss: 2.2420, Avg Acc@1: 0.5161, Avg Acc@5: 0.7660
2022-01-12 22:05:54,070 Val Step[0950/1563], Avg Loss: 2.2428, Avg Acc@1: 0.5153, Avg Acc@5: 0.7658
2022-01-12 22:05:55,996 Val Step[1000/1563], Avg Loss: 2.2423, Avg Acc@1: 0.5151, Avg Acc@5: 0.7657
2022-01-12 22:05:57,851 Val Step[1050/1563], Avg Loss: 2.2457, Avg Acc@1: 0.5146, Avg Acc@5: 0.7650
2022-01-12 22:05:59,712 Val Step[1100/1563], Avg Loss: 2.2437, Avg Acc@1: 0.5143, Avg Acc@5: 0.7654
2022-01-12 22:06:01,631 Val Step[1150/1563], Avg Loss: 2.2415, Avg Acc@1: 0.5147, Avg Acc@5: 0.7654
2022-01-12 22:06:03,421 Val Step[1200/1563], Avg Loss: 2.2413, Avg Acc@1: 0.5152, Avg Acc@5: 0.7658
2022-01-12 22:06:05,199 Val Step[1250/1563], Avg Loss: 2.2383, Avg Acc@1: 0.5153, Avg Acc@5: 0.7664
2022-01-12 22:06:07,016 Val Step[1300/1563], Avg Loss: 2.2402, Avg Acc@1: 0.5147, Avg Acc@5: 0.7662
2022-01-12 22:06:08,816 Val Step[1350/1563], Avg Loss: 2.2397, Avg Acc@1: 0.5144, Avg Acc@5: 0.7659
2022-01-12 22:06:10,702 Val Step[1400/1563], Avg Loss: 2.2399, Avg Acc@1: 0.5140, Avg Acc@5: 0.7658
2022-01-12 22:06:12,567 Val Step[1450/1563], Avg Loss: 2.2410, Avg Acc@1: 0.5136, Avg Acc@5: 0.7657
2022-01-12 22:06:14,421 Val Step[1500/1563], Avg Loss: 2.2404, Avg Acc@1: 0.5140, Avg Acc@5: 0.7658
2022-01-12 22:06:16,150 Val Step[1550/1563], Avg Loss: 2.2407, Avg Acc@1: 0.5140, Avg Acc@5: 0.7656
2022-01-12 22:06:17,986 ----- Epoch[014/300], Validation Loss: 2.2406, Validation Acc@1: 0.5142, Validation Acc@5: 0.7655, time: 136.16
2022-01-12 22:06:19,298 the pre best model acc:0.4803, at epoch 12
2022-01-12 22:06:19,574 current best model acc:0.5142, at epoch 14
2022-01-12 22:06:19,574 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-12 22:06:19,574 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-12 22:06:19,574 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-12 22:06:19,574 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-12 22:06:19,574 Now training epoch 15. LR=0.000750
2022-01-12 22:08:02,650 Epoch[015/300], Step[0000/1252], Avg Loss: 3.9024, Avg Acc: 0.3936
2022-01-12 22:09:27,916 Epoch[015/300], Step[0050/1252], Avg Loss: 4.3974, Avg Acc: 0.2507
2022-01-12 22:10:52,917 Epoch[015/300], Step[0100/1252], Avg Loss: 4.3724, Avg Acc: 0.2480
2022-01-12 22:12:17,576 Epoch[015/300], Step[0150/1252], Avg Loss: 4.3869, Avg Acc: 0.2509
2022-01-12 22:13:43,455 Epoch[015/300], Step[0200/1252], Avg Loss: 4.3897, Avg Acc: 0.2517
2022-01-12 22:15:09,075 Epoch[015/300], Step[0250/1252], Avg Loss: 4.4014, Avg Acc: 0.2504
2022-01-12 22:16:34,079 Epoch[015/300], Step[0300/1252], Avg Loss: 4.3954, Avg Acc: 0.2526
2022-01-12 22:17:58,676 Epoch[015/300], Step[0350/1252], Avg Loss: 4.4030, Avg Acc: 0.2546
2022-01-12 22:19:24,103 Epoch[015/300], Step[0400/1252], Avg Loss: 4.4066, Avg Acc: 0.2547
2022-01-12 22:20:48,023 Epoch[015/300], Step[0450/1252], Avg Loss: 4.4067, Avg Acc: 0.2549
2022-01-12 22:22:12,569 Epoch[015/300], Step[0500/1252], Avg Loss: 4.4004, Avg Acc: 0.2553
2022-01-12 22:23:38,392 Epoch[015/300], Step[0550/1252], Avg Loss: 4.4088, Avg Acc: 0.2537
2022-01-12 22:25:04,079 Epoch[015/300], Step[0600/1252], Avg Loss: 4.4123, Avg Acc: 0.2538
2022-01-12 22:26:29,861 Epoch[015/300], Step[0650/1252], Avg Loss: 4.4140, Avg Acc: 0.2545
2022-01-12 22:27:54,354 Epoch[015/300], Step[0700/1252], Avg Loss: 4.4166, Avg Acc: 0.2541
2022-01-12 22:29:19,691 Epoch[015/300], Step[0750/1252], Avg Loss: 4.4168, Avg Acc: 0.2535
2022-01-12 22:30:45,208 Epoch[015/300], Step[0800/1252], Avg Loss: 4.4148, Avg Acc: 0.2538
2022-01-12 22:32:11,016 Epoch[015/300], Step[0850/1252], Avg Loss: 4.4142, Avg Acc: 0.2542
2022-01-12 22:33:35,484 Epoch[015/300], Step[0900/1252], Avg Loss: 4.4156, Avg Acc: 0.2553
2022-01-12 22:35:01,536 Epoch[015/300], Step[0950/1252], Avg Loss: 4.4152, Avg Acc: 0.2558
2022-01-12 22:36:26,560 Epoch[015/300], Step[1000/1252], Avg Loss: 4.4119, Avg Acc: 0.2565
2022-01-12 22:37:51,841 Epoch[015/300], Step[1050/1252], Avg Loss: 4.4112, Avg Acc: 0.2567
2022-01-12 22:39:16,571 Epoch[015/300], Step[1100/1252], Avg Loss: 4.4088, Avg Acc: 0.2570
2022-01-12 22:40:42,559 Epoch[015/300], Step[1150/1252], Avg Loss: 4.4063, Avg Acc: 0.2572
2022-01-12 22:42:07,112 Epoch[015/300], Step[1200/1252], Avg Loss: 4.4056, Avg Acc: 0.2582
2022-01-12 22:43:29,416 Epoch[015/300], Step[1250/1252], Avg Loss: 4.4073, Avg Acc: 0.2574
2022-01-12 22:43:36,596 ----- Epoch[015/300], Train Loss: 4.4072, Train Acc: 0.2574, time: 2237.02, Best Val(epoch14) Acc@1: 0.5142
2022-01-12 22:43:36,596 Now training epoch 16. LR=0.000800
2022-01-12 22:45:26,754 Epoch[016/300], Step[0000/1252], Avg Loss: 4.6233, Avg Acc: 0.1357
2022-01-12 22:46:52,474 Epoch[016/300], Step[0050/1252], Avg Loss: 4.3193, Avg Acc: 0.2823
2022-01-12 22:48:18,577 Epoch[016/300], Step[0100/1252], Avg Loss: 4.3163, Avg Acc: 0.2794
2022-01-12 22:49:45,271 Epoch[016/300], Step[0150/1252], Avg Loss: 4.3473, Avg Acc: 0.2738
2022-01-12 22:51:11,286 Epoch[016/300], Step[0200/1252], Avg Loss: 4.3497, Avg Acc: 0.2713
2022-01-12 22:52:36,792 Epoch[016/300], Step[0250/1252], Avg Loss: 4.3605, Avg Acc: 0.2712
2022-01-12 22:54:03,454 Epoch[016/300], Step[0300/1252], Avg Loss: 4.3565, Avg Acc: 0.2664
2022-01-12 22:55:29,746 Epoch[016/300], Step[0350/1252], Avg Loss: 4.3600, Avg Acc: 0.2666
2022-01-12 22:56:57,022 Epoch[016/300], Step[0400/1252], Avg Loss: 4.3540, Avg Acc: 0.2627
2022-01-12 22:58:22,086 Epoch[016/300], Step[0450/1252], Avg Loss: 4.3621, Avg Acc: 0.2627
2022-01-12 22:59:48,955 Epoch[016/300], Step[0500/1252], Avg Loss: 4.3626, Avg Acc: 0.2636
2022-01-12 23:01:15,390 Epoch[016/300], Step[0550/1252], Avg Loss: 4.3610, Avg Acc: 0.2627
2022-01-12 23:02:40,812 Epoch[016/300], Step[0600/1252], Avg Loss: 4.3633, Avg Acc: 0.2640
2022-01-12 23:04:05,880 Epoch[016/300], Step[0650/1252], Avg Loss: 4.3646, Avg Acc: 0.2648
2022-01-12 23:05:30,853 Epoch[016/300], Step[0700/1252], Avg Loss: 4.3645, Avg Acc: 0.2640
2022-01-12 23:06:56,646 Epoch[016/300], Step[0750/1252], Avg Loss: 4.3637, Avg Acc: 0.2664
2022-01-12 23:08:22,607 Epoch[016/300], Step[0800/1252], Avg Loss: 4.3640, Avg Acc: 0.2662
2022-01-12 23:09:48,861 Epoch[016/300], Step[0850/1252], Avg Loss: 4.3626, Avg Acc: 0.2660
2022-01-12 23:11:15,125 Epoch[016/300], Step[0900/1252], Avg Loss: 4.3576, Avg Acc: 0.2662
2022-01-12 23:12:40,598 Epoch[016/300], Step[0950/1252], Avg Loss: 4.3594, Avg Acc: 0.2662
2022-01-12 23:14:06,699 Epoch[016/300], Step[1000/1252], Avg Loss: 4.3567, Avg Acc: 0.2673
2022-01-12 23:15:32,028 Epoch[016/300], Step[1050/1252], Avg Loss: 4.3552, Avg Acc: 0.2664
2022-01-12 23:16:58,098 Epoch[016/300], Step[1100/1252], Avg Loss: 4.3535, Avg Acc: 0.2665
2022-01-12 23:18:25,717 Epoch[016/300], Step[1150/1252], Avg Loss: 4.3522, Avg Acc: 0.2674
2022-01-12 23:19:52,448 Epoch[016/300], Step[1200/1252], Avg Loss: 4.3512, Avg Acc: 0.2679
2022-01-12 23:21:19,180 Epoch[016/300], Step[1250/1252], Avg Loss: 4.3515, Avg Acc: 0.2675
2022-01-12 23:21:26,407 ----- Epoch[016/300], Train Loss: 4.3515, Train Acc: 0.2675, time: 2269.81, Best Val(epoch14) Acc@1: 0.5142
2022-01-12 23:21:26,408 ----- Validation after Epoch: 16
2022-01-12 23:22:41,971 Val Step[0000/1563], Avg Loss: 2.0440, Avg Acc@1: 0.5625, Avg Acc@5: 0.7500
2022-01-12 23:22:43,957 Val Step[0050/1563], Avg Loss: 2.2053, Avg Acc@1: 0.5319, Avg Acc@5: 0.7819
2022-01-12 23:22:46,016 Val Step[0100/1563], Avg Loss: 2.2090, Avg Acc@1: 0.5306, Avg Acc@5: 0.7859
2022-01-12 23:22:47,899 Val Step[0150/1563], Avg Loss: 2.2163, Avg Acc@1: 0.5294, Avg Acc@5: 0.7839
2022-01-12 23:22:49,692 Val Step[0200/1563], Avg Loss: 2.2188, Avg Acc@1: 0.5283, Avg Acc@5: 0.7809
2022-01-12 23:22:51,693 Val Step[0250/1563], Avg Loss: 2.2006, Avg Acc@1: 0.5346, Avg Acc@5: 0.7851
2022-01-12 23:22:53,600 Val Step[0300/1563], Avg Loss: 2.1970, Avg Acc@1: 0.5379, Avg Acc@5: 0.7865
2022-01-12 23:22:55,513 Val Step[0350/1563], Avg Loss: 2.2032, Avg Acc@1: 0.5394, Avg Acc@5: 0.7850
2022-01-12 23:22:57,375 Val Step[0400/1563], Avg Loss: 2.2019, Avg Acc@1: 0.5390, Avg Acc@5: 0.7841
2022-01-12 23:22:59,217 Val Step[0450/1563], Avg Loss: 2.2089, Avg Acc@1: 0.5358, Avg Acc@5: 0.7815
2022-01-12 23:23:01,108 Val Step[0500/1563], Avg Loss: 2.2104, Avg Acc@1: 0.5356, Avg Acc@5: 0.7802
2022-01-12 23:23:02,925 Val Step[0550/1563], Avg Loss: 2.2119, Avg Acc@1: 0.5356, Avg Acc@5: 0.7799
2022-01-12 23:23:04,702 Val Step[0600/1563], Avg Loss: 2.2131, Avg Acc@1: 0.5359, Avg Acc@5: 0.7800
2022-01-12 23:23:06,563 Val Step[0650/1563], Avg Loss: 2.2161, Avg Acc@1: 0.5347, Avg Acc@5: 0.7788
2022-01-12 23:23:08,513 Val Step[0700/1563], Avg Loss: 2.2181, Avg Acc@1: 0.5345, Avg Acc@5: 0.7778
2022-01-12 23:23:10,401 Val Step[0750/1563], Avg Loss: 2.2226, Avg Acc@1: 0.5335, Avg Acc@5: 0.7773
2022-01-12 23:23:12,252 Val Step[0800/1563], Avg Loss: 2.2217, Avg Acc@1: 0.5332, Avg Acc@5: 0.7776
2022-01-12 23:23:14,112 Val Step[0850/1563], Avg Loss: 2.2225, Avg Acc@1: 0.5326, Avg Acc@5: 0.7777
2022-01-12 23:23:15,949 Val Step[0900/1563], Avg Loss: 2.2207, Avg Acc@1: 0.5331, Avg Acc@5: 0.7784
2022-01-12 23:23:17,798 Val Step[0950/1563], Avg Loss: 2.2196, Avg Acc@1: 0.5325, Avg Acc@5: 0.7787
2022-01-12 23:23:19,651 Val Step[1000/1563], Avg Loss: 2.2184, Avg Acc@1: 0.5327, Avg Acc@5: 0.7788
2022-01-12 23:23:21,645 Val Step[1050/1563], Avg Loss: 2.2228, Avg Acc@1: 0.5317, Avg Acc@5: 0.7778
2022-01-12 23:23:23,661 Val Step[1100/1563], Avg Loss: 2.2209, Avg Acc@1: 0.5312, Avg Acc@5: 0.7782
2022-01-12 23:23:25,578 Val Step[1150/1563], Avg Loss: 2.2182, Avg Acc@1: 0.5316, Avg Acc@5: 0.7787
2022-01-12 23:23:27,455 Val Step[1200/1563], Avg Loss: 2.2183, Avg Acc@1: 0.5313, Avg Acc@5: 0.7791
2022-01-12 23:23:29,322 Val Step[1250/1563], Avg Loss: 2.2167, Avg Acc@1: 0.5315, Avg Acc@5: 0.7797
2022-01-12 23:23:31,200 Val Step[1300/1563], Avg Loss: 2.2208, Avg Acc@1: 0.5304, Avg Acc@5: 0.7790
2022-01-12 23:23:32,971 Val Step[1350/1563], Avg Loss: 2.2206, Avg Acc@1: 0.5307, Avg Acc@5: 0.7786
2022-01-12 23:23:34,768 Val Step[1400/1563], Avg Loss: 2.2213, Avg Acc@1: 0.5298, Avg Acc@5: 0.7783
2022-01-12 23:23:36,587 Val Step[1450/1563], Avg Loss: 2.2213, Avg Acc@1: 0.5296, Avg Acc@5: 0.7787
2022-01-12 23:23:38,481 Val Step[1500/1563], Avg Loss: 2.2214, Avg Acc@1: 0.5298, Avg Acc@5: 0.7787
2022-01-12 23:23:40,296 Val Step[1550/1563], Avg Loss: 2.2222, Avg Acc@1: 0.5300, Avg Acc@5: 0.7783
2022-01-12 23:23:42,186 ----- Epoch[016/300], Validation Loss: 2.2221, Validation Acc@1: 0.5301, Validation Acc@5: 0.7783, time: 135.78
2022-01-12 23:23:43,508 the pre best model acc:0.5142, at epoch 14
2022-01-12 23:23:43,777 current best model acc:0.5301, at epoch 16
2022-01-12 23:23:43,777 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-12 23:23:43,777 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-12 23:23:43,777 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-12 23:23:43,777 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-12 23:23:43,777 Now training epoch 17. LR=0.000850
2022-01-12 23:25:26,424 Epoch[017/300], Step[0000/1252], Avg Loss: 4.5449, Avg Acc: 0.2656
2022-01-12 23:26:51,901 Epoch[017/300], Step[0050/1252], Avg Loss: 4.3784, Avg Acc: 0.2809
2022-01-12 23:28:16,907 Epoch[017/300], Step[0100/1252], Avg Loss: 4.3418, Avg Acc: 0.2836
2022-01-12 23:29:41,261 Epoch[017/300], Step[0150/1252], Avg Loss: 4.2928, Avg Acc: 0.2861
2022-01-12 23:31:07,675 Epoch[017/300], Step[0200/1252], Avg Loss: 4.2921, Avg Acc: 0.2824
2022-01-12 23:32:34,349 Epoch[017/300], Step[0250/1252], Avg Loss: 4.3015, Avg Acc: 0.2814
2022-01-12 23:33:59,018 Epoch[017/300], Step[0300/1252], Avg Loss: 4.2974, Avg Acc: 0.2837
2022-01-12 23:35:24,580 Epoch[017/300], Step[0350/1252], Avg Loss: 4.3043, Avg Acc: 0.2824
2022-01-12 23:36:50,745 Epoch[017/300], Step[0400/1252], Avg Loss: 4.3042, Avg Acc: 0.2815
2022-01-12 23:38:15,720 Epoch[017/300], Step[0450/1252], Avg Loss: 4.3161, Avg Acc: 0.2790
2022-01-12 23:39:41,989 Epoch[017/300], Step[0500/1252], Avg Loss: 4.3213, Avg Acc: 0.2788
2022-01-12 23:41:09,113 Epoch[017/300], Step[0550/1252], Avg Loss: 4.3155, Avg Acc: 0.2800
2022-01-12 23:42:33,737 Epoch[017/300], Step[0600/1252], Avg Loss: 4.3143, Avg Acc: 0.2785
2022-01-12 23:43:59,379 Epoch[017/300], Step[0650/1252], Avg Loss: 4.3200, Avg Acc: 0.2771
2022-01-12 23:45:25,229 Epoch[017/300], Step[0700/1252], Avg Loss: 4.3188, Avg Acc: 0.2753
2022-01-12 23:46:51,271 Epoch[017/300], Step[0750/1252], Avg Loss: 4.3208, Avg Acc: 0.2743
2022-01-12 23:48:17,999 Epoch[017/300], Step[0800/1252], Avg Loss: 4.3204, Avg Acc: 0.2741
2022-01-12 23:49:43,324 Epoch[017/300], Step[0850/1252], Avg Loss: 4.3176, Avg Acc: 0.2735
2022-01-12 23:51:08,679 Epoch[017/300], Step[0900/1252], Avg Loss: 4.3220, Avg Acc: 0.2734
2022-01-12 23:52:33,382 Epoch[017/300], Step[0950/1252], Avg Loss: 4.3177, Avg Acc: 0.2736
2022-01-12 23:53:59,135 Epoch[017/300], Step[1000/1252], Avg Loss: 4.3182, Avg Acc: 0.2735
2022-01-12 23:55:25,228 Epoch[017/300], Step[1050/1252], Avg Loss: 4.3152, Avg Acc: 0.2730
2022-01-12 23:56:50,520 Epoch[017/300], Step[1100/1252], Avg Loss: 4.3152, Avg Acc: 0.2729
2022-01-12 23:58:14,999 Epoch[017/300], Step[1150/1252], Avg Loss: 4.3144, Avg Acc: 0.2738
2022-01-12 23:59:40,131 Epoch[017/300], Step[1200/1252], Avg Loss: 4.3149, Avg Acc: 0.2741
2022-01-13 00:01:04,300 Epoch[017/300], Step[1250/1252], Avg Loss: 4.3169, Avg Acc: 0.2738
2022-01-13 00:01:11,270 ----- Epoch[017/300], Train Loss: 4.3169, Train Acc: 0.2738, time: 2247.49, Best Val(epoch16) Acc@1: 0.5301
2022-01-13 00:01:11,270 Now training epoch 18. LR=0.000900
2022-01-13 00:02:55,940 Epoch[018/300], Step[0000/1252], Avg Loss: 3.8373, Avg Acc: 0.3662
2022-01-13 00:04:21,728 Epoch[018/300], Step[0050/1252], Avg Loss: 4.2700, Avg Acc: 0.2427
2022-01-13 00:05:47,441 Epoch[018/300], Step[0100/1252], Avg Loss: 4.2915, Avg Acc: 0.2589
2022-01-13 00:07:13,372 Epoch[018/300], Step[0150/1252], Avg Loss: 4.2862, Avg Acc: 0.2656
2022-01-13 00:08:39,944 Epoch[018/300], Step[0200/1252], Avg Loss: 4.2814, Avg Acc: 0.2685
2022-01-13 00:10:05,853 Epoch[018/300], Step[0250/1252], Avg Loss: 4.2826, Avg Acc: 0.2736
2022-01-13 00:11:31,991 Epoch[018/300], Step[0300/1252], Avg Loss: 4.2789, Avg Acc: 0.2750
2022-01-13 00:12:57,616 Epoch[018/300], Step[0350/1252], Avg Loss: 4.2858, Avg Acc: 0.2740
2022-01-13 00:14:23,902 Epoch[018/300], Step[0400/1252], Avg Loss: 4.2834, Avg Acc: 0.2734
2022-01-13 00:15:49,396 Epoch[018/300], Step[0450/1252], Avg Loss: 4.2857, Avg Acc: 0.2737
2022-01-13 00:17:16,235 Epoch[018/300], Step[0500/1252], Avg Loss: 4.2850, Avg Acc: 0.2731
2022-01-13 00:18:42,445 Epoch[018/300], Step[0550/1252], Avg Loss: 4.2862, Avg Acc: 0.2739
2022-01-13 00:20:09,763 Epoch[018/300], Step[0600/1252], Avg Loss: 4.2841, Avg Acc: 0.2740
2022-01-13 00:21:36,333 Epoch[018/300], Step[0650/1252], Avg Loss: 4.2914, Avg Acc: 0.2737
2022-01-13 00:23:02,592 Epoch[018/300], Step[0700/1252], Avg Loss: 4.2870, Avg Acc: 0.2753
2022-01-13 00:24:29,397 Epoch[018/300], Step[0750/1252], Avg Loss: 4.2855, Avg Acc: 0.2751
2022-01-13 00:25:56,177 Epoch[018/300], Step[0800/1252], Avg Loss: 4.2863, Avg Acc: 0.2747
2022-01-13 00:27:22,369 Epoch[018/300], Step[0850/1252], Avg Loss: 4.2853, Avg Acc: 0.2745
2022-01-13 00:28:49,206 Epoch[018/300], Step[0900/1252], Avg Loss: 4.2868, Avg Acc: 0.2735
2022-01-13 00:30:15,238 Epoch[018/300], Step[0950/1252], Avg Loss: 4.2912, Avg Acc: 0.2725
2022-01-13 00:31:41,780 Epoch[018/300], Step[1000/1252], Avg Loss: 4.2939, Avg Acc: 0.2723
2022-01-13 00:33:07,647 Epoch[018/300], Step[1050/1252], Avg Loss: 4.2949, Avg Acc: 0.2718
2022-01-13 00:34:33,796 Epoch[018/300], Step[1100/1252], Avg Loss: 4.2937, Avg Acc: 0.2721
2022-01-13 00:36:00,408 Epoch[018/300], Step[1150/1252], Avg Loss: 4.2921, Avg Acc: 0.2720
2022-01-13 00:37:26,065 Epoch[018/300], Step[1200/1252], Avg Loss: 4.2925, Avg Acc: 0.2726
2022-01-13 00:38:52,504 Epoch[018/300], Step[1250/1252], Avg Loss: 4.2921, Avg Acc: 0.2730
2022-01-13 00:38:59,538 ----- Epoch[018/300], Train Loss: 4.2921, Train Acc: 0.2730, time: 2268.26, Best Val(epoch16) Acc@1: 0.5301
2022-01-13 00:38:59,538 ----- Validation after Epoch: 18
2022-01-13 00:40:09,895 Val Step[0000/1563], Avg Loss: 1.8539, Avg Acc@1: 0.5312, Avg Acc@5: 0.8125
2022-01-13 00:40:11,728 Val Step[0050/1563], Avg Loss: 2.0111, Avg Acc@1: 0.5533, Avg Acc@5: 0.8082
2022-01-13 00:40:13,511 Val Step[0100/1563], Avg Loss: 2.0324, Avg Acc@1: 0.5511, Avg Acc@5: 0.8085
2022-01-13 00:40:15,331 Val Step[0150/1563], Avg Loss: 2.0225, Avg Acc@1: 0.5532, Avg Acc@5: 0.8057
2022-01-13 00:40:17,198 Val Step[0200/1563], Avg Loss: 2.0354, Avg Acc@1: 0.5480, Avg Acc@5: 0.8029
2022-01-13 00:40:19,091 Val Step[0250/1563], Avg Loss: 2.0141, Avg Acc@1: 0.5534, Avg Acc@5: 0.8063
2022-01-13 00:40:20,981 Val Step[0300/1563], Avg Loss: 2.0045, Avg Acc@1: 0.5589, Avg Acc@5: 0.8080
2022-01-13 00:40:22,811 Val Step[0350/1563], Avg Loss: 2.0077, Avg Acc@1: 0.5598, Avg Acc@5: 0.8069
2022-01-13 00:40:24,700 Val Step[0400/1563], Avg Loss: 2.0054, Avg Acc@1: 0.5608, Avg Acc@5: 0.8075
2022-01-13 00:40:26,557 Val Step[0450/1563], Avg Loss: 2.0147, Avg Acc@1: 0.5576, Avg Acc@5: 0.8052
2022-01-13 00:40:28,445 Val Step[0500/1563], Avg Loss: 2.0172, Avg Acc@1: 0.5576, Avg Acc@5: 0.8040
2022-01-13 00:40:30,224 Val Step[0550/1563], Avg Loss: 2.0190, Avg Acc@1: 0.5567, Avg Acc@5: 0.8031
2022-01-13 00:40:32,053 Val Step[0600/1563], Avg Loss: 2.0206, Avg Acc@1: 0.5568, Avg Acc@5: 0.8025
2022-01-13 00:40:33,856 Val Step[0650/1563], Avg Loss: 2.0243, Avg Acc@1: 0.5561, Avg Acc@5: 0.8025
2022-01-13 00:40:35,641 Val Step[0700/1563], Avg Loss: 2.0226, Avg Acc@1: 0.5560, Avg Acc@5: 0.8027
2022-01-13 00:40:37,433 Val Step[0750/1563], Avg Loss: 2.0286, Avg Acc@1: 0.5548, Avg Acc@5: 0.8015
2022-01-13 00:40:39,234 Val Step[0800/1563], Avg Loss: 2.0274, Avg Acc@1: 0.5554, Avg Acc@5: 0.8023
2022-01-13 00:40:41,086 Val Step[0850/1563], Avg Loss: 2.0287, Avg Acc@1: 0.5555, Avg Acc@5: 0.8021
2022-01-13 00:40:42,875 Val Step[0900/1563], Avg Loss: 2.0259, Avg Acc@1: 0.5569, Avg Acc@5: 0.8022
2022-01-13 00:40:44,782 Val Step[0950/1563], Avg Loss: 2.0266, Avg Acc@1: 0.5563, Avg Acc@5: 0.8021
2022-01-13 00:40:46,724 Val Step[1000/1563], Avg Loss: 2.0260, Avg Acc@1: 0.5565, Avg Acc@5: 0.8020
2022-01-13 00:40:48,587 Val Step[1050/1563], Avg Loss: 2.0298, Avg Acc@1: 0.5556, Avg Acc@5: 0.8011
2022-01-13 00:40:50,479 Val Step[1100/1563], Avg Loss: 2.0282, Avg Acc@1: 0.5559, Avg Acc@5: 0.8015
2022-01-13 00:40:52,401 Val Step[1150/1563], Avg Loss: 2.0276, Avg Acc@1: 0.5559, Avg Acc@5: 0.8014
2022-01-13 00:40:54,193 Val Step[1200/1563], Avg Loss: 2.0275, Avg Acc@1: 0.5562, Avg Acc@5: 0.8012
2022-01-13 00:40:55,982 Val Step[1250/1563], Avg Loss: 2.0253, Avg Acc@1: 0.5564, Avg Acc@5: 0.8015
2022-01-13 00:40:57,885 Val Step[1300/1563], Avg Loss: 2.0292, Avg Acc@1: 0.5555, Avg Acc@5: 0.8007
2022-01-13 00:40:59,792 Val Step[1350/1563], Avg Loss: 2.0277, Avg Acc@1: 0.5559, Avg Acc@5: 0.8008
2022-01-13 00:41:01,575 Val Step[1400/1563], Avg Loss: 2.0279, Avg Acc@1: 0.5553, Avg Acc@5: 0.8005
2022-01-13 00:41:03,427 Val Step[1450/1563], Avg Loss: 2.0289, Avg Acc@1: 0.5552, Avg Acc@5: 0.8004
2022-01-13 00:41:05,199 Val Step[1500/1563], Avg Loss: 2.0294, Avg Acc@1: 0.5554, Avg Acc@5: 0.8004
2022-01-13 00:41:07,086 Val Step[1550/1563], Avg Loss: 2.0297, Avg Acc@1: 0.5554, Avg Acc@5: 0.8004
2022-01-13 00:41:09,009 ----- Epoch[018/300], Validation Loss: 2.0295, Validation Acc@1: 0.5555, Validation Acc@5: 0.8003, time: 129.47
2022-01-13 00:41:10,317 the pre best model acc:0.5301, at epoch 16
2022-01-13 00:41:10,591 current best model acc:0.5555, at epoch 18
2022-01-13 00:41:10,592 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 00:41:10,592 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 00:41:10,592 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 00:41:10,592 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 00:41:10,593 Now training epoch 19. LR=0.000950
2022-01-13 00:42:53,881 Epoch[019/300], Step[0000/1252], Avg Loss: 4.3288, Avg Acc: 0.2041
2022-01-13 00:44:19,205 Epoch[019/300], Step[0050/1252], Avg Loss: 4.3065, Avg Acc: 0.2857
2022-01-13 00:45:44,661 Epoch[019/300], Step[0100/1252], Avg Loss: 4.2450, Avg Acc: 0.2824
2022-01-13 00:47:10,158 Epoch[019/300], Step[0150/1252], Avg Loss: 4.2311, Avg Acc: 0.2844
2022-01-13 00:48:33,845 Epoch[019/300], Step[0200/1252], Avg Loss: 4.2471, Avg Acc: 0.2881
2022-01-13 00:49:59,082 Epoch[019/300], Step[0250/1252], Avg Loss: 4.2565, Avg Acc: 0.2834
2022-01-13 00:51:23,041 Epoch[019/300], Step[0300/1252], Avg Loss: 4.2550, Avg Acc: 0.2841
2022-01-13 00:52:47,935 Epoch[019/300], Step[0350/1252], Avg Loss: 4.2465, Avg Acc: 0.2843
2022-01-13 00:54:11,101 Epoch[019/300], Step[0400/1252], Avg Loss: 4.2498, Avg Acc: 0.2841
2022-01-13 00:55:35,459 Epoch[019/300], Step[0450/1252], Avg Loss: 4.2490, Avg Acc: 0.2831
2022-01-13 00:56:59,391 Epoch[019/300], Step[0500/1252], Avg Loss: 4.2544, Avg Acc: 0.2846
2022-01-13 00:58:24,582 Epoch[019/300], Step[0550/1252], Avg Loss: 4.2530, Avg Acc: 0.2846
2022-01-13 00:59:49,337 Epoch[019/300], Step[0600/1252], Avg Loss: 4.2524, Avg Acc: 0.2838
2022-01-13 01:01:14,035 Epoch[019/300], Step[0650/1252], Avg Loss: 4.2545, Avg Acc: 0.2838
2022-01-13 01:02:38,933 Epoch[019/300], Step[0700/1252], Avg Loss: 4.2550, Avg Acc: 0.2835
2022-01-13 01:04:02,847 Epoch[019/300], Step[0750/1252], Avg Loss: 4.2556, Avg Acc: 0.2837
2022-01-13 01:05:28,170 Epoch[019/300], Step[0800/1252], Avg Loss: 4.2573, Avg Acc: 0.2822
2022-01-13 01:06:53,121 Epoch[019/300], Step[0850/1252], Avg Loss: 4.2525, Avg Acc: 0.2812
2022-01-13 01:08:18,418 Epoch[019/300], Step[0900/1252], Avg Loss: 4.2496, Avg Acc: 0.2812
2022-01-13 01:09:43,666 Epoch[019/300], Step[0950/1252], Avg Loss: 4.2496, Avg Acc: 0.2810
2022-01-13 01:11:10,728 Epoch[019/300], Step[1000/1252], Avg Loss: 4.2512, Avg Acc: 0.2798
2022-01-13 01:12:37,346 Epoch[019/300], Step[1050/1252], Avg Loss: 4.2522, Avg Acc: 0.2801
2022-01-13 01:14:04,044 Epoch[019/300], Step[1100/1252], Avg Loss: 4.2564, Avg Acc: 0.2797
2022-01-13 01:15:30,778 Epoch[019/300], Step[1150/1252], Avg Loss: 4.2541, Avg Acc: 0.2800
2022-01-13 01:16:56,669 Epoch[019/300], Step[1200/1252], Avg Loss: 4.2551, Avg Acc: 0.2801
2022-01-13 01:18:23,967 Epoch[019/300], Step[1250/1252], Avg Loss: 4.2551, Avg Acc: 0.2800
2022-01-13 01:18:30,951 ----- Epoch[019/300], Train Loss: 4.2551, Train Acc: 0.2800, time: 2240.35, Best Val(epoch18) Acc@1: 0.5555
2022-01-13 01:18:30,951 Now training epoch 20. LR=0.001000
2022-01-13 01:20:17,365 Epoch[020/300], Step[0000/1252], Avg Loss: 4.1199, Avg Acc: 0.2500
2022-01-13 01:21:41,973 Epoch[020/300], Step[0050/1252], Avg Loss: 4.2555, Avg Acc: 0.2828
2022-01-13 01:23:07,476 Epoch[020/300], Step[0100/1252], Avg Loss: 4.2626, Avg Acc: 0.2773
2022-01-13 01:24:31,898 Epoch[020/300], Step[0150/1252], Avg Loss: 4.2537, Avg Acc: 0.2818
2022-01-13 01:25:57,122 Epoch[020/300], Step[0200/1252], Avg Loss: 4.2554, Avg Acc: 0.2866
2022-01-13 01:27:23,325 Epoch[020/300], Step[0250/1252], Avg Loss: 4.2390, Avg Acc: 0.2885
2022-01-13 01:28:48,387 Epoch[020/300], Step[0300/1252], Avg Loss: 4.2362, Avg Acc: 0.2882
2022-01-13 01:30:13,138 Epoch[020/300], Step[0350/1252], Avg Loss: 4.2397, Avg Acc: 0.2852
2022-01-13 01:31:38,917 Epoch[020/300], Step[0400/1252], Avg Loss: 4.2450, Avg Acc: 0.2843
2022-01-13 01:33:05,463 Epoch[020/300], Step[0450/1252], Avg Loss: 4.2398, Avg Acc: 0.2839
2022-01-13 01:34:31,591 Epoch[020/300], Step[0500/1252], Avg Loss: 4.2453, Avg Acc: 0.2832
2022-01-13 01:35:56,397 Epoch[020/300], Step[0550/1252], Avg Loss: 4.2458, Avg Acc: 0.2836
2022-01-13 01:37:22,413 Epoch[020/300], Step[0600/1252], Avg Loss: 4.2443, Avg Acc: 0.2840
2022-01-13 01:38:49,719 Epoch[020/300], Step[0650/1252], Avg Loss: 4.2415, Avg Acc: 0.2826
2022-01-13 01:40:15,470 Epoch[020/300], Step[0700/1252], Avg Loss: 4.2354, Avg Acc: 0.2824
2022-01-13 01:41:42,063 Epoch[020/300], Step[0750/1252], Avg Loss: 4.2356, Avg Acc: 0.2805
2022-01-13 01:43:07,982 Epoch[020/300], Step[0800/1252], Avg Loss: 4.2350, Avg Acc: 0.2809
2022-01-13 01:44:34,611 Epoch[020/300], Step[0850/1252], Avg Loss: 4.2358, Avg Acc: 0.2817
2022-01-13 01:46:01,117 Epoch[020/300], Step[0900/1252], Avg Loss: 4.2329, Avg Acc: 0.2821
2022-01-13 01:47:26,652 Epoch[020/300], Step[0950/1252], Avg Loss: 4.2361, Avg Acc: 0.2808
2022-01-13 01:48:53,102 Epoch[020/300], Step[1000/1252], Avg Loss: 4.2340, Avg Acc: 0.2814
2022-01-13 01:50:20,216 Epoch[020/300], Step[1050/1252], Avg Loss: 4.2346, Avg Acc: 0.2809
2022-01-13 01:51:46,709 Epoch[020/300], Step[1100/1252], Avg Loss: 4.2322, Avg Acc: 0.2814
2022-01-13 01:53:13,157 Epoch[020/300], Step[1150/1252], Avg Loss: 4.2301, Avg Acc: 0.2827
2022-01-13 01:54:40,095 Epoch[020/300], Step[1200/1252], Avg Loss: 4.2306, Avg Acc: 0.2823
2022-01-13 01:56:06,924 Epoch[020/300], Step[1250/1252], Avg Loss: 4.2296, Avg Acc: 0.2825
2022-01-13 01:56:13,907 ----- Epoch[020/300], Train Loss: 4.2296, Train Acc: 0.2825, time: 2262.95, Best Val(epoch18) Acc@1: 0.5555
2022-01-13 01:56:13,908 ----- Validation after Epoch: 20
2022-01-13 01:57:19,766 Val Step[0000/1563], Avg Loss: 1.7687, Avg Acc@1: 0.6250, Avg Acc@5: 0.8125
2022-01-13 01:57:21,645 Val Step[0050/1563], Avg Loss: 1.9282, Avg Acc@1: 0.5797, Avg Acc@5: 0.8223
2022-01-13 01:57:23,648 Val Step[0100/1563], Avg Loss: 1.9502, Avg Acc@1: 0.5774, Avg Acc@5: 0.8218
2022-01-13 01:57:25,708 Val Step[0150/1563], Avg Loss: 1.9479, Avg Acc@1: 0.5768, Avg Acc@5: 0.8160
2022-01-13 01:57:27,763 Val Step[0200/1563], Avg Loss: 1.9576, Avg Acc@1: 0.5752, Avg Acc@5: 0.8128
2022-01-13 01:57:29,807 Val Step[0250/1563], Avg Loss: 1.9379, Avg Acc@1: 0.5814, Avg Acc@5: 0.8160
2022-01-13 01:57:31,850 Val Step[0300/1563], Avg Loss: 1.9328, Avg Acc@1: 0.5816, Avg Acc@5: 0.8169
2022-01-13 01:57:33,803 Val Step[0350/1563], Avg Loss: 1.9372, Avg Acc@1: 0.5817, Avg Acc@5: 0.8166
2022-01-13 01:57:35,587 Val Step[0400/1563], Avg Loss: 1.9356, Avg Acc@1: 0.5817, Avg Acc@5: 0.8166
2022-01-13 01:57:37,381 Val Step[0450/1563], Avg Loss: 1.9443, Avg Acc@1: 0.5783, Avg Acc@5: 0.8153
2022-01-13 01:57:39,274 Val Step[0500/1563], Avg Loss: 1.9440, Avg Acc@1: 0.5780, Avg Acc@5: 0.8146
2022-01-13 01:57:41,069 Val Step[0550/1563], Avg Loss: 1.9482, Avg Acc@1: 0.5762, Avg Acc@5: 0.8141
2022-01-13 01:57:42,855 Val Step[0600/1563], Avg Loss: 1.9497, Avg Acc@1: 0.5760, Avg Acc@5: 0.8141
2022-01-13 01:57:44,632 Val Step[0650/1563], Avg Loss: 1.9524, Avg Acc@1: 0.5750, Avg Acc@5: 0.8141
2022-01-13 01:57:46,415 Val Step[0700/1563], Avg Loss: 1.9549, Avg Acc@1: 0.5744, Avg Acc@5: 0.8138
2022-01-13 01:57:48,195 Val Step[0750/1563], Avg Loss: 1.9598, Avg Acc@1: 0.5735, Avg Acc@5: 0.8131
2022-01-13 01:57:49,962 Val Step[0800/1563], Avg Loss: 1.9594, Avg Acc@1: 0.5736, Avg Acc@5: 0.8134
2022-01-13 01:57:51,772 Val Step[0850/1563], Avg Loss: 1.9611, Avg Acc@1: 0.5740, Avg Acc@5: 0.8127
2022-01-13 01:57:53,806 Val Step[0900/1563], Avg Loss: 1.9576, Avg Acc@1: 0.5748, Avg Acc@5: 0.8138
2022-01-13 01:57:55,895 Val Step[0950/1563], Avg Loss: 1.9591, Avg Acc@1: 0.5746, Avg Acc@5: 0.8137
2022-01-13 01:57:57,813 Val Step[1000/1563], Avg Loss: 1.9586, Avg Acc@1: 0.5740, Avg Acc@5: 0.8140
2022-01-13 01:57:59,690 Val Step[1050/1563], Avg Loss: 1.9625, Avg Acc@1: 0.5729, Avg Acc@5: 0.8137
2022-01-13 01:58:01,724 Val Step[1100/1563], Avg Loss: 1.9619, Avg Acc@1: 0.5726, Avg Acc@5: 0.8139
2022-01-13 01:58:03,647 Val Step[1150/1563], Avg Loss: 1.9604, Avg Acc@1: 0.5728, Avg Acc@5: 0.8140
2022-01-13 01:58:05,598 Val Step[1200/1563], Avg Loss: 1.9600, Avg Acc@1: 0.5726, Avg Acc@5: 0.8145
2022-01-13 01:58:07,723 Val Step[1250/1563], Avg Loss: 1.9567, Avg Acc@1: 0.5728, Avg Acc@5: 0.8150
2022-01-13 01:58:09,845 Val Step[1300/1563], Avg Loss: 1.9606, Avg Acc@1: 0.5722, Avg Acc@5: 0.8141
2022-01-13 01:58:11,954 Val Step[1350/1563], Avg Loss: 1.9589, Avg Acc@1: 0.5726, Avg Acc@5: 0.8140
2022-01-13 01:58:14,050 Val Step[1400/1563], Avg Loss: 1.9591, Avg Acc@1: 0.5726, Avg Acc@5: 0.8138
2022-01-13 01:58:16,160 Val Step[1450/1563], Avg Loss: 1.9597, Avg Acc@1: 0.5726, Avg Acc@5: 0.8136
2022-01-13 01:58:18,218 Val Step[1500/1563], Avg Loss: 1.9602, Avg Acc@1: 0.5727, Avg Acc@5: 0.8134
2022-01-13 01:58:20,223 Val Step[1550/1563], Avg Loss: 1.9606, Avg Acc@1: 0.5729, Avg Acc@5: 0.8131
2022-01-13 01:58:22,178 ----- Epoch[020/300], Validation Loss: 1.9609, Validation Acc@1: 0.5728, Validation Acc@5: 0.8131, time: 128.27
2022-01-13 01:58:23,518 the pre best model acc:0.5555, at epoch 18
2022-01-13 01:58:23,802 current best model acc:0.5728, at epoch 20
2022-01-13 01:58:23,802 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 01:58:23,802 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 01:58:23,802 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 01:58:23,802 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 01:58:24,471 ----- Save model: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-20-Loss-4.249183353401062.pdparams
2022-01-13 01:58:24,471 ----- Save optim: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-20-Loss-4.249183353401062.pdopt
2022-01-13 01:58:24,472 Now training epoch 21. LR=0.001000
2022-01-13 02:00:06,537 Epoch[021/300], Step[0000/1252], Avg Loss: 4.1586, Avg Acc: 0.4287
2022-01-13 02:01:31,202 Epoch[021/300], Step[0050/1252], Avg Loss: 4.2061, Avg Acc: 0.2721
2022-01-13 02:02:55,830 Epoch[021/300], Step[0100/1252], Avg Loss: 4.1867, Avg Acc: 0.2964
2022-01-13 02:04:20,171 Epoch[021/300], Step[0150/1252], Avg Loss: 4.1815, Avg Acc: 0.2947
2022-01-13 02:05:44,293 Epoch[021/300], Step[0200/1252], Avg Loss: 4.2171, Avg Acc: 0.2917
2022-01-13 02:07:09,886 Epoch[021/300], Step[0250/1252], Avg Loss: 4.2151, Avg Acc: 0.2957
2022-01-13 02:08:34,980 Epoch[021/300], Step[0300/1252], Avg Loss: 4.2187, Avg Acc: 0.2978
2022-01-13 02:09:58,991 Epoch[021/300], Step[0350/1252], Avg Loss: 4.2109, Avg Acc: 0.2975
2022-01-13 02:11:24,347 Epoch[021/300], Step[0400/1252], Avg Loss: 4.2132, Avg Acc: 0.2991
2022-01-13 02:12:48,900 Epoch[021/300], Step[0450/1252], Avg Loss: 4.2091, Avg Acc: 0.2978
2022-01-13 02:14:13,063 Epoch[021/300], Step[0500/1252], Avg Loss: 4.1997, Avg Acc: 0.2970
2022-01-13 02:15:37,357 Epoch[021/300], Step[0550/1252], Avg Loss: 4.1991, Avg Acc: 0.2974
2022-01-13 02:17:02,360 Epoch[021/300], Step[0600/1252], Avg Loss: 4.1946, Avg Acc: 0.2972
2022-01-13 02:18:27,598 Epoch[021/300], Step[0650/1252], Avg Loss: 4.1974, Avg Acc: 0.2962
2022-01-13 02:19:53,282 Epoch[021/300], Step[0700/1252], Avg Loss: 4.1956, Avg Acc: 0.2956
2022-01-13 02:21:18,826 Epoch[021/300], Step[0750/1252], Avg Loss: 4.1981, Avg Acc: 0.2956
2022-01-13 02:22:44,440 Epoch[021/300], Step[0800/1252], Avg Loss: 4.1975, Avg Acc: 0.2961
2022-01-13 02:24:09,314 Epoch[021/300], Step[0850/1252], Avg Loss: 4.1944, Avg Acc: 0.2966
2022-01-13 02:25:33,367 Epoch[021/300], Step[0900/1252], Avg Loss: 4.1934, Avg Acc: 0.2958
2022-01-13 02:26:55,605 Epoch[021/300], Step[0950/1252], Avg Loss: 4.1922, Avg Acc: 0.2962
2022-01-13 02:28:20,243 Epoch[021/300], Step[1000/1252], Avg Loss: 4.1915, Avg Acc: 0.2954
2022-01-13 02:29:45,079 Epoch[021/300], Step[1050/1252], Avg Loss: 4.1920, Avg Acc: 0.2956
2022-01-13 02:31:10,712 Epoch[021/300], Step[1100/1252], Avg Loss: 4.1924, Avg Acc: 0.2954
2022-01-13 02:32:35,377 Epoch[021/300], Step[1150/1252], Avg Loss: 4.1944, Avg Acc: 0.2947
2022-01-13 02:34:00,745 Epoch[021/300], Step[1200/1252], Avg Loss: 4.1901, Avg Acc: 0.2946
2022-01-13 02:35:26,402 Epoch[021/300], Step[1250/1252], Avg Loss: 4.1878, Avg Acc: 0.2940
2022-01-13 02:35:33,504 ----- Epoch[021/300], Train Loss: 4.1878, Train Acc: 0.2940, time: 2229.03, Best Val(epoch20) Acc@1: 0.5728
2022-01-13 02:35:33,504 Now training epoch 22. LR=0.001000
2022-01-13 02:37:14,199 Epoch[022/300], Step[0000/1252], Avg Loss: 3.8402, Avg Acc: 0.4102
2022-01-13 02:38:38,009 Epoch[022/300], Step[0050/1252], Avg Loss: 4.1213, Avg Acc: 0.2981
2022-01-13 02:40:02,397 Epoch[022/300], Step[0100/1252], Avg Loss: 4.1466, Avg Acc: 0.3095
2022-01-13 02:41:27,504 Epoch[022/300], Step[0150/1252], Avg Loss: 4.1434, Avg Acc: 0.3009
2022-01-13 02:42:52,090 Epoch[022/300], Step[0200/1252], Avg Loss: 4.1454, Avg Acc: 0.3019
2022-01-13 02:44:16,250 Epoch[022/300], Step[0250/1252], Avg Loss: 4.1578, Avg Acc: 0.3016
2022-01-13 02:45:40,967 Epoch[022/300], Step[0300/1252], Avg Loss: 4.1597, Avg Acc: 0.3040
2022-01-13 02:47:06,915 Epoch[022/300], Step[0350/1252], Avg Loss: 4.1559, Avg Acc: 0.3031
2022-01-13 02:48:32,229 Epoch[022/300], Step[0400/1252], Avg Loss: 4.1692, Avg Acc: 0.3009
2022-01-13 02:49:57,559 Epoch[022/300], Step[0450/1252], Avg Loss: 4.1700, Avg Acc: 0.3007
2022-01-13 02:51:23,129 Epoch[022/300], Step[0500/1252], Avg Loss: 4.1661, Avg Acc: 0.3005
2022-01-13 02:52:48,653 Epoch[022/300], Step[0550/1252], Avg Loss: 4.1725, Avg Acc: 0.3002
2022-01-13 02:54:13,531 Epoch[022/300], Step[0600/1252], Avg Loss: 4.1707, Avg Acc: 0.2986
2022-01-13 02:55:39,807 Epoch[022/300], Step[0650/1252], Avg Loss: 4.1696, Avg Acc: 0.2991
2022-01-13 02:57:05,675 Epoch[022/300], Step[0700/1252], Avg Loss: 4.1620, Avg Acc: 0.2989
2022-01-13 02:58:29,235 Epoch[022/300], Step[0750/1252], Avg Loss: 4.1616, Avg Acc: 0.2983
2022-01-13 02:59:54,496 Epoch[022/300], Step[0800/1252], Avg Loss: 4.1583, Avg Acc: 0.2981
2022-01-13 03:01:20,971 Epoch[022/300], Step[0850/1252], Avg Loss: 4.1609, Avg Acc: 0.2975
2022-01-13 03:02:46,608 Epoch[022/300], Step[0900/1252], Avg Loss: 4.1620, Avg Acc: 0.2971
2022-01-13 03:04:12,808 Epoch[022/300], Step[0950/1252], Avg Loss: 4.1600, Avg Acc: 0.2974
2022-01-13 03:05:37,691 Epoch[022/300], Step[1000/1252], Avg Loss: 4.1609, Avg Acc: 0.2964
2022-01-13 03:07:03,057 Epoch[022/300], Step[1050/1252], Avg Loss: 4.1602, Avg Acc: 0.2969
2022-01-13 03:08:29,307 Epoch[022/300], Step[1100/1252], Avg Loss: 4.1586, Avg Acc: 0.2980
2022-01-13 03:09:54,817 Epoch[022/300], Step[1150/1252], Avg Loss: 4.1551, Avg Acc: 0.2986
2022-01-13 03:11:20,373 Epoch[022/300], Step[1200/1252], Avg Loss: 4.1537, Avg Acc: 0.2987
2022-01-13 03:12:47,362 Epoch[022/300], Step[1250/1252], Avg Loss: 4.1561, Avg Acc: 0.2985
2022-01-13 03:12:54,457 ----- Epoch[022/300], Train Loss: 4.1560, Train Acc: 0.2985, time: 2240.95, Best Val(epoch20) Acc@1: 0.5728
2022-01-13 03:12:54,458 ----- Validation after Epoch: 22
2022-01-13 03:14:22,543 Val Step[0000/1563], Avg Loss: 1.6214, Avg Acc@1: 0.5938, Avg Acc@5: 0.9062
2022-01-13 03:14:24,623 Val Step[0050/1563], Avg Loss: 1.8949, Avg Acc@1: 0.5815, Avg Acc@5: 0.8223
2022-01-13 03:14:26,565 Val Step[0100/1563], Avg Loss: 1.9059, Avg Acc@1: 0.5795, Avg Acc@5: 0.8236
2022-01-13 03:14:28,406 Val Step[0150/1563], Avg Loss: 1.8967, Avg Acc@1: 0.5857, Avg Acc@5: 0.8231
2022-01-13 03:14:30,200 Val Step[0200/1563], Avg Loss: 1.8952, Avg Acc@1: 0.5864, Avg Acc@5: 0.8221
2022-01-13 03:14:31,987 Val Step[0250/1563], Avg Loss: 1.8741, Avg Acc@1: 0.5938, Avg Acc@5: 0.8242
2022-01-13 03:14:33,768 Val Step[0300/1563], Avg Loss: 1.8720, Avg Acc@1: 0.5962, Avg Acc@5: 0.8256
2022-01-13 03:14:35,574 Val Step[0350/1563], Avg Loss: 1.8747, Avg Acc@1: 0.5969, Avg Acc@5: 0.8250
2022-01-13 03:14:37,376 Val Step[0400/1563], Avg Loss: 1.8740, Avg Acc@1: 0.5963, Avg Acc@5: 0.8263
2022-01-13 03:14:39,286 Val Step[0450/1563], Avg Loss: 1.8823, Avg Acc@1: 0.5941, Avg Acc@5: 0.8246
2022-01-13 03:14:41,193 Val Step[0500/1563], Avg Loss: 1.8872, Avg Acc@1: 0.5931, Avg Acc@5: 0.8234
2022-01-13 03:14:42,976 Val Step[0550/1563], Avg Loss: 1.8903, Avg Acc@1: 0.5932, Avg Acc@5: 0.8232
2022-01-13 03:14:44,766 Val Step[0600/1563], Avg Loss: 1.8914, Avg Acc@1: 0.5923, Avg Acc@5: 0.8226
2022-01-13 03:14:46,554 Val Step[0650/1563], Avg Loss: 1.8942, Avg Acc@1: 0.5911, Avg Acc@5: 0.8228
2022-01-13 03:14:48,342 Val Step[0700/1563], Avg Loss: 1.8937, Avg Acc@1: 0.5904, Avg Acc@5: 0.8235
2022-01-13 03:14:50,122 Val Step[0750/1563], Avg Loss: 1.8986, Avg Acc@1: 0.5895, Avg Acc@5: 0.8229
2022-01-13 03:14:51,929 Val Step[0800/1563], Avg Loss: 1.8989, Avg Acc@1: 0.5890, Avg Acc@5: 0.8236
2022-01-13 03:14:53,709 Val Step[0850/1563], Avg Loss: 1.9011, Avg Acc@1: 0.5877, Avg Acc@5: 0.8236
2022-01-13 03:14:55,493 Val Step[0900/1563], Avg Loss: 1.8970, Avg Acc@1: 0.5888, Avg Acc@5: 0.8242
2022-01-13 03:14:57,288 Val Step[0950/1563], Avg Loss: 1.8974, Avg Acc@1: 0.5883, Avg Acc@5: 0.8244
2022-01-13 03:14:59,090 Val Step[1000/1563], Avg Loss: 1.8981, Avg Acc@1: 0.5876, Avg Acc@5: 0.8240
2022-01-13 03:15:01,210 Val Step[1050/1563], Avg Loss: 1.9014, Avg Acc@1: 0.5868, Avg Acc@5: 0.8234
2022-01-13 03:15:03,288 Val Step[1100/1563], Avg Loss: 1.9006, Avg Acc@1: 0.5868, Avg Acc@5: 0.8233
2022-01-13 03:15:05,165 Val Step[1150/1563], Avg Loss: 1.8990, Avg Acc@1: 0.5869, Avg Acc@5: 0.8236
2022-01-13 03:15:06,990 Val Step[1200/1563], Avg Loss: 1.8983, Avg Acc@1: 0.5870, Avg Acc@5: 0.8241
2022-01-13 03:15:08,799 Val Step[1250/1563], Avg Loss: 1.8962, Avg Acc@1: 0.5873, Avg Acc@5: 0.8243
2022-01-13 03:15:10,582 Val Step[1300/1563], Avg Loss: 1.8995, Avg Acc@1: 0.5869, Avg Acc@5: 0.8236
2022-01-13 03:15:12,424 Val Step[1350/1563], Avg Loss: 1.8993, Avg Acc@1: 0.5871, Avg Acc@5: 0.8234
2022-01-13 03:15:14,300 Val Step[1400/1563], Avg Loss: 1.8988, Avg Acc@1: 0.5868, Avg Acc@5: 0.8232
2022-01-13 03:15:16,146 Val Step[1450/1563], Avg Loss: 1.8997, Avg Acc@1: 0.5866, Avg Acc@5: 0.8231
2022-01-13 03:15:17,943 Val Step[1500/1563], Avg Loss: 1.8987, Avg Acc@1: 0.5870, Avg Acc@5: 0.8233
2022-01-13 03:15:19,701 Val Step[1550/1563], Avg Loss: 1.8993, Avg Acc@1: 0.5871, Avg Acc@5: 0.8230
2022-01-13 03:15:21,601 ----- Epoch[022/300], Validation Loss: 1.8992, Validation Acc@1: 0.5871, Validation Acc@5: 0.8230, time: 147.14
2022-01-13 03:15:23,009 the pre best model acc:0.5728, at epoch 20
2022-01-13 03:15:23,009 current best model acc:0.5871, at epoch 22
2022-01-13 03:15:23,010 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 03:15:23,010 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 03:15:23,010 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 03:15:23,010 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 03:15:23,010 Now training epoch 23. LR=0.001000
2022-01-13 03:17:18,830 Epoch[023/300], Step[0000/1252], Avg Loss: 3.9177, Avg Acc: 0.3916
2022-01-13 03:18:44,121 Epoch[023/300], Step[0050/1252], Avg Loss: 4.1029, Avg Acc: 0.2911
2022-01-13 03:20:09,560 Epoch[023/300], Step[0100/1252], Avg Loss: 4.1288, Avg Acc: 0.2909
2022-01-13 03:21:35,904 Epoch[023/300], Step[0150/1252], Avg Loss: 4.1113, Avg Acc: 0.2942
2022-01-13 03:23:02,091 Epoch[023/300], Step[0200/1252], Avg Loss: 4.1211, Avg Acc: 0.2961
2022-01-13 03:24:28,687 Epoch[023/300], Step[0250/1252], Avg Loss: 4.1184, Avg Acc: 0.2953
2022-01-13 03:25:54,258 Epoch[023/300], Step[0300/1252], Avg Loss: 4.1305, Avg Acc: 0.2958
2022-01-13 03:27:20,917 Epoch[023/300], Step[0350/1252], Avg Loss: 4.1246, Avg Acc: 0.2955
2022-01-13 03:28:46,646 Epoch[023/300], Step[0400/1252], Avg Loss: 4.1177, Avg Acc: 0.2965
2022-01-13 03:30:12,237 Epoch[023/300], Step[0450/1252], Avg Loss: 4.1187, Avg Acc: 0.2965
2022-01-13 03:31:37,550 Epoch[023/300], Step[0500/1252], Avg Loss: 4.1238, Avg Acc: 0.2985
2022-01-13 03:33:01,753 Epoch[023/300], Step[0550/1252], Avg Loss: 4.1252, Avg Acc: 0.3004
2022-01-13 03:34:26,966 Epoch[023/300], Step[0600/1252], Avg Loss: 4.1212, Avg Acc: 0.2987
2022-01-13 03:35:52,407 Epoch[023/300], Step[0650/1252], Avg Loss: 4.1230, Avg Acc: 0.2981
2022-01-13 03:37:17,361 Epoch[023/300], Step[0700/1252], Avg Loss: 4.1205, Avg Acc: 0.2999
2022-01-13 03:38:41,962 Epoch[023/300], Step[0750/1252], Avg Loss: 4.1190, Avg Acc: 0.3010
2022-01-13 03:40:07,636 Epoch[023/300], Step[0800/1252], Avg Loss: 4.1201, Avg Acc: 0.3012
2022-01-13 03:41:33,196 Epoch[023/300], Step[0850/1252], Avg Loss: 4.1183, Avg Acc: 0.3022
2022-01-13 03:42:59,567 Epoch[023/300], Step[0900/1252], Avg Loss: 4.1139, Avg Acc: 0.3034
2022-01-13 03:44:26,024 Epoch[023/300], Step[0950/1252], Avg Loss: 4.1133, Avg Acc: 0.3031
2022-01-13 03:45:53,300 Epoch[023/300], Step[1000/1252], Avg Loss: 4.1127, Avg Acc: 0.3029
2022-01-13 03:47:20,242 Epoch[023/300], Step[1050/1252], Avg Loss: 4.1143, Avg Acc: 0.3015
2022-01-13 03:48:46,683 Epoch[023/300], Step[1100/1252], Avg Loss: 4.1165, Avg Acc: 0.3003
2022-01-13 03:50:12,970 Epoch[023/300], Step[1150/1252], Avg Loss: 4.1201, Avg Acc: 0.2999
2022-01-13 03:51:40,444 Epoch[023/300], Step[1200/1252], Avg Loss: 4.1213, Avg Acc: 0.2999
2022-01-13 03:53:08,403 Epoch[023/300], Step[1250/1252], Avg Loss: 4.1224, Avg Acc: 0.2996
2022-01-13 03:53:15,508 ----- Epoch[023/300], Train Loss: 4.1225, Train Acc: 0.2996, time: 2272.49, Best Val(epoch22) Acc@1: 0.5871
2022-01-13 03:53:15,509 Now training epoch 24. LR=0.001000
2022-01-13 03:55:00,767 Epoch[024/300], Step[0000/1252], Avg Loss: 3.5118, Avg Acc: 0.0859
2022-01-13 03:56:26,162 Epoch[024/300], Step[0050/1252], Avg Loss: 4.1020, Avg Acc: 0.3096
2022-01-13 03:57:51,636 Epoch[024/300], Step[0100/1252], Avg Loss: 4.0862, Avg Acc: 0.2997
2022-01-13 03:59:16,913 Epoch[024/300], Step[0150/1252], Avg Loss: 4.0716, Avg Acc: 0.3106
2022-01-13 04:00:41,557 Epoch[024/300], Step[0200/1252], Avg Loss: 4.0655, Avg Acc: 0.3118
2022-01-13 04:02:06,562 Epoch[024/300], Step[0250/1252], Avg Loss: 4.0650, Avg Acc: 0.3030
2022-01-13 04:03:31,557 Epoch[024/300], Step[0300/1252], Avg Loss: 4.0794, Avg Acc: 0.3013
2022-01-13 04:04:56,005 Epoch[024/300], Step[0350/1252], Avg Loss: 4.0741, Avg Acc: 0.3059
2022-01-13 04:06:21,710 Epoch[024/300], Step[0400/1252], Avg Loss: 4.0737, Avg Acc: 0.3089
2022-01-13 04:07:48,097 Epoch[024/300], Step[0450/1252], Avg Loss: 4.0776, Avg Acc: 0.3083
2022-01-13 04:09:13,110 Epoch[024/300], Step[0500/1252], Avg Loss: 4.0825, Avg Acc: 0.3071
2022-01-13 04:10:39,040 Epoch[024/300], Step[0550/1252], Avg Loss: 4.0842, Avg Acc: 0.3078
2022-01-13 04:12:04,550 Epoch[024/300], Step[0600/1252], Avg Loss: 4.0826, Avg Acc: 0.3084
2022-01-13 04:13:30,693 Epoch[024/300], Step[0650/1252], Avg Loss: 4.0794, Avg Acc: 0.3077
2022-01-13 04:14:56,632 Epoch[024/300], Step[0700/1252], Avg Loss: 4.0821, Avg Acc: 0.3082
2022-01-13 04:16:22,870 Epoch[024/300], Step[0750/1252], Avg Loss: 4.0784, Avg Acc: 0.3087
2022-01-13 04:17:48,205 Epoch[024/300], Step[0800/1252], Avg Loss: 4.0774, Avg Acc: 0.3072
2022-01-13 04:19:14,042 Epoch[024/300], Step[0850/1252], Avg Loss: 4.0768, Avg Acc: 0.3075
2022-01-13 04:20:39,588 Epoch[024/300], Step[0900/1252], Avg Loss: 4.0801, Avg Acc: 0.3053
2022-01-13 04:22:05,397 Epoch[024/300], Step[0950/1252], Avg Loss: 4.0815, Avg Acc: 0.3053
2022-01-13 04:23:31,861 Epoch[024/300], Step[1000/1252], Avg Loss: 4.0786, Avg Acc: 0.3054
2022-01-13 04:24:57,669 Epoch[024/300], Step[1050/1252], Avg Loss: 4.0812, Avg Acc: 0.3039
2022-01-13 04:26:25,032 Epoch[024/300], Step[1100/1252], Avg Loss: 4.0789, Avg Acc: 0.3061
2022-01-13 04:27:50,039 Epoch[024/300], Step[1150/1252], Avg Loss: 4.0801, Avg Acc: 0.3070
2022-01-13 04:29:16,963 Epoch[024/300], Step[1200/1252], Avg Loss: 4.0834, Avg Acc: 0.3060
2022-01-13 04:30:44,730 Epoch[024/300], Step[1250/1252], Avg Loss: 4.0860, Avg Acc: 0.3062
2022-01-13 04:30:51,739 ----- Epoch[024/300], Train Loss: 4.0860, Train Acc: 0.3062, time: 2256.23, Best Val(epoch22) Acc@1: 0.5871
2022-01-13 04:30:51,739 ----- Validation after Epoch: 24
2022-01-13 04:32:18,331 Val Step[0000/1563], Avg Loss: 1.6331, Avg Acc@1: 0.6562, Avg Acc@5: 0.9062
2022-01-13 04:32:20,258 Val Step[0050/1563], Avg Loss: 1.8138, Avg Acc@1: 0.6078, Avg Acc@5: 0.8523
2022-01-13 04:32:22,085 Val Step[0100/1563], Avg Loss: 1.8429, Avg Acc@1: 0.6009, Avg Acc@5: 0.8453
2022-01-13 04:32:24,010 Val Step[0150/1563], Avg Loss: 1.8435, Avg Acc@1: 0.5998, Avg Acc@5: 0.8421
2022-01-13 04:32:25,812 Val Step[0200/1563], Avg Loss: 1.8466, Avg Acc@1: 0.6012, Avg Acc@5: 0.8403
2022-01-13 04:32:27,749 Val Step[0250/1563], Avg Loss: 1.8283, Avg Acc@1: 0.6068, Avg Acc@5: 0.8424
2022-01-13 04:32:29,788 Val Step[0300/1563], Avg Loss: 1.8273, Avg Acc@1: 0.6069, Avg Acc@5: 0.8425
2022-01-13 04:32:31,832 Val Step[0350/1563], Avg Loss: 1.8342, Avg Acc@1: 0.6064, Avg Acc@5: 0.8409
2022-01-13 04:32:33,895 Val Step[0400/1563], Avg Loss: 1.8334, Avg Acc@1: 0.6063, Avg Acc@5: 0.8404
2022-01-13 04:32:36,064 Val Step[0450/1563], Avg Loss: 1.8440, Avg Acc@1: 0.6021, Avg Acc@5: 0.8387
2022-01-13 04:32:38,185 Val Step[0500/1563], Avg Loss: 1.8461, Avg Acc@1: 0.6014, Avg Acc@5: 0.8387
2022-01-13 04:32:40,319 Val Step[0550/1563], Avg Loss: 1.8491, Avg Acc@1: 0.6000, Avg Acc@5: 0.8382
2022-01-13 04:32:42,443 Val Step[0600/1563], Avg Loss: 1.8486, Avg Acc@1: 0.6004, Avg Acc@5: 0.8381
2022-01-13 04:32:44,557 Val Step[0650/1563], Avg Loss: 1.8510, Avg Acc@1: 0.6000, Avg Acc@5: 0.8387
2022-01-13 04:32:46,676 Val Step[0700/1563], Avg Loss: 1.8495, Avg Acc@1: 0.6011, Avg Acc@5: 0.8396
2022-01-13 04:32:48,746 Val Step[0750/1563], Avg Loss: 1.8556, Avg Acc@1: 0.5992, Avg Acc@5: 0.8381
2022-01-13 04:32:50,876 Val Step[0800/1563], Avg Loss: 1.8533, Avg Acc@1: 0.6000, Avg Acc@5: 0.8391
2022-01-13 04:32:53,006 Val Step[0850/1563], Avg Loss: 1.8556, Avg Acc@1: 0.5990, Avg Acc@5: 0.8391
2022-01-13 04:32:55,148 Val Step[0900/1563], Avg Loss: 1.8531, Avg Acc@1: 0.5998, Avg Acc@5: 0.8396
2022-01-13 04:32:57,299 Val Step[0950/1563], Avg Loss: 1.8529, Avg Acc@1: 0.5997, Avg Acc@5: 0.8394
2022-01-13 04:32:59,431 Val Step[1000/1563], Avg Loss: 1.8514, Avg Acc@1: 0.6003, Avg Acc@5: 0.8390
2022-01-13 04:33:01,412 Val Step[1050/1563], Avg Loss: 1.8539, Avg Acc@1: 0.5993, Avg Acc@5: 0.8385
2022-01-13 04:33:03,242 Val Step[1100/1563], Avg Loss: 1.8531, Avg Acc@1: 0.5993, Avg Acc@5: 0.8382
2022-01-13 04:33:05,017 Val Step[1150/1563], Avg Loss: 1.8520, Avg Acc@1: 0.6002, Avg Acc@5: 0.8379
2022-01-13 04:33:06,937 Val Step[1200/1563], Avg Loss: 1.8519, Avg Acc@1: 0.6004, Avg Acc@5: 0.8379
2022-01-13 04:33:08,756 Val Step[1250/1563], Avg Loss: 1.8495, Avg Acc@1: 0.6013, Avg Acc@5: 0.8385
2022-01-13 04:33:10,605 Val Step[1300/1563], Avg Loss: 1.8540, Avg Acc@1: 0.6007, Avg Acc@5: 0.8375
2022-01-13 04:33:12,393 Val Step[1350/1563], Avg Loss: 1.8537, Avg Acc@1: 0.6009, Avg Acc@5: 0.8374
2022-01-13 04:33:14,244 Val Step[1400/1563], Avg Loss: 1.8538, Avg Acc@1: 0.6006, Avg Acc@5: 0.8373
2022-01-13 04:33:16,134 Val Step[1450/1563], Avg Loss: 1.8537, Avg Acc@1: 0.6007, Avg Acc@5: 0.8374
2022-01-13 04:33:17,975 Val Step[1500/1563], Avg Loss: 1.8528, Avg Acc@1: 0.6012, Avg Acc@5: 0.8376
2022-01-13 04:33:19,696 Val Step[1550/1563], Avg Loss: 1.8534, Avg Acc@1: 0.6011, Avg Acc@5: 0.8371
2022-01-13 04:33:21,593 ----- Epoch[024/300], Validation Loss: 1.8537, Validation Acc@1: 0.6010, Validation Acc@5: 0.8371, time: 149.85
2022-01-13 04:33:22,910 the pre best model acc:0.5871, at epoch 22
2022-01-13 04:33:23,178 current best model acc:0.6010, at epoch 24
2022-01-13 04:33:23,178 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 04:33:23,178 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 04:33:23,178 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 04:33:23,178 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 04:33:23,179 Now training epoch 25. LR=0.000999
2022-01-13 04:35:06,960 Epoch[025/300], Step[0000/1252], Avg Loss: 4.1476, Avg Acc: 0.3154
2022-01-13 04:36:31,985 Epoch[025/300], Step[0050/1252], Avg Loss: 4.0589, Avg Acc: 0.3004
2022-01-13 04:37:56,906 Epoch[025/300], Step[0100/1252], Avg Loss: 4.0986, Avg Acc: 0.2977
2022-01-13 04:39:23,562 Epoch[025/300], Step[0150/1252], Avg Loss: 4.1035, Avg Acc: 0.3026
2022-01-13 04:40:48,279 Epoch[025/300], Step[0200/1252], Avg Loss: 4.1033, Avg Acc: 0.3051
2022-01-13 04:42:13,984 Epoch[025/300], Step[0250/1252], Avg Loss: 4.1029, Avg Acc: 0.3031
2022-01-13 04:43:40,351 Epoch[025/300], Step[0300/1252], Avg Loss: 4.0975, Avg Acc: 0.3030
2022-01-13 04:45:05,978 Epoch[025/300], Step[0350/1252], Avg Loss: 4.0963, Avg Acc: 0.3034
2022-01-13 04:46:31,961 Epoch[025/300], Step[0400/1252], Avg Loss: 4.0984, Avg Acc: 0.3057
2022-01-13 04:47:57,517 Epoch[025/300], Step[0450/1252], Avg Loss: 4.1014, Avg Acc: 0.3077
2022-01-13 04:49:23,913 Epoch[025/300], Step[0500/1252], Avg Loss: 4.0957, Avg Acc: 0.3061
2022-01-13 04:50:49,971 Epoch[025/300], Step[0550/1252], Avg Loss: 4.0862, Avg Acc: 0.3073
2022-01-13 04:52:15,081 Epoch[025/300], Step[0600/1252], Avg Loss: 4.0833, Avg Acc: 0.3072
2022-01-13 04:53:40,652 Epoch[025/300], Step[0650/1252], Avg Loss: 4.0802, Avg Acc: 0.3068
2022-01-13 04:55:05,400 Epoch[025/300], Step[0700/1252], Avg Loss: 4.0798, Avg Acc: 0.3055
2022-01-13 04:56:30,853 Epoch[025/300], Step[0750/1252], Avg Loss: 4.0790, Avg Acc: 0.3052
2022-01-13 04:57:55,921 Epoch[025/300], Step[0800/1252], Avg Loss: 4.0853, Avg Acc: 0.3051
2022-01-13 04:59:21,949 Epoch[025/300], Step[0850/1252], Avg Loss: 4.0878, Avg Acc: 0.3048
2022-01-13 05:00:48,207 Epoch[025/300], Step[0900/1252], Avg Loss: 4.0854, Avg Acc: 0.3054
2022-01-13 05:02:15,137 Epoch[025/300], Step[0950/1252], Avg Loss: 4.0826, Avg Acc: 0.3058
2022-01-13 05:03:40,505 Epoch[025/300], Step[1000/1252], Avg Loss: 4.0807, Avg Acc: 0.3063
2022-01-13 05:05:05,816 Epoch[025/300], Step[1050/1252], Avg Loss: 4.0792, Avg Acc: 0.3064
2022-01-13 05:06:31,032 Epoch[025/300], Step[1100/1252], Avg Loss: 4.0834, Avg Acc: 0.3056
2022-01-13 05:07:56,609 Epoch[025/300], Step[1150/1252], Avg Loss: 4.0842, Avg Acc: 0.3062
2022-01-13 05:09:23,145 Epoch[025/300], Step[1200/1252], Avg Loss: 4.0831, Avg Acc: 0.3061
2022-01-13 05:10:50,998 Epoch[025/300], Step[1250/1252], Avg Loss: 4.0854, Avg Acc: 0.3062
2022-01-13 05:10:58,219 ----- Epoch[025/300], Train Loss: 4.0853, Train Acc: 0.3062, time: 2255.04, Best Val(epoch24) Acc@1: 0.6010
2022-01-13 05:10:58,219 Now training epoch 26. LR=0.000999
2022-01-13 05:12:42,079 Epoch[026/300], Step[0000/1252], Avg Loss: 4.5431, Avg Acc: 0.3184
2022-01-13 05:14:07,613 Epoch[026/300], Step[0050/1252], Avg Loss: 4.0979, Avg Acc: 0.3133
2022-01-13 05:15:31,770 Epoch[026/300], Step[0100/1252], Avg Loss: 4.0583, Avg Acc: 0.3161
2022-01-13 05:16:56,507 Epoch[026/300], Step[0150/1252], Avg Loss: 4.0588, Avg Acc: 0.3082
2022-01-13 05:18:22,341 Epoch[026/300], Step[0200/1252], Avg Loss: 4.0494, Avg Acc: 0.3078
2022-01-13 05:19:47,446 Epoch[026/300], Step[0250/1252], Avg Loss: 4.0357, Avg Acc: 0.3102
2022-01-13 05:21:13,664 Epoch[026/300], Step[0300/1252], Avg Loss: 4.0360, Avg Acc: 0.3110
2022-01-13 05:22:39,575 Epoch[026/300], Step[0350/1252], Avg Loss: 4.0322, Avg Acc: 0.3131
2022-01-13 05:24:05,597 Epoch[026/300], Step[0400/1252], Avg Loss: 4.0434, Avg Acc: 0.3111
2022-01-13 05:25:31,570 Epoch[026/300], Step[0450/1252], Avg Loss: 4.0468, Avg Acc: 0.3096
2022-01-13 05:26:58,423 Epoch[026/300], Step[0500/1252], Avg Loss: 4.0508, Avg Acc: 0.3066
2022-01-13 05:28:23,901 Epoch[026/300], Step[0550/1252], Avg Loss: 4.0504, Avg Acc: 0.3082
2022-01-13 05:29:50,697 Epoch[026/300], Step[0600/1252], Avg Loss: 4.0517, Avg Acc: 0.3068
2022-01-13 05:31:17,127 Epoch[026/300], Step[0650/1252], Avg Loss: 4.0499, Avg Acc: 0.3059
2022-01-13 05:32:43,307 Epoch[026/300], Step[0700/1252], Avg Loss: 4.0487, Avg Acc: 0.3068
2022-01-13 05:34:10,016 Epoch[026/300], Step[0750/1252], Avg Loss: 4.0465, Avg Acc: 0.3081
2022-01-13 05:35:35,439 Epoch[026/300], Step[0800/1252], Avg Loss: 4.0494, Avg Acc: 0.3091
2022-01-13 05:37:02,038 Epoch[026/300], Step[0850/1252], Avg Loss: 4.0510, Avg Acc: 0.3084
2022-01-13 05:38:27,823 Epoch[026/300], Step[0900/1252], Avg Loss: 4.0558, Avg Acc: 0.3082
2022-01-13 05:39:53,869 Epoch[026/300], Step[0950/1252], Avg Loss: 4.0542, Avg Acc: 0.3071
2022-01-13 05:41:19,484 Epoch[026/300], Step[1000/1252], Avg Loss: 4.0545, Avg Acc: 0.3064
2022-01-13 05:42:45,887 Epoch[026/300], Step[1050/1252], Avg Loss: 4.0557, Avg Acc: 0.3063
2022-01-13 05:44:12,461 Epoch[026/300], Step[1100/1252], Avg Loss: 4.0560, Avg Acc: 0.3065
2022-01-13 05:45:38,420 Epoch[026/300], Step[1150/1252], Avg Loss: 4.0551, Avg Acc: 0.3073
2022-01-13 05:47:03,007 Epoch[026/300], Step[1200/1252], Avg Loss: 4.0515, Avg Acc: 0.3085
2022-01-13 05:48:29,070 Epoch[026/300], Step[1250/1252], Avg Loss: 4.0508, Avg Acc: 0.3094
2022-01-13 05:48:36,367 ----- Epoch[026/300], Train Loss: 4.0507, Train Acc: 0.3094, time: 2258.14, Best Val(epoch24) Acc@1: 0.6010
2022-01-13 05:48:36,367 ----- Validation after Epoch: 26
2022-01-13 05:49:49,265 Val Step[0000/1563], Avg Loss: 1.5251, Avg Acc@1: 0.6562, Avg Acc@5: 0.8750
2022-01-13 05:49:51,221 Val Step[0050/1563], Avg Loss: 1.7420, Avg Acc@1: 0.6183, Avg Acc@5: 0.8462
2022-01-13 05:49:53,016 Val Step[0100/1563], Avg Loss: 1.7622, Avg Acc@1: 0.6126, Avg Acc@5: 0.8441
2022-01-13 05:49:54,859 Val Step[0150/1563], Avg Loss: 1.7621, Avg Acc@1: 0.6115, Avg Acc@5: 0.8431
2022-01-13 05:49:56,761 Val Step[0200/1563], Avg Loss: 1.7654, Avg Acc@1: 0.6118, Avg Acc@5: 0.8388
2022-01-13 05:49:58,555 Val Step[0250/1563], Avg Loss: 1.7462, Avg Acc@1: 0.6160, Avg Acc@5: 0.8434
2022-01-13 05:50:00,350 Val Step[0300/1563], Avg Loss: 1.7413, Avg Acc@1: 0.6176, Avg Acc@5: 0.8445
2022-01-13 05:50:02,230 Val Step[0350/1563], Avg Loss: 1.7485, Avg Acc@1: 0.6165, Avg Acc@5: 0.8431
2022-01-13 05:50:04,158 Val Step[0400/1563], Avg Loss: 1.7457, Avg Acc@1: 0.6185, Avg Acc@5: 0.8436
2022-01-13 05:50:06,079 Val Step[0450/1563], Avg Loss: 1.7552, Avg Acc@1: 0.6150, Avg Acc@5: 0.8417
2022-01-13 05:50:08,021 Val Step[0500/1563], Avg Loss: 1.7597, Avg Acc@1: 0.6148, Avg Acc@5: 0.8407
2022-01-13 05:50:09,871 Val Step[0550/1563], Avg Loss: 1.7626, Avg Acc@1: 0.6149, Avg Acc@5: 0.8403
2022-01-13 05:50:11,748 Val Step[0600/1563], Avg Loss: 1.7614, Avg Acc@1: 0.6155, Avg Acc@5: 0.8405
2022-01-13 05:50:13,586 Val Step[0650/1563], Avg Loss: 1.7617, Avg Acc@1: 0.6151, Avg Acc@5: 0.8405
2022-01-13 05:50:15,470 Val Step[0700/1563], Avg Loss: 1.7620, Avg Acc@1: 0.6151, Avg Acc@5: 0.8410
2022-01-13 05:50:17,401 Val Step[0750/1563], Avg Loss: 1.7677, Avg Acc@1: 0.6138, Avg Acc@5: 0.8406
2022-01-13 05:50:19,341 Val Step[0800/1563], Avg Loss: 1.7658, Avg Acc@1: 0.6147, Avg Acc@5: 0.8412
2022-01-13 05:50:21,276 Val Step[0850/1563], Avg Loss: 1.7676, Avg Acc@1: 0.6144, Avg Acc@5: 0.8408
2022-01-13 05:50:23,154 Val Step[0900/1563], Avg Loss: 1.7654, Avg Acc@1: 0.6143, Avg Acc@5: 0.8416
2022-01-13 05:50:25,089 Val Step[0950/1563], Avg Loss: 1.7657, Avg Acc@1: 0.6137, Avg Acc@5: 0.8419
2022-01-13 05:50:26,904 Val Step[1000/1563], Avg Loss: 1.7658, Avg Acc@1: 0.6142, Avg Acc@5: 0.8414
2022-01-13 05:50:28,738 Val Step[1050/1563], Avg Loss: 1.7699, Avg Acc@1: 0.6131, Avg Acc@5: 0.8407
2022-01-13 05:50:30,551 Val Step[1100/1563], Avg Loss: 1.7688, Avg Acc@1: 0.6129, Avg Acc@5: 0.8406
2022-01-13 05:50:32,413 Val Step[1150/1563], Avg Loss: 1.7674, Avg Acc@1: 0.6129, Avg Acc@5: 0.8407
2022-01-13 05:50:34,407 Val Step[1200/1563], Avg Loss: 1.7664, Avg Acc@1: 0.6133, Avg Acc@5: 0.8411
2022-01-13 05:50:36,278 Val Step[1250/1563], Avg Loss: 1.7651, Avg Acc@1: 0.6134, Avg Acc@5: 0.8412
2022-01-13 05:50:38,069 Val Step[1300/1563], Avg Loss: 1.7675, Avg Acc@1: 0.6129, Avg Acc@5: 0.8412
2022-01-13 05:50:39,852 Val Step[1350/1563], Avg Loss: 1.7668, Avg Acc@1: 0.6132, Avg Acc@5: 0.8414
2022-01-13 05:50:41,646 Val Step[1400/1563], Avg Loss: 1.7660, Avg Acc@1: 0.6130, Avg Acc@5: 0.8413
2022-01-13 05:50:43,488 Val Step[1450/1563], Avg Loss: 1.7661, Avg Acc@1: 0.6129, Avg Acc@5: 0.8410
2022-01-13 05:50:45,524 Val Step[1500/1563], Avg Loss: 1.7650, Avg Acc@1: 0.6135, Avg Acc@5: 0.8414
2022-01-13 05:50:47,571 Val Step[1550/1563], Avg Loss: 1.7658, Avg Acc@1: 0.6135, Avg Acc@5: 0.8413
2022-01-13 05:50:49,526 ----- Epoch[026/300], Validation Loss: 1.7660, Validation Acc@1: 0.6135, Validation Acc@5: 0.8412, time: 133.16
2022-01-13 05:50:50,853 the pre best model acc:0.6010, at epoch 24
2022-01-13 05:50:51,120 current best model acc:0.6135, at epoch 26
2022-01-13 05:50:51,121 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 05:50:51,121 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 05:50:51,121 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 05:50:51,121 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 05:50:51,121 Now training epoch 27. LR=0.000998
2022-01-13 05:52:32,369 Epoch[027/300], Step[0000/1252], Avg Loss: 4.4491, Avg Acc: 0.3096
2022-01-13 05:53:58,411 Epoch[027/300], Step[0050/1252], Avg Loss: 4.0943, Avg Acc: 0.3220
2022-01-13 05:55:23,474 Epoch[027/300], Step[0100/1252], Avg Loss: 4.0633, Avg Acc: 0.3273
2022-01-13 05:56:49,357 Epoch[027/300], Step[0150/1252], Avg Loss: 4.0483, Avg Acc: 0.3220
2022-01-13 05:58:14,766 Epoch[027/300], Step[0200/1252], Avg Loss: 4.0371, Avg Acc: 0.3160
2022-01-13 05:59:40,831 Epoch[027/300], Step[0250/1252], Avg Loss: 4.0355, Avg Acc: 0.3169
2022-01-13 06:01:05,996 Epoch[027/300], Step[0300/1252], Avg Loss: 4.0419, Avg Acc: 0.3168
2022-01-13 06:02:31,475 Epoch[027/300], Step[0350/1252], Avg Loss: 4.0322, Avg Acc: 0.3171
2022-01-13 06:03:56,782 Epoch[027/300], Step[0400/1252], Avg Loss: 4.0311, Avg Acc: 0.3156
2022-01-13 06:05:22,637 Epoch[027/300], Step[0450/1252], Avg Loss: 4.0239, Avg Acc: 0.3161
2022-01-13 06:06:47,583 Epoch[027/300], Step[0500/1252], Avg Loss: 4.0323, Avg Acc: 0.3149
2022-01-13 06:08:12,510 Epoch[027/300], Step[0550/1252], Avg Loss: 4.0325, Avg Acc: 0.3146
2022-01-13 06:09:38,560 Epoch[027/300], Step[0600/1252], Avg Loss: 4.0315, Avg Acc: 0.3135
2022-01-13 06:11:03,861 Epoch[027/300], Step[0650/1252], Avg Loss: 4.0350, Avg Acc: 0.3128
2022-01-13 06:12:27,992 Epoch[027/300], Step[0700/1252], Avg Loss: 4.0350, Avg Acc: 0.3143
2022-01-13 06:13:52,887 Epoch[027/300], Step[0750/1252], Avg Loss: 4.0361, Avg Acc: 0.3126
2022-01-13 06:15:18,252 Epoch[027/300], Step[0800/1252], Avg Loss: 4.0387, Avg Acc: 0.3108
2022-01-13 06:16:42,853 Epoch[027/300], Step[0850/1252], Avg Loss: 4.0369, Avg Acc: 0.3123
2022-01-13 06:18:08,026 Epoch[027/300], Step[0900/1252], Avg Loss: 4.0344, Avg Acc: 0.3131
2022-01-13 06:19:32,352 Epoch[027/300], Step[0950/1252], Avg Loss: 4.0362, Avg Acc: 0.3136
2022-01-13 06:20:55,735 Epoch[027/300], Step[1000/1252], Avg Loss: 4.0365, Avg Acc: 0.3141
2022-01-13 06:22:21,363 Epoch[027/300], Step[1050/1252], Avg Loss: 4.0351, Avg Acc: 0.3140
2022-01-13 06:23:47,370 Epoch[027/300], Step[1100/1252], Avg Loss: 4.0337, Avg Acc: 0.3149
2022-01-13 06:25:14,087 Epoch[027/300], Step[1150/1252], Avg Loss: 4.0313, Avg Acc: 0.3157
2022-01-13 06:26:39,288 Epoch[027/300], Step[1200/1252], Avg Loss: 4.0328, Avg Acc: 0.3158
2022-01-13 06:28:06,626 Epoch[027/300], Step[1250/1252], Avg Loss: 4.0303, Avg Acc: 0.3161
2022-01-13 06:28:13,862 ----- Epoch[027/300], Train Loss: 4.0303, Train Acc: 0.3161, time: 2242.74, Best Val(epoch26) Acc@1: 0.6135
2022-01-13 06:28:13,863 Now training epoch 28. LR=0.000998
2022-01-13 06:29:53,307 Epoch[028/300], Step[0000/1252], Avg Loss: 4.3102, Avg Acc: 0.3184
2022-01-13 06:31:18,746 Epoch[028/300], Step[0050/1252], Avg Loss: 4.0547, Avg Acc: 0.3120
2022-01-13 06:32:42,558 Epoch[028/300], Step[0100/1252], Avg Loss: 4.0356, Avg Acc: 0.3059
2022-01-13 06:34:07,397 Epoch[028/300], Step[0150/1252], Avg Loss: 4.0250, Avg Acc: 0.3060
2022-01-13 06:35:33,607 Epoch[028/300], Step[0200/1252], Avg Loss: 4.0214, Avg Acc: 0.3117
2022-01-13 06:36:58,841 Epoch[028/300], Step[0250/1252], Avg Loss: 4.0158, Avg Acc: 0.3138
2022-01-13 06:38:25,041 Epoch[028/300], Step[0300/1252], Avg Loss: 4.0129, Avg Acc: 0.3133
2022-01-13 06:39:50,076 Epoch[028/300], Step[0350/1252], Avg Loss: 4.0076, Avg Acc: 0.3157
2022-01-13 06:41:15,883 Epoch[028/300], Step[0400/1252], Avg Loss: 4.0003, Avg Acc: 0.3164
2022-01-13 06:42:42,717 Epoch[028/300], Step[0450/1252], Avg Loss: 4.0014, Avg Acc: 0.3158
2022-01-13 06:44:09,082 Epoch[028/300], Step[0500/1252], Avg Loss: 3.9973, Avg Acc: 0.3177
2022-01-13 06:45:35,172 Epoch[028/300], Step[0550/1252], Avg Loss: 4.0000, Avg Acc: 0.3195
2022-01-13 06:47:01,199 Epoch[028/300], Step[0600/1252], Avg Loss: 4.0074, Avg Acc: 0.3192
2022-01-13 06:48:27,551 Epoch[028/300], Step[0650/1252], Avg Loss: 4.0022, Avg Acc: 0.3196
2022-01-13 06:49:53,444 Epoch[028/300], Step[0700/1252], Avg Loss: 4.0019, Avg Acc: 0.3186
2022-01-13 06:51:19,679 Epoch[028/300], Step[0750/1252], Avg Loss: 4.0036, Avg Acc: 0.3192
2022-01-13 06:52:43,842 Epoch[028/300], Step[0800/1252], Avg Loss: 4.0020, Avg Acc: 0.3194
2022-01-13 06:54:09,901 Epoch[028/300], Step[0850/1252], Avg Loss: 3.9984, Avg Acc: 0.3198
2022-01-13 06:55:36,187 Epoch[028/300], Step[0900/1252], Avg Loss: 4.0016, Avg Acc: 0.3195
2022-01-13 06:57:01,521 Epoch[028/300], Step[0950/1252], Avg Loss: 4.0036, Avg Acc: 0.3195
2022-01-13 06:58:28,245 Epoch[028/300], Step[1000/1252], Avg Loss: 4.0049, Avg Acc: 0.3199
2022-01-13 06:59:54,209 Epoch[028/300], Step[1050/1252], Avg Loss: 4.0049, Avg Acc: 0.3199
2022-01-13 07:01:19,618 Epoch[028/300], Step[1100/1252], Avg Loss: 4.0033, Avg Acc: 0.3193
2022-01-13 07:02:44,568 Epoch[028/300], Step[1150/1252], Avg Loss: 4.0058, Avg Acc: 0.3186
2022-01-13 07:04:11,220 Epoch[028/300], Step[1200/1252], Avg Loss: 4.0042, Avg Acc: 0.3200
2022-01-13 07:05:37,193 Epoch[028/300], Step[1250/1252], Avg Loss: 4.0021, Avg Acc: 0.3200
2022-01-13 07:05:44,394 ----- Epoch[028/300], Train Loss: 4.0021, Train Acc: 0.3199, time: 2250.53, Best Val(epoch26) Acc@1: 0.6135
2022-01-13 07:05:44,394 ----- Validation after Epoch: 28
2022-01-13 07:06:50,071 Val Step[0000/1563], Avg Loss: 1.5701, Avg Acc@1: 0.6250, Avg Acc@5: 0.9375
2022-01-13 07:06:52,015 Val Step[0050/1563], Avg Loss: 1.7532, Avg Acc@1: 0.6219, Avg Acc@5: 0.8456
2022-01-13 07:06:53,967 Val Step[0100/1563], Avg Loss: 1.7729, Avg Acc@1: 0.6170, Avg Acc@5: 0.8444
2022-01-13 07:06:55,747 Val Step[0150/1563], Avg Loss: 1.7667, Avg Acc@1: 0.6173, Avg Acc@5: 0.8448
2022-01-13 07:06:57,527 Val Step[0200/1563], Avg Loss: 1.7693, Avg Acc@1: 0.6166, Avg Acc@5: 0.8431
2022-01-13 07:06:59,424 Val Step[0250/1563], Avg Loss: 1.7419, Avg Acc@1: 0.6219, Avg Acc@5: 0.8470
2022-01-13 07:07:01,257 Val Step[0300/1563], Avg Loss: 1.7395, Avg Acc@1: 0.6235, Avg Acc@5: 0.8472
2022-01-13 07:07:03,058 Val Step[0350/1563], Avg Loss: 1.7457, Avg Acc@1: 0.6222, Avg Acc@5: 0.8467
2022-01-13 07:07:04,897 Val Step[0400/1563], Avg Loss: 1.7445, Avg Acc@1: 0.6213, Avg Acc@5: 0.8469
2022-01-13 07:07:06,804 Val Step[0450/1563], Avg Loss: 1.7515, Avg Acc@1: 0.6173, Avg Acc@5: 0.8458
2022-01-13 07:07:08,703 Val Step[0500/1563], Avg Loss: 1.7537, Avg Acc@1: 0.6170, Avg Acc@5: 0.8446
2022-01-13 07:07:10,547 Val Step[0550/1563], Avg Loss: 1.7553, Avg Acc@1: 0.6165, Avg Acc@5: 0.8444
2022-01-13 07:07:12,355 Val Step[0600/1563], Avg Loss: 1.7541, Avg Acc@1: 0.6170, Avg Acc@5: 0.8445
2022-01-13 07:07:14,135 Val Step[0650/1563], Avg Loss: 1.7550, Avg Acc@1: 0.6167, Avg Acc@5: 0.8449
2022-01-13 07:07:15,985 Val Step[0700/1563], Avg Loss: 1.7545, Avg Acc@1: 0.6165, Avg Acc@5: 0.8453
2022-01-13 07:07:17,785 Val Step[0750/1563], Avg Loss: 1.7604, Avg Acc@1: 0.6146, Avg Acc@5: 0.8442
2022-01-13 07:07:19,627 Val Step[0800/1563], Avg Loss: 1.7582, Avg Acc@1: 0.6154, Avg Acc@5: 0.8452
2022-01-13 07:07:21,430 Val Step[0850/1563], Avg Loss: 1.7596, Avg Acc@1: 0.6148, Avg Acc@5: 0.8448
2022-01-13 07:07:23,300 Val Step[0900/1563], Avg Loss: 1.7556, Avg Acc@1: 0.6158, Avg Acc@5: 0.8456
2022-01-13 07:07:25,187 Val Step[0950/1563], Avg Loss: 1.7551, Avg Acc@1: 0.6153, Avg Acc@5: 0.8456
2022-01-13 07:07:27,008 Val Step[1000/1563], Avg Loss: 1.7534, Avg Acc@1: 0.6155, Avg Acc@5: 0.8455
2022-01-13 07:07:28,807 Val Step[1050/1563], Avg Loss: 1.7560, Avg Acc@1: 0.6152, Avg Acc@5: 0.8449
2022-01-13 07:07:30,591 Val Step[1100/1563], Avg Loss: 1.7555, Avg Acc@1: 0.6151, Avg Acc@5: 0.8447
2022-01-13 07:07:32,386 Val Step[1150/1563], Avg Loss: 1.7544, Avg Acc@1: 0.6151, Avg Acc@5: 0.8448
2022-01-13 07:07:34,173 Val Step[1200/1563], Avg Loss: 1.7545, Avg Acc@1: 0.6156, Avg Acc@5: 0.8446
2022-01-13 07:07:35,948 Val Step[1250/1563], Avg Loss: 1.7537, Avg Acc@1: 0.6160, Avg Acc@5: 0.8448
2022-01-13 07:07:37,751 Val Step[1300/1563], Avg Loss: 1.7567, Avg Acc@1: 0.6155, Avg Acc@5: 0.8445
2022-01-13 07:07:39,611 Val Step[1350/1563], Avg Loss: 1.7561, Avg Acc@1: 0.6152, Avg Acc@5: 0.8447
2022-01-13 07:07:41,594 Val Step[1400/1563], Avg Loss: 1.7555, Avg Acc@1: 0.6148, Avg Acc@5: 0.8447
2022-01-13 07:07:43,585 Val Step[1450/1563], Avg Loss: 1.7552, Avg Acc@1: 0.6148, Avg Acc@5: 0.8448
2022-01-13 07:07:45,498 Val Step[1500/1563], Avg Loss: 1.7547, Avg Acc@1: 0.6155, Avg Acc@5: 0.8448
2022-01-13 07:07:47,317 Val Step[1550/1563], Avg Loss: 1.7557, Avg Acc@1: 0.6157, Avg Acc@5: 0.8447
2022-01-13 07:07:49,303 ----- Epoch[028/300], Validation Loss: 1.7556, Validation Acc@1: 0.6157, Validation Acc@5: 0.8448, time: 124.91
2022-01-13 07:07:50,614 the pre best model acc:0.6135, at epoch 26
2022-01-13 07:07:50,879 current best model acc:0.6157, at epoch 28
2022-01-13 07:07:50,879 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 07:07:50,879 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 07:07:50,879 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 07:07:50,880 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 07:07:50,880 Now training epoch 29. LR=0.000997
2022-01-13 07:09:27,600 Epoch[029/300], Step[0000/1252], Avg Loss: 3.9969, Avg Acc: 0.2373
2022-01-13 07:10:51,561 Epoch[029/300], Step[0050/1252], Avg Loss: 3.9899, Avg Acc: 0.3385
2022-01-13 07:12:16,851 Epoch[029/300], Step[0100/1252], Avg Loss: 4.0539, Avg Acc: 0.3178
2022-01-13 07:13:42,728 Epoch[029/300], Step[0150/1252], Avg Loss: 4.0403, Avg Acc: 0.3154
2022-01-13 07:15:08,317 Epoch[029/300], Step[0200/1252], Avg Loss: 4.0383, Avg Acc: 0.3182
2022-01-13 07:16:34,894 Epoch[029/300], Step[0250/1252], Avg Loss: 4.0227, Avg Acc: 0.3132
2022-01-13 07:17:59,403 Epoch[029/300], Step[0300/1252], Avg Loss: 4.0126, Avg Acc: 0.3136
2022-01-13 07:19:25,770 Epoch[029/300], Step[0350/1252], Avg Loss: 4.0066, Avg Acc: 0.3140
2022-01-13 07:20:52,072 Epoch[029/300], Step[0400/1252], Avg Loss: 4.0040, Avg Acc: 0.3144
2022-01-13 07:22:17,914 Epoch[029/300], Step[0450/1252], Avg Loss: 3.9987, Avg Acc: 0.3137
2022-01-13 07:23:44,579 Epoch[029/300], Step[0500/1252], Avg Loss: 3.9959, Avg Acc: 0.3132
2022-01-13 07:25:10,448 Epoch[029/300], Step[0550/1252], Avg Loss: 3.9961, Avg Acc: 0.3137
2022-01-13 07:26:36,732 Epoch[029/300], Step[0600/1252], Avg Loss: 3.9894, Avg Acc: 0.3136
2022-01-13 07:28:02,717 Epoch[029/300], Step[0650/1252], Avg Loss: 3.9905, Avg Acc: 0.3136
2022-01-13 07:29:29,556 Epoch[029/300], Step[0700/1252], Avg Loss: 3.9855, Avg Acc: 0.3147
2022-01-13 07:30:56,198 Epoch[029/300], Step[0750/1252], Avg Loss: 3.9836, Avg Acc: 0.3136
2022-01-13 07:32:21,590 Epoch[029/300], Step[0800/1252], Avg Loss: 3.9852, Avg Acc: 0.3153
2022-01-13 07:33:46,876 Epoch[029/300], Step[0850/1252], Avg Loss: 3.9811, Avg Acc: 0.3164
2022-01-13 07:35:13,841 Epoch[029/300], Step[0900/1252], Avg Loss: 3.9804, Avg Acc: 0.3160
2022-01-13 07:36:40,533 Epoch[029/300], Step[0950/1252], Avg Loss: 3.9796, Avg Acc: 0.3171
2022-01-13 07:38:07,055 Epoch[029/300], Step[1000/1252], Avg Loss: 3.9801, Avg Acc: 0.3181
2022-01-13 07:39:33,588 Epoch[029/300], Step[1050/1252], Avg Loss: 3.9806, Avg Acc: 0.3179
2022-01-13 07:41:00,379 Epoch[029/300], Step[1100/1252], Avg Loss: 3.9798, Avg Acc: 0.3171
2022-01-13 07:42:26,255 Epoch[029/300], Step[1150/1252], Avg Loss: 3.9792, Avg Acc: 0.3170
2022-01-13 07:43:52,398 Epoch[029/300], Step[1200/1252], Avg Loss: 3.9787, Avg Acc: 0.3178
2022-01-13 07:45:19,608 Epoch[029/300], Step[1250/1252], Avg Loss: 3.9795, Avg Acc: 0.3173
2022-01-13 07:45:26,749 ----- Epoch[029/300], Train Loss: 3.9795, Train Acc: 0.3173, time: 2255.86, Best Val(epoch28) Acc@1: 0.6157
2022-01-13 07:45:26,749 Now training epoch 30. LR=0.000997
2022-01-13 07:47:06,056 Epoch[030/300], Step[0000/1252], Avg Loss: 4.0808, Avg Acc: 0.2549
2022-01-13 07:48:31,364 Epoch[030/300], Step[0050/1252], Avg Loss: 3.9364, Avg Acc: 0.3292
2022-01-13 07:49:57,366 Epoch[030/300], Step[0100/1252], Avg Loss: 3.9733, Avg Acc: 0.3236
2022-01-13 07:51:23,386 Epoch[030/300], Step[0150/1252], Avg Loss: 3.9600, Avg Acc: 0.3199
2022-01-13 07:52:49,070 Epoch[030/300], Step[0200/1252], Avg Loss: 3.9446, Avg Acc: 0.3236
2022-01-13 07:54:14,818 Epoch[030/300], Step[0250/1252], Avg Loss: 3.9561, Avg Acc: 0.3233
2022-01-13 07:55:40,279 Epoch[030/300], Step[0300/1252], Avg Loss: 3.9606, Avg Acc: 0.3248
2022-01-13 07:57:05,986 Epoch[030/300], Step[0350/1252], Avg Loss: 3.9580, Avg Acc: 0.3271
2022-01-13 07:58:31,852 Epoch[030/300], Step[0400/1252], Avg Loss: 3.9601, Avg Acc: 0.3253
2022-01-13 07:59:56,646 Epoch[030/300], Step[0450/1252], Avg Loss: 3.9611, Avg Acc: 0.3262
2022-01-13 08:01:23,829 Epoch[030/300], Step[0500/1252], Avg Loss: 3.9599, Avg Acc: 0.3254
2022-01-13 08:02:50,553 Epoch[030/300], Step[0550/1252], Avg Loss: 3.9568, Avg Acc: 0.3228
2022-01-13 08:04:17,518 Epoch[030/300], Step[0600/1252], Avg Loss: 3.9608, Avg Acc: 0.3220
2022-01-13 08:05:43,040 Epoch[030/300], Step[0650/1252], Avg Loss: 3.9612, Avg Acc: 0.3227
2022-01-13 08:07:10,090 Epoch[030/300], Step[0700/1252], Avg Loss: 3.9661, Avg Acc: 0.3211
2022-01-13 08:08:35,730 Epoch[030/300], Step[0750/1252], Avg Loss: 3.9652, Avg Acc: 0.3203
2022-01-13 08:10:02,010 Epoch[030/300], Step[0800/1252], Avg Loss: 3.9669, Avg Acc: 0.3206
2022-01-13 08:11:28,022 Epoch[030/300], Step[0850/1252], Avg Loss: 3.9677, Avg Acc: 0.3198
2022-01-13 08:12:52,956 Epoch[030/300], Step[0900/1252], Avg Loss: 3.9646, Avg Acc: 0.3208
2022-01-13 08:14:17,624 Epoch[030/300], Step[0950/1252], Avg Loss: 3.9646, Avg Acc: 0.3221
2022-01-13 08:15:43,669 Epoch[030/300], Step[1000/1252], Avg Loss: 3.9660, Avg Acc: 0.3218
2022-01-13 08:17:09,341 Epoch[030/300], Step[1050/1252], Avg Loss: 3.9627, Avg Acc: 0.3225
2022-01-13 08:18:35,419 Epoch[030/300], Step[1100/1252], Avg Loss: 3.9638, Avg Acc: 0.3219
2022-01-13 08:20:00,960 Epoch[030/300], Step[1150/1252], Avg Loss: 3.9637, Avg Acc: 0.3215
2022-01-13 08:21:25,443 Epoch[030/300], Step[1200/1252], Avg Loss: 3.9630, Avg Acc: 0.3223
2022-01-13 08:22:52,444 Epoch[030/300], Step[1250/1252], Avg Loss: 3.9627, Avg Acc: 0.3216
2022-01-13 08:22:59,564 ----- Epoch[030/300], Train Loss: 3.9627, Train Acc: 0.3216, time: 2252.81, Best Val(epoch28) Acc@1: 0.6157
2022-01-13 08:22:59,564 ----- Validation after Epoch: 30
2022-01-13 08:24:02,394 Val Step[0000/1563], Avg Loss: 1.6248, Avg Acc@1: 0.6250, Avg Acc@5: 0.8438
2022-01-13 08:24:04,587 Val Step[0050/1563], Avg Loss: 1.7505, Avg Acc@1: 0.6201, Avg Acc@5: 0.8523
2022-01-13 08:24:06,738 Val Step[0100/1563], Avg Loss: 1.7592, Avg Acc@1: 0.6176, Avg Acc@5: 0.8537
2022-01-13 08:24:08,797 Val Step[0150/1563], Avg Loss: 1.7537, Avg Acc@1: 0.6227, Avg Acc@5: 0.8539
2022-01-13 08:24:10,843 Val Step[0200/1563], Avg Loss: 1.7624, Avg Acc@1: 0.6234, Avg Acc@5: 0.8497
2022-01-13 08:24:12,897 Val Step[0250/1563], Avg Loss: 1.7465, Avg Acc@1: 0.6287, Avg Acc@5: 0.8523
2022-01-13 08:24:14,943 Val Step[0300/1563], Avg Loss: 1.7444, Avg Acc@1: 0.6306, Avg Acc@5: 0.8540
2022-01-13 08:24:17,010 Val Step[0350/1563], Avg Loss: 1.7485, Avg Acc@1: 0.6301, Avg Acc@5: 0.8543
2022-01-13 08:24:19,104 Val Step[0400/1563], Avg Loss: 1.7464, Avg Acc@1: 0.6305, Avg Acc@5: 0.8544
2022-01-13 08:24:21,170 Val Step[0450/1563], Avg Loss: 1.7540, Avg Acc@1: 0.6271, Avg Acc@5: 0.8519
2022-01-13 08:24:23,249 Val Step[0500/1563], Avg Loss: 1.7589, Avg Acc@1: 0.6248, Avg Acc@5: 0.8512
2022-01-13 08:24:25,342 Val Step[0550/1563], Avg Loss: 1.7608, Avg Acc@1: 0.6242, Avg Acc@5: 0.8513
2022-01-13 08:24:27,428 Val Step[0600/1563], Avg Loss: 1.7612, Avg Acc@1: 0.6236, Avg Acc@5: 0.8508
2022-01-13 08:24:29,492 Val Step[0650/1563], Avg Loss: 1.7635, Avg Acc@1: 0.6236, Avg Acc@5: 0.8510
2022-01-13 08:24:31,566 Val Step[0700/1563], Avg Loss: 1.7629, Avg Acc@1: 0.6236, Avg Acc@5: 0.8507
2022-01-13 08:24:33,716 Val Step[0750/1563], Avg Loss: 1.7671, Avg Acc@1: 0.6230, Avg Acc@5: 0.8500
2022-01-13 08:24:35,927 Val Step[0800/1563], Avg Loss: 1.7650, Avg Acc@1: 0.6226, Avg Acc@5: 0.8509
2022-01-13 08:24:38,068 Val Step[0850/1563], Avg Loss: 1.7662, Avg Acc@1: 0.6221, Avg Acc@5: 0.8507
2022-01-13 08:24:39,978 Val Step[0900/1563], Avg Loss: 1.7617, Avg Acc@1: 0.6231, Avg Acc@5: 0.8516
2022-01-13 08:24:41,939 Val Step[0950/1563], Avg Loss: 1.7625, Avg Acc@1: 0.6227, Avg Acc@5: 0.8515
2022-01-13 08:24:43,869 Val Step[1000/1563], Avg Loss: 1.7619, Avg Acc@1: 0.6226, Avg Acc@5: 0.8515
2022-01-13 08:24:45,734 Val Step[1050/1563], Avg Loss: 1.7645, Avg Acc@1: 0.6224, Avg Acc@5: 0.8507
2022-01-13 08:24:47,654 Val Step[1100/1563], Avg Loss: 1.7649, Avg Acc@1: 0.6218, Avg Acc@5: 0.8505
2022-01-13 08:24:49,750 Val Step[1150/1563], Avg Loss: 1.7644, Avg Acc@1: 0.6220, Avg Acc@5: 0.8502
2022-01-13 08:24:51,898 Val Step[1200/1563], Avg Loss: 1.7633, Avg Acc@1: 0.6225, Avg Acc@5: 0.8504
2022-01-13 08:24:53,879 Val Step[1250/1563], Avg Loss: 1.7627, Avg Acc@1: 0.6227, Avg Acc@5: 0.8507
2022-01-13 08:24:55,787 Val Step[1300/1563], Avg Loss: 1.7651, Avg Acc@1: 0.6219, Avg Acc@5: 0.8503
2022-01-13 08:24:57,690 Val Step[1350/1563], Avg Loss: 1.7661, Avg Acc@1: 0.6219, Avg Acc@5: 0.8499
2022-01-13 08:24:59,736 Val Step[1400/1563], Avg Loss: 1.7661, Avg Acc@1: 0.6214, Avg Acc@5: 0.8497
2022-01-13 08:25:01,794 Val Step[1450/1563], Avg Loss: 1.7658, Avg Acc@1: 0.6211, Avg Acc@5: 0.8501
2022-01-13 08:25:03,869 Val Step[1500/1563], Avg Loss: 1.7657, Avg Acc@1: 0.6214, Avg Acc@5: 0.8501
2022-01-13 08:25:05,869 Val Step[1550/1563], Avg Loss: 1.7666, Avg Acc@1: 0.6213, Avg Acc@5: 0.8498
2022-01-13 08:25:07,822 ----- Epoch[030/300], Validation Loss: 1.7667, Validation Acc@1: 0.6214, Validation Acc@5: 0.8498, time: 128.25
2022-01-13 08:25:09,148 the pre best model acc:0.6157, at epoch 28
2022-01-13 08:25:09,416 current best model acc:0.6214, at epoch 30
2022-01-13 08:25:09,416 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 08:25:09,416 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 08:25:09,417 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 08:25:09,417 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 08:25:10,057 ----- Save model: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-30-Loss-3.955713431600219.pdparams
2022-01-13 08:25:10,058 ----- Save optim: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-30-Loss-3.955713431600219.pdopt
2022-01-13 08:25:10,058 Now training epoch 31. LR=0.000996
2022-01-13 08:26:45,676 Epoch[031/300], Step[0000/1252], Avg Loss: 4.5060, Avg Acc: 0.3359
2022-01-13 08:28:13,203 Epoch[031/300], Step[0050/1252], Avg Loss: 3.9494, Avg Acc: 0.3424
2022-01-13 08:29:39,454 Epoch[031/300], Step[0100/1252], Avg Loss: 3.9774, Avg Acc: 0.3315
2022-01-13 08:31:05,254 Epoch[031/300], Step[0150/1252], Avg Loss: 3.9637, Avg Acc: 0.3362
2022-01-13 08:32:31,369 Epoch[031/300], Step[0200/1252], Avg Loss: 3.9451, Avg Acc: 0.3385
2022-01-13 08:33:58,018 Epoch[031/300], Step[0250/1252], Avg Loss: 3.9421, Avg Acc: 0.3332
2022-01-13 08:35:24,358 Epoch[031/300], Step[0300/1252], Avg Loss: 3.9436, Avg Acc: 0.3349
2022-01-13 08:36:49,957 Epoch[031/300], Step[0350/1252], Avg Loss: 3.9408, Avg Acc: 0.3350
2022-01-13 08:38:15,491 Epoch[031/300], Step[0400/1252], Avg Loss: 3.9319, Avg Acc: 0.3350
2022-01-13 08:39:41,153 Epoch[031/300], Step[0450/1252], Avg Loss: 3.9239, Avg Acc: 0.3353
2022-01-13 08:41:06,318 Epoch[031/300], Step[0500/1252], Avg Loss: 3.9208, Avg Acc: 0.3350
2022-01-13 08:42:32,065 Epoch[031/300], Step[0550/1252], Avg Loss: 3.9230, Avg Acc: 0.3349
2022-01-13 08:43:59,416 Epoch[031/300], Step[0600/1252], Avg Loss: 3.9293, Avg Acc: 0.3320
2022-01-13 08:45:25,707 Epoch[031/300], Step[0650/1252], Avg Loss: 3.9362, Avg Acc: 0.3335
2022-01-13 08:46:52,068 Epoch[031/300], Step[0700/1252], Avg Loss: 3.9447, Avg Acc: 0.3332
2022-01-13 08:48:18,230 Epoch[031/300], Step[0750/1252], Avg Loss: 3.9470, Avg Acc: 0.3337
2022-01-13 08:49:44,950 Epoch[031/300], Step[0800/1252], Avg Loss: 3.9483, Avg Acc: 0.3334
2022-01-13 08:51:11,517 Epoch[031/300], Step[0850/1252], Avg Loss: 3.9487, Avg Acc: 0.3330
2022-01-13 08:52:36,449 Epoch[031/300], Step[0900/1252], Avg Loss: 3.9475, Avg Acc: 0.3344
2022-01-13 08:54:02,000 Epoch[031/300], Step[0950/1252], Avg Loss: 3.9503, Avg Acc: 0.3356
2022-01-13 08:55:28,293 Epoch[031/300], Step[1000/1252], Avg Loss: 3.9499, Avg Acc: 0.3344
2022-01-13 08:56:54,801 Epoch[031/300], Step[1050/1252], Avg Loss: 3.9489, Avg Acc: 0.3345
2022-01-13 08:58:22,030 Epoch[031/300], Step[1100/1252], Avg Loss: 3.9467, Avg Acc: 0.3341
2022-01-13 08:59:48,618 Epoch[031/300], Step[1150/1252], Avg Loss: 3.9462, Avg Acc: 0.3344
2022-01-13 09:01:15,343 Epoch[031/300], Step[1200/1252], Avg Loss: 3.9469, Avg Acc: 0.3348
2022-01-13 09:02:43,552 Epoch[031/300], Step[1250/1252], Avg Loss: 3.9456, Avg Acc: 0.3341
2022-01-13 09:02:50,639 ----- Epoch[031/300], Train Loss: 3.9456, Train Acc: 0.3341, time: 2260.58, Best Val(epoch30) Acc@1: 0.6214
2022-01-13 09:02:50,639 Now training epoch 32. LR=0.000996
2022-01-13 09:04:31,171 Epoch[032/300], Step[0000/1252], Avg Loss: 3.8965, Avg Acc: 0.3242
2022-01-13 09:05:56,340 Epoch[032/300], Step[0050/1252], Avg Loss: 3.8940, Avg Acc: 0.3426
2022-01-13 09:07:21,180 Epoch[032/300], Step[0100/1252], Avg Loss: 3.9166, Avg Acc: 0.3402
2022-01-13 09:08:45,790 Epoch[032/300], Step[0150/1252], Avg Loss: 3.9219, Avg Acc: 0.3296
2022-01-13 09:10:11,077 Epoch[032/300], Step[0200/1252], Avg Loss: 3.9203, Avg Acc: 0.3304
2022-01-13 09:11:36,484 Epoch[032/300], Step[0250/1252], Avg Loss: 3.9238, Avg Acc: 0.3303
2022-01-13 09:13:02,471 Epoch[032/300], Step[0300/1252], Avg Loss: 3.9295, Avg Acc: 0.3312
2022-01-13 09:14:27,242 Epoch[032/300], Step[0350/1252], Avg Loss: 3.9379, Avg Acc: 0.3294
2022-01-13 09:15:53,562 Epoch[032/300], Step[0400/1252], Avg Loss: 3.9406, Avg Acc: 0.3303
2022-01-13 09:17:18,474 Epoch[032/300], Step[0450/1252], Avg Loss: 3.9451, Avg Acc: 0.3314
2022-01-13 09:18:43,578 Epoch[032/300], Step[0500/1252], Avg Loss: 3.9457, Avg Acc: 0.3324
2022-01-13 09:20:10,057 Epoch[032/300], Step[0550/1252], Avg Loss: 3.9477, Avg Acc: 0.3296
2022-01-13 09:21:36,417 Epoch[032/300], Step[0600/1252], Avg Loss: 3.9504, Avg Acc: 0.3287
2022-01-13 09:23:02,450 Epoch[032/300], Step[0650/1252], Avg Loss: 3.9495, Avg Acc: 0.3300
2022-01-13 09:24:28,278 Epoch[032/300], Step[0700/1252], Avg Loss: 3.9519, Avg Acc: 0.3293
2022-01-13 09:25:53,766 Epoch[032/300], Step[0750/1252], Avg Loss: 3.9494, Avg Acc: 0.3295
2022-01-13 09:27:18,564 Epoch[032/300], Step[0800/1252], Avg Loss: 3.9557, Avg Acc: 0.3298
2022-01-13 09:28:44,035 Epoch[032/300], Step[0850/1252], Avg Loss: 3.9563, Avg Acc: 0.3303
2022-01-13 09:30:08,768 Epoch[032/300], Step[0900/1252], Avg Loss: 3.9538, Avg Acc: 0.3312
2022-01-13 09:31:35,235 Epoch[032/300], Step[0950/1252], Avg Loss: 3.9538, Avg Acc: 0.3309
2022-01-13 09:33:00,201 Epoch[032/300], Step[1000/1252], Avg Loss: 3.9506, Avg Acc: 0.3314
2022-01-13 09:34:25,188 Epoch[032/300], Step[1050/1252], Avg Loss: 3.9490, Avg Acc: 0.3324
2022-01-13 09:35:50,644 Epoch[032/300], Step[1100/1252], Avg Loss: 3.9467, Avg Acc: 0.3324
2022-01-13 09:37:16,765 Epoch[032/300], Step[1150/1252], Avg Loss: 3.9458, Avg Acc: 0.3327
2022-01-13 09:38:43,250 Epoch[032/300], Step[1200/1252], Avg Loss: 3.9463, Avg Acc: 0.3329
2022-01-13 09:40:09,123 Epoch[032/300], Step[1250/1252], Avg Loss: 3.9436, Avg Acc: 0.3340
2022-01-13 09:40:16,034 ----- Epoch[032/300], Train Loss: 3.9436, Train Acc: 0.3340, time: 2245.39, Best Val(epoch30) Acc@1: 0.6214
2022-01-13 09:40:16,034 ----- Validation after Epoch: 32
2022-01-13 09:41:22,204 Val Step[0000/1563], Avg Loss: 1.5960, Avg Acc@1: 0.6250, Avg Acc@5: 0.8125
2022-01-13 09:41:24,261 Val Step[0050/1563], Avg Loss: 1.6192, Avg Acc@1: 0.6385, Avg Acc@5: 0.8554
2022-01-13 09:41:26,301 Val Step[0100/1563], Avg Loss: 1.6261, Avg Acc@1: 0.6439, Avg Acc@5: 0.8583
2022-01-13 09:41:28,349 Val Step[0150/1563], Avg Loss: 1.6225, Avg Acc@1: 0.6422, Avg Acc@5: 0.8572
2022-01-13 09:41:30,386 Val Step[0200/1563], Avg Loss: 1.6305, Avg Acc@1: 0.6407, Avg Acc@5: 0.8529
2022-01-13 09:41:32,421 Val Step[0250/1563], Avg Loss: 1.6118, Avg Acc@1: 0.6436, Avg Acc@5: 0.8567
2022-01-13 09:41:34,495 Val Step[0300/1563], Avg Loss: 1.6161, Avg Acc@1: 0.6425, Avg Acc@5: 0.8570
2022-01-13 09:41:36,532 Val Step[0350/1563], Avg Loss: 1.6222, Avg Acc@1: 0.6423, Avg Acc@5: 0.8575
2022-01-13 09:41:38,590 Val Step[0400/1563], Avg Loss: 1.6199, Avg Acc@1: 0.6435, Avg Acc@5: 0.8572
2022-01-13 09:41:40,631 Val Step[0450/1563], Avg Loss: 1.6243, Avg Acc@1: 0.6406, Avg Acc@5: 0.8565
2022-01-13 09:41:42,672 Val Step[0500/1563], Avg Loss: 1.6282, Avg Acc@1: 0.6402, Avg Acc@5: 0.8556
2022-01-13 09:41:44,706 Val Step[0550/1563], Avg Loss: 1.6288, Avg Acc@1: 0.6396, Avg Acc@5: 0.8561
2022-01-13 09:41:46,766 Val Step[0600/1563], Avg Loss: 1.6307, Avg Acc@1: 0.6386, Avg Acc@5: 0.8561
2022-01-13 09:41:48,809 Val Step[0650/1563], Avg Loss: 1.6329, Avg Acc@1: 0.6391, Avg Acc@5: 0.8562
2022-01-13 09:41:50,857 Val Step[0700/1563], Avg Loss: 1.6309, Avg Acc@1: 0.6391, Avg Acc@5: 0.8573
2022-01-13 09:41:52,895 Val Step[0750/1563], Avg Loss: 1.6357, Avg Acc@1: 0.6376, Avg Acc@5: 0.8567
2022-01-13 09:41:54,761 Val Step[0800/1563], Avg Loss: 1.6336, Avg Acc@1: 0.6377, Avg Acc@5: 0.8571
2022-01-13 09:41:56,589 Val Step[0850/1563], Avg Loss: 1.6339, Avg Acc@1: 0.6369, Avg Acc@5: 0.8569
2022-01-13 09:41:58,472 Val Step[0900/1563], Avg Loss: 1.6322, Avg Acc@1: 0.6374, Avg Acc@5: 0.8572
2022-01-13 09:42:00,364 Val Step[0950/1563], Avg Loss: 1.6316, Avg Acc@1: 0.6368, Avg Acc@5: 0.8576
2022-01-13 09:42:02,156 Val Step[1000/1563], Avg Loss: 1.6307, Avg Acc@1: 0.6375, Avg Acc@5: 0.8573
2022-01-13 09:42:03,952 Val Step[1050/1563], Avg Loss: 1.6334, Avg Acc@1: 0.6367, Avg Acc@5: 0.8568
2022-01-13 09:42:05,788 Val Step[1100/1563], Avg Loss: 1.6337, Avg Acc@1: 0.6359, Avg Acc@5: 0.8571
2022-01-13 09:42:07,852 Val Step[1150/1563], Avg Loss: 1.6336, Avg Acc@1: 0.6365, Avg Acc@5: 0.8570
2022-01-13 09:42:09,906 Val Step[1200/1563], Avg Loss: 1.6322, Avg Acc@1: 0.6372, Avg Acc@5: 0.8573
2022-01-13 09:42:11,959 Val Step[1250/1563], Avg Loss: 1.6310, Avg Acc@1: 0.6370, Avg Acc@5: 0.8576
2022-01-13 09:42:13,993 Val Step[1300/1563], Avg Loss: 1.6335, Avg Acc@1: 0.6368, Avg Acc@5: 0.8572
2022-01-13 09:42:15,940 Val Step[1350/1563], Avg Loss: 1.6338, Avg Acc@1: 0.6367, Avg Acc@5: 0.8572
2022-01-13 09:42:17,849 Val Step[1400/1563], Avg Loss: 1.6328, Avg Acc@1: 0.6365, Avg Acc@5: 0.8572
2022-01-13 09:42:19,825 Val Step[1450/1563], Avg Loss: 1.6329, Avg Acc@1: 0.6360, Avg Acc@5: 0.8571
2022-01-13 09:42:21,683 Val Step[1500/1563], Avg Loss: 1.6322, Avg Acc@1: 0.6366, Avg Acc@5: 0.8573
2022-01-13 09:42:23,428 Val Step[1550/1563], Avg Loss: 1.6331, Avg Acc@1: 0.6364, Avg Acc@5: 0.8570
2022-01-13 09:42:25,316 ----- Epoch[032/300], Validation Loss: 1.6330, Validation Acc@1: 0.6367, Validation Acc@5: 0.8570, time: 129.28
2022-01-13 09:42:26,637 the pre best model acc:0.6214, at epoch 30
2022-01-13 09:42:26,917 current best model acc:0.6367, at epoch 32
2022-01-13 09:42:26,917 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 09:42:26,917 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 09:42:26,917 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 09:42:26,917 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 09:42:26,918 Now training epoch 33. LR=0.000995
2022-01-13 09:44:13,066 Epoch[033/300], Step[0000/1252], Avg Loss: 3.8699, Avg Acc: 0.5244
2022-01-13 09:45:39,193 Epoch[033/300], Step[0050/1252], Avg Loss: 3.9460, Avg Acc: 0.3362
2022-01-13 09:47:04,472 Epoch[033/300], Step[0100/1252], Avg Loss: 3.9472, Avg Acc: 0.3371
2022-01-13 09:48:30,674 Epoch[033/300], Step[0150/1252], Avg Loss: 3.9361, Avg Acc: 0.3363
2022-01-13 09:49:56,576 Epoch[033/300], Step[0200/1252], Avg Loss: 3.9279, Avg Acc: 0.3295
2022-01-13 09:51:21,356 Epoch[033/300], Step[0250/1252], Avg Loss: 3.9315, Avg Acc: 0.3270
2022-01-13 09:52:46,932 Epoch[033/300], Step[0300/1252], Avg Loss: 3.9250, Avg Acc: 0.3311
2022-01-13 09:54:12,419 Epoch[033/300], Step[0350/1252], Avg Loss: 3.9221, Avg Acc: 0.3324
2022-01-13 09:55:38,299 Epoch[033/300], Step[0400/1252], Avg Loss: 3.9297, Avg Acc: 0.3300
2022-01-13 09:57:04,145 Epoch[033/300], Step[0450/1252], Avg Loss: 3.9420, Avg Acc: 0.3289
2022-01-13 09:58:29,632 Epoch[033/300], Step[0500/1252], Avg Loss: 3.9382, Avg Acc: 0.3302
2022-01-13 09:59:53,612 Epoch[033/300], Step[0550/1252], Avg Loss: 3.9367, Avg Acc: 0.3317
2022-01-13 10:01:17,585 Epoch[033/300], Step[0600/1252], Avg Loss: 3.9355, Avg Acc: 0.3340
2022-01-13 10:02:43,685 Epoch[033/300], Step[0650/1252], Avg Loss: 3.9359, Avg Acc: 0.3331
2022-01-13 10:04:10,346 Epoch[033/300], Step[0700/1252], Avg Loss: 3.9333, Avg Acc: 0.3339
2022-01-13 10:05:36,249 Epoch[033/300], Step[0750/1252], Avg Loss: 3.9339, Avg Acc: 0.3331
2022-01-13 10:07:02,539 Epoch[033/300], Step[0800/1252], Avg Loss: 3.9298, Avg Acc: 0.3319
2022-01-13 10:08:29,341 Epoch[033/300], Step[0850/1252], Avg Loss: 3.9307, Avg Acc: 0.3318
2022-01-13 10:09:54,777 Epoch[033/300], Step[0900/1252], Avg Loss: 3.9316, Avg Acc: 0.3317
2022-01-13 10:11:20,443 Epoch[033/300], Step[0950/1252], Avg Loss: 3.9312, Avg Acc: 0.3313
2022-01-13 10:12:45,495 Epoch[033/300], Step[1000/1252], Avg Loss: 3.9306, Avg Acc: 0.3320
2022-01-13 10:14:10,959 Epoch[033/300], Step[1050/1252], Avg Loss: 3.9331, Avg Acc: 0.3314
2022-01-13 10:15:37,155 Epoch[033/300], Step[1100/1252], Avg Loss: 3.9332, Avg Acc: 0.3308
2022-01-13 10:17:01,224 Epoch[033/300], Step[1150/1252], Avg Loss: 3.9346, Avg Acc: 0.3309
2022-01-13 10:18:26,304 Epoch[033/300], Step[1200/1252], Avg Loss: 3.9339, Avg Acc: 0.3316
2022-01-13 10:19:51,783 Epoch[033/300], Step[1250/1252], Avg Loss: 3.9365, Avg Acc: 0.3314
2022-01-13 10:19:59,042 ----- Epoch[033/300], Train Loss: 3.9365, Train Acc: 0.3314, time: 2252.12, Best Val(epoch32) Acc@1: 0.6367
2022-01-13 10:19:59,042 Now training epoch 34. LR=0.000994
2022-01-13 10:21:43,659 Epoch[034/300], Step[0000/1252], Avg Loss: 3.5575, Avg Acc: 0.2744
2022-01-13 10:23:08,785 Epoch[034/300], Step[0050/1252], Avg Loss: 3.8633, Avg Acc: 0.3251
2022-01-13 10:24:34,927 Epoch[034/300], Step[0100/1252], Avg Loss: 3.8895, Avg Acc: 0.3266
2022-01-13 10:25:59,514 Epoch[034/300], Step[0150/1252], Avg Loss: 3.9033, Avg Acc: 0.3321
2022-01-13 10:27:25,436 Epoch[034/300], Step[0200/1252], Avg Loss: 3.8918, Avg Acc: 0.3365
2022-01-13 10:28:48,559 Epoch[034/300], Step[0250/1252], Avg Loss: 3.8818, Avg Acc: 0.3419
2022-01-13 10:30:13,824 Epoch[034/300], Step[0300/1252], Avg Loss: 3.8809, Avg Acc: 0.3417
2022-01-13 10:31:38,185 Epoch[034/300], Step[0350/1252], Avg Loss: 3.8720, Avg Acc: 0.3439
2022-01-13 10:33:03,099 Epoch[034/300], Step[0400/1252], Avg Loss: 3.8718, Avg Acc: 0.3418
2022-01-13 10:34:28,753 Epoch[034/300], Step[0450/1252], Avg Loss: 3.8726, Avg Acc: 0.3395
2022-01-13 10:35:54,627 Epoch[034/300], Step[0500/1252], Avg Loss: 3.8785, Avg Acc: 0.3393
2022-01-13 10:37:19,988 Epoch[034/300], Step[0550/1252], Avg Loss: 3.8803, Avg Acc: 0.3402
2022-01-13 10:38:45,610 Epoch[034/300], Step[0600/1252], Avg Loss: 3.8854, Avg Acc: 0.3386
2022-01-13 10:40:11,303 Epoch[034/300], Step[0650/1252], Avg Loss: 3.8873, Avg Acc: 0.3385
2022-01-13 10:41:36,525 Epoch[034/300], Step[0700/1252], Avg Loss: 3.8953, Avg Acc: 0.3371
2022-01-13 10:43:02,015 Epoch[034/300], Step[0750/1252], Avg Loss: 3.8960, Avg Acc: 0.3366
2022-01-13 10:44:28,098 Epoch[034/300], Step[0800/1252], Avg Loss: 3.8940, Avg Acc: 0.3371
2022-01-13 10:45:53,519 Epoch[034/300], Step[0850/1252], Avg Loss: 3.8945, Avg Acc: 0.3373
2022-01-13 10:47:20,124 Epoch[034/300], Step[0900/1252], Avg Loss: 3.8975, Avg Acc: 0.3375
2022-01-13 10:48:45,413 Epoch[034/300], Step[0950/1252], Avg Loss: 3.8979, Avg Acc: 0.3376
2022-01-13 10:50:11,150 Epoch[034/300], Step[1000/1252], Avg Loss: 3.8943, Avg Acc: 0.3380
2022-01-13 10:51:36,775 Epoch[034/300], Step[1050/1252], Avg Loss: 3.8952, Avg Acc: 0.3376
2022-01-13 10:53:02,032 Epoch[034/300], Step[1100/1252], Avg Loss: 3.8955, Avg Acc: 0.3373
2022-01-13 10:54:29,070 Epoch[034/300], Step[1150/1252], Avg Loss: 3.8978, Avg Acc: 0.3362
2022-01-13 10:55:55,533 Epoch[034/300], Step[1200/1252], Avg Loss: 3.8996, Avg Acc: 0.3360
2022-01-13 10:57:22,298 Epoch[034/300], Step[1250/1252], Avg Loss: 3.8966, Avg Acc: 0.3366
2022-01-13 10:57:29,499 ----- Epoch[034/300], Train Loss: 3.8966, Train Acc: 0.3366, time: 2250.45, Best Val(epoch32) Acc@1: 0.6367
2022-01-13 10:57:29,499 ----- Validation after Epoch: 34
2022-01-13 10:58:41,211 Val Step[0000/1563], Avg Loss: 1.4208, Avg Acc@1: 0.6562, Avg Acc@5: 0.9062
2022-01-13 10:58:43,277 Val Step[0050/1563], Avg Loss: 1.6246, Avg Acc@1: 0.6452, Avg Acc@5: 0.8664
2022-01-13 10:58:45,313 Val Step[0100/1563], Avg Loss: 1.6536, Avg Acc@1: 0.6405, Avg Acc@5: 0.8673
2022-01-13 10:58:47,213 Val Step[0150/1563], Avg Loss: 1.6394, Avg Acc@1: 0.6461, Avg Acc@5: 0.8630
2022-01-13 10:58:49,181 Val Step[0200/1563], Avg Loss: 1.6438, Avg Acc@1: 0.6438, Avg Acc@5: 0.8593
2022-01-13 10:58:51,096 Val Step[0250/1563], Avg Loss: 1.6300, Avg Acc@1: 0.6474, Avg Acc@5: 0.8601
2022-01-13 10:58:52,993 Val Step[0300/1563], Avg Loss: 1.6278, Avg Acc@1: 0.6472, Avg Acc@5: 0.8608
2022-01-13 10:58:54,922 Val Step[0350/1563], Avg Loss: 1.6356, Avg Acc@1: 0.6470, Avg Acc@5: 0.8606
2022-01-13 10:58:56,820 Val Step[0400/1563], Avg Loss: 1.6312, Avg Acc@1: 0.6487, Avg Acc@5: 0.8612
2022-01-13 10:58:58,642 Val Step[0450/1563], Avg Loss: 1.6393, Avg Acc@1: 0.6457, Avg Acc@5: 0.8593
2022-01-13 10:59:00,450 Val Step[0500/1563], Avg Loss: 1.6389, Avg Acc@1: 0.6454, Avg Acc@5: 0.8588
2022-01-13 10:59:02,524 Val Step[0550/1563], Avg Loss: 1.6419, Avg Acc@1: 0.6441, Avg Acc@5: 0.8589
2022-01-13 10:59:04,596 Val Step[0600/1563], Avg Loss: 1.6428, Avg Acc@1: 0.6439, Avg Acc@5: 0.8594
2022-01-13 10:59:06,669 Val Step[0650/1563], Avg Loss: 1.6463, Avg Acc@1: 0.6439, Avg Acc@5: 0.8588
2022-01-13 10:59:08,709 Val Step[0700/1563], Avg Loss: 1.6457, Avg Acc@1: 0.6431, Avg Acc@5: 0.8589
2022-01-13 10:59:10,811 Val Step[0750/1563], Avg Loss: 1.6522, Avg Acc@1: 0.6421, Avg Acc@5: 0.8578
2022-01-13 10:59:12,732 Val Step[0800/1563], Avg Loss: 1.6505, Avg Acc@1: 0.6430, Avg Acc@5: 0.8580
2022-01-13 10:59:14,558 Val Step[0850/1563], Avg Loss: 1.6526, Avg Acc@1: 0.6420, Avg Acc@5: 0.8579
2022-01-13 10:59:16,404 Val Step[0900/1563], Avg Loss: 1.6497, Avg Acc@1: 0.6422, Avg Acc@5: 0.8582
2022-01-13 10:59:18,242 Val Step[0950/1563], Avg Loss: 1.6494, Avg Acc@1: 0.6422, Avg Acc@5: 0.8588
2022-01-13 10:59:20,066 Val Step[1000/1563], Avg Loss: 1.6494, Avg Acc@1: 0.6420, Avg Acc@5: 0.8587
2022-01-13 10:59:21,964 Val Step[1050/1563], Avg Loss: 1.6528, Avg Acc@1: 0.6411, Avg Acc@5: 0.8581
2022-01-13 10:59:23,766 Val Step[1100/1563], Avg Loss: 1.6535, Avg Acc@1: 0.6405, Avg Acc@5: 0.8579
2022-01-13 10:59:25,553 Val Step[1150/1563], Avg Loss: 1.6535, Avg Acc@1: 0.6403, Avg Acc@5: 0.8576
2022-01-13 10:59:27,355 Val Step[1200/1563], Avg Loss: 1.6524, Avg Acc@1: 0.6408, Avg Acc@5: 0.8578
2022-01-13 10:59:29,203 Val Step[1250/1563], Avg Loss: 1.6511, Avg Acc@1: 0.6407, Avg Acc@5: 0.8581
2022-01-13 10:59:30,975 Val Step[1300/1563], Avg Loss: 1.6543, Avg Acc@1: 0.6399, Avg Acc@5: 0.8575
2022-01-13 10:59:32,771 Val Step[1350/1563], Avg Loss: 1.6536, Avg Acc@1: 0.6398, Avg Acc@5: 0.8578
2022-01-13 10:59:34,612 Val Step[1400/1563], Avg Loss: 1.6529, Avg Acc@1: 0.6394, Avg Acc@5: 0.8580
2022-01-13 10:59:36,398 Val Step[1450/1563], Avg Loss: 1.6529, Avg Acc@1: 0.6389, Avg Acc@5: 0.8582
2022-01-13 10:59:38,209 Val Step[1500/1563], Avg Loss: 1.6527, Avg Acc@1: 0.6391, Avg Acc@5: 0.8579
2022-01-13 10:59:40,013 Val Step[1550/1563], Avg Loss: 1.6535, Avg Acc@1: 0.6390, Avg Acc@5: 0.8577
2022-01-13 10:59:41,864 ----- Epoch[034/300], Validation Loss: 1.6538, Validation Acc@1: 0.6390, Validation Acc@5: 0.8577, time: 132.36
2022-01-13 10:59:43,254 the pre best model acc:0.6367, at epoch 32
2022-01-13 10:59:43,447 current best model acc:0.6390, at epoch 34
2022-01-13 10:59:43,447 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 10:59:43,447 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 10:59:43,447 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 10:59:43,447 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 10:59:43,448 Now training epoch 35. LR=0.000993
2022-01-13 11:01:24,497 Epoch[035/300], Step[0000/1252], Avg Loss: 4.6720, Avg Acc: 0.2939
2022-01-13 11:02:50,368 Epoch[035/300], Step[0050/1252], Avg Loss: 3.8392, Avg Acc: 0.3404
2022-01-13 11:04:15,811 Epoch[035/300], Step[0100/1252], Avg Loss: 3.8992, Avg Acc: 0.3400
2022-01-13 11:05:41,493 Epoch[035/300], Step[0150/1252], Avg Loss: 3.8830, Avg Acc: 0.3429
2022-01-13 11:07:06,355 Epoch[035/300], Step[0200/1252], Avg Loss: 3.8910, Avg Acc: 0.3395
2022-01-13 11:08:32,705 Epoch[035/300], Step[0250/1252], Avg Loss: 3.8926, Avg Acc: 0.3338
2022-01-13 11:09:57,532 Epoch[035/300], Step[0300/1252], Avg Loss: 3.8893, Avg Acc: 0.3320
2022-01-13 11:11:22,209 Epoch[035/300], Step[0350/1252], Avg Loss: 3.8869, Avg Acc: 0.3367
2022-01-13 11:12:47,480 Epoch[035/300], Step[0400/1252], Avg Loss: 3.8890, Avg Acc: 0.3359
2022-01-13 11:14:12,819 Epoch[035/300], Step[0450/1252], Avg Loss: 3.8930, Avg Acc: 0.3346
2022-01-13 11:15:38,906 Epoch[035/300], Step[0500/1252], Avg Loss: 3.8959, Avg Acc: 0.3339
2022-01-13 11:17:05,219 Epoch[035/300], Step[0550/1252], Avg Loss: 3.9006, Avg Acc: 0.3340
2022-01-13 11:18:31,118 Epoch[035/300], Step[0600/1252], Avg Loss: 3.8976, Avg Acc: 0.3361
2022-01-13 11:19:57,445 Epoch[035/300], Step[0650/1252], Avg Loss: 3.9015, Avg Acc: 0.3345
2022-01-13 11:21:24,479 Epoch[035/300], Step[0700/1252], Avg Loss: 3.9057, Avg Acc: 0.3347
2022-01-13 11:22:50,276 Epoch[035/300], Step[0750/1252], Avg Loss: 3.9086, Avg Acc: 0.3337
2022-01-13 11:24:16,796 Epoch[035/300], Step[0800/1252], Avg Loss: 3.9037, Avg Acc: 0.3354
2022-01-13 11:25:43,839 Epoch[035/300], Step[0850/1252], Avg Loss: 3.9039, Avg Acc: 0.3356
2022-01-13 11:27:11,527 Epoch[035/300], Step[0900/1252], Avg Loss: 3.9033, Avg Acc: 0.3356
2022-01-13 11:28:38,778 Epoch[035/300], Step[0950/1252], Avg Loss: 3.9045, Avg Acc: 0.3350
2022-01-13 11:30:03,467 Epoch[035/300], Step[1000/1252], Avg Loss: 3.9050, Avg Acc: 0.3340
2022-01-13 11:31:30,139 Epoch[035/300], Step[1050/1252], Avg Loss: 3.9066, Avg Acc: 0.3343
2022-01-13 11:32:57,131 Epoch[035/300], Step[1100/1252], Avg Loss: 3.9054, Avg Acc: 0.3340
2022-01-13 11:34:23,725 Epoch[035/300], Step[1150/1252], Avg Loss: 3.9083, Avg Acc: 0.3338
2022-01-13 11:35:49,731 Epoch[035/300], Step[1200/1252], Avg Loss: 3.9079, Avg Acc: 0.3339
2022-01-13 11:37:16,972 Epoch[035/300], Step[1250/1252], Avg Loss: 3.9059, Avg Acc: 0.3338
2022-01-13 11:37:24,137 ----- Epoch[035/300], Train Loss: 3.9060, Train Acc: 0.3338, time: 2260.68, Best Val(epoch34) Acc@1: 0.6390
2022-01-13 11:37:24,138 Now training epoch 36. LR=0.000992
2022-01-13 11:39:07,618 Epoch[036/300], Step[0000/1252], Avg Loss: 4.3905, Avg Acc: 0.3457
2022-01-13 11:40:32,356 Epoch[036/300], Step[0050/1252], Avg Loss: 3.9751, Avg Acc: 0.3337
2022-01-13 11:41:57,077 Epoch[036/300], Step[0100/1252], Avg Loss: 3.9168, Avg Acc: 0.3406
2022-01-13 11:43:21,025 Epoch[036/300], Step[0150/1252], Avg Loss: 3.9137, Avg Acc: 0.3406
2022-01-13 11:44:44,810 Epoch[036/300], Step[0200/1252], Avg Loss: 3.9141, Avg Acc: 0.3363
2022-01-13 11:46:08,827 Epoch[036/300], Step[0250/1252], Avg Loss: 3.9236, Avg Acc: 0.3385
2022-01-13 11:47:31,796 Epoch[036/300], Step[0300/1252], Avg Loss: 3.9264, Avg Acc: 0.3326
2022-01-13 11:48:56,616 Epoch[036/300], Step[0350/1252], Avg Loss: 3.9215, Avg Acc: 0.3322
2022-01-13 11:50:22,676 Epoch[036/300], Step[0400/1252], Avg Loss: 3.9171, Avg Acc: 0.3300
2022-01-13 11:51:47,700 Epoch[036/300], Step[0450/1252], Avg Loss: 3.9187, Avg Acc: 0.3326
2022-01-13 11:53:12,428 Epoch[036/300], Step[0500/1252], Avg Loss: 3.9126, Avg Acc: 0.3327
2022-01-13 11:54:37,726 Epoch[036/300], Step[0550/1252], Avg Loss: 3.9146, Avg Acc: 0.3328
2022-01-13 11:56:03,443 Epoch[036/300], Step[0600/1252], Avg Loss: 3.9137, Avg Acc: 0.3340
2022-01-13 11:57:27,451 Epoch[036/300], Step[0650/1252], Avg Loss: 3.9136, Avg Acc: 0.3345
2022-01-13 11:58:51,614 Epoch[036/300], Step[0700/1252], Avg Loss: 3.9118, Avg Acc: 0.3346
2022-01-13 12:00:17,057 Epoch[036/300], Step[0750/1252], Avg Loss: 3.9069, Avg Acc: 0.3322
2022-01-13 12:01:41,509 Epoch[036/300], Step[0800/1252], Avg Loss: 3.9081, Avg Acc: 0.3326
2022-01-13 12:03:07,752 Epoch[036/300], Step[0850/1252], Avg Loss: 3.9059, Avg Acc: 0.3317
2022-01-13 12:04:34,020 Epoch[036/300], Step[0900/1252], Avg Loss: 3.9045, Avg Acc: 0.3331
2022-01-13 12:05:59,083 Epoch[036/300], Step[0950/1252], Avg Loss: 3.9064, Avg Acc: 0.3329
2022-01-13 12:07:23,843 Epoch[036/300], Step[1000/1252], Avg Loss: 3.9077, Avg Acc: 0.3328
2022-01-13 12:08:49,074 Epoch[036/300], Step[1050/1252], Avg Loss: 3.9018, Avg Acc: 0.3325
2022-01-13 12:10:14,970 Epoch[036/300], Step[1100/1252], Avg Loss: 3.8999, Avg Acc: 0.3332
2022-01-13 12:11:39,322 Epoch[036/300], Step[1150/1252], Avg Loss: 3.8973, Avg Acc: 0.3340
2022-01-13 12:13:05,346 Epoch[036/300], Step[1200/1252], Avg Loss: 3.8978, Avg Acc: 0.3336
2022-01-13 12:14:30,832 Epoch[036/300], Step[1250/1252], Avg Loss: 3.8956, Avg Acc: 0.3334
2022-01-13 12:14:37,992 ----- Epoch[036/300], Train Loss: 3.8956, Train Acc: 0.3334, time: 2233.85, Best Val(epoch34) Acc@1: 0.6390
2022-01-13 12:14:37,993 ----- Validation after Epoch: 36
2022-01-13 12:15:55,938 Val Step[0000/1563], Avg Loss: 1.4561, Avg Acc@1: 0.6562, Avg Acc@5: 0.9062
2022-01-13 12:15:58,011 Val Step[0050/1563], Avg Loss: 1.5895, Avg Acc@1: 0.6415, Avg Acc@5: 0.8676
2022-01-13 12:15:59,810 Val Step[0100/1563], Avg Loss: 1.6080, Avg Acc@1: 0.6414, Avg Acc@5: 0.8651
2022-01-13 12:16:01,760 Val Step[0150/1563], Avg Loss: 1.6011, Avg Acc@1: 0.6440, Avg Acc@5: 0.8661
2022-01-13 12:16:03,816 Val Step[0200/1563], Avg Loss: 1.6078, Avg Acc@1: 0.6432, Avg Acc@5: 0.8624
2022-01-13 12:16:05,855 Val Step[0250/1563], Avg Loss: 1.5908, Avg Acc@1: 0.6469, Avg Acc@5: 0.8629
2022-01-13 12:16:07,970 Val Step[0300/1563], Avg Loss: 1.5892, Avg Acc@1: 0.6487, Avg Acc@5: 0.8629
2022-01-13 12:16:10,045 Val Step[0350/1563], Avg Loss: 1.5968, Avg Acc@1: 0.6458, Avg Acc@5: 0.8615
2022-01-13 12:16:12,108 Val Step[0400/1563], Avg Loss: 1.5913, Avg Acc@1: 0.6471, Avg Acc@5: 0.8622
2022-01-13 12:16:14,149 Val Step[0450/1563], Avg Loss: 1.6006, Avg Acc@1: 0.6447, Avg Acc@5: 0.8598
2022-01-13 12:16:16,213 Val Step[0500/1563], Avg Loss: 1.6053, Avg Acc@1: 0.6428, Avg Acc@5: 0.8596
2022-01-13 12:16:18,267 Val Step[0550/1563], Avg Loss: 1.6087, Avg Acc@1: 0.6406, Avg Acc@5: 0.8600
2022-01-13 12:16:20,330 Val Step[0600/1563], Avg Loss: 1.6088, Avg Acc@1: 0.6398, Avg Acc@5: 0.8599
2022-01-13 12:16:22,395 Val Step[0650/1563], Avg Loss: 1.6097, Avg Acc@1: 0.6397, Avg Acc@5: 0.8603
2022-01-13 12:16:24,570 Val Step[0700/1563], Avg Loss: 1.6069, Avg Acc@1: 0.6407, Avg Acc@5: 0.8606
2022-01-13 12:16:26,492 Val Step[0750/1563], Avg Loss: 1.6135, Avg Acc@1: 0.6392, Avg Acc@5: 0.8596
2022-01-13 12:16:28,278 Val Step[0800/1563], Avg Loss: 1.6106, Avg Acc@1: 0.6400, Avg Acc@5: 0.8604
2022-01-13 12:16:30,077 Val Step[0850/1563], Avg Loss: 1.6123, Avg Acc@1: 0.6399, Avg Acc@5: 0.8602
2022-01-13 12:16:31,966 Val Step[0900/1563], Avg Loss: 1.6100, Avg Acc@1: 0.6396, Avg Acc@5: 0.8607
2022-01-13 12:16:33,955 Val Step[0950/1563], Avg Loss: 1.6096, Avg Acc@1: 0.6392, Avg Acc@5: 0.8611
2022-01-13 12:16:35,818 Val Step[1000/1563], Avg Loss: 1.6088, Avg Acc@1: 0.6394, Avg Acc@5: 0.8613
2022-01-13 12:16:37,623 Val Step[1050/1563], Avg Loss: 1.6123, Avg Acc@1: 0.6387, Avg Acc@5: 0.8610
2022-01-13 12:16:39,420 Val Step[1100/1563], Avg Loss: 1.6117, Avg Acc@1: 0.6384, Avg Acc@5: 0.8611
2022-01-13 12:16:41,354 Val Step[1150/1563], Avg Loss: 1.6121, Avg Acc@1: 0.6384, Avg Acc@5: 0.8608
2022-01-13 12:16:43,180 Val Step[1200/1563], Avg Loss: 1.6118, Avg Acc@1: 0.6385, Avg Acc@5: 0.8607
2022-01-13 12:16:45,031 Val Step[1250/1563], Avg Loss: 1.6097, Avg Acc@1: 0.6394, Avg Acc@5: 0.8610
2022-01-13 12:16:46,822 Val Step[1300/1563], Avg Loss: 1.6135, Avg Acc@1: 0.6387, Avg Acc@5: 0.8606
2022-01-13 12:16:48,594 Val Step[1350/1563], Avg Loss: 1.6137, Avg Acc@1: 0.6388, Avg Acc@5: 0.8604
2022-01-13 12:16:50,427 Val Step[1400/1563], Avg Loss: 1.6125, Avg Acc@1: 0.6388, Avg Acc@5: 0.8605
2022-01-13 12:16:52,201 Val Step[1450/1563], Avg Loss: 1.6126, Avg Acc@1: 0.6387, Avg Acc@5: 0.8607
2022-01-13 12:16:54,024 Val Step[1500/1563], Avg Loss: 1.6130, Avg Acc@1: 0.6388, Avg Acc@5: 0.8610
2022-01-13 12:16:55,802 Val Step[1550/1563], Avg Loss: 1.6144, Avg Acc@1: 0.6385, Avg Acc@5: 0.8606
2022-01-13 12:16:57,699 ----- Epoch[036/300], Validation Loss: 1.6144, Validation Acc@1: 0.6384, Validation Acc@5: 0.8606, time: 139.70
2022-01-13 12:16:57,710 Now training epoch 37. LR=0.000991
2022-01-13 12:18:48,336 Epoch[037/300], Step[0000/1252], Avg Loss: 3.4799, Avg Acc: 0.4932
2022-01-13 12:20:13,796 Epoch[037/300], Step[0050/1252], Avg Loss: 3.8777, Avg Acc: 0.3103
2022-01-13 12:21:36,865 Epoch[037/300], Step[0100/1252], Avg Loss: 3.8599, Avg Acc: 0.3330
2022-01-13 12:23:01,916 Epoch[037/300], Step[0150/1252], Avg Loss: 3.8582, Avg Acc: 0.3341
2022-01-13 12:24:27,192 Epoch[037/300], Step[0200/1252], Avg Loss: 3.8617, Avg Acc: 0.3374
2022-01-13 12:25:51,992 Epoch[037/300], Step[0250/1252], Avg Loss: 3.8732, Avg Acc: 0.3360
2022-01-13 12:27:16,492 Epoch[037/300], Step[0300/1252], Avg Loss: 3.8712, Avg Acc: 0.3369
2022-01-13 12:28:42,922 Epoch[037/300], Step[0350/1252], Avg Loss: 3.8815, Avg Acc: 0.3357
2022-01-13 12:30:08,955 Epoch[037/300], Step[0400/1252], Avg Loss: 3.8816, Avg Acc: 0.3383
2022-01-13 12:31:35,071 Epoch[037/300], Step[0450/1252], Avg Loss: 3.8767, Avg Acc: 0.3377
2022-01-13 12:33:01,143 Epoch[037/300], Step[0500/1252], Avg Loss: 3.8777, Avg Acc: 0.3374
2022-01-13 12:34:28,193 Epoch[037/300], Step[0550/1252], Avg Loss: 3.8795, Avg Acc: 0.3370
2022-01-13 12:35:54,337 Epoch[037/300], Step[0600/1252], Avg Loss: 3.8807, Avg Acc: 0.3379
2022-01-13 12:37:20,240 Epoch[037/300], Step[0650/1252], Avg Loss: 3.8849, Avg Acc: 0.3390
2022-01-13 12:38:46,187 Epoch[037/300], Step[0700/1252], Avg Loss: 3.8781, Avg Acc: 0.3398
2022-01-13 12:40:11,612 Epoch[037/300], Step[0750/1252], Avg Loss: 3.8764, Avg Acc: 0.3418
2022-01-13 12:41:36,812 Epoch[037/300], Step[0800/1252], Avg Loss: 3.8743, Avg Acc: 0.3424
2022-01-13 12:43:02,932 Epoch[037/300], Step[0850/1252], Avg Loss: 3.8769, Avg Acc: 0.3423
2022-01-13 12:44:29,452 Epoch[037/300], Step[0900/1252], Avg Loss: 3.8794, Avg Acc: 0.3418
2022-01-13 12:45:56,098 Epoch[037/300], Step[0950/1252], Avg Loss: 3.8802, Avg Acc: 0.3412
2022-01-13 12:47:22,209 Epoch[037/300], Step[1000/1252], Avg Loss: 3.8821, Avg Acc: 0.3414
2022-01-13 12:48:48,747 Epoch[037/300], Step[1050/1252], Avg Loss: 3.8824, Avg Acc: 0.3415
2022-01-13 12:50:15,880 Epoch[037/300], Step[1100/1252], Avg Loss: 3.8840, Avg Acc: 0.3422
2022-01-13 12:51:43,125 Epoch[037/300], Step[1150/1252], Avg Loss: 3.8845, Avg Acc: 0.3422
2022-01-13 12:53:08,844 Epoch[037/300], Step[1200/1252], Avg Loss: 3.8847, Avg Acc: 0.3437
2022-01-13 12:54:34,813 Epoch[037/300], Step[1250/1252], Avg Loss: 3.8842, Avg Acc: 0.3434
2022-01-13 12:54:41,864 ----- Epoch[037/300], Train Loss: 3.8842, Train Acc: 0.3434, time: 2264.15, Best Val(epoch34) Acc@1: 0.6390
2022-01-13 12:54:41,864 Now training epoch 38. LR=0.000990
2022-01-13 12:56:26,469 Epoch[038/300], Step[0000/1252], Avg Loss: 3.5951, Avg Acc: 0.5273
2022-01-13 12:57:52,096 Epoch[038/300], Step[0050/1252], Avg Loss: 3.8968, Avg Acc: 0.3500
2022-01-13 12:59:17,623 Epoch[038/300], Step[0100/1252], Avg Loss: 3.8992, Avg Acc: 0.3402
2022-01-13 13:00:42,087 Epoch[038/300], Step[0150/1252], Avg Loss: 3.8829, Avg Acc: 0.3353
2022-01-13 13:02:06,959 Epoch[038/300], Step[0200/1252], Avg Loss: 3.8737, Avg Acc: 0.3374
2022-01-13 13:03:32,307 Epoch[038/300], Step[0250/1252], Avg Loss: 3.8744, Avg Acc: 0.3408
2022-01-13 13:04:57,383 Epoch[038/300], Step[0300/1252], Avg Loss: 3.8668, Avg Acc: 0.3431
2022-01-13 13:06:23,432 Epoch[038/300], Step[0350/1252], Avg Loss: 3.8622, Avg Acc: 0.3437
2022-01-13 13:07:47,804 Epoch[038/300], Step[0400/1252], Avg Loss: 3.8643, Avg Acc: 0.3437
2022-01-13 13:09:13,496 Epoch[038/300], Step[0450/1252], Avg Loss: 3.8638, Avg Acc: 0.3408
2022-01-13 13:10:37,534 Epoch[038/300], Step[0500/1252], Avg Loss: 3.8704, Avg Acc: 0.3416
2022-01-13 13:12:02,675 Epoch[038/300], Step[0550/1252], Avg Loss: 3.8684, Avg Acc: 0.3429
2022-01-13 13:13:29,230 Epoch[038/300], Step[0600/1252], Avg Loss: 3.8601, Avg Acc: 0.3420
2022-01-13 13:14:54,288 Epoch[038/300], Step[0650/1252], Avg Loss: 3.8657, Avg Acc: 0.3412
2022-01-13 13:16:20,020 Epoch[038/300], Step[0700/1252], Avg Loss: 3.8683, Avg Acc: 0.3398
2022-01-13 13:17:46,456 Epoch[038/300], Step[0750/1252], Avg Loss: 3.8651, Avg Acc: 0.3421
2022-01-13 13:19:13,499 Epoch[038/300], Step[0800/1252], Avg Loss: 3.8692, Avg Acc: 0.3421
2022-01-13 13:20:39,694 Epoch[038/300], Step[0850/1252], Avg Loss: 3.8676, Avg Acc: 0.3420
2022-01-13 13:22:05,887 Epoch[038/300], Step[0900/1252], Avg Loss: 3.8735, Avg Acc: 0.3413
2022-01-13 13:23:32,588 Epoch[038/300], Step[0950/1252], Avg Loss: 3.8742, Avg Acc: 0.3411
2022-01-13 13:24:59,943 Epoch[038/300], Step[1000/1252], Avg Loss: 3.8731, Avg Acc: 0.3404
2022-01-13 13:26:26,962 Epoch[038/300], Step[1050/1252], Avg Loss: 3.8741, Avg Acc: 0.3395
2022-01-13 13:27:54,573 Epoch[038/300], Step[1100/1252], Avg Loss: 3.8786, Avg Acc: 0.3394
2022-01-13 13:29:21,476 Epoch[038/300], Step[1150/1252], Avg Loss: 3.8767, Avg Acc: 0.3394
2022-01-13 13:30:48,974 Epoch[038/300], Step[1200/1252], Avg Loss: 3.8766, Avg Acc: 0.3387
2022-01-13 13:32:15,071 Epoch[038/300], Step[1250/1252], Avg Loss: 3.8775, Avg Acc: 0.3393
2022-01-13 13:32:22,096 ----- Epoch[038/300], Train Loss: 3.8774, Train Acc: 0.3393, time: 2260.23, Best Val(epoch34) Acc@1: 0.6390
2022-01-13 13:32:22,096 ----- Validation after Epoch: 38
2022-01-13 13:33:37,484 Val Step[0000/1563], Avg Loss: 1.5429, Avg Acc@1: 0.6562, Avg Acc@5: 0.9375
2022-01-13 13:33:39,433 Val Step[0050/1563], Avg Loss: 1.6062, Avg Acc@1: 0.6513, Avg Acc@5: 0.8683
2022-01-13 13:33:41,263 Val Step[0100/1563], Avg Loss: 1.6467, Avg Acc@1: 0.6380, Avg Acc@5: 0.8626
2022-01-13 13:33:43,103 Val Step[0150/1563], Avg Loss: 1.6278, Avg Acc@1: 0.6413, Avg Acc@5: 0.8634
2022-01-13 13:33:45,005 Val Step[0200/1563], Avg Loss: 1.6266, Avg Acc@1: 0.6427, Avg Acc@5: 0.8613
2022-01-13 13:33:46,921 Val Step[0250/1563], Avg Loss: 1.6086, Avg Acc@1: 0.6467, Avg Acc@5: 0.8645
2022-01-13 13:33:48,763 Val Step[0300/1563], Avg Loss: 1.6026, Avg Acc@1: 0.6470, Avg Acc@5: 0.8642
2022-01-13 13:33:50,620 Val Step[0350/1563], Avg Loss: 1.6110, Avg Acc@1: 0.6461, Avg Acc@5: 0.8648
2022-01-13 13:33:52,443 Val Step[0400/1563], Avg Loss: 1.6105, Avg Acc@1: 0.6452, Avg Acc@5: 0.8653
2022-01-13 13:33:54,285 Val Step[0450/1563], Avg Loss: 1.6154, Avg Acc@1: 0.6429, Avg Acc@5: 0.8654
2022-01-13 13:33:56,202 Val Step[0500/1563], Avg Loss: 1.6165, Avg Acc@1: 0.6437, Avg Acc@5: 0.8653
2022-01-13 13:33:58,023 Val Step[0550/1563], Avg Loss: 1.6188, Avg Acc@1: 0.6424, Avg Acc@5: 0.8647
2022-01-13 13:33:59,813 Val Step[0600/1563], Avg Loss: 1.6172, Avg Acc@1: 0.6424, Avg Acc@5: 0.8650
2022-01-13 13:34:01,599 Val Step[0650/1563], Avg Loss: 1.6192, Avg Acc@1: 0.6421, Avg Acc@5: 0.8653
2022-01-13 13:34:03,408 Val Step[0700/1563], Avg Loss: 1.6175, Avg Acc@1: 0.6423, Avg Acc@5: 0.8658
2022-01-13 13:34:05,192 Val Step[0750/1563], Avg Loss: 1.6227, Avg Acc@1: 0.6416, Avg Acc@5: 0.8649
2022-01-13 13:34:07,020 Val Step[0800/1563], Avg Loss: 1.6202, Avg Acc@1: 0.6427, Avg Acc@5: 0.8658
2022-01-13 13:34:08,796 Val Step[0850/1563], Avg Loss: 1.6210, Avg Acc@1: 0.6418, Avg Acc@5: 0.8654
2022-01-13 13:34:10,584 Val Step[0900/1563], Avg Loss: 1.6169, Avg Acc@1: 0.6426, Avg Acc@5: 0.8657
2022-01-13 13:34:12,375 Val Step[0950/1563], Avg Loss: 1.6165, Avg Acc@1: 0.6428, Avg Acc@5: 0.8659
2022-01-13 13:34:14,171 Val Step[1000/1563], Avg Loss: 1.6154, Avg Acc@1: 0.6430, Avg Acc@5: 0.8662
2022-01-13 13:34:16,320 Val Step[1050/1563], Avg Loss: 1.6184, Avg Acc@1: 0.6422, Avg Acc@5: 0.8656
2022-01-13 13:34:18,505 Val Step[1100/1563], Avg Loss: 1.6172, Avg Acc@1: 0.6424, Avg Acc@5: 0.8656
2022-01-13 13:34:20,494 Val Step[1150/1563], Avg Loss: 1.6158, Avg Acc@1: 0.6428, Avg Acc@5: 0.8654
2022-01-13 13:34:22,403 Val Step[1200/1563], Avg Loss: 1.6157, Avg Acc@1: 0.6430, Avg Acc@5: 0.8656
2022-01-13 13:34:24,421 Val Step[1250/1563], Avg Loss: 1.6144, Avg Acc@1: 0.6428, Avg Acc@5: 0.8658
2022-01-13 13:34:26,319 Val Step[1300/1563], Avg Loss: 1.6184, Avg Acc@1: 0.6423, Avg Acc@5: 0.8650
2022-01-13 13:34:28,213 Val Step[1350/1563], Avg Loss: 1.6178, Avg Acc@1: 0.6421, Avg Acc@5: 0.8648
2022-01-13 13:34:30,097 Val Step[1400/1563], Avg Loss: 1.6174, Avg Acc@1: 0.6416, Avg Acc@5: 0.8648
2022-01-13 13:34:32,026 Val Step[1450/1563], Avg Loss: 1.6173, Avg Acc@1: 0.6412, Avg Acc@5: 0.8647
2022-01-13 13:34:33,910 Val Step[1500/1563], Avg Loss: 1.6169, Avg Acc@1: 0.6411, Avg Acc@5: 0.8651
2022-01-13 13:34:35,668 Val Step[1550/1563], Avg Loss: 1.6191, Avg Acc@1: 0.6409, Avg Acc@5: 0.8647
2022-01-13 13:34:37,624 ----- Epoch[038/300], Validation Loss: 1.6194, Validation Acc@1: 0.6410, Validation Acc@5: 0.8647, time: 135.53
2022-01-13 13:34:38,942 the pre best model acc:0.6390, at epoch 34
2022-01-13 13:34:39,223 current best model acc:0.6410, at epoch 38
2022-01-13 13:34:39,223 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 13:34:39,223 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 13:34:39,223 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 13:34:39,223 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 13:34:39,224 Now training epoch 39. LR=0.000989
2022-01-13 13:36:17,232 Epoch[039/300], Step[0000/1252], Avg Loss: 4.0884, Avg Acc: 0.3223
2022-01-13 13:37:43,091 Epoch[039/300], Step[0050/1252], Avg Loss: 3.9036, Avg Acc: 0.3359
2022-01-13 13:39:08,814 Epoch[039/300], Step[0100/1252], Avg Loss: 3.8681, Avg Acc: 0.3322
2022-01-13 13:40:34,713 Epoch[039/300], Step[0150/1252], Avg Loss: 3.8834, Avg Acc: 0.3340
2022-01-13 13:41:59,422 Epoch[039/300], Step[0200/1252], Avg Loss: 3.8860, Avg Acc: 0.3381
2022-01-13 13:43:25,601 Epoch[039/300], Step[0250/1252], Avg Loss: 3.8845, Avg Acc: 0.3380
2022-01-13 13:44:51,044 Epoch[039/300], Step[0300/1252], Avg Loss: 3.8805, Avg Acc: 0.3399
2022-01-13 13:46:16,546 Epoch[039/300], Step[0350/1252], Avg Loss: 3.8784, Avg Acc: 0.3448
2022-01-13 13:47:42,270 Epoch[039/300], Step[0400/1252], Avg Loss: 3.8719, Avg Acc: 0.3454
2022-01-13 13:49:08,016 Epoch[039/300], Step[0450/1252], Avg Loss: 3.8682, Avg Acc: 0.3451
2022-01-13 13:50:34,814 Epoch[039/300], Step[0500/1252], Avg Loss: 3.8694, Avg Acc: 0.3462
2022-01-13 13:52:00,577 Epoch[039/300], Step[0550/1252], Avg Loss: 3.8646, Avg Acc: 0.3458
2022-01-13 13:53:25,508 Epoch[039/300], Step[0600/1252], Avg Loss: 3.8697, Avg Acc: 0.3453
2022-01-13 13:54:51,973 Epoch[039/300], Step[0650/1252], Avg Loss: 3.8713, Avg Acc: 0.3451
2022-01-13 13:56:17,151 Epoch[039/300], Step[0700/1252], Avg Loss: 3.8696, Avg Acc: 0.3453
2022-01-13 13:57:43,243 Epoch[039/300], Step[0750/1252], Avg Loss: 3.8705, Avg Acc: 0.3431
2022-01-13 13:59:09,603 Epoch[039/300], Step[0800/1252], Avg Loss: 3.8713, Avg Acc: 0.3418
2022-01-13 14:00:35,521 Epoch[039/300], Step[0850/1252], Avg Loss: 3.8690, Avg Acc: 0.3420
2022-01-13 14:02:02,023 Epoch[039/300], Step[0900/1252], Avg Loss: 3.8729, Avg Acc: 0.3413
2022-01-13 14:03:28,281 Epoch[039/300], Step[0950/1252], Avg Loss: 3.8744, Avg Acc: 0.3403
2022-01-13 14:04:54,557 Epoch[039/300], Step[1000/1252], Avg Loss: 3.8729, Avg Acc: 0.3408
2022-01-13 14:06:20,686 Epoch[039/300], Step[1050/1252], Avg Loss: 3.8755, Avg Acc: 0.3403
2022-01-13 14:07:47,098 Epoch[039/300], Step[1100/1252], Avg Loss: 3.8728, Avg Acc: 0.3406
2022-01-13 14:09:14,615 Epoch[039/300], Step[1150/1252], Avg Loss: 3.8687, Avg Acc: 0.3414
2022-01-13 14:10:40,499 Epoch[039/300], Step[1200/1252], Avg Loss: 3.8702, Avg Acc: 0.3416
2022-01-13 14:12:07,287 Epoch[039/300], Step[1250/1252], Avg Loss: 3.8689, Avg Acc: 0.3425
2022-01-13 14:12:14,439 ----- Epoch[039/300], Train Loss: 3.8689, Train Acc: 0.3425, time: 2255.21, Best Val(epoch38) Acc@1: 0.6410
2022-01-13 14:12:14,439 Now training epoch 40. LR=0.000988
2022-01-13 14:14:12,307 Epoch[040/300], Step[0000/1252], Avg Loss: 3.8031, Avg Acc: 0.3408
2022-01-13 14:15:38,358 Epoch[040/300], Step[0050/1252], Avg Loss: 3.8495, Avg Acc: 0.3585
2022-01-13 14:17:03,592 Epoch[040/300], Step[0100/1252], Avg Loss: 3.8889, Avg Acc: 0.3402
2022-01-13 14:18:29,471 Epoch[040/300], Step[0150/1252], Avg Loss: 3.8803, Avg Acc: 0.3417
2022-01-13 14:19:55,400 Epoch[040/300], Step[0200/1252], Avg Loss: 3.8634, Avg Acc: 0.3464
2022-01-13 14:21:21,692 Epoch[040/300], Step[0250/1252], Avg Loss: 3.8598, Avg Acc: 0.3430
2022-01-13 14:22:45,990 Epoch[040/300], Step[0300/1252], Avg Loss: 3.8627, Avg Acc: 0.3409
2022-01-13 14:24:11,718 Epoch[040/300], Step[0350/1252], Avg Loss: 3.8616, Avg Acc: 0.3423
2022-01-13 14:25:37,752 Epoch[040/300], Step[0400/1252], Avg Loss: 3.8655, Avg Acc: 0.3424
2022-01-13 14:27:04,216 Epoch[040/300], Step[0450/1252], Avg Loss: 3.8617, Avg Acc: 0.3403
2022-01-13 14:28:29,713 Epoch[040/300], Step[0500/1252], Avg Loss: 3.8586, Avg Acc: 0.3424
2022-01-13 14:29:56,655 Epoch[040/300], Step[0550/1252], Avg Loss: 3.8640, Avg Acc: 0.3423
2022-01-13 14:31:23,088 Epoch[040/300], Step[0600/1252], Avg Loss: 3.8646, Avg Acc: 0.3427
2022-01-13 14:32:49,157 Epoch[040/300], Step[0650/1252], Avg Loss: 3.8600, Avg Acc: 0.3437
2022-01-13 14:34:15,283 Epoch[040/300], Step[0700/1252], Avg Loss: 3.8520, Avg Acc: 0.3436
2022-01-13 14:35:40,927 Epoch[040/300], Step[0750/1252], Avg Loss: 3.8508, Avg Acc: 0.3436
2022-01-13 14:37:06,084 Epoch[040/300], Step[0800/1252], Avg Loss: 3.8492, Avg Acc: 0.3420
2022-01-13 14:38:32,905 Epoch[040/300], Step[0850/1252], Avg Loss: 3.8504, Avg Acc: 0.3419
2022-01-13 14:40:00,047 Epoch[040/300], Step[0900/1252], Avg Loss: 3.8478, Avg Acc: 0.3412
2022-01-13 14:41:27,241 Epoch[040/300], Step[0950/1252], Avg Loss: 3.8480, Avg Acc: 0.3386
2022-01-13 14:42:53,578 Epoch[040/300], Step[1000/1252], Avg Loss: 3.8501, Avg Acc: 0.3377
2022-01-13 14:44:19,746 Epoch[040/300], Step[1050/1252], Avg Loss: 3.8539, Avg Acc: 0.3373
2022-01-13 14:45:46,573 Epoch[040/300], Step[1100/1252], Avg Loss: 3.8564, Avg Acc: 0.3369
2022-01-13 14:47:13,386 Epoch[040/300], Step[1150/1252], Avg Loss: 3.8582, Avg Acc: 0.3371
2022-01-13 14:48:40,199 Epoch[040/300], Step[1200/1252], Avg Loss: 3.8565, Avg Acc: 0.3373
2022-01-13 14:50:05,140 Epoch[040/300], Step[1250/1252], Avg Loss: 3.8564, Avg Acc: 0.3376
2022-01-13 14:50:12,413 ----- Epoch[040/300], Train Loss: 3.8563, Train Acc: 0.3376, time: 2277.97, Best Val(epoch38) Acc@1: 0.6410
2022-01-13 14:50:12,413 ----- Validation after Epoch: 40
2022-01-13 14:51:29,516 Val Step[0000/1563], Avg Loss: 1.3707, Avg Acc@1: 0.6875, Avg Acc@5: 0.9375
2022-01-13 14:51:31,615 Val Step[0050/1563], Avg Loss: 1.5691, Avg Acc@1: 0.6691, Avg Acc@5: 0.8664
2022-01-13 14:51:33,637 Val Step[0100/1563], Avg Loss: 1.5653, Avg Acc@1: 0.6584, Avg Acc@5: 0.8744
2022-01-13 14:51:35,647 Val Step[0150/1563], Avg Loss: 1.5735, Avg Acc@1: 0.6581, Avg Acc@5: 0.8711
2022-01-13 14:51:37,697 Val Step[0200/1563], Avg Loss: 1.5830, Avg Acc@1: 0.6566, Avg Acc@5: 0.8689
2022-01-13 14:51:39,709 Val Step[0250/1563], Avg Loss: 1.5665, Avg Acc@1: 0.6607, Avg Acc@5: 0.8705
2022-01-13 14:51:41,742 Val Step[0300/1563], Avg Loss: 1.5650, Avg Acc@1: 0.6610, Avg Acc@5: 0.8706
2022-01-13 14:51:43,790 Val Step[0350/1563], Avg Loss: 1.5747, Avg Acc@1: 0.6595, Avg Acc@5: 0.8696
2022-01-13 14:51:45,844 Val Step[0400/1563], Avg Loss: 1.5729, Avg Acc@1: 0.6590, Avg Acc@5: 0.8697
2022-01-13 14:51:47,893 Val Step[0450/1563], Avg Loss: 1.5825, Avg Acc@1: 0.6565, Avg Acc@5: 0.8675
2022-01-13 14:51:49,938 Val Step[0500/1563], Avg Loss: 1.5845, Avg Acc@1: 0.6560, Avg Acc@5: 0.8675
2022-01-13 14:51:52,005 Val Step[0550/1563], Avg Loss: 1.5877, Avg Acc@1: 0.6542, Avg Acc@5: 0.8669
2022-01-13 14:51:54,021 Val Step[0600/1563], Avg Loss: 1.5858, Avg Acc@1: 0.6548, Avg Acc@5: 0.8675
2022-01-13 14:51:56,041 Val Step[0650/1563], Avg Loss: 1.5860, Avg Acc@1: 0.6544, Avg Acc@5: 0.8673
2022-01-13 14:51:58,117 Val Step[0700/1563], Avg Loss: 1.5849, Avg Acc@1: 0.6542, Avg Acc@5: 0.8675
2022-01-13 14:52:00,198 Val Step[0750/1563], Avg Loss: 1.5909, Avg Acc@1: 0.6540, Avg Acc@5: 0.8665
2022-01-13 14:52:02,287 Val Step[0800/1563], Avg Loss: 1.5896, Avg Acc@1: 0.6549, Avg Acc@5: 0.8665
2022-01-13 14:52:04,386 Val Step[0850/1563], Avg Loss: 1.5911, Avg Acc@1: 0.6541, Avg Acc@5: 0.8657
2022-01-13 14:52:06,443 Val Step[0900/1563], Avg Loss: 1.5871, Avg Acc@1: 0.6548, Avg Acc@5: 0.8665
2022-01-13 14:52:08,495 Val Step[0950/1563], Avg Loss: 1.5861, Avg Acc@1: 0.6547, Avg Acc@5: 0.8669
2022-01-13 14:52:10,533 Val Step[1000/1563], Avg Loss: 1.5855, Avg Acc@1: 0.6545, Avg Acc@5: 0.8667
2022-01-13 14:52:12,564 Val Step[1050/1563], Avg Loss: 1.5880, Avg Acc@1: 0.6532, Avg Acc@5: 0.8665
2022-01-13 14:52:14,598 Val Step[1100/1563], Avg Loss: 1.5877, Avg Acc@1: 0.6527, Avg Acc@5: 0.8664
2022-01-13 14:52:16,633 Val Step[1150/1563], Avg Loss: 1.5855, Avg Acc@1: 0.6528, Avg Acc@5: 0.8664
2022-01-13 14:52:18,695 Val Step[1200/1563], Avg Loss: 1.5855, Avg Acc@1: 0.6527, Avg Acc@5: 0.8664
2022-01-13 14:52:20,805 Val Step[1250/1563], Avg Loss: 1.5849, Avg Acc@1: 0.6529, Avg Acc@5: 0.8662
2022-01-13 14:52:22,869 Val Step[1300/1563], Avg Loss: 1.5875, Avg Acc@1: 0.6528, Avg Acc@5: 0.8659
2022-01-13 14:52:24,961 Val Step[1350/1563], Avg Loss: 1.5879, Avg Acc@1: 0.6529, Avg Acc@5: 0.8659
2022-01-13 14:52:27,064 Val Step[1400/1563], Avg Loss: 1.5877, Avg Acc@1: 0.6520, Avg Acc@5: 0.8660
2022-01-13 14:52:28,947 Val Step[1450/1563], Avg Loss: 1.5881, Avg Acc@1: 0.6517, Avg Acc@5: 0.8658
2022-01-13 14:52:30,737 Val Step[1500/1563], Avg Loss: 1.5876, Avg Acc@1: 0.6522, Avg Acc@5: 0.8659
2022-01-13 14:52:32,487 Val Step[1550/1563], Avg Loss: 1.5881, Avg Acc@1: 0.6520, Avg Acc@5: 0.8659
2022-01-13 14:52:34,315 ----- Epoch[040/300], Validation Loss: 1.5881, Validation Acc@1: 0.6521, Validation Acc@5: 0.8660, time: 141.90
2022-01-13 14:52:35,639 the pre best model acc:0.6410, at epoch 38
2022-01-13 14:52:35,906 current best model acc:0.6521, at epoch 40
2022-01-13 14:52:35,906 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 14:52:35,906 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 14:52:35,906 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 14:52:35,906 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 14:52:36,620 ----- Save model: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-40-Loss-3.8698712083248346.pdparams
2022-01-13 14:52:36,621 ----- Save optim: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-40-Loss-3.8698712083248346.pdopt
2022-01-13 14:52:36,621 Now training epoch 41. LR=0.000986
2022-01-13 14:54:20,661 Epoch[041/300], Step[0000/1252], Avg Loss: 3.9178, Avg Acc: 0.3496
2022-01-13 14:55:47,122 Epoch[041/300], Step[0050/1252], Avg Loss: 3.7434, Avg Acc: 0.3590
2022-01-13 14:57:13,074 Epoch[041/300], Step[0100/1252], Avg Loss: 3.7866, Avg Acc: 0.3479
2022-01-13 14:58:38,830 Epoch[041/300], Step[0150/1252], Avg Loss: 3.7940, Avg Acc: 0.3535
2022-01-13 15:00:04,532 Epoch[041/300], Step[0200/1252], Avg Loss: 3.8008, Avg Acc: 0.3452
2022-01-13 15:01:31,122 Epoch[041/300], Step[0250/1252], Avg Loss: 3.8194, Avg Acc: 0.3449
2022-01-13 15:02:57,339 Epoch[041/300], Step[0300/1252], Avg Loss: 3.8257, Avg Acc: 0.3454
2022-01-13 15:04:23,491 Epoch[041/300], Step[0350/1252], Avg Loss: 3.8266, Avg Acc: 0.3474
2022-01-13 15:05:50,166 Epoch[041/300], Step[0400/1252], Avg Loss: 3.8410, Avg Acc: 0.3456
2022-01-13 15:07:16,125 Epoch[041/300], Step[0450/1252], Avg Loss: 3.8330, Avg Acc: 0.3474
2022-01-13 15:08:40,879 Epoch[041/300], Step[0500/1252], Avg Loss: 3.8413, Avg Acc: 0.3438
2022-01-13 15:10:05,125 Epoch[041/300], Step[0550/1252], Avg Loss: 3.8352, Avg Acc: 0.3423
2022-01-13 15:11:28,769 Epoch[041/300], Step[0600/1252], Avg Loss: 3.8358, Avg Acc: 0.3418
2022-01-13 15:12:53,918 Epoch[041/300], Step[0650/1252], Avg Loss: 3.8385, Avg Acc: 0.3411
2022-01-13 15:14:17,410 Epoch[041/300], Step[0700/1252], Avg Loss: 3.8369, Avg Acc: 0.3418
2022-01-13 15:15:41,447 Epoch[041/300], Step[0750/1252], Avg Loss: 3.8395, Avg Acc: 0.3422
2022-01-13 15:17:05,856 Epoch[041/300], Step[0800/1252], Avg Loss: 3.8410, Avg Acc: 0.3426
2022-01-13 15:18:31,476 Epoch[041/300], Step[0850/1252], Avg Loss: 3.8421, Avg Acc: 0.3425
2022-01-13 15:19:56,904 Epoch[041/300], Step[0900/1252], Avg Loss: 3.8419, Avg Acc: 0.3430
2022-01-13 15:21:22,299 Epoch[041/300], Step[0950/1252], Avg Loss: 3.8409, Avg Acc: 0.3431
2022-01-13 15:22:47,012 Epoch[041/300], Step[1000/1252], Avg Loss: 3.8415, Avg Acc: 0.3428
2022-01-13 15:24:12,554 Epoch[041/300], Step[1050/1252], Avg Loss: 3.8404, Avg Acc: 0.3427
2022-01-13 15:25:37,373 Epoch[041/300], Step[1100/1252], Avg Loss: 3.8432, Avg Acc: 0.3429
2022-01-13 15:27:01,728 Epoch[041/300], Step[1150/1252], Avg Loss: 3.8425, Avg Acc: 0.3433
2022-01-13 15:28:27,624 Epoch[041/300], Step[1200/1252], Avg Loss: 3.8403, Avg Acc: 0.3429
2022-01-13 15:29:53,115 Epoch[041/300], Step[1250/1252], Avg Loss: 3.8414, Avg Acc: 0.3423
2022-01-13 15:30:00,309 ----- Epoch[041/300], Train Loss: 3.8414, Train Acc: 0.3423, time: 2243.68, Best Val(epoch40) Acc@1: 0.6521
2022-01-13 15:30:00,310 Now training epoch 42. LR=0.000985
2022-01-13 15:31:46,597 Epoch[042/300], Step[0000/1252], Avg Loss: 4.2056, Avg Acc: 0.3018
2022-01-13 15:33:11,521 Epoch[042/300], Step[0050/1252], Avg Loss: 3.9218, Avg Acc: 0.3437
2022-01-13 15:34:37,300 Epoch[042/300], Step[0100/1252], Avg Loss: 3.8985, Avg Acc: 0.3385
2022-01-13 15:36:03,159 Epoch[042/300], Step[0150/1252], Avg Loss: 3.8637, Avg Acc: 0.3383
2022-01-13 15:37:29,508 Epoch[042/300], Step[0200/1252], Avg Loss: 3.8688, Avg Acc: 0.3435
2022-01-13 15:38:55,435 Epoch[042/300], Step[0250/1252], Avg Loss: 3.8606, Avg Acc: 0.3481
2022-01-13 15:40:21,512 Epoch[042/300], Step[0300/1252], Avg Loss: 3.8601, Avg Acc: 0.3482
2022-01-13 15:41:47,317 Epoch[042/300], Step[0350/1252], Avg Loss: 3.8571, Avg Acc: 0.3491
2022-01-13 15:43:13,389 Epoch[042/300], Step[0400/1252], Avg Loss: 3.8539, Avg Acc: 0.3462
2022-01-13 15:44:40,121 Epoch[042/300], Step[0450/1252], Avg Loss: 3.8513, Avg Acc: 0.3442
2022-01-13 15:46:06,464 Epoch[042/300], Step[0500/1252], Avg Loss: 3.8503, Avg Acc: 0.3431
2022-01-13 15:47:32,755 Epoch[042/300], Step[0550/1252], Avg Loss: 3.8486, Avg Acc: 0.3435
2022-01-13 15:48:58,595 Epoch[042/300], Step[0600/1252], Avg Loss: 3.8566, Avg Acc: 0.3423
2022-01-13 15:50:24,138 Epoch[042/300], Step[0650/1252], Avg Loss: 3.8541, Avg Acc: 0.3433
2022-01-13 15:51:49,426 Epoch[042/300], Step[0700/1252], Avg Loss: 3.8472, Avg Acc: 0.3458
2022-01-13 15:53:15,445 Epoch[042/300], Step[0750/1252], Avg Loss: 3.8432, Avg Acc: 0.3467
2022-01-13 15:54:41,878 Epoch[042/300], Step[0800/1252], Avg Loss: 3.8453, Avg Acc: 0.3463
2022-01-13 15:56:08,012 Epoch[042/300], Step[0850/1252], Avg Loss: 3.8491, Avg Acc: 0.3456
2022-01-13 15:57:32,420 Epoch[042/300], Step[0900/1252], Avg Loss: 3.8488, Avg Acc: 0.3454
2022-01-13 15:58:58,607 Epoch[042/300], Step[0950/1252], Avg Loss: 3.8502, Avg Acc: 0.3451
2022-01-13 16:00:25,059 Epoch[042/300], Step[1000/1252], Avg Loss: 3.8489, Avg Acc: 0.3449
2022-01-13 16:01:50,772 Epoch[042/300], Step[1050/1252], Avg Loss: 3.8511, Avg Acc: 0.3444
2022-01-13 16:03:15,531 Epoch[042/300], Step[1100/1252], Avg Loss: 3.8497, Avg Acc: 0.3444
2022-01-13 16:04:40,556 Epoch[042/300], Step[1150/1252], Avg Loss: 3.8497, Avg Acc: 0.3442
2022-01-13 16:06:05,273 Epoch[042/300], Step[1200/1252], Avg Loss: 3.8460, Avg Acc: 0.3448
2022-01-13 16:07:30,967 Epoch[042/300], Step[1250/1252], Avg Loss: 3.8487, Avg Acc: 0.3438
2022-01-13 16:07:38,099 ----- Epoch[042/300], Train Loss: 3.8486, Train Acc: 0.3438, time: 2257.79, Best Val(epoch40) Acc@1: 0.6521
2022-01-13 16:07:38,099 ----- Validation after Epoch: 42
2022-01-13 16:08:50,132 Val Step[0000/1563], Avg Loss: 1.4596, Avg Acc@1: 0.6875, Avg Acc@5: 0.8438
2022-01-13 16:08:51,997 Val Step[0050/1563], Avg Loss: 1.5840, Avg Acc@1: 0.6544, Avg Acc@5: 0.8609
2022-01-13 16:08:54,026 Val Step[0100/1563], Avg Loss: 1.5800, Avg Acc@1: 0.6516, Avg Acc@5: 0.8651
2022-01-13 16:08:56,062 Val Step[0150/1563], Avg Loss: 1.5758, Avg Acc@1: 0.6527, Avg Acc@5: 0.8657
2022-01-13 16:08:58,117 Val Step[0200/1563], Avg Loss: 1.5754, Avg Acc@1: 0.6545, Avg Acc@5: 0.8630
2022-01-13 16:09:00,150 Val Step[0250/1563], Avg Loss: 1.5562, Avg Acc@1: 0.6584, Avg Acc@5: 0.8654
2022-01-13 16:09:02,237 Val Step[0300/1563], Avg Loss: 1.5538, Avg Acc@1: 0.6607, Avg Acc@5: 0.8659
2022-01-13 16:09:04,345 Val Step[0350/1563], Avg Loss: 1.5573, Avg Acc@1: 0.6596, Avg Acc@5: 0.8657
2022-01-13 16:09:06,496 Val Step[0400/1563], Avg Loss: 1.5521, Avg Acc@1: 0.6606, Avg Acc@5: 0.8669
2022-01-13 16:09:08,616 Val Step[0450/1563], Avg Loss: 1.5642, Avg Acc@1: 0.6583, Avg Acc@5: 0.8654
2022-01-13 16:09:10,704 Val Step[0500/1563], Avg Loss: 1.5646, Avg Acc@1: 0.6570, Avg Acc@5: 0.8656
2022-01-13 16:09:12,768 Val Step[0550/1563], Avg Loss: 1.5678, Avg Acc@1: 0.6561, Avg Acc@5: 0.8650
2022-01-13 16:09:14,835 Val Step[0600/1563], Avg Loss: 1.5688, Avg Acc@1: 0.6556, Avg Acc@5: 0.8654
2022-01-13 16:09:16,881 Val Step[0650/1563], Avg Loss: 1.5692, Avg Acc@1: 0.6562, Avg Acc@5: 0.8661
2022-01-13 16:09:18,925 Val Step[0700/1563], Avg Loss: 1.5671, Avg Acc@1: 0.6572, Avg Acc@5: 0.8673
2022-01-13 16:09:20,965 Val Step[0750/1563], Avg Loss: 1.5732, Avg Acc@1: 0.6558, Avg Acc@5: 0.8669
2022-01-13 16:09:23,013 Val Step[0800/1563], Avg Loss: 1.5697, Avg Acc@1: 0.6569, Avg Acc@5: 0.8681
2022-01-13 16:09:24,986 Val Step[0850/1563], Avg Loss: 1.5725, Avg Acc@1: 0.6561, Avg Acc@5: 0.8672
2022-01-13 16:09:26,880 Val Step[0900/1563], Avg Loss: 1.5695, Avg Acc@1: 0.6566, Avg Acc@5: 0.8680
2022-01-13 16:09:28,846 Val Step[0950/1563], Avg Loss: 1.5676, Avg Acc@1: 0.6564, Avg Acc@5: 0.8688
2022-01-13 16:09:30,783 Val Step[1000/1563], Avg Loss: 1.5659, Avg Acc@1: 0.6566, Avg Acc@5: 0.8693
2022-01-13 16:09:32,631 Val Step[1050/1563], Avg Loss: 1.5675, Avg Acc@1: 0.6562, Avg Acc@5: 0.8687
2022-01-13 16:09:34,483 Val Step[1100/1563], Avg Loss: 1.5669, Avg Acc@1: 0.6559, Avg Acc@5: 0.8688
2022-01-13 16:09:36,282 Val Step[1150/1563], Avg Loss: 1.5664, Avg Acc@1: 0.6559, Avg Acc@5: 0.8687
2022-01-13 16:09:38,075 Val Step[1200/1563], Avg Loss: 1.5662, Avg Acc@1: 0.6562, Avg Acc@5: 0.8687
2022-01-13 16:09:39,928 Val Step[1250/1563], Avg Loss: 1.5645, Avg Acc@1: 0.6565, Avg Acc@5: 0.8690
2022-01-13 16:09:41,810 Val Step[1300/1563], Avg Loss: 1.5686, Avg Acc@1: 0.6558, Avg Acc@5: 0.8685
2022-01-13 16:09:43,621 Val Step[1350/1563], Avg Loss: 1.5681, Avg Acc@1: 0.6561, Avg Acc@5: 0.8685
2022-01-13 16:09:45,475 Val Step[1400/1563], Avg Loss: 1.5678, Avg Acc@1: 0.6556, Avg Acc@5: 0.8683
2022-01-13 16:09:47,349 Val Step[1450/1563], Avg Loss: 1.5670, Avg Acc@1: 0.6555, Avg Acc@5: 0.8685
2022-01-13 16:09:49,179 Val Step[1500/1563], Avg Loss: 1.5667, Avg Acc@1: 0.6557, Avg Acc@5: 0.8688
2022-01-13 16:09:50,999 Val Step[1550/1563], Avg Loss: 1.5675, Avg Acc@1: 0.6553, Avg Acc@5: 0.8686
2022-01-13 16:09:52,896 ----- Epoch[042/300], Validation Loss: 1.5676, Validation Acc@1: 0.6553, Validation Acc@5: 0.8686, time: 134.79
2022-01-13 16:09:54,209 the pre best model acc:0.6521, at epoch 40
2022-01-13 16:09:54,477 current best model acc:0.6553, at epoch 42
2022-01-13 16:09:54,478 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 16:09:54,478 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 16:09:54,478 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 16:09:54,478 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 16:09:54,478 Now training epoch 43. LR=0.000984
2022-01-13 16:11:38,480 Epoch[043/300], Step[0000/1252], Avg Loss: 3.9414, Avg Acc: 0.2695
2022-01-13 16:13:04,285 Epoch[043/300], Step[0050/1252], Avg Loss: 3.8443, Avg Acc: 0.3421
2022-01-13 16:14:30,095 Epoch[043/300], Step[0100/1252], Avg Loss: 3.7978, Avg Acc: 0.3396
2022-01-13 16:15:55,901 Epoch[043/300], Step[0150/1252], Avg Loss: 3.7922, Avg Acc: 0.3415
2022-01-13 16:17:22,609 Epoch[043/300], Step[0200/1252], Avg Loss: 3.7852, Avg Acc: 0.3433
2022-01-13 16:18:48,570 Epoch[043/300], Step[0250/1252], Avg Loss: 3.7955, Avg Acc: 0.3457
2022-01-13 16:20:13,957 Epoch[043/300], Step[0300/1252], Avg Loss: 3.8087, Avg Acc: 0.3478
2022-01-13 16:21:38,349 Epoch[043/300], Step[0350/1252], Avg Loss: 3.8032, Avg Acc: 0.3511
2022-01-13 16:23:02,954 Epoch[043/300], Step[0400/1252], Avg Loss: 3.8001, Avg Acc: 0.3512
2022-01-13 16:24:28,541 Epoch[043/300], Step[0450/1252], Avg Loss: 3.8036, Avg Acc: 0.3483
2022-01-13 16:25:55,178 Epoch[043/300], Step[0500/1252], Avg Loss: 3.8090, Avg Acc: 0.3504
2022-01-13 16:27:20,437 Epoch[043/300], Step[0550/1252], Avg Loss: 3.8047, Avg Acc: 0.3521
2022-01-13 16:28:46,345 Epoch[043/300], Step[0600/1252], Avg Loss: 3.8042, Avg Acc: 0.3520
2022-01-13 16:30:12,818 Epoch[043/300], Step[0650/1252], Avg Loss: 3.8098, Avg Acc: 0.3506
2022-01-13 16:31:38,649 Epoch[043/300], Step[0700/1252], Avg Loss: 3.8099, Avg Acc: 0.3496
2022-01-13 16:33:03,887 Epoch[043/300], Step[0750/1252], Avg Loss: 3.8137, Avg Acc: 0.3503
2022-01-13 16:34:28,713 Epoch[043/300], Step[0800/1252], Avg Loss: 3.8200, Avg Acc: 0.3497
2022-01-13 16:35:54,319 Epoch[043/300], Step[0850/1252], Avg Loss: 3.8249, Avg Acc: 0.3495
2022-01-13 16:37:19,390 Epoch[043/300], Step[0900/1252], Avg Loss: 3.8211, Avg Acc: 0.3514
2022-01-13 16:38:44,790 Epoch[043/300], Step[0950/1252], Avg Loss: 3.8270, Avg Acc: 0.3512
2022-01-13 16:40:10,799 Epoch[043/300], Step[1000/1252], Avg Loss: 3.8270, Avg Acc: 0.3512
2022-01-13 16:41:36,850 Epoch[043/300], Step[1050/1252], Avg Loss: 3.8293, Avg Acc: 0.3519
2022-01-13 16:43:02,015 Epoch[043/300], Step[1100/1252], Avg Loss: 3.8284, Avg Acc: 0.3515
2022-01-13 16:44:27,496 Epoch[043/300], Step[1150/1252], Avg Loss: 3.8298, Avg Acc: 0.3517
2022-01-13 16:45:54,006 Epoch[043/300], Step[1200/1252], Avg Loss: 3.8294, Avg Acc: 0.3513
2022-01-13 16:47:20,185 Epoch[043/300], Step[1250/1252], Avg Loss: 3.8289, Avg Acc: 0.3508
2022-01-13 16:47:27,230 ----- Epoch[043/300], Train Loss: 3.8289, Train Acc: 0.3508, time: 2252.75, Best Val(epoch42) Acc@1: 0.6553
2022-01-13 16:47:27,230 Now training epoch 44. LR=0.000982
2022-01-13 16:49:11,359 Epoch[044/300], Step[0000/1252], Avg Loss: 4.0079, Avg Acc: 0.0625
2022-01-13 16:50:35,645 Epoch[044/300], Step[0050/1252], Avg Loss: 3.8735, Avg Acc: 0.3322
2022-01-13 16:52:00,513 Epoch[044/300], Step[0100/1252], Avg Loss: 3.8309, Avg Acc: 0.3405
2022-01-13 16:53:26,247 Epoch[044/300], Step[0150/1252], Avg Loss: 3.8223, Avg Acc: 0.3397
2022-01-13 16:54:50,434 Epoch[044/300], Step[0200/1252], Avg Loss: 3.8119, Avg Acc: 0.3458
2022-01-13 16:56:15,893 Epoch[044/300], Step[0250/1252], Avg Loss: 3.8316, Avg Acc: 0.3447
2022-01-13 16:57:41,671 Epoch[044/300], Step[0300/1252], Avg Loss: 3.8217, Avg Acc: 0.3435
2022-01-13 16:59:06,559 Epoch[044/300], Step[0350/1252], Avg Loss: 3.8157, Avg Acc: 0.3482
2022-01-13 17:00:31,435 Epoch[044/300], Step[0400/1252], Avg Loss: 3.8183, Avg Acc: 0.3476
2022-01-13 17:01:55,882 Epoch[044/300], Step[0450/1252], Avg Loss: 3.8179, Avg Acc: 0.3465
2022-01-13 17:03:19,915 Epoch[044/300], Step[0500/1252], Avg Loss: 3.8182, Avg Acc: 0.3485
2022-01-13 17:04:44,537 Epoch[044/300], Step[0550/1252], Avg Loss: 3.8145, Avg Acc: 0.3499
2022-01-13 17:06:09,724 Epoch[044/300], Step[0600/1252], Avg Loss: 3.8126, Avg Acc: 0.3521
2022-01-13 17:07:34,907 Epoch[044/300], Step[0650/1252], Avg Loss: 3.8141, Avg Acc: 0.3510
2022-01-13 17:09:00,437 Epoch[044/300], Step[0700/1252], Avg Loss: 3.8134, Avg Acc: 0.3502
2022-01-13 17:10:25,690 Epoch[044/300], Step[0750/1252], Avg Loss: 3.8138, Avg Acc: 0.3499
2022-01-13 17:11:51,596 Epoch[044/300], Step[0800/1252], Avg Loss: 3.8152, Avg Acc: 0.3483
2022-01-13 17:13:16,400 Epoch[044/300], Step[0850/1252], Avg Loss: 3.8156, Avg Acc: 0.3485
2022-01-13 17:14:41,917 Epoch[044/300], Step[0900/1252], Avg Loss: 3.8130, Avg Acc: 0.3491
2022-01-13 17:16:08,362 Epoch[044/300], Step[0950/1252], Avg Loss: 3.8116, Avg Acc: 0.3493
2022-01-13 17:17:34,457 Epoch[044/300], Step[1000/1252], Avg Loss: 3.8149, Avg Acc: 0.3485
2022-01-13 17:19:01,207 Epoch[044/300], Step[1050/1252], Avg Loss: 3.8200, Avg Acc: 0.3475
2022-01-13 17:20:27,225 Epoch[044/300], Step[1100/1252], Avg Loss: 3.8200, Avg Acc: 0.3490
2022-01-13 17:21:52,547 Epoch[044/300], Step[1150/1252], Avg Loss: 3.8191, Avg Acc: 0.3486
2022-01-13 17:23:18,552 Epoch[044/300], Step[1200/1252], Avg Loss: 3.8214, Avg Acc: 0.3483
2022-01-13 17:24:45,001 Epoch[044/300], Step[1250/1252], Avg Loss: 3.8209, Avg Acc: 0.3486
2022-01-13 17:24:52,019 ----- Epoch[044/300], Train Loss: 3.8209, Train Acc: 0.3486, time: 2244.78, Best Val(epoch42) Acc@1: 0.6553
2022-01-13 17:24:52,019 ----- Validation after Epoch: 44
2022-01-13 17:26:02,130 Val Step[0000/1563], Avg Loss: 1.3739, Avg Acc@1: 0.6875, Avg Acc@5: 0.9375
2022-01-13 17:26:04,132 Val Step[0050/1563], Avg Loss: 1.5762, Avg Acc@1: 0.6544, Avg Acc@5: 0.8732
2022-01-13 17:26:06,245 Val Step[0100/1563], Avg Loss: 1.5806, Avg Acc@1: 0.6550, Avg Acc@5: 0.8722
2022-01-13 17:26:08,100 Val Step[0150/1563], Avg Loss: 1.5801, Avg Acc@1: 0.6581, Avg Acc@5: 0.8733
2022-01-13 17:26:09,917 Val Step[0200/1563], Avg Loss: 1.5831, Avg Acc@1: 0.6592, Avg Acc@5: 0.8711
2022-01-13 17:26:11,715 Val Step[0250/1563], Avg Loss: 1.5628, Avg Acc@1: 0.6628, Avg Acc@5: 0.8728
2022-01-13 17:26:13,491 Val Step[0300/1563], Avg Loss: 1.5644, Avg Acc@1: 0.6635, Avg Acc@5: 0.8719
2022-01-13 17:26:15,306 Val Step[0350/1563], Avg Loss: 1.5701, Avg Acc@1: 0.6619, Avg Acc@5: 0.8717
2022-01-13 17:26:17,401 Val Step[0400/1563], Avg Loss: 1.5688, Avg Acc@1: 0.6620, Avg Acc@5: 0.8717
2022-01-13 17:26:19,550 Val Step[0450/1563], Avg Loss: 1.5810, Avg Acc@1: 0.6596, Avg Acc@5: 0.8708
2022-01-13 17:26:21,461 Val Step[0500/1563], Avg Loss: 1.5822, Avg Acc@1: 0.6602, Avg Acc@5: 0.8710
2022-01-13 17:26:23,369 Val Step[0550/1563], Avg Loss: 1.5875, Avg Acc@1: 0.6581, Avg Acc@5: 0.8702
2022-01-13 17:26:25,148 Val Step[0600/1563], Avg Loss: 1.5871, Avg Acc@1: 0.6578, Avg Acc@5: 0.8701
2022-01-13 17:26:27,016 Val Step[0650/1563], Avg Loss: 1.5886, Avg Acc@1: 0.6570, Avg Acc@5: 0.8708
2022-01-13 17:26:28,827 Val Step[0700/1563], Avg Loss: 1.5866, Avg Acc@1: 0.6572, Avg Acc@5: 0.8714
2022-01-13 17:26:30,607 Val Step[0750/1563], Avg Loss: 1.5922, Avg Acc@1: 0.6558, Avg Acc@5: 0.8708
2022-01-13 17:26:32,387 Val Step[0800/1563], Avg Loss: 1.5926, Avg Acc@1: 0.6556, Avg Acc@5: 0.8711
2022-01-13 17:26:34,165 Val Step[0850/1563], Avg Loss: 1.5944, Avg Acc@1: 0.6553, Avg Acc@5: 0.8704
2022-01-13 17:26:35,940 Val Step[0900/1563], Avg Loss: 1.5897, Avg Acc@1: 0.6559, Avg Acc@5: 0.8713
2022-01-13 17:26:37,788 Val Step[0950/1563], Avg Loss: 1.5895, Avg Acc@1: 0.6557, Avg Acc@5: 0.8716
2022-01-13 17:26:39,634 Val Step[1000/1563], Avg Loss: 1.5888, Avg Acc@1: 0.6558, Avg Acc@5: 0.8712
2022-01-13 17:26:41,570 Val Step[1050/1563], Avg Loss: 1.5920, Avg Acc@1: 0.6558, Avg Acc@5: 0.8702
2022-01-13 17:26:43,389 Val Step[1100/1563], Avg Loss: 1.5915, Avg Acc@1: 0.6559, Avg Acc@5: 0.8701
2022-01-13 17:26:45,162 Val Step[1150/1563], Avg Loss: 1.5900, Avg Acc@1: 0.6563, Avg Acc@5: 0.8702
2022-01-13 17:26:46,955 Val Step[1200/1563], Avg Loss: 1.5888, Avg Acc@1: 0.6573, Avg Acc@5: 0.8702
2022-01-13 17:26:48,775 Val Step[1250/1563], Avg Loss: 1.5874, Avg Acc@1: 0.6574, Avg Acc@5: 0.8706
2022-01-13 17:26:50,685 Val Step[1300/1563], Avg Loss: 1.5903, Avg Acc@1: 0.6573, Avg Acc@5: 0.8701
2022-01-13 17:26:52,545 Val Step[1350/1563], Avg Loss: 1.5901, Avg Acc@1: 0.6571, Avg Acc@5: 0.8700
2022-01-13 17:26:54,372 Val Step[1400/1563], Avg Loss: 1.5885, Avg Acc@1: 0.6568, Avg Acc@5: 0.8702
2022-01-13 17:26:56,138 Val Step[1450/1563], Avg Loss: 1.5887, Avg Acc@1: 0.6570, Avg Acc@5: 0.8701
2022-01-13 17:26:58,077 Val Step[1500/1563], Avg Loss: 1.5883, Avg Acc@1: 0.6575, Avg Acc@5: 0.8703
2022-01-13 17:26:59,938 Val Step[1550/1563], Avg Loss: 1.5891, Avg Acc@1: 0.6575, Avg Acc@5: 0.8702
2022-01-13 17:27:01,774 ----- Epoch[044/300], Validation Loss: 1.5892, Validation Acc@1: 0.6574, Validation Acc@5: 0.8702, time: 129.75
2022-01-13 17:27:03,110 the pre best model acc:0.6553, at epoch 42
2022-01-13 17:27:03,111 current best model acc:0.6574, at epoch 44
2022-01-13 17:27:03,111 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 17:27:03,111 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 17:27:03,111 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 17:27:03,111 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 17:27:03,112 Now training epoch 45. LR=0.000981
2022-01-13 17:28:52,924 Epoch[045/300], Step[0000/1252], Avg Loss: 3.8240, Avg Acc: 0.4434
2022-01-13 17:30:18,579 Epoch[045/300], Step[0050/1252], Avg Loss: 3.8119, Avg Acc: 0.3425
2022-01-13 17:31:43,063 Epoch[045/300], Step[0100/1252], Avg Loss: 3.7823, Avg Acc: 0.3576
2022-01-13 17:33:08,888 Epoch[045/300], Step[0150/1252], Avg Loss: 3.7840, Avg Acc: 0.3591
2022-01-13 17:34:34,144 Epoch[045/300], Step[0200/1252], Avg Loss: 3.7730, Avg Acc: 0.3608
2022-01-13 17:35:59,325 Epoch[045/300], Step[0250/1252], Avg Loss: 3.7724, Avg Acc: 0.3589
2022-01-13 17:37:25,573 Epoch[045/300], Step[0300/1252], Avg Loss: 3.7897, Avg Acc: 0.3560
2022-01-13 17:38:51,272 Epoch[045/300], Step[0350/1252], Avg Loss: 3.7860, Avg Acc: 0.3556
2022-01-13 17:40:17,012 Epoch[045/300], Step[0400/1252], Avg Loss: 3.7960, Avg Acc: 0.3518
2022-01-13 17:41:43,691 Epoch[045/300], Step[0450/1252], Avg Loss: 3.7954, Avg Acc: 0.3498
2022-01-13 17:43:08,764 Epoch[045/300], Step[0500/1252], Avg Loss: 3.7991, Avg Acc: 0.3508
2022-01-13 17:44:33,323 Epoch[045/300], Step[0550/1252], Avg Loss: 3.7993, Avg Acc: 0.3475
2022-01-13 17:45:58,867 Epoch[045/300], Step[0600/1252], Avg Loss: 3.8004, Avg Acc: 0.3465
2022-01-13 17:47:24,073 Epoch[045/300], Step[0650/1252], Avg Loss: 3.8003, Avg Acc: 0.3465
2022-01-13 17:48:48,273 Epoch[045/300], Step[0700/1252], Avg Loss: 3.7995, Avg Acc: 0.3464
2022-01-13 17:50:13,474 Epoch[045/300], Step[0750/1252], Avg Loss: 3.7987, Avg Acc: 0.3474
2022-01-13 17:51:38,987 Epoch[045/300], Step[0800/1252], Avg Loss: 3.7986, Avg Acc: 0.3478
2022-01-13 17:53:02,808 Epoch[045/300], Step[0850/1252], Avg Loss: 3.8004, Avg Acc: 0.3476
2022-01-13 17:54:28,568 Epoch[045/300], Step[0900/1252], Avg Loss: 3.8021, Avg Acc: 0.3477
2022-01-13 17:55:53,889 Epoch[045/300], Step[0950/1252], Avg Loss: 3.8030, Avg Acc: 0.3478
2022-01-13 17:57:18,995 Epoch[045/300], Step[1000/1252], Avg Loss: 3.8071, Avg Acc: 0.3485
2022-01-13 17:58:44,362 Epoch[045/300], Step[1050/1252], Avg Loss: 3.8077, Avg Acc: 0.3475
2022-01-13 18:00:09,428 Epoch[045/300], Step[1100/1252], Avg Loss: 3.8044, Avg Acc: 0.3473
2022-01-13 18:01:34,778 Epoch[045/300], Step[1150/1252], Avg Loss: 3.8041, Avg Acc: 0.3477
2022-01-13 18:03:00,401 Epoch[045/300], Step[1200/1252], Avg Loss: 3.8021, Avg Acc: 0.3489
2022-01-13 18:04:25,579 Epoch[045/300], Step[1250/1252], Avg Loss: 3.8047, Avg Acc: 0.3489
2022-01-13 18:04:32,801 ----- Epoch[045/300], Train Loss: 3.8047, Train Acc: 0.3489, time: 2249.69, Best Val(epoch44) Acc@1: 0.6574
2022-01-13 18:04:32,802 Now training epoch 46. LR=0.000979
2022-01-13 18:06:16,782 Epoch[046/300], Step[0000/1252], Avg Loss: 3.9334, Avg Acc: 0.2412
2022-01-13 18:07:42,813 Epoch[046/300], Step[0050/1252], Avg Loss: 3.7977, Avg Acc: 0.3521
2022-01-13 18:09:05,886 Epoch[046/300], Step[0100/1252], Avg Loss: 3.7961, Avg Acc: 0.3619
2022-01-13 18:10:31,188 Epoch[046/300], Step[0150/1252], Avg Loss: 3.7951, Avg Acc: 0.3507
2022-01-13 18:11:56,714 Epoch[046/300], Step[0200/1252], Avg Loss: 3.7876, Avg Acc: 0.3577
2022-01-13 18:13:20,999 Epoch[046/300], Step[0250/1252], Avg Loss: 3.7848, Avg Acc: 0.3543
2022-01-13 18:14:45,739 Epoch[046/300], Step[0300/1252], Avg Loss: 3.7831, Avg Acc: 0.3534
2022-01-13 18:16:10,929 Epoch[046/300], Step[0350/1252], Avg Loss: 3.7828, Avg Acc: 0.3531
2022-01-13 18:17:35,575 Epoch[046/300], Step[0400/1252], Avg Loss: 3.7944, Avg Acc: 0.3514
2022-01-13 18:18:59,438 Epoch[046/300], Step[0450/1252], Avg Loss: 3.7974, Avg Acc: 0.3513
2022-01-13 18:20:25,255 Epoch[046/300], Step[0500/1252], Avg Loss: 3.7989, Avg Acc: 0.3502
2022-01-13 18:21:49,911 Epoch[046/300], Step[0550/1252], Avg Loss: 3.7965, Avg Acc: 0.3520
2022-01-13 18:23:16,539 Epoch[046/300], Step[0600/1252], Avg Loss: 3.7989, Avg Acc: 0.3518
2022-01-13 18:24:42,865 Epoch[046/300], Step[0650/1252], Avg Loss: 3.8023, Avg Acc: 0.3514
2022-01-13 18:26:09,850 Epoch[046/300], Step[0700/1252], Avg Loss: 3.8029, Avg Acc: 0.3493
2022-01-13 18:27:36,105 Epoch[046/300], Step[0750/1252], Avg Loss: 3.8034, Avg Acc: 0.3506
2022-01-13 18:29:02,295 Epoch[046/300], Step[0800/1252], Avg Loss: 3.8071, Avg Acc: 0.3497
2022-01-13 18:30:28,602 Epoch[046/300], Step[0850/1252], Avg Loss: 3.8073, Avg Acc: 0.3499
2022-01-13 18:31:54,610 Epoch[046/300], Step[0900/1252], Avg Loss: 3.8062, Avg Acc: 0.3511
2022-01-13 18:33:21,011 Epoch[046/300], Step[0950/1252], Avg Loss: 3.8011, Avg Acc: 0.3511
2022-01-13 18:34:47,591 Epoch[046/300], Step[1000/1252], Avg Loss: 3.8019, Avg Acc: 0.3510
2022-01-13 18:36:14,868 Epoch[046/300], Step[1050/1252], Avg Loss: 3.8035, Avg Acc: 0.3498
2022-01-13 18:37:41,344 Epoch[046/300], Step[1100/1252], Avg Loss: 3.8020, Avg Acc: 0.3492
2022-01-13 18:39:08,671 Epoch[046/300], Step[1150/1252], Avg Loss: 3.7983, Avg Acc: 0.3488
2022-01-13 18:40:34,435 Epoch[046/300], Step[1200/1252], Avg Loss: 3.7976, Avg Acc: 0.3493
2022-01-13 18:42:02,150 Epoch[046/300], Step[1250/1252], Avg Loss: 3.7996, Avg Acc: 0.3485
2022-01-13 18:42:09,338 ----- Epoch[046/300], Train Loss: 3.7996, Train Acc: 0.3485, time: 2256.53, Best Val(epoch44) Acc@1: 0.6574
2022-01-13 18:42:09,338 ----- Validation after Epoch: 46
2022-01-13 18:43:24,990 Val Step[0000/1563], Avg Loss: 1.4480, Avg Acc@1: 0.6875, Avg Acc@5: 0.9375
2022-01-13 18:43:27,247 Val Step[0050/1563], Avg Loss: 1.5863, Avg Acc@1: 0.6667, Avg Acc@5: 0.8750
2022-01-13 18:43:29,299 Val Step[0100/1563], Avg Loss: 1.5879, Avg Acc@1: 0.6593, Avg Acc@5: 0.8738
2022-01-13 18:43:31,189 Val Step[0150/1563], Avg Loss: 1.5892, Avg Acc@1: 0.6589, Avg Acc@5: 0.8731
2022-01-13 18:43:33,008 Val Step[0200/1563], Avg Loss: 1.5923, Avg Acc@1: 0.6573, Avg Acc@5: 0.8686
2022-01-13 18:43:34,807 Val Step[0250/1563], Avg Loss: 1.5717, Avg Acc@1: 0.6636, Avg Acc@5: 0.8710
2022-01-13 18:43:36,719 Val Step[0300/1563], Avg Loss: 1.5718, Avg Acc@1: 0.6653, Avg Acc@5: 0.8711
2022-01-13 18:43:38,622 Val Step[0350/1563], Avg Loss: 1.5751, Avg Acc@1: 0.6646, Avg Acc@5: 0.8712
2022-01-13 18:43:40,483 Val Step[0400/1563], Avg Loss: 1.5710, Avg Acc@1: 0.6656, Avg Acc@5: 0.8723
2022-01-13 18:43:42,363 Val Step[0450/1563], Avg Loss: 1.5819, Avg Acc@1: 0.6611, Avg Acc@5: 0.8702
2022-01-13 18:43:44,188 Val Step[0500/1563], Avg Loss: 1.5855, Avg Acc@1: 0.6592, Avg Acc@5: 0.8703
2022-01-13 18:43:46,072 Val Step[0550/1563], Avg Loss: 1.5889, Avg Acc@1: 0.6583, Avg Acc@5: 0.8698
2022-01-13 18:43:47,934 Val Step[0600/1563], Avg Loss: 1.5875, Avg Acc@1: 0.6585, Avg Acc@5: 0.8707
2022-01-13 18:43:49,814 Val Step[0650/1563], Avg Loss: 1.5901, Avg Acc@1: 0.6592, Avg Acc@5: 0.8708
2022-01-13 18:43:51,765 Val Step[0700/1563], Avg Loss: 1.5876, Avg Acc@1: 0.6594, Avg Acc@5: 0.8717
2022-01-13 18:43:53,578 Val Step[0750/1563], Avg Loss: 1.5931, Avg Acc@1: 0.6583, Avg Acc@5: 0.8708
2022-01-13 18:43:55,617 Val Step[0800/1563], Avg Loss: 1.5912, Avg Acc@1: 0.6589, Avg Acc@5: 0.8716
2022-01-13 18:43:57,668 Val Step[0850/1563], Avg Loss: 1.5919, Avg Acc@1: 0.6584, Avg Acc@5: 0.8716
2022-01-13 18:43:59,711 Val Step[0900/1563], Avg Loss: 1.5875, Avg Acc@1: 0.6598, Avg Acc@5: 0.8724
2022-01-13 18:44:01,739 Val Step[0950/1563], Avg Loss: 1.5879, Avg Acc@1: 0.6594, Avg Acc@5: 0.8728
2022-01-13 18:44:03,848 Val Step[1000/1563], Avg Loss: 1.5880, Avg Acc@1: 0.6595, Avg Acc@5: 0.8729
2022-01-13 18:44:06,021 Val Step[1050/1563], Avg Loss: 1.5908, Avg Acc@1: 0.6586, Avg Acc@5: 0.8726
2022-01-13 18:44:08,175 Val Step[1100/1563], Avg Loss: 1.5898, Avg Acc@1: 0.6585, Avg Acc@5: 0.8731
2022-01-13 18:44:10,235 Val Step[1150/1563], Avg Loss: 1.5874, Avg Acc@1: 0.6587, Avg Acc@5: 0.8732
2022-01-13 18:44:12,289 Val Step[1200/1563], Avg Loss: 1.5851, Avg Acc@1: 0.6593, Avg Acc@5: 0.8735
2022-01-13 18:44:14,349 Val Step[1250/1563], Avg Loss: 1.5841, Avg Acc@1: 0.6596, Avg Acc@5: 0.8735
2022-01-13 18:44:16,487 Val Step[1300/1563], Avg Loss: 1.5868, Avg Acc@1: 0.6591, Avg Acc@5: 0.8729
2022-01-13 18:44:18,585 Val Step[1350/1563], Avg Loss: 1.5861, Avg Acc@1: 0.6590, Avg Acc@5: 0.8730
2022-01-13 18:44:20,665 Val Step[1400/1563], Avg Loss: 1.5854, Avg Acc@1: 0.6584, Avg Acc@5: 0.8730
2022-01-13 18:44:22,756 Val Step[1450/1563], Avg Loss: 1.5857, Avg Acc@1: 0.6580, Avg Acc@5: 0.8730
2022-01-13 18:44:24,822 Val Step[1500/1563], Avg Loss: 1.5855, Avg Acc@1: 0.6583, Avg Acc@5: 0.8734
2022-01-13 18:44:26,829 Val Step[1550/1563], Avg Loss: 1.5864, Avg Acc@1: 0.6579, Avg Acc@5: 0.8730
2022-01-13 18:44:28,806 ----- Epoch[046/300], Validation Loss: 1.5864, Validation Acc@1: 0.6579, Validation Acc@5: 0.8731, time: 139.47
2022-01-13 18:44:30,249 the pre best model acc:0.6574, at epoch 44
2022-01-13 18:44:30,449 current best model acc:0.6579, at epoch 46
2022-01-13 18:44:30,450 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 18:44:30,450 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 18:44:30,450 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 18:44:30,450 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 18:44:30,450 Now training epoch 47. LR=0.000977
2022-01-13 18:46:14,825 Epoch[047/300], Step[0000/1252], Avg Loss: 3.1341, Avg Acc: 0.4678
2022-01-13 18:47:39,122 Epoch[047/300], Step[0050/1252], Avg Loss: 3.8075, Avg Acc: 0.3276
2022-01-13 18:49:04,495 Epoch[047/300], Step[0100/1252], Avg Loss: 3.7966, Avg Acc: 0.3407
2022-01-13 18:50:27,732 Epoch[047/300], Step[0150/1252], Avg Loss: 3.7973, Avg Acc: 0.3490
2022-01-13 18:51:52,894 Epoch[047/300], Step[0200/1252], Avg Loss: 3.7870, Avg Acc: 0.3469
2022-01-13 18:53:17,868 Epoch[047/300], Step[0250/1252], Avg Loss: 3.7682, Avg Acc: 0.3515
2022-01-13 18:54:42,588 Epoch[047/300], Step[0300/1252], Avg Loss: 3.7626, Avg Acc: 0.3526
2022-01-13 18:56:08,534 Epoch[047/300], Step[0350/1252], Avg Loss: 3.7706, Avg Acc: 0.3481
2022-01-13 18:57:34,806 Epoch[047/300], Step[0400/1252], Avg Loss: 3.7758, Avg Acc: 0.3484
2022-01-13 18:58:59,003 Epoch[047/300], Step[0450/1252], Avg Loss: 3.7865, Avg Acc: 0.3471
2022-01-13 19:00:24,330 Epoch[047/300], Step[0500/1252], Avg Loss: 3.7905, Avg Acc: 0.3489
2022-01-13 19:01:48,746 Epoch[047/300], Step[0550/1252], Avg Loss: 3.7892, Avg Acc: 0.3477
2022-01-13 19:03:13,131 Epoch[047/300], Step[0600/1252], Avg Loss: 3.7871, Avg Acc: 0.3485
2022-01-13 19:04:38,735 Epoch[047/300], Step[0650/1252], Avg Loss: 3.7946, Avg Acc: 0.3477
2022-01-13 19:06:04,941 Epoch[047/300], Step[0700/1252], Avg Loss: 3.7932, Avg Acc: 0.3481
2022-01-13 19:07:30,587 Epoch[047/300], Step[0750/1252], Avg Loss: 3.7880, Avg Acc: 0.3489
2022-01-13 19:08:56,856 Epoch[047/300], Step[0800/1252], Avg Loss: 3.7850, Avg Acc: 0.3503
2022-01-13 19:10:24,587 Epoch[047/300], Step[0850/1252], Avg Loss: 3.7897, Avg Acc: 0.3501
2022-01-13 19:11:52,228 Epoch[047/300], Step[0900/1252], Avg Loss: 3.7934, Avg Acc: 0.3486
2022-01-13 19:13:18,695 Epoch[047/300], Step[0950/1252], Avg Loss: 3.7938, Avg Acc: 0.3491
2022-01-13 19:14:45,116 Epoch[047/300], Step[1000/1252], Avg Loss: 3.7935, Avg Acc: 0.3507
2022-01-13 19:16:12,359 Epoch[047/300], Step[1050/1252], Avg Loss: 3.7923, Avg Acc: 0.3505
2022-01-13 19:17:39,776 Epoch[047/300], Step[1100/1252], Avg Loss: 3.7966, Avg Acc: 0.3507
2022-01-13 19:19:06,835 Epoch[047/300], Step[1150/1252], Avg Loss: 3.7981, Avg Acc: 0.3492
2022-01-13 19:20:34,242 Epoch[047/300], Step[1200/1252], Avg Loss: 3.8018, Avg Acc: 0.3490
2022-01-13 19:22:02,956 Epoch[047/300], Step[1250/1252], Avg Loss: 3.8009, Avg Acc: 0.3490
2022-01-13 19:22:10,246 ----- Epoch[047/300], Train Loss: 3.8009, Train Acc: 0.3490, time: 2259.79, Best Val(epoch46) Acc@1: 0.6579
2022-01-13 19:22:10,246 Now training epoch 48. LR=0.000976
2022-01-13 19:24:01,356 Epoch[048/300], Step[0000/1252], Avg Loss: 3.2287, Avg Acc: 0.4385
2022-01-13 19:25:27,189 Epoch[048/300], Step[0050/1252], Avg Loss: 3.8050, Avg Acc: 0.3599
2022-01-13 19:26:53,437 Epoch[048/300], Step[0100/1252], Avg Loss: 3.7673, Avg Acc: 0.3618
2022-01-13 19:28:19,343 Epoch[048/300], Step[0150/1252], Avg Loss: 3.7892, Avg Acc: 0.3587
2022-01-13 19:29:44,507 Epoch[048/300], Step[0200/1252], Avg Loss: 3.7835, Avg Acc: 0.3617
2022-01-13 19:31:10,283 Epoch[048/300], Step[0250/1252], Avg Loss: 3.7856, Avg Acc: 0.3583
2022-01-13 19:32:36,158 Epoch[048/300], Step[0300/1252], Avg Loss: 3.7789, Avg Acc: 0.3593
2022-01-13 19:34:01,982 Epoch[048/300], Step[0350/1252], Avg Loss: 3.7793, Avg Acc: 0.3566
2022-01-13 19:35:28,144 Epoch[048/300], Step[0400/1252], Avg Loss: 3.7849, Avg Acc: 0.3562
2022-01-13 19:36:54,031 Epoch[048/300], Step[0450/1252], Avg Loss: 3.7903, Avg Acc: 0.3549
2022-01-13 19:38:19,128 Epoch[048/300], Step[0500/1252], Avg Loss: 3.7910, Avg Acc: 0.3535
2022-01-13 19:39:43,408 Epoch[048/300], Step[0550/1252], Avg Loss: 3.7891, Avg Acc: 0.3547
2022-01-13 19:41:08,832 Epoch[048/300], Step[0600/1252], Avg Loss: 3.7954, Avg Acc: 0.3528
2022-01-13 19:42:32,945 Epoch[048/300], Step[0650/1252], Avg Loss: 3.7910, Avg Acc: 0.3529
2022-01-13 19:43:58,238 Epoch[048/300], Step[0700/1252], Avg Loss: 3.7901, Avg Acc: 0.3546
2022-01-13 19:45:22,735 Epoch[048/300], Step[0750/1252], Avg Loss: 3.7912, Avg Acc: 0.3531
2022-01-13 19:46:49,411 Epoch[048/300], Step[0800/1252], Avg Loss: 3.7903, Avg Acc: 0.3525
2022-01-13 19:48:15,797 Epoch[048/300], Step[0850/1252], Avg Loss: 3.7876, Avg Acc: 0.3512
2022-01-13 19:49:40,758 Epoch[048/300], Step[0900/1252], Avg Loss: 3.7858, Avg Acc: 0.3511
2022-01-13 19:51:06,884 Epoch[048/300], Step[0950/1252], Avg Loss: 3.7817, Avg Acc: 0.3502
2022-01-13 19:52:32,695 Epoch[048/300], Step[1000/1252], Avg Loss: 3.7829, Avg Acc: 0.3502
2022-01-13 19:53:59,230 Epoch[048/300], Step[1050/1252], Avg Loss: 3.7850, Avg Acc: 0.3489
2022-01-13 19:55:25,638 Epoch[048/300], Step[1100/1252], Avg Loss: 3.7837, Avg Acc: 0.3495
2022-01-13 19:56:50,949 Epoch[048/300], Step[1150/1252], Avg Loss: 3.7852, Avg Acc: 0.3493
2022-01-13 19:58:16,297 Epoch[048/300], Step[1200/1252], Avg Loss: 3.7877, Avg Acc: 0.3499
2022-01-13 19:59:42,928 Epoch[048/300], Step[1250/1252], Avg Loss: 3.7882, Avg Acc: 0.3498
2022-01-13 19:59:50,041 ----- Epoch[048/300], Train Loss: 3.7882, Train Acc: 0.3498, time: 2259.79, Best Val(epoch46) Acc@1: 0.6579
2022-01-13 19:59:50,041 ----- Validation after Epoch: 48
2022-01-13 20:01:03,499 Val Step[0000/1563], Avg Loss: 1.2193, Avg Acc@1: 0.6562, Avg Acc@5: 0.9375
2022-01-13 20:01:05,445 Val Step[0050/1563], Avg Loss: 1.5075, Avg Acc@1: 0.6618, Avg Acc@5: 0.8805
2022-01-13 20:01:07,385 Val Step[0100/1563], Avg Loss: 1.5246, Avg Acc@1: 0.6578, Avg Acc@5: 0.8796
2022-01-13 20:01:09,262 Val Step[0150/1563], Avg Loss: 1.5250, Avg Acc@1: 0.6573, Avg Acc@5: 0.8771
2022-01-13 20:01:11,036 Val Step[0200/1563], Avg Loss: 1.5271, Avg Acc@1: 0.6595, Avg Acc@5: 0.8758
2022-01-13 20:01:12,803 Val Step[0250/1563], Avg Loss: 1.5080, Avg Acc@1: 0.6656, Avg Acc@5: 0.8762
2022-01-13 20:01:14,663 Val Step[0300/1563], Avg Loss: 1.5107, Avg Acc@1: 0.6661, Avg Acc@5: 0.8765
2022-01-13 20:01:16,583 Val Step[0350/1563], Avg Loss: 1.5184, Avg Acc@1: 0.6655, Avg Acc@5: 0.8759
2022-01-13 20:01:18,446 Val Step[0400/1563], Avg Loss: 1.5147, Avg Acc@1: 0.6651, Avg Acc@5: 0.8777
2022-01-13 20:01:20,254 Val Step[0450/1563], Avg Loss: 1.5257, Avg Acc@1: 0.6612, Avg Acc@5: 0.8754
2022-01-13 20:01:22,020 Val Step[0500/1563], Avg Loss: 1.5262, Avg Acc@1: 0.6616, Avg Acc@5: 0.8749
2022-01-13 20:01:23,943 Val Step[0550/1563], Avg Loss: 1.5279, Avg Acc@1: 0.6606, Avg Acc@5: 0.8747
2022-01-13 20:01:25,931 Val Step[0600/1563], Avg Loss: 1.5263, Avg Acc@1: 0.6608, Avg Acc@5: 0.8752
2022-01-13 20:01:27,855 Val Step[0650/1563], Avg Loss: 1.5248, Avg Acc@1: 0.6626, Avg Acc@5: 0.8758
2022-01-13 20:01:29,698 Val Step[0700/1563], Avg Loss: 1.5221, Avg Acc@1: 0.6637, Avg Acc@5: 0.8765
2022-01-13 20:01:31,795 Val Step[0750/1563], Avg Loss: 1.5287, Avg Acc@1: 0.6625, Avg Acc@5: 0.8756
2022-01-13 20:01:33,917 Val Step[0800/1563], Avg Loss: 1.5272, Avg Acc@1: 0.6626, Avg Acc@5: 0.8761
2022-01-13 20:01:35,961 Val Step[0850/1563], Avg Loss: 1.5285, Avg Acc@1: 0.6622, Avg Acc@5: 0.8758
2022-01-13 20:01:38,012 Val Step[0900/1563], Avg Loss: 1.5265, Avg Acc@1: 0.6621, Avg Acc@5: 0.8765
2022-01-13 20:01:40,116 Val Step[0950/1563], Avg Loss: 1.5260, Avg Acc@1: 0.6623, Avg Acc@5: 0.8768
2022-01-13 20:01:42,167 Val Step[1000/1563], Avg Loss: 1.5268, Avg Acc@1: 0.6616, Avg Acc@5: 0.8766
2022-01-13 20:01:44,288 Val Step[1050/1563], Avg Loss: 1.5297, Avg Acc@1: 0.6614, Avg Acc@5: 0.8758
2022-01-13 20:01:46,410 Val Step[1100/1563], Avg Loss: 1.5298, Avg Acc@1: 0.6615, Avg Acc@5: 0.8758
2022-01-13 20:01:48,530 Val Step[1150/1563], Avg Loss: 1.5292, Avg Acc@1: 0.6615, Avg Acc@5: 0.8757
2022-01-13 20:01:50,597 Val Step[1200/1563], Avg Loss: 1.5284, Avg Acc@1: 0.6621, Avg Acc@5: 0.8757
2022-01-13 20:01:52,644 Val Step[1250/1563], Avg Loss: 1.5271, Avg Acc@1: 0.6626, Avg Acc@5: 0.8760
2022-01-13 20:01:54,681 Val Step[1300/1563], Avg Loss: 1.5307, Avg Acc@1: 0.6624, Avg Acc@5: 0.8756
2022-01-13 20:01:56,542 Val Step[1350/1563], Avg Loss: 1.5307, Avg Acc@1: 0.6622, Avg Acc@5: 0.8753
2022-01-13 20:01:58,318 Val Step[1400/1563], Avg Loss: 1.5307, Avg Acc@1: 0.6617, Avg Acc@5: 0.8752
2022-01-13 20:02:00,093 Val Step[1450/1563], Avg Loss: 1.5307, Avg Acc@1: 0.6615, Avg Acc@5: 0.8753
2022-01-13 20:02:01,870 Val Step[1500/1563], Avg Loss: 1.5301, Avg Acc@1: 0.6616, Avg Acc@5: 0.8757
2022-01-13 20:02:03,602 Val Step[1550/1563], Avg Loss: 1.5313, Avg Acc@1: 0.6615, Avg Acc@5: 0.8754
2022-01-13 20:02:05,509 ----- Epoch[048/300], Validation Loss: 1.5317, Validation Acc@1: 0.6615, Validation Acc@5: 0.8753, time: 135.46
2022-01-13 20:02:06,826 the pre best model acc:0.6579, at epoch 46
2022-01-13 20:02:07,102 current best model acc:0.6615, at epoch 48
2022-01-13 20:02:07,102 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 20:02:07,102 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 20:02:07,102 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 20:02:07,102 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 20:02:07,102 Now training epoch 49. LR=0.000974
2022-01-13 20:03:51,519 Epoch[049/300], Step[0000/1252], Avg Loss: 3.6690, Avg Acc: 0.3955
2022-01-13 20:05:16,320 Epoch[049/300], Step[0050/1252], Avg Loss: 3.8048, Avg Acc: 0.3631
2022-01-13 20:06:40,446 Epoch[049/300], Step[0100/1252], Avg Loss: 3.7831, Avg Acc: 0.3619
2022-01-13 20:08:05,130 Epoch[049/300], Step[0150/1252], Avg Loss: 3.7808, Avg Acc: 0.3622
2022-01-13 20:09:29,004 Epoch[049/300], Step[0200/1252], Avg Loss: 3.7939, Avg Acc: 0.3583
2022-01-13 20:10:52,964 Epoch[049/300], Step[0250/1252], Avg Loss: 3.7774, Avg Acc: 0.3615
2022-01-13 20:12:17,517 Epoch[049/300], Step[0300/1252], Avg Loss: 3.7834, Avg Acc: 0.3611
2022-01-13 20:13:42,380 Epoch[049/300], Step[0350/1252], Avg Loss: 3.7799, Avg Acc: 0.3554
2022-01-13 20:15:07,392 Epoch[049/300], Step[0400/1252], Avg Loss: 3.7745, Avg Acc: 0.3564
2022-01-13 20:16:32,837 Epoch[049/300], Step[0450/1252], Avg Loss: 3.7765, Avg Acc: 0.3569
2022-01-13 20:17:57,110 Epoch[049/300], Step[0500/1252], Avg Loss: 3.7792, Avg Acc: 0.3553
2022-01-13 20:19:21,990 Epoch[049/300], Step[0550/1252], Avg Loss: 3.7841, Avg Acc: 0.3535
2022-01-13 20:20:46,156 Epoch[049/300], Step[0600/1252], Avg Loss: 3.7873, Avg Acc: 0.3532
2022-01-13 20:22:11,032 Epoch[049/300], Step[0650/1252], Avg Loss: 3.7856, Avg Acc: 0.3519
2022-01-13 20:23:36,200 Epoch[049/300], Step[0700/1252], Avg Loss: 3.7870, Avg Acc: 0.3520
2022-01-13 20:25:02,272 Epoch[049/300], Step[0750/1252], Avg Loss: 3.7906, Avg Acc: 0.3513
2022-01-13 20:26:28,229 Epoch[049/300], Step[0800/1252], Avg Loss: 3.7891, Avg Acc: 0.3522
2022-01-13 20:27:54,599 Epoch[049/300], Step[0850/1252], Avg Loss: 3.7877, Avg Acc: 0.3515
2022-01-13 20:29:20,844 Epoch[049/300], Step[0900/1252], Avg Loss: 3.7911, Avg Acc: 0.3517
2022-01-13 20:30:46,618 Epoch[049/300], Step[0950/1252], Avg Loss: 3.7945, Avg Acc: 0.3516
2022-01-13 20:32:12,886 Epoch[049/300], Step[1000/1252], Avg Loss: 3.7937, Avg Acc: 0.3503
2022-01-13 20:33:38,286 Epoch[049/300], Step[1050/1252], Avg Loss: 3.7943, Avg Acc: 0.3511
2022-01-13 20:35:05,608 Epoch[049/300], Step[1100/1252], Avg Loss: 3.7918, Avg Acc: 0.3511
2022-01-13 20:36:32,384 Epoch[049/300], Step[1150/1252], Avg Loss: 3.7928, Avg Acc: 0.3504
2022-01-13 20:37:58,978 Epoch[049/300], Step[1200/1252], Avg Loss: 3.7936, Avg Acc: 0.3502
2022-01-13 20:39:25,922 Epoch[049/300], Step[1250/1252], Avg Loss: 3.7956, Avg Acc: 0.3488
2022-01-13 20:39:33,016 ----- Epoch[049/300], Train Loss: 3.7956, Train Acc: 0.3488, time: 2245.91, Best Val(epoch48) Acc@1: 0.6615
2022-01-13 20:39:33,016 Now training epoch 50. LR=0.000972
2022-01-13 20:41:20,247 Epoch[050/300], Step[0000/1252], Avg Loss: 3.2678, Avg Acc: 0.5693
2022-01-13 20:42:45,692 Epoch[050/300], Step[0050/1252], Avg Loss: 3.6580, Avg Acc: 0.3718
2022-01-13 20:44:12,164 Epoch[050/300], Step[0100/1252], Avg Loss: 3.6681, Avg Acc: 0.3777
2022-01-13 20:45:39,374 Epoch[050/300], Step[0150/1252], Avg Loss: 3.6745, Avg Acc: 0.3668
2022-01-13 20:47:05,167 Epoch[050/300], Step[0200/1252], Avg Loss: 3.6853, Avg Acc: 0.3684
2022-01-13 20:48:31,888 Epoch[050/300], Step[0250/1252], Avg Loss: 3.7079, Avg Acc: 0.3633
2022-01-13 20:49:58,462 Epoch[050/300], Step[0300/1252], Avg Loss: 3.7099, Avg Acc: 0.3614
2022-01-13 20:51:24,709 Epoch[050/300], Step[0350/1252], Avg Loss: 3.7274, Avg Acc: 0.3631
2022-01-13 20:52:50,563 Epoch[050/300], Step[0400/1252], Avg Loss: 3.7335, Avg Acc: 0.3645
2022-01-13 20:54:17,906 Epoch[050/300], Step[0450/1252], Avg Loss: 3.7366, Avg Acc: 0.3617
2022-01-13 20:55:44,359 Epoch[050/300], Step[0500/1252], Avg Loss: 3.7404, Avg Acc: 0.3588
2022-01-13 20:57:11,435 Epoch[050/300], Step[0550/1252], Avg Loss: 3.7442, Avg Acc: 0.3593
2022-01-13 20:58:38,961 Epoch[050/300], Step[0600/1252], Avg Loss: 3.7503, Avg Acc: 0.3580
2022-01-13 21:00:04,186 Epoch[050/300], Step[0650/1252], Avg Loss: 3.7499, Avg Acc: 0.3587
2022-01-13 21:01:31,388 Epoch[050/300], Step[0700/1252], Avg Loss: 3.7531, Avg Acc: 0.3569
2022-01-13 21:02:58,534 Epoch[050/300], Step[0750/1252], Avg Loss: 3.7502, Avg Acc: 0.3569
2022-01-13 21:04:25,548 Epoch[050/300], Step[0800/1252], Avg Loss: 3.7542, Avg Acc: 0.3568
2022-01-13 21:05:52,730 Epoch[050/300], Step[0850/1252], Avg Loss: 3.7578, Avg Acc: 0.3553
2022-01-13 21:07:20,231 Epoch[050/300], Step[0900/1252], Avg Loss: 3.7571, Avg Acc: 0.3545
2022-01-13 21:08:46,974 Epoch[050/300], Step[0950/1252], Avg Loss: 3.7576, Avg Acc: 0.3554
2022-01-13 21:10:14,111 Epoch[050/300], Step[1000/1252], Avg Loss: 3.7591, Avg Acc: 0.3559
2022-01-13 21:11:41,823 Epoch[050/300], Step[1050/1252], Avg Loss: 3.7605, Avg Acc: 0.3548
2022-01-13 21:13:08,042 Epoch[050/300], Step[1100/1252], Avg Loss: 3.7599, Avg Acc: 0.3558
2022-01-13 21:14:33,903 Epoch[050/300], Step[1150/1252], Avg Loss: 3.7613, Avg Acc: 0.3551
2022-01-13 21:15:59,613 Epoch[050/300], Step[1200/1252], Avg Loss: 3.7635, Avg Acc: 0.3553
2022-01-13 21:17:25,103 Epoch[050/300], Step[1250/1252], Avg Loss: 3.7646, Avg Acc: 0.3539
2022-01-13 21:17:32,272 ----- Epoch[050/300], Train Loss: 3.7646, Train Acc: 0.3539, time: 2279.25, Best Val(epoch48) Acc@1: 0.6615
2022-01-13 21:17:32,272 ----- Validation after Epoch: 50
2022-01-13 21:18:51,785 Val Step[0000/1563], Avg Loss: 1.4222, Avg Acc@1: 0.5625, Avg Acc@5: 0.8438
2022-01-13 21:18:53,658 Val Step[0050/1563], Avg Loss: 1.5012, Avg Acc@1: 0.6575, Avg Acc@5: 0.8725
2022-01-13 21:18:55,525 Val Step[0100/1563], Avg Loss: 1.5019, Avg Acc@1: 0.6590, Avg Acc@5: 0.8759
2022-01-13 21:18:57,303 Val Step[0150/1563], Avg Loss: 1.5107, Avg Acc@1: 0.6560, Avg Acc@5: 0.8742
2022-01-13 21:18:59,220 Val Step[0200/1563], Avg Loss: 1.5155, Avg Acc@1: 0.6592, Avg Acc@5: 0.8744
2022-01-13 21:19:01,124 Val Step[0250/1563], Avg Loss: 1.4971, Avg Acc@1: 0.6646, Avg Acc@5: 0.8756
2022-01-13 21:19:03,021 Val Step[0300/1563], Avg Loss: 1.4959, Avg Acc@1: 0.6664, Avg Acc@5: 0.8760
2022-01-13 21:19:04,849 Val Step[0350/1563], Avg Loss: 1.4990, Avg Acc@1: 0.6647, Avg Acc@5: 0.8760
2022-01-13 21:19:06,723 Val Step[0400/1563], Avg Loss: 1.4940, Avg Acc@1: 0.6662, Avg Acc@5: 0.8755
2022-01-13 21:19:08,652 Val Step[0450/1563], Avg Loss: 1.4993, Avg Acc@1: 0.6643, Avg Acc@5: 0.8747
2022-01-13 21:19:10,529 Val Step[0500/1563], Avg Loss: 1.4997, Avg Acc@1: 0.6635, Avg Acc@5: 0.8747
2022-01-13 21:19:12,461 Val Step[0550/1563], Avg Loss: 1.4987, Avg Acc@1: 0.6628, Avg Acc@5: 0.8751
2022-01-13 21:19:14,341 Val Step[0600/1563], Avg Loss: 1.4978, Avg Acc@1: 0.6628, Avg Acc@5: 0.8756
2022-01-13 21:19:16,216 Val Step[0650/1563], Avg Loss: 1.4987, Avg Acc@1: 0.6628, Avg Acc@5: 0.8766
2022-01-13 21:19:18,132 Val Step[0700/1563], Avg Loss: 1.4943, Avg Acc@1: 0.6641, Avg Acc@5: 0.8779
2022-01-13 21:19:19,960 Val Step[0750/1563], Avg Loss: 1.5015, Avg Acc@1: 0.6632, Avg Acc@5: 0.8765
2022-01-13 21:19:21,804 Val Step[0800/1563], Avg Loss: 1.4984, Avg Acc@1: 0.6644, Avg Acc@5: 0.8771
2022-01-13 21:19:23,727 Val Step[0850/1563], Avg Loss: 1.5009, Avg Acc@1: 0.6635, Avg Acc@5: 0.8768
2022-01-13 21:19:25,674 Val Step[0900/1563], Avg Loss: 1.4974, Avg Acc@1: 0.6641, Avg Acc@5: 0.8773
2022-01-13 21:19:27,566 Val Step[0950/1563], Avg Loss: 1.4972, Avg Acc@1: 0.6643, Avg Acc@5: 0.8778
2022-01-13 21:19:29,450 Val Step[1000/1563], Avg Loss: 1.4974, Avg Acc@1: 0.6636, Avg Acc@5: 0.8777
2022-01-13 21:19:31,377 Val Step[1050/1563], Avg Loss: 1.4995, Avg Acc@1: 0.6633, Avg Acc@5: 0.8767
2022-01-13 21:19:33,316 Val Step[1100/1563], Avg Loss: 1.4994, Avg Acc@1: 0.6627, Avg Acc@5: 0.8770
2022-01-13 21:19:35,249 Val Step[1150/1563], Avg Loss: 1.4973, Avg Acc@1: 0.6629, Avg Acc@5: 0.8770
2022-01-13 21:19:37,053 Val Step[1200/1563], Avg Loss: 1.4967, Avg Acc@1: 0.6630, Avg Acc@5: 0.8773
2022-01-13 21:19:38,909 Val Step[1250/1563], Avg Loss: 1.4956, Avg Acc@1: 0.6629, Avg Acc@5: 0.8772
2022-01-13 21:19:40,745 Val Step[1300/1563], Avg Loss: 1.4984, Avg Acc@1: 0.6628, Avg Acc@5: 0.8769
2022-01-13 21:19:42,525 Val Step[1350/1563], Avg Loss: 1.4975, Avg Acc@1: 0.6630, Avg Acc@5: 0.8769
2022-01-13 21:19:44,363 Val Step[1400/1563], Avg Loss: 1.4958, Avg Acc@1: 0.6633, Avg Acc@5: 0.8770
2022-01-13 21:19:46,266 Val Step[1450/1563], Avg Loss: 1.4952, Avg Acc@1: 0.6633, Avg Acc@5: 0.8770
2022-01-13 21:19:48,172 Val Step[1500/1563], Avg Loss: 1.4945, Avg Acc@1: 0.6638, Avg Acc@5: 0.8773
2022-01-13 21:19:49,928 Val Step[1550/1563], Avg Loss: 1.4955, Avg Acc@1: 0.6635, Avg Acc@5: 0.8768
2022-01-13 21:19:51,780 ----- Epoch[050/300], Validation Loss: 1.4957, Validation Acc@1: 0.6634, Validation Acc@5: 0.8768, time: 139.50
2022-01-13 21:19:53,094 the pre best model acc:0.6615, at epoch 48
2022-01-13 21:19:53,362 current best model acc:0.6634, at epoch 50
2022-01-13 21:19:53,363 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 21:19:53,363 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 21:19:53,363 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 21:19:53,363 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 21:19:54,002 ----- Save model: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-50-Loss-3.7495067538594338.pdparams
2022-01-13 21:19:54,002 ----- Save optim: /root/paddlejob/workspace/output//train/CycleMLP-Epoch-50-Loss-3.7495067538594338.pdopt
2022-01-13 21:19:54,003 Now training epoch 51. LR=0.000970
2022-01-13 21:21:36,076 Epoch[051/300], Step[0000/1252], Avg Loss: 3.7028, Avg Acc: 0.2705
2022-01-13 21:23:01,444 Epoch[051/300], Step[0050/1252], Avg Loss: 3.8330, Avg Acc: 0.3406
2022-01-13 21:24:25,872 Epoch[051/300], Step[0100/1252], Avg Loss: 3.8182, Avg Acc: 0.3517
2022-01-13 21:25:49,481 Epoch[051/300], Step[0150/1252], Avg Loss: 3.7878, Avg Acc: 0.3512
2022-01-13 21:27:14,825 Epoch[051/300], Step[0200/1252], Avg Loss: 3.7754, Avg Acc: 0.3551
2022-01-13 21:28:39,409 Epoch[051/300], Step[0250/1252], Avg Loss: 3.7688, Avg Acc: 0.3640
2022-01-13 21:30:05,050 Epoch[051/300], Step[0300/1252], Avg Loss: 3.7800, Avg Acc: 0.3558
2022-01-13 21:31:29,285 Epoch[051/300], Step[0350/1252], Avg Loss: 3.7719, Avg Acc: 0.3576
2022-01-13 21:32:54,116 Epoch[051/300], Step[0400/1252], Avg Loss: 3.7816, Avg Acc: 0.3561
2022-01-13 21:34:19,137 Epoch[051/300], Step[0450/1252], Avg Loss: 3.7868, Avg Acc: 0.3535
2022-01-13 21:35:44,342 Epoch[051/300], Step[0500/1252], Avg Loss: 3.7856, Avg Acc: 0.3540
2022-01-13 21:37:10,221 Epoch[051/300], Step[0550/1252], Avg Loss: 3.7842, Avg Acc: 0.3521
2022-01-13 21:38:35,494 Epoch[051/300], Step[0600/1252], Avg Loss: 3.7820, Avg Acc: 0.3520
2022-01-13 21:40:02,164 Epoch[051/300], Step[0650/1252], Avg Loss: 3.7814, Avg Acc: 0.3512
2022-01-13 21:41:28,167 Epoch[051/300], Step[0700/1252], Avg Loss: 3.7797, Avg Acc: 0.3498
2022-01-13 21:42:54,904 Epoch[051/300], Step[0750/1252], Avg Loss: 3.7842, Avg Acc: 0.3486
2022-01-13 21:44:20,056 Epoch[051/300], Step[0800/1252], Avg Loss: 3.7846, Avg Acc: 0.3511
2022-01-13 21:45:45,938 Epoch[051/300], Step[0850/1252], Avg Loss: 3.7849, Avg Acc: 0.3516
2022-01-13 21:47:11,456 Epoch[051/300], Step[0900/1252], Avg Loss: 3.7848, Avg Acc: 0.3507
2022-01-13 21:48:37,018 Epoch[051/300], Step[0950/1252], Avg Loss: 3.7857, Avg Acc: 0.3503
2022-01-13 21:50:02,910 Epoch[051/300], Step[1000/1252], Avg Loss: 3.7829, Avg Acc: 0.3505
2022-01-13 21:51:27,925 Epoch[051/300], Step[1050/1252], Avg Loss: 3.7842, Avg Acc: 0.3509
2022-01-13 21:52:53,699 Epoch[051/300], Step[1100/1252], Avg Loss: 3.7854, Avg Acc: 0.3499
2022-01-13 21:54:18,996 Epoch[051/300], Step[1150/1252], Avg Loss: 3.7848, Avg Acc: 0.3507
2022-01-13 21:55:45,900 Epoch[051/300], Step[1200/1252], Avg Loss: 3.7877, Avg Acc: 0.3511
2022-01-13 21:57:13,717 Epoch[051/300], Step[1250/1252], Avg Loss: 3.7834, Avg Acc: 0.3522
2022-01-13 21:57:20,877 ----- Epoch[051/300], Train Loss: 3.7835, Train Acc: 0.3522, time: 2246.87, Best Val(epoch50) Acc@1: 0.6634
2022-01-13 21:57:20,878 Now training epoch 52. LR=0.000968
2022-01-13 21:59:13,294 Epoch[052/300], Step[0000/1252], Avg Loss: 4.0569, Avg Acc: 0.1953
2022-01-13 22:00:38,495 Epoch[052/300], Step[0050/1252], Avg Loss: 3.7278, Avg Acc: 0.3490
2022-01-13 22:02:02,648 Epoch[052/300], Step[0100/1252], Avg Loss: 3.7350, Avg Acc: 0.3653
2022-01-13 22:03:28,469 Epoch[052/300], Step[0150/1252], Avg Loss: 3.7454, Avg Acc: 0.3522
2022-01-13 22:04:54,328 Epoch[052/300], Step[0200/1252], Avg Loss: 3.7353, Avg Acc: 0.3473
2022-01-13 22:06:19,722 Epoch[052/300], Step[0250/1252], Avg Loss: 3.7446, Avg Acc: 0.3506
2022-01-13 22:07:44,022 Epoch[052/300], Step[0300/1252], Avg Loss: 3.7543, Avg Acc: 0.3532
2022-01-13 22:09:09,311 Epoch[052/300], Step[0350/1252], Avg Loss: 3.7443, Avg Acc: 0.3595
2022-01-13 22:10:35,354 Epoch[052/300], Step[0400/1252], Avg Loss: 3.7465, Avg Acc: 0.3622
2022-01-13 22:12:00,519 Epoch[052/300], Step[0450/1252], Avg Loss: 3.7400, Avg Acc: 0.3638
2022-01-13 22:13:25,870 Epoch[052/300], Step[0500/1252], Avg Loss: 3.7435, Avg Acc: 0.3631
2022-01-13 22:14:51,760 Epoch[052/300], Step[0550/1252], Avg Loss: 3.7464, Avg Acc: 0.3598
2022-01-13 22:16:17,726 Epoch[052/300], Step[0600/1252], Avg Loss: 3.7474, Avg Acc: 0.3593
2022-01-13 22:17:43,843 Epoch[052/300], Step[0650/1252], Avg Loss: 3.7466, Avg Acc: 0.3597
2022-01-13 22:19:09,013 Epoch[052/300], Step[0700/1252], Avg Loss: 3.7399, Avg Acc: 0.3614
2022-01-13 22:20:34,123 Epoch[052/300], Step[0750/1252], Avg Loss: 3.7399, Avg Acc: 0.3609
2022-01-13 22:21:59,742 Epoch[052/300], Step[0800/1252], Avg Loss: 3.7412, Avg Acc: 0.3598
2022-01-13 22:23:24,659 Epoch[052/300], Step[0850/1252], Avg Loss: 3.7455, Avg Acc: 0.3580
2022-01-13 22:24:50,630 Epoch[052/300], Step[0900/1252], Avg Loss: 3.7453, Avg Acc: 0.3566
2022-01-13 22:26:16,338 Epoch[052/300], Step[0950/1252], Avg Loss: 3.7445, Avg Acc: 0.3567
2022-01-13 22:27:42,376 Epoch[052/300], Step[1000/1252], Avg Loss: 3.7427, Avg Acc: 0.3580
2022-01-13 22:29:07,997 Epoch[052/300], Step[1050/1252], Avg Loss: 3.7475, Avg Acc: 0.3578
2022-01-13 22:30:33,723 Epoch[052/300], Step[1100/1252], Avg Loss: 3.7505, Avg Acc: 0.3568
2022-01-13 22:31:59,351 Epoch[052/300], Step[1150/1252], Avg Loss: 3.7505, Avg Acc: 0.3580
2022-01-13 22:33:25,185 Epoch[052/300], Step[1200/1252], Avg Loss: 3.7515, Avg Acc: 0.3565
2022-01-13 22:34:51,311 Epoch[052/300], Step[1250/1252], Avg Loss: 3.7546, Avg Acc: 0.3559
2022-01-13 22:34:58,544 ----- Epoch[052/300], Train Loss: 3.7546, Train Acc: 0.3559, time: 2257.66, Best Val(epoch50) Acc@1: 0.6634
2022-01-13 22:34:58,544 ----- Validation after Epoch: 52
2022-01-13 22:36:16,752 Val Step[0000/1563], Avg Loss: 1.2367, Avg Acc@1: 0.7188, Avg Acc@5: 0.9062
2022-01-13 22:36:18,895 Val Step[0050/1563], Avg Loss: 1.5005, Avg Acc@1: 0.6648, Avg Acc@5: 0.8848
2022-01-13 22:36:20,825 Val Step[0100/1563], Avg Loss: 1.5158, Avg Acc@1: 0.6634, Avg Acc@5: 0.8827
2022-01-13 22:36:22,618 Val Step[0150/1563], Avg Loss: 1.5139, Avg Acc@1: 0.6631, Avg Acc@5: 0.8793
2022-01-13 22:36:24,411 Val Step[0200/1563], Avg Loss: 1.5170, Avg Acc@1: 0.6657, Avg Acc@5: 0.8753
2022-01-13 22:36:26,198 Val Step[0250/1563], Avg Loss: 1.4934, Avg Acc@1: 0.6712, Avg Acc@5: 0.8775
2022-01-13 22:36:28,009 Val Step[0300/1563], Avg Loss: 1.4952, Avg Acc@1: 0.6715, Avg Acc@5: 0.8776
2022-01-13 22:36:29,838 Val Step[0350/1563], Avg Loss: 1.4999, Avg Acc@1: 0.6702, Avg Acc@5: 0.8779
2022-01-13 22:36:31,627 Val Step[0400/1563], Avg Loss: 1.4947, Avg Acc@1: 0.6709, Avg Acc@5: 0.8793
2022-01-13 22:36:33,419 Val Step[0450/1563], Avg Loss: 1.5033, Avg Acc@1: 0.6676, Avg Acc@5: 0.8780
2022-01-13 22:36:35,224 Val Step[0500/1563], Avg Loss: 1.5044, Avg Acc@1: 0.6675, Avg Acc@5: 0.8779
2022-01-13 22:36:37,026 Val Step[0550/1563], Avg Loss: 1.5047, Avg Acc@1: 0.6658, Avg Acc@5: 0.8786
2022-01-13 22:36:38,820 Val Step[0600/1563], Avg Loss: 1.5050, Avg Acc@1: 0.6651, Avg Acc@5: 0.8787
2022-01-13 22:36:40,724 Val Step[0650/1563], Avg Loss: 1.5066, Avg Acc@1: 0.6656, Avg Acc@5: 0.8790
2022-01-13 22:36:42,639 Val Step[0700/1563], Avg Loss: 1.5059, Avg Acc@1: 0.6661, Avg Acc@5: 0.8795
2022-01-13 22:36:44,547 Val Step[0750/1563], Avg Loss: 1.5132, Avg Acc@1: 0.6645, Avg Acc@5: 0.8785
2022-01-13 22:36:46,444 Val Step[0800/1563], Avg Loss: 1.5133, Avg Acc@1: 0.6653, Avg Acc@5: 0.8788
2022-01-13 22:36:48,232 Val Step[0850/1563], Avg Loss: 1.5147, Avg Acc@1: 0.6651, Avg Acc@5: 0.8785
2022-01-13 22:36:50,071 Val Step[0900/1563], Avg Loss: 1.5112, Avg Acc@1: 0.6657, Avg Acc@5: 0.8789
2022-01-13 22:36:51,900 Val Step[0950/1563], Avg Loss: 1.5104, Avg Acc@1: 0.6660, Avg Acc@5: 0.8791
2022-01-13 22:36:53,784 Val Step[1000/1563], Avg Loss: 1.5102, Avg Acc@1: 0.6661, Avg Acc@5: 0.8788
2022-01-13 22:36:55,710 Val Step[1050/1563], Avg Loss: 1.5122, Avg Acc@1: 0.6659, Avg Acc@5: 0.8784
2022-01-13 22:36:57,617 Val Step[1100/1563], Avg Loss: 1.5120, Avg Acc@1: 0.6652, Avg Acc@5: 0.8783
2022-01-13 22:36:59,486 Val Step[1150/1563], Avg Loss: 1.5104, Avg Acc@1: 0.6654, Avg Acc@5: 0.8784
2022-01-13 22:37:01,402 Val Step[1200/1563], Avg Loss: 1.5087, Avg Acc@1: 0.6662, Avg Acc@5: 0.8788
2022-01-13 22:37:03,314 Val Step[1250/1563], Avg Loss: 1.5072, Avg Acc@1: 0.6664, Avg Acc@5: 0.8791
2022-01-13 22:37:05,163 Val Step[1300/1563], Avg Loss: 1.5101, Avg Acc@1: 0.6660, Avg Acc@5: 0.8788
2022-01-13 22:37:07,024 Val Step[1350/1563], Avg Loss: 1.5094, Avg Acc@1: 0.6662, Avg Acc@5: 0.8789
2022-01-13 22:37:08,896 Val Step[1400/1563], Avg Loss: 1.5082, Avg Acc@1: 0.6660, Avg Acc@5: 0.8789
2022-01-13 22:37:10,752 Val Step[1450/1563], Avg Loss: 1.5083, Avg Acc@1: 0.6658, Avg Acc@5: 0.8791
2022-01-13 22:37:12,534 Val Step[1500/1563], Avg Loss: 1.5075, Avg Acc@1: 0.6663, Avg Acc@5: 0.8792
2022-01-13 22:37:14,238 Val Step[1550/1563], Avg Loss: 1.5080, Avg Acc@1: 0.6663, Avg Acc@5: 0.8792
2022-01-13 22:37:16,221 ----- Epoch[052/300], Validation Loss: 1.5082, Validation Acc@1: 0.6664, Validation Acc@5: 0.8791, time: 137.67
2022-01-13 22:37:17,547 the pre best model acc:0.6634, at epoch 50
2022-01-13 22:37:17,819 current best model acc:0.6664, at epoch 52
2022-01-13 22:37:17,819 ----- Save BEST model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 22:37:17,819 ----- Save BEST optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 22:37:17,820 ----- Save model: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdparams
2022-01-13 22:37:17,820 ----- Save optim: /root/paddlejob/workspace/output//train/Best_CycleMLP.pdopt
2022-01-13 22:37:17,820 Now training epoch 53. LR=0.000966
2022-01-13 22:39:05,581 Epoch[053/300], Step[0000/1252], Avg Loss: 3.7864, Avg Acc: 0.4756
2022-01-13 22:40:31,080 Epoch[053/300], Step[0050/1252], Avg Loss: 3.7085, Avg Acc: 0.3920
2022-01-13 22:41:56,382 Epoch[053/300], Step[0100/1252], Avg Loss: 3.7453, Avg Acc: 0.3609
2022-01-13 22:43:22,513 Epoch[053/300], Step[0150/1252], Avg Loss: 3.7610, Avg Acc: 0.3577
2022-01-13 22:44:48,440 Epoch[053/300], Step[0200/1252], Avg Loss: 3.7569, Avg Acc: 0.3632
2022-01-13 22:46:13,760 Epoch[053/300], Step[0250/1252], Avg Loss: 3.7600, Avg Acc: 0.3628
2022-01-13 22:47:40,735 Epoch[053/300], Step[0300/1252], Avg Loss: 3.7650, Avg Acc: 0.3591
2022-01-13 22:49:05,706 Epoch[053/300], Step[0350/1252], Avg Loss: 3.7703, Avg Acc: 0.3605
2022-01-13 22:50:31,383 Epoch[053/300], Step[0400/1252], Avg Loss: 3.7746, Avg Acc: 0.3615
2022-01-13 22:51:57,831 Epoch[053/300], Step[0450/1252], Avg Loss: 3.7767, Avg Acc: 0.3588
2022-01-13 22:53:22,911 Epoch[053/300], Step[0500/1252], Avg Loss: 3.7698, Avg Acc: 0.3606
2022-01-13 22:54:49,683 Epoch[053/300], Step[0550/1252], Avg Loss: 3.7646, Avg Acc: 0.3619
2022-01-13 22:56:15,718 Epoch[053/300], Step[0600/1252], Avg Loss: 3.7646, Avg Acc: 0.3623
2022-01-13 22:57:42,290 Epoch[053/300], Step[0650/1252], Avg Loss: 3.7597, Avg Acc: 0.3632
2022-01-13 22:59:09,509 Epoch[053/300], Step[0700/1252], Avg Loss: 3.7564, Avg Acc: 0.3622
2022-01-13 23:00:36,476 Epoch[053/300], Step[0750/1252], Avg Loss: 3.7517, Avg Acc: 0.3625
2022-01-13 23:02:03,833 Epoch[053/300], Step[0800/1252], Avg Loss: 3.7537, Avg Acc: 0.3609
2022-01-13 23:03:30,934 Epoch[053/300], Step[0850/1252], Avg Loss: 3.7554, Avg Acc: 0.3603
2022-01-13 23:04:57,376 Epoch[053/300], Step[0900/1252], Avg Loss: 3.7540, Avg Acc: 0.3597
2022-01-13 23:06:23,963 Epoch[053/300], Step[0950/1252], Avg Loss: 3.7583, Avg Acc: 0.3589
2022-01-13 23:07:49,915 Epoch[053/300], Step[1000/1252], Avg Loss: 3.7579, Avg Acc: 0.3576
2022-01-13 23:09:15,986 Epoch[053/300], Step[1050/1252], Avg Loss: 3.7616, Avg Acc: 0.3569
2022-01-13 23:10:42,582 Epoch[053/300], Step[1100/1252], Avg Loss: 3.7593, Avg Acc: 0.3562
2022-01-13 23:12:09,652 Epoch[053/300], Step[1150/1252], Avg Loss: 3.7582, Avg Acc: 0.3553
2022-01-13 23:13:37,196 Epoch[053/300], Step[1200/1252], Avg Loss: 3.7576, Avg Acc: 0.3550
2022-01-13 23:15:04,066 Epoch[053/300], Step[1250/1252], Avg Loss: 3.7558, Avg Acc: 0.3546
2022-01-13 23:15:11,117 ----- Epoch[053/300], Train Loss: 3.7558, Train Acc: 0.3546, time: 2273.29, Best Val(epoch52) Acc@1: 0.6664
2022-01-13 23:15:11,117 Now training epoch 54. LR=0.000964
2022-01-13 23:16:58,606 Epoch[054/300], Step[0000/1252], Avg Loss: 3.6131, Avg Acc: 0.5098
2022-01-13 23:18:24,726 Epoch[054/300], Step[0050/1252], Avg Loss: 3.7143, Avg Acc: 0.3793
2022-01-13 23:19:50,614 Epoch[054/300], Step[0100/1252], Avg Loss: 3.7064, Avg Acc: 0.3592
2022-01-13 23:21:16,764 Epoch[054/300], Step[0150/1252], Avg Loss: 3.7069, Avg Acc: 0.3469
2022-01-13 23:22:42,915 Epoch[054/300], Step[0200/1252], Avg Loss: 3.7169, Avg Acc: 0.3431
2022-01-13 23:24:08,215 Epoch[054/300], Step[0250/1252], Avg Loss: 3.7273, Avg Acc: 0.3487
2022-01-13 23:25:33,989 Epoch[054/300], Step[0300/1252], Avg Loss: 3.7342, Avg Acc: 0.3536
2022-01-13 23:26:58,490 Epoch[054/300], Step[0350/1252], Avg Loss: 3.7374, Avg Acc: 0.3501
2022-01-13 23:28:24,021 Epoch[054/300], Step[0400/1252], Avg Loss: 3.7392, Avg Acc: 0.3470
2022-01-13 23:29:49,887 Epoch[054/300], Step[0450/1252], Avg Loss: 3.7444, Avg Acc: 0.3471
2022-01-13 23:31:15,912 Epoch[054/300], Step[0500/1252], Avg Loss: 3.7525, Avg Acc: 0.3433
2022-01-13 23:32:42,078 Epoch[054/300], Step[0550/1252], Avg Loss: 3.7489, Avg Acc: 0.3441
2022-01-13 23:34:08,432 Epoch[054/300], Step[0600/1252], Avg Loss: 3.7486, Avg Acc: 0.3440
2022-01-13 23:35:35,030 Epoch[054/300], Step[0650/1252], Avg Loss: 3.7479, Avg Acc: 0.3454
2022-01-13 23:37:01,676 Epoch[054/300], Step[0700/1252], Avg Loss: 3.7498, Avg Acc: 0.3473
2022-01-13 23:38:27,783 Epoch[054/300], Step[0750/1252], Avg Loss: 3.7468, Avg Acc: 0.3483
2022-01-13 23:39:54,990 Epoch[054/300], Step[0800/1252], Avg Loss: 3.7464, Avg Acc: 0.3506
2022-01-13 23:41:22,576 Epoch[054/300], Step[0850/1252], Avg Loss: 3.7471, Avg Acc: 0.3506
2022-01-13 23:42:49,599 Epoch[054/300], Step[0900/1252], Avg Loss: 3.7450, Avg Acc: 0.3517
2022-01-13 23:44:14,867 Epoch[054/300], Step[0950/1252], Avg Loss: 3.7482, Avg Acc: 0.3517
2022-01-13 23:45:41,104 Epoch[054/300], Step[1000/1252], Avg Loss: 3.7473, Avg Acc: 0.3517
2022-01-13 23:47:07,186 Epoch[054/300], Step[1050/1252], Avg Loss: 3.7470, Avg Acc: 0.3516
2022-01-13 23:48:33,701 Epoch[054/300], Step[1100/1252], Avg Loss: 3.7465, Avg Acc: 0.3521
2022-01-13 23:50:02,649 Epoch[054/300], Step[1150/1252], Avg Loss: 3.7455, Avg Acc: 0.3509
2022-01-13 23:51:29,456 Epoch[054/300], Step[1200/1252], Avg Loss: 3.7430, Avg Acc: 0.3510
2022-01-13 23:52:57,812 Epoch[054/300], Step[1250/1252], Avg Loss: 3.7435, Avg Acc: 0.3519
2022-01-13 23:53:04,493 ----- Epoch[054/300], Train Loss: 3.7435, Train Acc: 0.3519, time: 2273.37, Best Val(epoch52) Acc@1: 0.6664
2022-01-13 23:53:04,493 ----- Validation after Epoch: 54
2022-01-14 00:17:12,297 Val Step[0000/1563], Avg Loss: 1.4629, Avg Acc@1: 0.6562, Avg Acc@5: 0.9375
2022-01-14 00:17:14,478 Val Step[0050/1563], Avg Loss: 1.5380, Avg Acc@1: 0.6661, Avg Acc@5: 0.8732
2022-01-14 00:17:16,981 Val Step[0100/1563], Avg Loss: 1.5545, Avg Acc@1: 0.6618, Avg Acc@5: 0.8722
2022-01-14 00:17:19,355 Val Step[0150/1563], Avg Loss: 1.5533, Avg Acc@1: 0.6606, Avg Acc@5: 0.8723
2022-01-14 00:17:21,637 Val Step[0200/1563], Avg Loss: 1.5533, Avg Acc@1: 0.6650, Avg Acc@5: 0.8714
2022-01-14 00:17:23,997 Val Step[0250/1563], Avg Loss: 1.5368, Avg Acc@1: 0.6688, Avg Acc@5: 0.8731
2022-01-14 00:17:26,188 Val Step[0300/1563], Avg Loss: 1.5375, Avg Acc@1: 0.6696, Avg Acc@5: 0.8734
2022-01-14 00:17:28,257 Val Step[0350/1563], Avg Loss: 1.5401, Avg Acc@1: 0.6700, Avg Acc@5: 0.8738
2022-01-14 00:17:30,610 Val Step[0400/1563], Avg Loss: 1.5358, Avg Acc@1: 0.6705, Avg Acc@5: 0.8749
2022-01-14 00:17:32,758 Val Step[0450/1563], Avg Loss: 1.5425, Avg Acc@1: 0.6678, Avg Acc@5: 0.8734
2022-01-14 00:17:34,900 Val Step[0500/1563], Avg Loss: 1.5467, Avg Acc@1: 0.6664, Avg Acc@5: 0.8734
2022-01-14 00:17:37,018 Val Step[0550/1563], Avg Loss: 1.5458, Avg Acc@1: 0.6662, Avg Acc@5: 0.8736
2022-01-14 00:17:39,129 Val Step[0600/1563], Avg Loss: 1.5480, Avg Acc@1: 0.6654, Avg Acc@5: 0.8741
2022-01-14 00:17:41,207 Val Step[0650/1563], Avg Loss: 1.5501, Avg Acc@1: 0.6656, Avg Acc@5: 0.8747
2022-01-14 00:17:43,391 Val Step[0700/1563], Avg Loss: 1.5470, Avg Acc@1: 0.6660, Avg Acc@5: 0.8754
2022-01-14 00:17:45,617 Val Step[0750/1563], Avg Loss: 1.5536, Avg Acc@1: 0.6654, Avg Acc@5: 0.8746
2022-01-14 00:17:47,960 Val Step[0800/1563], Avg Loss: 1.5500, Avg Acc@1: 0.6671, Avg Acc@5: 0.8757
2022-01-14 00:17:50,461 Val Step[0850/1563], Avg Loss: 1.5512, Avg Acc@1: 0.6675, Avg Acc@5: 0.8756
2022-01-14 00:17:52,700 Val Step[0900/1563], Avg Loss: 1.5469, Avg Acc@1: 0.6679, Avg Acc@5: 0.8765
2022-01-14 00:17:54,876 Val Step[0950/1563], Avg Loss: 1.5469, Avg Acc@1: 0.6675, Avg Acc@5: 0.8766
2022-01-14 00:17:56,973 Val Step[1000/1563], Avg Loss: 1.5470, Avg Acc@1: 0.6678, Avg Acc@5: 0.8763
2022-01-14 00:17:59,237 Val Step[1050/1563], Avg Loss: 1.5503, Avg Acc@1: 0.6666, Avg Acc@5: 0.8759
2022-01-14 00:18:01,372 Val Step[1100/1563], Avg Loss: 1.5498, Avg Acc@1: 0.6664, Avg Acc@5: 0.8757
2022-01-14 00:18:03,494 Val Step[1150/1563], Avg Loss: 1.5488, Avg Acc@1: 0.6666, Avg Acc@5: 0.8756
2022-01-14 00:18:05,687 Val Step[1200/1563], Avg Loss: 1.5476, Avg Acc@1: 0.6671, Avg Acc@5: 0.8758
2022-01-14 00:18:08,065 Val Step[1250/1563], Avg Loss: 1.5466, Avg Acc@1: 0.6672, Avg Acc@5: 0.8763
2022-01-14 00:18:10,289 Val Step[1300/1563], Avg Loss: 1.5489, Avg Acc@1: 0.6668, Avg Acc@5: 0.8761
2022-01-14 00:18:12,461 Val Step[1350/1563], Avg Loss: 1.5479, Avg Acc@1: 0.6667, Avg Acc@5: 0.8760
2022-01-14 00:18:14,596 Val Step[1400/1563], Avg Loss: 1.5474, Avg Acc@1: 0.6662, Avg Acc@5: 0.8761
2022-01-14 00:18:16,615 Val Step[1450/1563], Avg Loss: 1.5481, Avg Acc@1: 0.6660, Avg Acc@5: 0.8760
2022-01-14 00:18:18,845 Val Step[1500/1563], Avg Loss: 1.5477, Avg Acc@1: 0.6662, Avg Acc@5: 0.8761
2022-01-14 00:18:20,916 Val Step[1550/1563], Avg Loss: 1.5480, Avg Acc@1: 0.6664, Avg Acc@5: 0.8762
2022-01-14 00:18:22,703 ----- Epoch[054/300], Validation Loss: 1.5484, Validation Acc@1: 0.6662, Validation Acc@5: 0.8763, time: 1518.21
2022-01-14 00:18:22,703 Now training epoch 55. LR=0.000962
2022-01-14 00:20:17,974 Epoch[055/300], Step[0000/1252], Avg Loss: 4.0225, Avg Acc: 0.4316
2022-01-14 00:21:48,932 Epoch[055/300], Step[0050/1252], Avg Loss: 3.8507, Avg Acc: 0.3472
2022-01-14 00:23:21,017 Epoch[055/300], Step[0100/1252], Avg Loss: 3.8302, Avg Acc: 0.3507
2022-01-14 00:24:54,964 Epoch[055/300], Step[0150/1252], Avg Loss: 3.8002, Avg Acc: 0.3478
2022-01-14 00:26:27,882 Epoch[055/300], Step[0200/1252], Avg Loss: 3.8053, Avg Acc: 0.3510
2022-01-14 00:27:59,823 Epoch[055/300], Step[0250/1252], Avg Loss: 3.7904, Avg Acc: 0.3562
2022-01-14 00:29:31,802 Epoch[055/300], Step[0300/1252], Avg Loss: 3.7845, Avg Acc: 0.3528
2022-01-14 00:31:04,538 Epoch[055/300], Step[0350/1252], Avg Loss: 3.7687, Avg Acc: 0.3528
2022-01-14 00:32:38,422 Epoch[055/300], Step[0400/1252], Avg Loss: 3.7567, Avg Acc: 0.3575
2022-01-14 00:34:09,836 Epoch[055/300], Step[0450/1252], Avg Loss: 3.7550, Avg Acc: 0.3589
2022-01-14 00:35:43,040 Epoch[055/300], Step[0500/1252], Avg Loss: 3.7509, Avg Acc: 0.3582
2022-01-14 00:37:15,152 Epoch[055/300], Step[0550/1252], Avg Loss: 3.7522, Avg Acc: 0.3573
2022-01-14 00:38:48,319 Epoch[055/300], Step[0600/1252], Avg Loss: 3.7559, Avg Acc: 0.3542
2022-01-14 00:40:20,068 Epoch[055/300], Step[0650/1252], Avg Loss: 3.7596, Avg Acc: 0.3554
2022-01-14 00:41:50,161 Epoch[055/300], Step[0700/1252], Avg Loss: 3.7607, Avg Acc: 0.3556
2022-01-14 00:43:21,777 Epoch[055/300], Step[0750/1252], Avg Loss: 3.7626, Avg Acc: 0.3552
2022-01-14 00:44:54,655 Epoch[055/300], Step[0800/1252], Avg Loss: 3.7643, Avg Acc: 0.3562
2022-01-14 00:46:28,117 Epoch[055/300], Step[0850/1252], Avg Loss: 3.7628, Avg Acc: 0.3572
2022-01-14 00:48:01,410 Epoch[055/300], Step[0900/1252], Avg Loss: 3.7606, Avg Acc: 0.3590
2022-01-14 00:49:36,670 Epoch[055/300], Step[0950/1252], Avg Loss: 3.7602, Avg Acc: 0.3576
2022-01-14 00:51:12,501 Epoch[055/300], Step[1000/1252], Avg Loss: 3.7618, Avg Acc: 0.3576
2022-01-14 00:52:47,392 Epoch[055/300], Step[1050/1252], Avg Loss: 3.7644, Avg Acc: 0.3567
2022-01-14 00:54:22,155 Epoch[055/300], Step[1100/1252], Avg Loss: 3.7642, Avg Acc: 0.3565
2022-01-14 00:55:55,863 Epoch[055/300], Step[1150/1252], Avg Loss: 3.7670, Avg Acc: 0.3570
2022-01-14 00:57:30,366 Epoch[055/300], Step[1200/1252], Avg Loss: 3.7655, Avg Acc: 0.3582
2022-01-14 00:59:02,454 Epoch[055/300], Step[1250/1252], Avg Loss: 3.7644, Avg Acc: 0.3584
2022-01-14 00:59:09,705 ----- Epoch[055/300], Train Loss: 3.7643, Train Acc: 0.3584, time: 2447.00, Best Val(epoch52) Acc@1: 0.6664
2022-01-14 00:59:09,706 Now training epoch 56. LR=0.000960
